[[1;34mINFO[m] Scanning for projects...
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] [1mReactor Build Order:[m
[[1;34mINFO[m] 
[[1;34mINFO[m] Strimzi - Apache Kafka on Kubernetes and OpenShift                 [pom]
[[1;34mINFO[m] test                                                               [jar]
[[1;34mINFO[m] crd-annotations                                                    [jar]
[[1;34mINFO[m] crd-generator                                                      [jar]
[[1;34mINFO[m] api                                                                [jar]
[[1;34mINFO[m] mockkube                                                           [jar]
[[1;34mINFO[m] config-model                                                       [jar]
[[1;34mINFO[m] certificate-manager                                                [jar]
[[1;34mINFO[m] operator-common                                                    [jar]
[[1;34mINFO[m] systemtest                                                         [jar]
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m-------------------------< [0;36mio.strimzi:strimzi[0;1m >-------------------------[m
[[1;34mINFO[m] [1mBuilding Strimzi - Apache Kafka on Kubernetes and OpenShift 0.29.0-SNAPSHOT [1/10][m
[[1;34mINFO[m] [1m--------------------------------[ pom ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mstrimzi[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mstrimzi[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] No tests to run.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Skipping pom project
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--------------------------< [0;36mio.strimzi:test[0;1m >---------------------------[m
[[1;34mINFO[m] [1mBuilding test 0.29.0-SNAPSHOT                                     [2/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/test/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/test/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/test/src/test/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] No sources to compile
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] No tests to run.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mtest[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/test/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mtest[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/test/target/test-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] No tests to run.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m---------------------< [0;36mio.strimzi:crd-annotations[0;1m >---------------------[m
[[1;34mINFO[m] [1mBuilding crd-annotations 0.29.0-SNAPSHOT                          [3/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/crd-annotations/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/crd-annotations/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/crd-annotations/src/test/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mcrd-annotations[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/crd-annotations/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mcrd-annotations[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/crd-annotations/target/crd-annotations-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mcrd-annotations[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m----------------------< [0;36mio.strimzi:crd-generator[0;1m >----------------------[m
[[1;34mINFO[m] [1mBuilding crd-generator 0.29.0-SNAPSHOT                            [4/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/crd-generator/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/crd-generator/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 7 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mcrd-generator[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/crd-generator/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mcrd-generator[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-shade-plugin:3.1.0:shade[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Including io.strimzi:crd-annotations:jar:0.29.0-SNAPSHOT in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.core:jackson-core:jar:2.12.6 in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.core:jackson-databind:jar:2.12.6 in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:jar:2.12.6 in the shaded jar.
[[1;34mINFO[m] Including org.yaml:snakeyaml:jar:1.27 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-client:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-rbac:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-admissionregistration:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-apps:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-autoscaling:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-apiextensions:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-batch:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-certificates:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-coordination:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-discovery:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-events:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-extensions:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-flowcontrol:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-networking:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-metrics:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-policy:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-scheduling:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-storageclass:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-node:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including com.squareup.okhttp3:okhttp:jar:3.12.12 in the shaded jar.
[[1;34mINFO[m] Including com.squareup.okio:okio:jar:1.15.0 in the shaded jar.
[[1;34mINFO[m] Including com.squareup.okhttp3:logging-interceptor:jar:3.12.12 in the shaded jar.
[[1;34mINFO[m] Including org.slf4j:slf4j-api:jar:1.7.36 in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.datatype:jackson-datatype-jsr310:jar:2.13.1 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:zjsonpatch:jar:0.3.0 in the shaded jar.
[[1;34mINFO[m] Including com.github.mifmif:generex:jar:1.0.2 in the shaded jar.
[[1;34mINFO[m] Including dk.brics.automaton:automaton:jar:1.11-8 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-core:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-common:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.core:jackson-annotations:jar:2.12.6 in the shaded jar.
[[1;33mWARNING[m] Discovered module-info.class. Shading will break its strong encapsulation.
[[1;33mWARNING[m] Discovered module-info.class. Shading will break its strong encapsulation.
[[1;33mWARNING[m] Discovered module-info.class. Shading will break its strong encapsulation.
[[1;33mWARNING[m] Discovered module-info.class. Shading will break its strong encapsulation.
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, generex-1.0.2.jar define 7 overlapping classes: 
[[1;33mWARNING[m]   - com.mifmif.common.regex.GenerexIterator
[[1;33mWARNING[m]   - com.mifmif.common.regex.Generex
[[1;33mWARNING[m]   - com.mifmif.common.regex.GenerexIterator$Step
[[1;33mWARNING[m]   - com.mifmif.common.regex.Node
[[1;33mWARNING[m]   - com.mifmif.common.regex.Main
[[1;33mWARNING[m]   - com.mifmif.common.regex.util.Iterable
[[1;33mWARNING[m]   - com.mifmif.common.regex.util.Iterator
[[1;33mWARNING[m] kubernetes-model-rbac-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 80 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.SubjectBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.RoleListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingFluent$MetadataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.RoleBindingBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.AggregationRuleFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.SubjectFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.PolicyRuleFluent
[[1;33mWARNING[m]   - 70 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, jackson-annotations-2.12.6.jar define 71 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonAutoDetect
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonInclude
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.ObjectIdGenerators
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonFormat$Features
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonFormat$Feature
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonIgnore
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonSetter
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonTypeInfo$None
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonFormat$Shape
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonSubTypes
[[1;33mWARNING[m]   - 61 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-extensions-5.12.0.jar define 264 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetConditionBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicyListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.DeploymentStrategyFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.IngressListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicySpecFluent$IngressNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.IngressStatus
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetFluentImpl$SpecNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.IngressSpecFluent$RulesNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetSpecFluent$UpdateStrategyNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicyPeerBuilder
[[1;33mWARNING[m]   - 254 more...
[[1;33mWARNING[m] kubernetes-model-autoscaling-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 350 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.MetricSpecFluentImpl$ObjectNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.CrossVersionObjectReferenceBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerStatusFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerFluent$SpecNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.ContainerResourceMetricStatusFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerStatus
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.MetricStatusFluent$ObjectNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerSpecFluent$ScaleTargetRefNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerFluent$SpecNested
[[1;33mWARNING[m]   - 340 more...
[[1;33mWARNING[m] kubernetes-model-storageclass-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 172 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSIStorageCapacityListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.CSINodeFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.CSINodeDriverFluentImpl$AllocatableNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.StorageClass
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSourceBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSourceFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.TokenRequestFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSINodeDriverFluent$AllocatableNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSIDriverSpecBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSpecFluent
[[1;33mWARNING[m]   - 162 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-batch-5.12.0.jar define 112 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.JobFluentImpl$StatusNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.CronJobStatusFluentImpl$ActiveNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.JobSpecFluent$TemplateNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobSpecFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.JobSpecFluentImpl$TemplateNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.Job
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobListFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.CronJobListFluent
[[1;33mWARNING[m]   - 102 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-apiextensions-5.12.0.jar define 350 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionVersionBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrBoolBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionSpecFluent$ValidationNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionVersionFluentImpl$SchemaNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrStringArraySerDe$Deserializer$1
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceValidationFluentImpl$OpenAPIV3SchemaNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsFluentImpl$NotNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.WebhookClientConfigFluentImpl$ServiceNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrArrayFluent$SchemaNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1.JSONSchemaPropsOrBoolSerDe
[[1;33mWARNING[m]   - 340 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-discovery-5.12.0.jar define 88 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointSliceFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointSliceBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointPort
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.ForZoneBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointFluent$TargetRefNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointFluentImpl$ConditionsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointSliceListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointConditionsFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointSliceListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - 78 more...
[[1;33mWARNING[m] okhttp-3.12.12.jar, crd-generator-0.29.0-SNAPSHOT.jar define 208 overlapping classes: 
[[1;33mWARNING[m]   - okhttp3.WebSocket
[[1;33mWARNING[m]   - okhttp3.Cookie$Builder
[[1;33mWARNING[m]   - okhttp3.internal.http.HttpHeaders
[[1;33mWARNING[m]   - okhttp3.internal.http2.Http2Connection$ReaderRunnable
[[1;33mWARNING[m]   - okhttp3.internal.http2.Http2Reader$ContinuationSource
[[1;33mWARNING[m]   - okhttp3.internal.tls.OkHostnameVerifier
[[1;33mWARNING[m]   - okhttp3.Cache$Entry
[[1;33mWARNING[m]   - okhttp3.internal.http2.Http2Connection$3
[[1;33mWARNING[m]   - okhttp3.internal.ws.RealWebSocket$Streams
[[1;33mWARNING[m]   - okhttp3.CacheControl$Builder
[[1;33mWARNING[m]   - 198 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-metrics-5.12.0.jar define 30 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.ContainerMetricsFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetrics
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsFluent$MetadataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsFluentImpl$ContainersNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsListBuilder
[[1;33mWARNING[m]   - 20 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-flowcontrol-5.12.0.jar define 132 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationFluentImpl$SpecNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.FlowSchemaConditionFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.FlowDistinguisherMethodBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationReferenceBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.QueuingConfiguration
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.QueuingConfigurationFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfiguration
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationReference
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PolicyRulesWithSubjects
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationListFluent$ItemsNested
[[1;33mWARNING[m]   - 122 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-events-5.12.0.jar define 44 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$MetadataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$SeriesNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1.EventList
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1.EventListBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$RegardingNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1.EventSeriesFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - 34 more...
[[1;33mWARNING[m] automaton-1.11-8.jar, crd-generator-0.29.0-SNAPSHOT.jar define 25 overlapping classes: 
[[1;33mWARNING[m]   - dk.brics.automaton.AutomatonMatcher
[[1;33mWARNING[m]   - dk.brics.automaton.ShuffleOperations$ShuffleConfiguration
[[1;33mWARNING[m]   - dk.brics.automaton.RegExp$Kind
[[1;33mWARNING[m]   - dk.brics.automaton.RunAutomaton
[[1;33mWARNING[m]   - dk.brics.automaton.Automaton
[[1;33mWARNING[m]   - dk.brics.automaton.RegExp
[[1;33mWARNING[m]   - dk.brics.automaton.AutomatonProvider
[[1;33mWARNING[m]   - dk.brics.automaton.RegExp$1
[[1;33mWARNING[m]   - dk.brics.automaton.MinimizationOperations$StateListNode
[[1;33mWARNING[m]   - dk.brics.automaton.State
[[1;33mWARNING[m]   - 15 more...
[[1;33mWARNING[m] jackson-core-2.12.6.jar, crd-generator-0.29.0-SNAPSHOT.jar define 124 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.JsonGenerator$Feature
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.json.JsonReadFeature
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.util.ThreadLocalBufferManager$ThreadLocalBufferManagerHolder
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.util.Separators
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.io.SegmentedStringWriter
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.TreeNode
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.sym.Name
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.util.RequestPayload
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.util.JsonGeneratorDelegate
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.async.NonBlockingInputFeeder
[[1;33mWARNING[m]   - 114 more...
[[1;33mWARNING[m] kubernetes-model-networking-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 234 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.IngressServiceBackend
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.NetworkPolicyPort
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressFluentImpl$StatusNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.IngressStatus
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.IngressClassFluent$SpecNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.NetworkPolicyBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.IngressRuleFluentImpl$HttpNestedImpl
[[1;33mWARNING[m]   - 224 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-coordination-5.12.0.jar define 18 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluentImpl$SpecNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluent$SpecNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpecBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpecFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluentImpl$MetadataNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluent
[[1;33mWARNING[m]   - 8 more...
[[1;33mWARNING[m] zjsonpatch-0.3.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 24 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.sequence.InsertCommand
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.Operation
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.sequence.CommandVisitor
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.guava.Strings
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.sequence.EditCommand
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.JsonDiff$EncodePathFunction
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.sequence.SequencesComparator
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.Diff
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.ListUtils
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.JsonPatch
[[1;33mWARNING[m]   - 14 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-common-5.12.0.jar define 16 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.Plural
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.Group
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.jackson.JsonUnwrappedDeserializer$CancelUnwrapped
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.jackson.JsonUnwrappedDeserializer
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.PrinterColumn
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.jackson.UnwrappedTypeResolverBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.Singular
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.StatusReplicas
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.SpecReplicas
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.Version
[[1;33mWARNING[m]   - 6 more...
[[1;33mWARNING[m] kubernetes-model-admissionregistration-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 362 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookFluent$ObjectSelectorNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authorization.v1.SubjectAccessReviewSpecFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SubjectRulesReviewStatusBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1.ValidatingWebhookConfigurationBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authentication.TokenReviewFluentImpl$MetadataNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SelfSubjectRulesReviewSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookFluentImpl$NamespaceSelectorNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookConfigurationFluentImpl$WebhooksNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SelfSubjectAccessReviewFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1.MutatingWebhookFluent$ClientConfigNested
[[1;33mWARNING[m]   - 352 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, okio-1.15.0.jar define 44 overlapping classes: 
[[1;33mWARNING[m]   - okio.ByteString
[[1;33mWARNING[m]   - okio.Source
[[1;33mWARNING[m]   - okio.ForwardingSink
[[1;33mWARNING[m]   - okio.BufferedSource
[[1;33mWARNING[m]   - okio.Util
[[1;33mWARNING[m]   - okio.AsyncTimeout$1
[[1;33mWARNING[m]   - okio.HashingSource
[[1;33mWARNING[m]   - okio.GzipSink
[[1;33mWARNING[m]   - okio.Okio$1
[[1;33mWARNING[m]   - okio.Pipe$PipeSink
[[1;33mWARNING[m]   - 34 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-certificates-5.12.0.jar define 60 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestStatusFluent$ConditionsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluent$SpecNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluentImpl$StatusNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestStatusFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestFluent$MetadataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestConditionFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestStatusBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestStatusFluent$ConditionsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluentImpl
[[1;33mWARNING[m]   - 50 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, jackson-datatype-jsr310-2.13.1.jar define 59 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.deser.LocalDateDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.deser.key.Jsr310KeyDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.PackageVersion
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.deser.YearDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.ser.key.Jsr310NullKeySerializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.deser.key.LocalDateTimeKeyDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.util.DurationUnitConverter
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.ser.InstantSerializerBase
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.ser.LocalDateTimeSerializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.ser.OffsetDateTimeSerializer
[[1;33mWARNING[m]   - 49 more...
[[1;33mWARNING[m] crd-annotations-0.29.0-SNAPSHOT.jar, crd-generator-0.29.0-SNAPSHOT.jar define 8 overlapping classes: 
[[1;33mWARNING[m]   - io.strimzi.api.annotations.VersionRange
[[1;33mWARNING[m]   - io.strimzi.api.annotations.ApiVersion
[[1;33mWARNING[m]   - io.strimzi.api.annotations.ApiVersion$Stability
[[1;33mWARNING[m]   - io.strimzi.api.annotations.ApiVersion$1
[[1;33mWARNING[m]   - io.strimzi.api.annotations.DeprecatedType
[[1;33mWARNING[m]   - io.strimzi.api.annotations.DeprecatedProperty
[[1;33mWARNING[m]   - io.strimzi.api.annotations.VersionRange$VersionParser
[[1;33mWARNING[m]   - io.strimzi.api.annotations.KubeVersion
[[1;33mWARNING[m] kubernetes-model-apps-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 212 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.StatefulSetConditionFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.DeploymentStrategyFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ControllerRevisionFluent$DeploymentDataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetSpecFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.DeploymentFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetStatusFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ControllerRevisionFluentImpl$PersistentVolumeClaimDataNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetCondition
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.StatefulSetSpecFluent$UpdateStrategyNested
[[1;33mWARNING[m]   - 202 more...
[[1;33mWARNING[m] logging-interceptor-3.12.12.jar, crd-generator-0.29.0-SNAPSHOT.jar define 8 overlapping classes: 
[[1;33mWARNING[m]   - okhttp3.logging.HttpLoggingInterceptor$Logger$1
[[1;33mWARNING[m]   - okhttp3.logging.LoggingEventListener$Factory
[[1;33mWARNING[m]   - okhttp3.logging.HttpLoggingInterceptor$Level
[[1;33mWARNING[m]   - okhttp3.logging.HttpLoggingInterceptor
[[1;33mWARNING[m]   - okhttp3.logging.package-info
[[1;33mWARNING[m]   - okhttp3.logging.LoggingEventListener
[[1;33mWARNING[m]   - okhttp3.logging.LoggingEventListener$1
[[1;33mWARNING[m]   - okhttp3.logging.HttpLoggingInterceptor$Logger
[[1;33mWARNING[m] jackson-databind-2.12.6.jar, crd-generator-0.29.0-SNAPSHOT.jar define 700 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.introspect.AnnotationCollector$NoAnnotations
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.jsontype.BasicPolymorphicTypeValidator$Builder
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.BeanDescription
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.deser.impl.BeanAsArrayBuilderDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.introspect.AnnotatedMethodMap
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.SerializerProvider
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.introspect.AnnotationCollector$OneAnnotation
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.ser.std.StaticListSerializerBase
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.ser.std.NumberSerializers$ShortSerializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.ser.BeanSerializerFactory
[[1;33mWARNING[m]   - 690 more...
[[1;33mWARNING[m] jackson-dataformat-yaml-2.12.6.jar, crd-generator-0.29.0-SNAPSHOT.jar define 17 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLMapper$Builder
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.snakeyaml.error.Mark
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLGenerator
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.UTF8Reader
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLParser
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.util.StringQuotingChecker
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLGenerator$Feature
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.JacksonYAMLParseException
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLParser$Feature
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.util.StringQuotingChecker$Default
[[1;33mWARNING[m]   - 7 more...
[[1;33mWARNING[m] kubernetes-model-core-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 2394 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.BaseKubernetesListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.StatusBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.KubeSchemaFluentImpl$APIResourceNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.NodeListBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.ResourceQuotaListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.WatchEventFluentImpl$APIServiceStatusObjectNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.WatchEventFluent$VsphereVirtualDiskVolumeSourceObjectNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.ProbeFluentImpl$HttpGetNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.PatchOptionsFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.ServerAddressByClientCIDRFluentImpl
[[1;33mWARNING[m]   - 2384 more...
[[1;33mWARNING[m] slf4j-api-1.7.36.jar, crd-generator-0.29.0-SNAPSHOT.jar define 34 overlapping classes: 
[[1;33mWARNING[m]   - org.slf4j.helpers.SubstituteLogger
[[1;33mWARNING[m]   - org.slf4j.helpers.NamedLoggerBase
[[1;33mWARNING[m]   - org.slf4j.helpers.NOPMDCAdapter
[[1;33mWARNING[m]   - org.slf4j.MarkerFactory
[[1;33mWARNING[m]   - org.slf4j.helpers.BasicMarker
[[1;33mWARNING[m]   - org.slf4j.spi.LoggerFactoryBinder
[[1;33mWARNING[m]   - org.slf4j.MDC$MDCCloseable
[[1;33mWARNING[m]   - org.slf4j.spi.LocationAwareLogger
[[1;33mWARNING[m]   - org.slf4j.helpers.MessageFormatter
[[1;33mWARNING[m]   - org.slf4j.helpers.Util$ClassContextSecurityManager
[[1;33mWARNING[m]   - 24 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-node-5.12.0.jar define 78 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1beta1.OverheadBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1alpha1.Scheduling
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1beta1.RuntimeClassFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1beta1.RuntimeClassListBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1alpha1.SchedulingFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1alpha1.RuntimeClassSpecFluent$OverheadNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassFluentImpl
[[1;33mWARNING[m]   - 68 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, snakeyaml-1.27.jar define 216 overlapping classes: 
[[1;33mWARNING[m]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockMappingValue
[[1;33mWARNING[m]   - org.yaml.snakeyaml.parser.ParserImpl$ParseBlockNode
[[1;33mWARNING[m]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockMappingSimpleValue
[[1;33mWARNING[m]   - org.yaml.snakeyaml.emitter.Emitter$ExpectDocumentEnd
[[1;33mWARNING[m]   - org.yaml.snakeyaml.Yaml$3
[[1;33mWARNING[m]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockSequenceItem
[[1;33mWARNING[m]   - org.yaml.snakeyaml.parser.ParserImpl$ParseBlockSequenceEntry
[[1;33mWARNING[m]   - org.yaml.snakeyaml.util.ArrayUtils
[[1;33mWARNING[m]   - org.yaml.snakeyaml.tokens.Token$ID
[[1;33mWARNING[m]   - org.yaml.snakeyaml.reader.StreamReader
[[1;33mWARNING[m]   - 206 more...
[[1;33mWARNING[m] kubernetes-client-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 536 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.internal.CertUtils
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.CustomResource
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.osgi.ManagedKubernetesClient
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.V1beta1ApiextensionAPIGroupDSL
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.internal.PatchUtils$SingletonHolder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.VersionInfo$1
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.utils.ReplaceValueStream
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.dsl.CreateFromServerGettable
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.dsl.ApiextensionsAPIGroupDSL
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.dsl.Containerable
[[1;33mWARNING[m]   - 526 more...
[[1;33mWARNING[m] kubernetes-model-scheduling-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 24 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassFluentImpl$MetadataNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassList
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClass
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassFluent$MetadataNested
[[1;33mWARNING[m]   - 14 more...
[[1;33mWARNING[m] kubernetes-model-policy-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 162 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1.PodDisruptionBudgetList
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.HostPortRangeBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.EvictionFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicyFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicySpecFluentImpl$AllowedCSIDriversNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicyListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.AllowedFlexVolumeBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.IDRangeFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.SELinuxStrategyOptionsFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicySpecFluentImpl$FsGroupNestedImpl
[[1;33mWARNING[m]   - 152 more...
[[1;33mWARNING[m] maven-shade-plugin has detected that some class files are
[[1;33mWARNING[m] present in two or more JARs. When this happens, only one
[[1;33mWARNING[m] single version of the class is copied to the uber jar.
[[1;33mWARNING[m] Usually this is not harmful and you can skip these warnings,
[[1;33mWARNING[m] otherwise try to manually exclude artifacts based on
[[1;33mWARNING[m] mvn dependency:tree -Ddetail=true and the above output.
[[1;33mWARNING[m] See http://maven.apache.org/plugins/maven-shade-plugin/
[[1;34mINFO[m] Replacing original artifact with shaded artifact.
[[1;34mINFO[m] Replacing /home/ec2-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT.jar with /home/ec2-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT-shaded.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m---------------------------< [0;36mio.strimzi:api[0;1m >---------------------------[m
[[1;34mINFO[m] [1mBuilding api 0.29.0-SNAPSHOT                                      [5/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/api/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/api/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mexec-maven-plugin:1.6.0:exec[m [1m(generate-crd-co-install-v1)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mexec-maven-plugin:1.6.0:exec[m [1m(generate-crd-co-install-v1-eo)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mexec-maven-plugin:1.6.0:exec[m [1m(generate-doc)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 99 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-test-compile)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mapi[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/api/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mapi[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:test-jar[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m------------------------< [0;36mio.strimzi:mockkube[0;1m >-------------------------[m
[[1;34mINFO[m] [1mBuilding mockkube 0.29.0-SNAPSHOT                                 [6/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/mockkube/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/mockkube/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 1 resource
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mmockkube[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/mockkube/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mmockkube[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/mockkube/target/mockkube-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mmockkube[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m----------------------< [0;36mio.strimzi:config-model[0;1m >-----------------------[m
[[1;34mINFO[m] [1mBuilding config-model 0.29.0-SNAPSHOT                             [7/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/config-model/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/config-model/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/config-model/src/test/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mconfig-model[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/config-model/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mconfig-model[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/config-model/target/config-model-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mconfig-model[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m-------------------< [0;36mio.strimzi:certificate-manager[0;1m >-------------------[m
[[1;34mINFO[m] [1mBuilding certificate-manager 0.29.0-SNAPSHOT                      [8/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/certificate-manager/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 1 resource
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 1 resource
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mcertificate-manager[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/certificate-manager/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mcertificate-manager[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/certificate-manager/target/certificate-manager-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mcertificate-manager[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m---------------------< [0;36mio.strimzi:operator-common[0;1m >---------------------[m
[[1;34mINFO[m] [1mBuilding operator-common 0.29.0-SNAPSHOT                          [9/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/operator-common/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/operator-common/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 9 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36moperator-common[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/operator-common/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36moperator-common[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/operator-common/target/operator-common-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:test-jar[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m-----------------------< [0;36mio.strimzi:systemtest[0;1m >------------------------[m
[[1;34mINFO[m] [1mBuilding systemtest 0.29.0-SNAPSHOT                              [10/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/systemtest/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] Copying 2 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 32 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;33mWARNING[m] Corrupted STDOUT by directly writing to native stream in forked JVM 1. See FAQ web page and the dump file /home/ec2-user/strimzi-kafka-operator/systemtest/target/surefire-reports/2022-03-30T21-28-18_388-jvmRun1.dumpstream
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36msystemtest[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/systemtest/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36msystemtest[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36msystemtest[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
2022-03-30 21:28:36 [main] [32mINFO [m [TestExecutionListener:29] =======================================================================
2022-03-30 21:28:36 [main] [32mINFO [m [TestExecutionListener:30] =======================================================================
2022-03-30 21:28:36 [main] [32mINFO [m [TestExecutionListener:31]                         Test run started
2022-03-30 21:28:36 [main] [32mINFO [m [TestExecutionListener:32] =======================================================================
2022-03-30 21:28:36 [main] [32mINFO [m [TestExecutionListener:33] =======================================================================
2022-03-30 21:28:36 [main] [32mINFO [m [TestExecutionListener:48] Following testclasses are selected for run:
2022-03-30 21:28:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.operators.user.UserST
2022-03-30 21:28:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.bridge.HttpBridgeTlsST
2022-03-30 21:28:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.cruisecontrol.CruiseControlApiST
2022-03-30 21:28:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.cruisecontrol.CruiseControlST
2022-03-30 21:28:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.kafka.listeners.ListenersST
2022-03-30 21:28:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.security.SecurityST
2022-03-30 21:28:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.rollingupdate.RollingUpdateST
2022-03-30 21:28:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.specific.SpecificIsolatedST
2022-03-30 21:28:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.watcher.AllNamespaceIsolatedST
2022-03-30 21:28:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.connect.ConnectIsolatedST
2022-03-30 21:28:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST
2022-03-30 21:28:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST
2022-03-30 21:28:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.metrics.MetricsIsolatedST
2022-03-30 21:28:36 [main] [32mINFO [m [TestExecutionListener:52] =======================================================================
2022-03-30 21:28:36 [main] [32mINFO [m [TestExecutionListener:53] =======================================================================
[[1;34mINFO[m] Running io.strimzi.systemtest.bridge.HttpBridgeTlsST
[[1;34mINFO[m] Running io.strimzi.systemtest.cruisecontrol.CruiseControlApiST
[[1;34mINFO[m] Running io.strimzi.systemtest.cruisecontrol.CruiseControlST
[[1;34mINFO[m] Running io.strimzi.systemtest.metrics.MetricsIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.operators.user.UserST
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Environment:271] Json configuration is not provided or cannot be processed!
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:219] Used environment variables:
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:220] CONFIG: /home/ec2-user/strimzi-kafka-operator/systemtest/config.json
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] STRIMZI_RBAC_SCOPE: CLUSTER
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] OLM_APP_BUNDLE_PREFIX: strimzi-cluster-operator
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_CLIENTS_VERSION: 0.2.0
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] OLM_SOURCE_NAMESPACE: openshift-marketplace
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] CLUSTER_OPERATOR_INSTALL_TYPE: BUNDLE
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] STRIMZI_COMPONENTS_LOG_LEVEL: INFO
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] SKIP_TEARDOWN: false
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] LB_FINALIZERS: false
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] OLM_OPERATOR_DEPLOYMENT_NAME: strimzi-cluster-operator
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] DOCKER_ORG: strimzi
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_LOG_DIR: /home/ec2-user/strimzi-kafka-operator/systemtest/../systemtest/target/logs/
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] COMPONENTS_IMAGE_PULL_POLICY: IfNotPresent
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] DOCKER_REGISTRY: quay.io
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_CLIENT_IMAGE: quay.io/strimzi/test-client:latest-kafka-3.1.0
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] SYSTEM_TEST_STRIMZI_IMAGE_PULL_SECRET: 
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_ADMIN_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-admin:0.2.0-kafka-3.1.0
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_HTTP_PRODUCER_IMAGE: quay.io/strimzi-test-clients/test-client-http-producer:0.2.0
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] OLM_OPERATOR_NAME: strimzi-kafka-operator
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] DOCKER_TAG: latest
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] OLM_SOURCE_NAME: community-operators
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] STRIMZI_FEATURE_GATES: 
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] CLIENTS_KAFKA_VERSION: 3.1.0
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_HTTP_CONSUMER_IMAGE: quay.io/strimzi-test-clients/test-client-http-consumer:0.2.0
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] STRIMZI_LOG_LEVEL: DEBUG
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] ST_KAFKA_VERSION: 3.1.0
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] OPERATOR_IMAGE_PULL_POLICY: Always
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] DEFAULT_TO_DENY_NETWORK_POLICIES: true
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_PRODUCER_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-producer:0.2.0-kafka-3.1.0
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] BRIDGE_IMAGE: latest-released
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_STREAMS_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-streams:0.2.0-kafka-3.1.0
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_CONSUMER_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-consumer:0.2.0-kafka-3.1.0
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] OLM_OPERATOR_VERSION: 
2022-03-30 21:28:36 [ForkJoinPool-3-worker-11] [36mDEBUG[m [BeforeAllOnce:51] ============================================================================
2022-03-30 21:28:36 [ForkJoinPool-3-worker-11] [36mDEBUG[m [BeforeAllOnce:52] [io.strimzi.systemtest.cruisecontrol.CruiseControlST - Before Suite] - Setup Suite environment
2022-03-30 21:28:36 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Config:540] Trying to configure client from Kubernetes config...
2022-03-30 21:28:36 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Config:549] Found for Kubernetes config at: [/home/ec2-user/.kube/config].
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 21:28:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:667] [metrics.MetricsIsolatedST - Before All] - Setup test suite environment
2022-03-30 21:28:36 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Config:540] Trying to configure client from Kubernetes config...
2022-03-30 21:28:36 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Config:549] Found for Kubernetes config at: [/home/ec2-user/.kube/config].
2022-03-30 21:28:36 [ForkJoinPool-3-worker-11] [36mDEBUG[m [KubeCluster:71] Cluster minikube is installed
2022-03-30 21:28:36 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - minikube status
2022-03-30 21:28:37 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: minikube status
2022-03-30 21:28:37 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:28:37 [ForkJoinPool-3-worker-11] [36mDEBUG[m [KubeCluster:73] Cluster minikube is running
2022-03-30 21:28:37 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeCluster:87] Using cluster: minikube
2022-03-30 21:28:37 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:60] Cluster default namespace is 'default'
2022-03-30 21:28:37 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 21:28:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:28:37 [ForkJoinPool-3-worker-11] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@26fcd291
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-30 21:28:37 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@3f2e6467, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@26fcd291, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-30 21:28:37 [ForkJoinPool-3-worker-11] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-30 21:28:38 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-30 21:28:38 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-30 21:28:38 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 21:28:38 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 21:28:38 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:28:38 [ForkJoinPool-3-worker-11] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T21:28:38Z",
        "name": "infra-namespace",
        "resourceVersion": "97467",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "9ad42b0a-5f48-48a1-b078-f271a7a65ceb"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 21:28:38 [ForkJoinPool-3-worker-11] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-30 21:28:38 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 21:28:38 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-30 21:28:38 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 21:28:38 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 21:28:38 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 21:28:38 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 21:28:38 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 21:28:38 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 21:28:38 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 21:28:38 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 21:28:38 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 21:28:38 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 21:28:38 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 21:28:38 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 21:28:38 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 21:28:38 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 21:28:38 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 21:28:38 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 21:28:38 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 21:28:38 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:28:38 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 21:28:38 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 21:28:38 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:28:38 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 21:28:39 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 21:28:40 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 21:28:40 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 21:28:40 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 21:28:40 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 21:28:40 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479982ms till timeout)
2022-03-30 21:28:41 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478978ms till timeout)
2022-03-30 21:28:42 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477971ms till timeout)
2022-03-30 21:28:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:28:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:28:43 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476966ms till timeout)
2022-03-30 21:28:44 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475960ms till timeout)
2022-03-30 21:28:45 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474955ms till timeout)
2022-03-30 21:28:46 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473949ms till timeout)
2022-03-30 21:28:47 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472944ms till timeout)
2022-03-30 21:28:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:28:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:28:48 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471940ms till timeout)
2022-03-30 21:28:49 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470934ms till timeout)
2022-03-30 21:28:50 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469922ms till timeout)
2022-03-30 21:28:51 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468915ms till timeout)
2022-03-30 21:28:52 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467907ms till timeout)
2022-03-30 21:28:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:28:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:28:53 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466902ms till timeout)
2022-03-30 21:28:54 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465897ms till timeout)
2022-03-30 21:28:55 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464892ms till timeout)
2022-03-30 21:28:56 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463887ms till timeout)
2022-03-30 21:28:57 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462883ms till timeout)
2022-03-30 21:28:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:28:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:28:58 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461878ms till timeout)
2022-03-30 21:28:59 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (460874ms till timeout)
2022-03-30 21:29:00 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (459869ms till timeout)
2022-03-30 21:29:01 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (458865ms till timeout)
2022-03-30 21:29:02 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (457860ms till timeout)
2022-03-30 21:29:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:29:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:29:03 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (456855ms till timeout)
2022-03-30 21:29:04 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (455851ms till timeout)
2022-03-30 21:29:05 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (454846ms till timeout)
2022-03-30 21:29:06 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (453841ms till timeout)
2022-03-30 21:29:07 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (452837ms till timeout)
2022-03-30 21:29:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:29:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:29:08 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (451833ms till timeout)
2022-03-30 21:29:09 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (450828ms till timeout)
2022-03-30 21:29:10 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (449823ms till timeout)
2022-03-30 21:29:11 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (448819ms till timeout)
2022-03-30 21:29:12 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (447814ms till timeout)
2022-03-30 21:29:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:29:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:29:13 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (446810ms till timeout)
2022-03-30 21:29:14 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-30 21:29:14 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-30 21:29:14 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-30 21:29:14 [ForkJoinPool-3-worker-11] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-sjcmm not ready: strimzi-cluster-operator)
2022-03-30 21:29:14 [ForkJoinPool-3-worker-11] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-sjcmm are ready
2022-03-30 21:29:14 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599982ms till timeout)
2022-03-30 21:29:15 [ForkJoinPool-3-worker-11] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-sjcmm not ready: strimzi-cluster-operator)
2022-03-30 21:29:15 [ForkJoinPool-3-worker-11] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-sjcmm are ready
2022-03-30 21:29:15 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598976ms till timeout)
2022-03-30 21:29:16 [ForkJoinPool-3-worker-11] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-sjcmm not ready: strimzi-cluster-operator)
2022-03-30 21:29:16 [ForkJoinPool-3-worker-11] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-sjcmm are ready
2022-03-30 21:29:16 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597971ms till timeout)
2022-03-30 21:29:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:29:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:29:17 [ForkJoinPool-3-worker-11] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-sjcmm not ready: strimzi-cluster-operator)
2022-03-30 21:29:17 [ForkJoinPool-3-worker-11] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-sjcmm are ready
2022-03-30 21:29:17 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596965ms till timeout)
2022-03-30 21:29:18 [ForkJoinPool-3-worker-11] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-sjcmm not ready: strimzi-cluster-operator)
2022-03-30 21:29:18 [ForkJoinPool-3-worker-11] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-sjcmm are ready
2022-03-30 21:29:18 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595959ms till timeout)
2022-03-30 21:29:19 [ForkJoinPool-3-worker-11] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-sjcmm not ready: strimzi-cluster-operator)
2022-03-30 21:29:19 [ForkJoinPool-3-worker-11] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-sjcmm are ready
2022-03-30 21:29:19 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594954ms till timeout)
2022-03-30 21:29:20 [ForkJoinPool-3-worker-11] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-sjcmm not ready: strimzi-cluster-operator)
2022-03-30 21:29:20 [ForkJoinPool-3-worker-11] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-sjcmm are ready
2022-03-30 21:29:20 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593949ms till timeout)
2022-03-30 21:29:21 [ForkJoinPool-3-worker-11] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-sjcmm not ready: strimzi-cluster-operator)
2022-03-30 21:29:21 [ForkJoinPool-3-worker-11] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-sjcmm are ready
2022-03-30 21:29:21 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592942ms till timeout)
2022-03-30 21:29:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:29:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:29:22 [ForkJoinPool-3-worker-11] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-sjcmm not ready: strimzi-cluster-operator)
2022-03-30 21:29:22 [ForkJoinPool-3-worker-11] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-sjcmm are ready
2022-03-30 21:29:22 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591937ms till timeout)
2022-03-30 21:29:23 [ForkJoinPool-3-worker-11] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-sjcmm not ready: strimzi-cluster-operator)
2022-03-30 21:29:23 [ForkJoinPool-3-worker-11] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-sjcmm are ready
2022-03-30 21:29:23 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590931ms till timeout)
2022-03-30 21:29:24 [ForkJoinPool-3-worker-11] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-sjcmm not ready: strimzi-cluster-operator)
2022-03-30 21:29:24 [ForkJoinPool-3-worker-11] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-sjcmm are ready
2022-03-30 21:29:24 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-30 21:29:24 [ForkJoinPool-3-worker-11] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 21:29:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 21:29:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 21:29:24 [ForkJoinPool-3-worker-11] [36mDEBUG[m [AbstractST:667] [cruisecontrol.CruiseControlST - Before All] - Setup test suite environment
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 21:29:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:667] [operators.user.UserST - Before All] - Setup test suite environment
2022-03-30 21:29:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:667] [bridge.HttpBridgeTlsST - Before All] - Setup test suite environment
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:667] [cruisecontrol.CruiseControlApiST - Before All] - Setup test suite environment
2022-03-30 21:29:24 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:69] [cruisecontrol.CruiseControlST] - Adding parallel suite: CruiseControlST
2022-03-30 21:29:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:69] [bridge.HttpBridgeTlsST] - Adding parallel suite: HttpBridgeTlsST
2022-03-30 21:29:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:69] [operators.user.UserST] - Adding parallel suite: UserST
2022-03-30 21:29:24 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:73] [cruisecontrol.CruiseControlST] - Parallel suites count: 1
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:69] [cruisecontrol.CruiseControlApiST] - Adding parallel suite: CruiseControlApiST
2022-03-30 21:29:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:73] [bridge.HttpBridgeTlsST] - Parallel suites count: 2
2022-03-30 21:29:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:73] [operators.user.UserST] - Parallel suites count: 3
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:73] [cruisecontrol.CruiseControlApiST] - Parallel suites count: 4
2022-03-30 21:29:24 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:184] CruiseControlST suite now can proceed its execution
2022-03-30 21:29:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:184] HttpBridgeTlsST suite now can proceed its execution
2022-03-30 21:29:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:184] UserST suite now can proceed its execution
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:184] CruiseControlApiST suite now can proceed its execution
2022-03-30 21:29:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-30 21:29:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-30 21:29:24 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-30 21:29:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestSuiteNamespaceManager:129] Test suite `UserST` creates these additional namespaces:[user-st]
2022-03-30 21:29:24 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestSuiteNamespaceManager:129] Test suite `CruiseControlST` creates these additional namespaces:[cruise-control-st]
2022-03-30 21:29:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestSuiteNamespaceManager:129] Test suite `HttpBridgeTlsST` creates these additional namespaces:[http-bridge-tls-st]
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestSuiteNamespaceManager:129] Test suite `CruiseControlApiST` creates these additional namespaces:[cruise-control-api-st]
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:156] Creating Namespace: cruise-control-api-st
2022-03-30 21:29:24 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: user-st
2022-03-30 21:29:24 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:156] Creating Namespace: cruise-control-st
2022-03-30 21:29:24 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: http-bridge-tls-st
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Namespace cruise-control-api-st
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o json
2022-03-30 21:29:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Namespace http-bridge-tls-st
2022-03-30 21:29:24 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Namespace cruise-control-st
2022-03-30 21:29:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace http-bridge-tls-st -o json
2022-03-30 21:29:24 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o json
2022-03-30 21:29:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Namespace user-st
2022-03-30 21:29:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace user-st -o json
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o json
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T21:29:24Z",
        "name": "cruise-control-api-st",
        "resourceVersion": "97574",
        "selfLink": "/api/v1/namespaces/cruise-control-api-st",
        "uid": "da600b3a-58ed-468a-8e55-b77b041795b1"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st]}
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:82] Client use Namespace: cruise-control-api-st
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=cruise-control-api-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: cruise-control-api-st
2022-03-30 21:29:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace http-bridge-tls-st -o json
2022-03-30 21:29:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:29:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T21:29:24Z",
        "name": "http-bridge-tls-st",
        "resourceVersion": "97577",
        "selfLink": "/api/v1/namespaces/http-bridge-tls-st",
        "uid": "f201e195-a3eb-42fd-a23d-3b2f50785dac"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 21:29:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st]}
2022-03-30 21:29:24 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: http-bridge-tls-st
2022-03-30 21:29:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace user-st -o json
2022-03-30 21:29:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:29:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=http-bridge-tls-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 21:29:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T21:29:24Z",
        "name": "user-st",
        "resourceVersion": "97575",
        "selfLink": "/api/v1/namespaces/user-st",
        "uid": "7126424c-d92f-4896-8633-8c37442f0c42"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 21:29:24 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: http-bridge-tls-st
2022-03-30 21:29:24 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o json
2022-03-30 21:29:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st]}
2022-03-30 21:29:24 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:29:24 [ForkJoinPool-3-worker-11] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T21:29:24Z",
        "name": "cruise-control-st",
        "resourceVersion": "97576",
        "selfLink": "/api/v1/namespaces/cruise-control-st",
        "uid": "ec48a15c-a7bf-44c6-af98-c36b50741f92"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 21:29:24 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: user-st
2022-03-30 21:29:24 [ForkJoinPool-3-worker-11] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st]}
2022-03-30 21:29:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=user-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 21:29:24 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:82] Client use Namespace: cruise-control-st
2022-03-30 21:29:24 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: user-st
2022-03-30 21:29:24 [ForkJoinPool-3-worker-11] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=cruise-control-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 21:29:24 [ForkJoinPool-3-worker-11] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: cruise-control-st
2022-03-30 21:29:24 [ForkJoinPool-3-worker-7] [32mINFO [m [HttpBridgeTlsST:129] Deploy Kafka and KafkaBridge before tests
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 21:29:24 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 21:29:24 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlApiST.testCruiseControlBasicAPIRequests-STARTED
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlApiST.testCruiseControlBasicAPIRequestsWithSecurityDisabled-STARTED
2022-03-30 21:29:24 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 21:29:24 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:659] [cruisecontrol.CruiseControlApiST - Before Each] - Setup test case environment
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:659] [cruisecontrol.CruiseControlApiST - Before Each] - Setup test case environment
2022-03-30 21:29:24 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:77] [cruisecontrol.CruiseControlApiST] - Adding parallel test: testCruiseControlBasicAPIRequests
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:77] [cruisecontrol.CruiseControlApiST] - Adding parallel test: testCruiseControlBasicAPIRequestsWithSecurityDisabled
2022-03-30 21:29:24 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:81] [cruisecontrol.CruiseControlApiST] - Parallel test count: 1
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:81] [cruisecontrol.CruiseControlApiST] - Parallel test count: 2
2022-03-30 21:29:24 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:230] testCruiseControlBasicAPIRequests test now can proceed its execution
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:230] testCruiseControlBasicAPIRequestsWithSecurityDisabled test now can proceed its execution
2022-03-30 21:29:24 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 21:29:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testCruiseControlBasicAPIRequests=my-cluster-cdcd0608}
2022-03-30 21:29:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testCruiseControlBasicAPIRequests=my-user-507552110-1812270342}
2022-03-30 21:29:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testCruiseControlBasicAPIRequests=my-topic-716947866-29609027}
2022-03-30 21:29:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients}
2022-03-30 21:29:24 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-0 for test case:testCruiseControlBasicAPIRequests
2022-03-30 21:29:24 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-0
2022-03-30 21:29:24 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-0
2022-03-30 21:29:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace namespace-0 -o json
2022-03-30 21:29:24 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace namespace-0 -o json
2022-03-30 21:29:24 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:29:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T21:29:24Z",
        "name": "namespace-0",
        "resourceVersion": "97590",
        "selfLink": "/api/v1/namespaces/namespace-0",
        "uid": "5459598a-591a-490f-87dc-5cc99abc6b32"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 21:29:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-0]}
2022-03-30 21:29:24 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-0
2022-03-30 21:29:24 [ForkJoinPool-3-worker-13] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-0, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 21:29:24 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-0
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09}
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659}
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393}
[[1;34mINFO[m] Running io.strimzi.systemtest.kafka.listeners.ListenersST
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients}
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-1 for test case:testCruiseControlBasicAPIRequestsWithSecurityDisabled
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-1
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-1
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 get Namespace namespace-1 -o json
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 get Namespace namespace-1 -o json
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T21:29:24Z",
        "name": "namespace-1",
        "resourceVersion": "97594",
        "selfLink": "/api/v1/namespaces/namespace-1",
        "uid": "40215ea4-00ba-471b-ac8c-b5f636902c88"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@12282135=[namespace-1], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-0]}
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-1
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-1, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 21:29:24 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-1
2022-03-30 21:29:25 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka user-cluster-name in namespace user-st
2022-03-30 21:29:25 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-e1c9825e in namespace namespace-1
2022-03-30 21:29:25 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-0
2022-03-30 21:29:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Kafka cruise-control-api-cluster-name in namespace namespace-1
2022-03-30 21:29:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-1
2022-03-30 21:29:25 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-30 21:29:25 [ForkJoinPool-3-worker-7] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkas' with unstable version 'v1beta2'
2022-03-30 21:29:25 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-e1c9825e
2022-03-30 21:29:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:http-bridge-tls-cluster-name
2022-03-30 21:29:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:cruise-control-api-cluster-name
2022-03-30 21:29:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:user-cluster-name
2022-03-30 21:29:25 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-e1c9825e will have desired state: Ready
2022-03-30 21:29:25 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-e1c9825e will have desired state: Ready
2022-03-30 21:29:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for Kafka: cruise-control-api-cluster-name will have desired state: Ready
2022-03-30 21:29:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: cruise-control-api-cluster-name will have desired state: Ready
2022-03-30 21:29:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1319993ms till timeout)
2022-03-30 21:29:25 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-30 21:29:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-30 21:29:25 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: user-cluster-name will have desired state: Ready
2022-03-30 21:29:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: user-cluster-name will have desired state: Ready
2022-03-30 21:29:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (839994ms till timeout)
2022-03-30 21:29:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1319990ms till timeout)
2022-03-30 21:29:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (839995ms till timeout)
2022-03-30 21:29:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1318988ms till timeout)
2022-03-30 21:29:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (838990ms till timeout)
2022-03-30 21:29:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (838991ms till timeout)
2022-03-30 21:29:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1318981ms till timeout)
2022-03-30 21:29:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 21:29:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:667] [kafka.listeners.ListenersST - Before All] - Setup test suite environment
2022-03-30 21:29:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:69] [kafka.listeners.ListenersST] - Adding parallel suite: ListenersST
2022-03-30 21:29:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:73] [kafka.listeners.ListenersST] - Parallel suites count: 5
2022-03-30 21:29:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:184] ListenersST suite now can proceed its execution
2022-03-30 21:29:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-30 21:29:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestSuiteNamespaceManager:129] Test suite `ListenersST` creates these additional namespaces:[listeners-st]
2022-03-30 21:29:26 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: listeners-st
2022-03-30 21:29:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Namespace listeners-st
2022-03-30 21:29:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace listeners-st -o json
2022-03-30 21:29:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace listeners-st -o json
2022-03-30 21:29:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:29:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T21:29:26Z",
        "name": "listeners-st",
        "resourceVersion": "97610",
        "selfLink": "/api/v1/namespaces/listeners-st",
        "uid": "01d71dc4-ff3b-4ce9-9e3e-d3fe7fc307da"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 21:29:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@12282135=[namespace-1], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-0]}
2022-03-30 21:29:26 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: listeners-st
2022-03-30 21:29:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=listeners-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 21:29:26 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: listeners-st
2022-03-30 21:29:26 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 21:29:26 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesTlsScramSha-STARTED
2022-03-30 21:29:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 21:29:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:659] [kafka.listeners.ListenersST - Before Each] - Setup test case environment
2022-03-30 21:29:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:77] [kafka.listeners.ListenersST] - Adding parallel test: testSendMessagesTlsScramSha
2022-03-30 21:29:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:81] [kafka.listeners.ListenersST] - Parallel test count: 3
2022-03-30 21:29:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:230] testSendMessagesTlsScramSha test now can proceed its execution
2022-03-30 21:29:26 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 21:29:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendMessagesTlsScramSha=my-cluster-380e6586, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09}
2022-03-30 21:29:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendMessagesTlsScramSha=my-user-2039401547-533835955, testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659}
2022-03-30 21:29:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendMessagesTlsScramSha=my-topic-988094002-1400465950, testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393}
2022-03-30 21:29:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendMessagesTlsScramSha=my-cluster-380e6586-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients}
2022-03-30 21:29:26 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-2 for test case:testSendMessagesTlsScramSha
2022-03-30 21:29:26 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-2
2022-03-30 21:29:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-2
2022-03-30 21:29:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace listeners-st get Namespace namespace-2 -o json
2022-03-30 21:29:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace listeners-st get Namespace namespace-2 -o json
2022-03-30 21:29:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:29:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T21:29:26Z",
        "name": "namespace-2",
        "resourceVersion": "97614",
        "selfLink": "/api/v1/namespaces/namespace-2",
        "uid": "888e02c7-6890-4d65-8fd2-069b4f999dc9"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 21:29:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@6e85bf86=[namespace-2], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@12282135=[namespace-1], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-0]}
2022-03-30 21:29:27 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-2
2022-03-30 21:29:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-2, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 21:29:27 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-2
2022-03-30 21:29:27 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-380e6586 in namespace namespace-2
2022-03-30 21:29:27 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-2
2022-03-30 21:29:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-380e6586
2022-03-30 21:29:27 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-380e6586 will have desired state: Ready
2022-03-30 21:29:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-380e6586 will have desired state: Ready
2022-03-30 21:29:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (839995ms till timeout)
2022-03-30 21:29:27 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1317984ms till timeout)
2022-03-30 21:29:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (837987ms till timeout)
2022-03-30 21:29:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (837988ms till timeout)
2022-03-30 21:29:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1317978ms till timeout)
2022-03-30 21:29:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:29:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:29:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (838991ms till timeout)
2022-03-30 21:29:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1316980ms till timeout)
2022-03-30 21:29:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (836983ms till timeout)
2022-03-30 21:29:28 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (836985ms till timeout)
2022-03-30 21:29:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1316975ms till timeout)
2022-03-30 21:29:29 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (837986ms till timeout)
2022-03-30 21:29:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1315977ms till timeout)
2022-03-30 21:29:29 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (835980ms till timeout)
2022-03-30 21:29:29 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (835982ms till timeout)
2022-03-30 21:29:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1315970ms till timeout)
2022-03-30 21:29:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (836982ms till timeout)
2022-03-30 21:29:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1314973ms till timeout)
2022-03-30 21:29:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (834977ms till timeout)
2022-03-30 21:29:30 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (834979ms till timeout)
2022-03-30 21:29:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1314967ms till timeout)
2022-03-30 21:29:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (835977ms till timeout)
2022-03-30 21:29:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1313970ms till timeout)
2022-03-30 21:29:31 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (833974ms till timeout)
2022-03-30 21:29:31 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (833976ms till timeout)
2022-03-30 21:29:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1313964ms till timeout)
2022-03-30 21:29:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (834973ms till timeout)
2022-03-30 21:29:32 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1312966ms till timeout)
2022-03-30 21:29:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (832971ms till timeout)
2022-03-30 21:29:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (832973ms till timeout)
2022-03-30 21:29:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1312961ms till timeout)
2022-03-30 21:29:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:29:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:29:33 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (833968ms till timeout)
2022-03-30 21:29:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1311963ms till timeout)
2022-03-30 21:29:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (831967ms till timeout)
2022-03-30 21:29:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (831961ms till timeout)
2022-03-30 21:29:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1311958ms till timeout)
2022-03-30 21:29:34 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (832964ms till timeout)
2022-03-30 21:29:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1310960ms till timeout)
2022-03-30 21:29:34 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (830964ms till timeout)
2022-03-30 21:29:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (830958ms till timeout)
2022-03-30 21:29:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1310955ms till timeout)
2022-03-30 21:29:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (831959ms till timeout)
2022-03-30 21:29:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1309956ms till timeout)
2022-03-30 21:29:35 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (829961ms till timeout)
2022-03-30 21:29:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (829955ms till timeout)
2022-03-30 21:29:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1309951ms till timeout)
2022-03-30 21:29:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (830955ms till timeout)
2022-03-30 21:29:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1308953ms till timeout)
2022-03-30 21:29:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (828956ms till timeout)
2022-03-30 21:29:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (828949ms till timeout)
2022-03-30 21:29:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1308947ms till timeout)
2022-03-30 21:29:37 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (829951ms till timeout)
2022-03-30 21:29:37 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1307950ms till timeout)
2022-03-30 21:29:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (827953ms till timeout)
2022-03-30 21:29:37 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (827946ms till timeout)
2022-03-30 21:29:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1307943ms till timeout)
2022-03-30 21:29:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:29:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:29:38 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (828946ms till timeout)
2022-03-30 21:29:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1306946ms till timeout)
2022-03-30 21:29:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (826950ms till timeout)
2022-03-30 21:29:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (826943ms till timeout)
2022-03-30 21:29:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1306939ms till timeout)
2022-03-30 21:29:39 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (827941ms till timeout)
2022-03-30 21:29:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1305942ms till timeout)
2022-03-30 21:29:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (825945ms till timeout)
2022-03-30 21:29:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (825934ms till timeout)
2022-03-30 21:29:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1305919ms till timeout)
2022-03-30 21:29:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (826935ms till timeout)
2022-03-30 21:29:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1304938ms till timeout)
2022-03-30 21:29:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (824941ms till timeout)
2022-03-30 21:29:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (824930ms till timeout)
2022-03-30 21:29:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1304915ms till timeout)
2022-03-30 21:29:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (825930ms till timeout)
2022-03-30 21:29:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1303933ms till timeout)
2022-03-30 21:29:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (823938ms till timeout)
2022-03-30 21:29:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (823917ms till timeout)
2022-03-30 21:29:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1303910ms till timeout)
2022-03-30 21:29:42 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (824925ms till timeout)
2022-03-30 21:29:42 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1302926ms till timeout)
2022-03-30 21:29:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (822934ms till timeout)
2022-03-30 21:29:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (822913ms till timeout)
2022-03-30 21:29:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1302905ms till timeout)
2022-03-30 21:29:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:29:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:29:43 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (823918ms till timeout)
2022-03-30 21:29:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1301918ms till timeout)
2022-03-30 21:29:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (821928ms till timeout)
2022-03-30 21:29:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (821909ms till timeout)
2022-03-30 21:29:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1301897ms till timeout)
2022-03-30 21:29:44 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (822912ms till timeout)
2022-03-30 21:29:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1300910ms till timeout)
2022-03-30 21:29:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (820921ms till timeout)
2022-03-30 21:29:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (820895ms till timeout)
2022-03-30 21:29:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1300873ms till timeout)
2022-03-30 21:29:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (821904ms till timeout)
2022-03-30 21:29:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1299906ms till timeout)
2022-03-30 21:29:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (819916ms till timeout)
2022-03-30 21:29:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (819887ms till timeout)
2022-03-30 21:29:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1299868ms till timeout)
2022-03-30 21:29:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (820893ms till timeout)
2022-03-30 21:29:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1298887ms till timeout)
2022-03-30 21:29:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (818897ms till timeout)
2022-03-30 21:29:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (818882ms till timeout)
2022-03-30 21:29:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1298862ms till timeout)
2022-03-30 21:29:47 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (819886ms till timeout)
2022-03-30 21:29:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1297869ms till timeout)
2022-03-30 21:29:47 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (817876ms till timeout)
2022-03-30 21:29:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (817877ms till timeout)
2022-03-30 21:29:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1297856ms till timeout)
2022-03-30 21:29:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:29:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:29:48 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (818881ms till timeout)
2022-03-30 21:29:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1296859ms till timeout)
2022-03-30 21:29:48 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (816870ms till timeout)
2022-03-30 21:29:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (816865ms till timeout)
2022-03-30 21:29:48 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1296851ms till timeout)
2022-03-30 21:29:49 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (817877ms till timeout)
2022-03-30 21:29:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1295852ms till timeout)
2022-03-30 21:29:49 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (815859ms till timeout)
2022-03-30 21:29:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (815863ms till timeout)
2022-03-30 21:29:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1295847ms till timeout)
2022-03-30 21:29:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (816871ms till timeout)
2022-03-30 21:29:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1294845ms till timeout)
2022-03-30 21:29:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (814847ms till timeout)
2022-03-30 21:29:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (814851ms till timeout)
2022-03-30 21:29:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1294842ms till timeout)
2022-03-30 21:29:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (815867ms till timeout)
2022-03-30 21:29:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1293841ms till timeout)
2022-03-30 21:29:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (813843ms till timeout)
2022-03-30 21:29:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (813846ms till timeout)
2022-03-30 21:29:51 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1293838ms till timeout)
2022-03-30 21:29:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (814862ms till timeout)
2022-03-30 21:29:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1292837ms till timeout)
2022-03-30 21:29:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (812837ms till timeout)
2022-03-30 21:29:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (812841ms till timeout)
2022-03-30 21:29:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1292833ms till timeout)
2022-03-30 21:29:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:29:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:29:53 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (813855ms till timeout)
2022-03-30 21:29:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1291832ms till timeout)
2022-03-30 21:29:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (811831ms till timeout)
2022-03-30 21:29:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1291826ms till timeout)
2022-03-30 21:29:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (811832ms till timeout)
2022-03-30 21:29:54 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (812850ms till timeout)
2022-03-30 21:29:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1290827ms till timeout)
2022-03-30 21:29:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (810827ms till timeout)
2022-03-30 21:29:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1290819ms till timeout)
2022-03-30 21:29:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (810823ms till timeout)
2022-03-30 21:29:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (811834ms till timeout)
2022-03-30 21:29:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1289820ms till timeout)
2022-03-30 21:29:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (809812ms till timeout)
2022-03-30 21:29:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (809812ms till timeout)
2022-03-30 21:29:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1289806ms till timeout)
2022-03-30 21:29:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (810827ms till timeout)
2022-03-30 21:29:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1288805ms till timeout)
2022-03-30 21:29:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (808802ms till timeout)
2022-03-30 21:29:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1288797ms till timeout)
2022-03-30 21:29:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (808803ms till timeout)
2022-03-30 21:29:57 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (809819ms till timeout)
2022-03-30 21:29:57 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1287800ms till timeout)
2022-03-30 21:29:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:29:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:29:57 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (807798ms till timeout)
2022-03-30 21:29:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (807793ms till timeout)
2022-03-30 21:29:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1287785ms till timeout)
2022-03-30 21:29:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (808814ms till timeout)
2022-03-30 21:29:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1286796ms till timeout)
2022-03-30 21:29:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (806795ms till timeout)
2022-03-30 21:29:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (806790ms till timeout)
2022-03-30 21:29:58 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1286781ms till timeout)
2022-03-30 21:29:59 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (807810ms till timeout)
2022-03-30 21:29:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1285793ms till timeout)
2022-03-30 21:29:59 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (805792ms till timeout)
2022-03-30 21:29:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (805787ms till timeout)
2022-03-30 21:29:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1285778ms till timeout)
2022-03-30 21:30:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (806806ms till timeout)
2022-03-30 21:30:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1284789ms till timeout)
2022-03-30 21:30:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (804789ms till timeout)
2022-03-30 21:30:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (804784ms till timeout)
2022-03-30 21:30:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1284775ms till timeout)
2022-03-30 21:30:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (805802ms till timeout)
2022-03-30 21:30:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1283785ms till timeout)
2022-03-30 21:30:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (803786ms till timeout)
2022-03-30 21:30:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (803781ms till timeout)
2022-03-30 21:30:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1283772ms till timeout)
2022-03-30 21:30:02 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (804798ms till timeout)
2022-03-30 21:30:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:30:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:30:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1282781ms till timeout)
2022-03-30 21:30:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (802783ms till timeout)
2022-03-30 21:30:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (802779ms till timeout)
2022-03-30 21:30:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1282769ms till timeout)
2022-03-30 21:30:03 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (803794ms till timeout)
2022-03-30 21:30:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1281778ms till timeout)
2022-03-30 21:30:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (801780ms till timeout)
2022-03-30 21:30:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (801776ms till timeout)
2022-03-30 21:30:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1281766ms till timeout)
2022-03-30 21:30:04 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (802790ms till timeout)
2022-03-30 21:30:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1280775ms till timeout)
2022-03-30 21:30:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (800777ms till timeout)
2022-03-30 21:30:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (800773ms till timeout)
2022-03-30 21:30:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1280763ms till timeout)
2022-03-30 21:30:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (801786ms till timeout)
2022-03-30 21:30:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1279772ms till timeout)
2022-03-30 21:30:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (799774ms till timeout)
2022-03-30 21:30:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (799770ms till timeout)
2022-03-30 21:30:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1279760ms till timeout)
2022-03-30 21:30:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (800781ms till timeout)
2022-03-30 21:30:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1278768ms till timeout)
2022-03-30 21:30:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (798771ms till timeout)
2022-03-30 21:30:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (798767ms till timeout)
2022-03-30 21:30:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1278757ms till timeout)
2022-03-30 21:30:07 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (799778ms till timeout)
2022-03-30 21:30:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:30:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:30:07 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1277765ms till timeout)
2022-03-30 21:30:07 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (797768ms till timeout)
2022-03-30 21:30:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (797764ms till timeout)
2022-03-30 21:30:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1277755ms till timeout)
2022-03-30 21:30:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (798774ms till timeout)
2022-03-30 21:30:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1276762ms till timeout)
2022-03-30 21:30:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (796765ms till timeout)
2022-03-30 21:30:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (796761ms till timeout)
2022-03-30 21:30:08 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1276752ms till timeout)
2022-03-30 21:30:09 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (797770ms till timeout)
2022-03-30 21:30:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1275759ms till timeout)
2022-03-30 21:30:09 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (795763ms till timeout)
2022-03-30 21:30:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (795758ms till timeout)
2022-03-30 21:30:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1275749ms till timeout)
2022-03-30 21:30:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (796765ms till timeout)
2022-03-30 21:30:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1274756ms till timeout)
2022-03-30 21:30:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (794760ms till timeout)
2022-03-30 21:30:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (794756ms till timeout)
2022-03-30 21:30:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1274746ms till timeout)
2022-03-30 21:30:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (795761ms till timeout)
2022-03-30 21:30:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1273752ms till timeout)
2022-03-30 21:30:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (793757ms till timeout)
2022-03-30 21:30:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (793753ms till timeout)
2022-03-30 21:30:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1273744ms till timeout)
2022-03-30 21:30:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:30:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:30:12 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (794756ms till timeout)
2022-03-30 21:30:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1272749ms till timeout)
2022-03-30 21:30:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (792754ms till timeout)
2022-03-30 21:30:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (792750ms till timeout)
2022-03-30 21:30:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1272741ms till timeout)
2022-03-30 21:30:13 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (793752ms till timeout)
2022-03-30 21:30:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1271746ms till timeout)
2022-03-30 21:30:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (791751ms till timeout)
2022-03-30 21:30:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (791747ms till timeout)
2022-03-30 21:30:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1271738ms till timeout)
2022-03-30 21:30:14 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (792748ms till timeout)
2022-03-30 21:30:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1270742ms till timeout)
2022-03-30 21:30:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (790749ms till timeout)
2022-03-30 21:30:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (790744ms till timeout)
2022-03-30 21:30:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1270735ms till timeout)
2022-03-30 21:30:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (791744ms till timeout)
2022-03-30 21:30:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1269738ms till timeout)
2022-03-30 21:30:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (789744ms till timeout)
2022-03-30 21:30:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (789740ms till timeout)
2022-03-30 21:30:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1269732ms till timeout)
2022-03-30 21:30:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (790740ms till timeout)
2022-03-30 21:30:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1268734ms till timeout)
2022-03-30 21:30:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (788741ms till timeout)
2022-03-30 21:30:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (788738ms till timeout)
2022-03-30 21:30:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1268729ms till timeout)
2022-03-30 21:30:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:30:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:30:17 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (789733ms till timeout)
2022-03-30 21:30:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1267731ms till timeout)
2022-03-30 21:30:17 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (787738ms till timeout)
2022-03-30 21:30:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (787735ms till timeout)
2022-03-30 21:30:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1267726ms till timeout)
2022-03-30 21:30:18 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (788729ms till timeout)
2022-03-30 21:30:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1266726ms till timeout)
2022-03-30 21:30:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (786734ms till timeout)
2022-03-30 21:30:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (786731ms till timeout)
2022-03-30 21:30:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1266722ms till timeout)
2022-03-30 21:30:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (787725ms till timeout)
2022-03-30 21:30:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1265723ms till timeout)
2022-03-30 21:30:19 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (785730ms till timeout)
2022-03-30 21:30:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (785728ms till timeout)
2022-03-30 21:30:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1265720ms till timeout)
2022-03-30 21:30:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (786721ms till timeout)
2022-03-30 21:30:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1264720ms till timeout)
2022-03-30 21:30:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (784727ms till timeout)
2022-03-30 21:30:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (784726ms till timeout)
2022-03-30 21:30:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1264717ms till timeout)
2022-03-30 21:30:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (785717ms till timeout)
2022-03-30 21:30:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (783724ms till timeout)
2022-03-30 21:30:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1263713ms till timeout)
2022-03-30 21:30:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (783723ms till timeout)
2022-03-30 21:30:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1263714ms till timeout)
2022-03-30 21:30:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:30:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:30:22 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (784713ms till timeout)
2022-03-30 21:30:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (782720ms till timeout)
2022-03-30 21:30:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1262709ms till timeout)
2022-03-30 21:30:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (782721ms till timeout)
2022-03-30 21:30:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1262711ms till timeout)
2022-03-30 21:30:23 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (783709ms till timeout)
2022-03-30 21:30:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (781716ms till timeout)
2022-03-30 21:30:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1261704ms till timeout)
2022-03-30 21:30:23 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (781717ms till timeout)
2022-03-30 21:30:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1261708ms till timeout)
2022-03-30 21:30:24 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (782705ms till timeout)
2022-03-30 21:30:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (780713ms till timeout)
2022-03-30 21:30:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1260701ms till timeout)
2022-03-30 21:30:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (780712ms till timeout)
2022-03-30 21:30:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1260705ms till timeout)
2022-03-30 21:30:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (781699ms till timeout)
2022-03-30 21:30:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (779708ms till timeout)
2022-03-30 21:30:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1259698ms till timeout)
2022-03-30 21:30:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (779708ms till timeout)
2022-03-30 21:30:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1259700ms till timeout)
2022-03-30 21:30:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (780696ms till timeout)
2022-03-30 21:30:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (778705ms till timeout)
2022-03-30 21:30:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1258694ms till timeout)
2022-03-30 21:30:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (778705ms till timeout)
2022-03-30 21:30:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1258697ms till timeout)
2022-03-30 21:30:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:30:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:30:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (779692ms till timeout)
2022-03-30 21:30:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (777702ms till timeout)
2022-03-30 21:30:27 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1257691ms till timeout)
2022-03-30 21:30:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (777703ms till timeout)
2022-03-30 21:30:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1257694ms till timeout)
2022-03-30 21:30:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (778688ms till timeout)
2022-03-30 21:30:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (776699ms till timeout)
2022-03-30 21:30:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1256689ms till timeout)
2022-03-30 21:30:28 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (776700ms till timeout)
2022-03-30 21:30:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1256692ms till timeout)
2022-03-30 21:30:29 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (777684ms till timeout)
2022-03-30 21:30:29 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (775696ms till timeout)
2022-03-30 21:30:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1255685ms till timeout)
2022-03-30 21:30:29 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (775698ms till timeout)
2022-03-30 21:30:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1255689ms till timeout)
2022-03-30 21:30:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (776680ms till timeout)
2022-03-30 21:30:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (774693ms till timeout)
2022-03-30 21:30:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1254682ms till timeout)
2022-03-30 21:30:30 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (774695ms till timeout)
2022-03-30 21:30:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1254686ms till timeout)
2022-03-30 21:30:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (775676ms till timeout)
2022-03-30 21:30:31 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (773690ms till timeout)
2022-03-30 21:30:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1253679ms till timeout)
2022-03-30 21:30:31 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (773692ms till timeout)
2022-03-30 21:30:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1253684ms till timeout)
2022-03-30 21:30:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:30:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:30:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (774672ms till timeout)
2022-03-30 21:30:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (772687ms till timeout)
2022-03-30 21:30:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (772690ms till timeout)
2022-03-30 21:30:32 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1252676ms till timeout)
2022-03-30 21:30:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1252681ms till timeout)
2022-03-30 21:30:33 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (773668ms till timeout)
2022-03-30 21:30:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (771684ms till timeout)
2022-03-30 21:30:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (771687ms till timeout)
2022-03-30 21:30:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1251672ms till timeout)
2022-03-30 21:30:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1251678ms till timeout)
2022-03-30 21:30:34 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (772664ms till timeout)
2022-03-30 21:30:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (770681ms till timeout)
2022-03-30 21:30:34 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (770684ms till timeout)
2022-03-30 21:30:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1250669ms till timeout)
2022-03-30 21:30:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1250674ms till timeout)
2022-03-30 21:30:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (771660ms till timeout)
2022-03-30 21:30:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (769678ms till timeout)
2022-03-30 21:30:35 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (769679ms till timeout)
2022-03-30 21:30:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1249666ms till timeout)
2022-03-30 21:30:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1249671ms till timeout)
2022-03-30 21:30:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (770656ms till timeout)
2022-03-30 21:30:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (768675ms till timeout)
2022-03-30 21:30:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1248663ms till timeout)
2022-03-30 21:30:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (768675ms till timeout)
2022-03-30 21:30:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1248668ms till timeout)
2022-03-30 21:30:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:30:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:30:37 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (769652ms till timeout)
2022-03-30 21:30:37 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (767671ms till timeout)
2022-03-30 21:30:37 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1247660ms till timeout)
2022-03-30 21:30:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (767670ms till timeout)
2022-03-30 21:30:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1247664ms till timeout)
2022-03-30 21:30:38 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (768648ms till timeout)
2022-03-30 21:30:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (766668ms till timeout)
2022-03-30 21:30:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1246656ms till timeout)
2022-03-30 21:30:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (766666ms till timeout)
2022-03-30 21:30:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1246657ms till timeout)
2022-03-30 21:30:39 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (767643ms till timeout)
2022-03-30 21:30:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (765665ms till timeout)
2022-03-30 21:30:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1245653ms till timeout)
2022-03-30 21:30:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (765663ms till timeout)
2022-03-30 21:30:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1245650ms till timeout)
2022-03-30 21:30:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (766638ms till timeout)
2022-03-30 21:30:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (764659ms till timeout)
2022-03-30 21:30:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1244649ms till timeout)
2022-03-30 21:30:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (764660ms till timeout)
2022-03-30 21:30:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1244647ms till timeout)
2022-03-30 21:30:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (765633ms till timeout)
2022-03-30 21:30:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (763655ms till timeout)
2022-03-30 21:30:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1243644ms till timeout)
2022-03-30 21:30:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (763654ms till timeout)
2022-03-30 21:30:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1243642ms till timeout)
2022-03-30 21:30:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:30:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:30:42 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (764628ms till timeout)
2022-03-30 21:30:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (762648ms till timeout)
2022-03-30 21:30:42 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1242630ms till timeout)
2022-03-30 21:30:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (762642ms till timeout)
2022-03-30 21:30:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1242637ms till timeout)
2022-03-30 21:30:43 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (763623ms till timeout)
2022-03-30 21:30:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (761599ms till timeout)
2022-03-30 21:30:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1241591ms till timeout)
2022-03-30 21:30:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (761597ms till timeout)
2022-03-30 21:30:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1241583ms till timeout)
2022-03-30 21:30:44 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (762616ms till timeout)
2022-03-30 21:30:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (760594ms till timeout)
2022-03-30 21:30:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1240587ms till timeout)
2022-03-30 21:30:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1240574ms till timeout)
2022-03-30 21:30:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (760586ms till timeout)
2022-03-30 21:30:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (761604ms till timeout)
2022-03-30 21:30:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (759588ms till timeout)
2022-03-30 21:30:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1239581ms till timeout)
2022-03-30 21:30:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1239568ms till timeout)
2022-03-30 21:30:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (759579ms till timeout)
2022-03-30 21:30:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (760598ms till timeout)
2022-03-30 21:30:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (758583ms till timeout)
2022-03-30 21:30:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1238569ms till timeout)
2022-03-30 21:30:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1238560ms till timeout)
2022-03-30 21:30:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (758573ms till timeout)
2022-03-30 21:30:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:30:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:30:47 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (759593ms till timeout)
2022-03-30 21:30:47 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (757572ms till timeout)
2022-03-30 21:30:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1237564ms till timeout)
2022-03-30 21:30:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1237555ms till timeout)
2022-03-30 21:30:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (757567ms till timeout)
2022-03-30 21:30:48 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (758589ms till timeout)
2022-03-30 21:30:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (756568ms till timeout)
2022-03-30 21:30:48 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1236560ms till timeout)
2022-03-30 21:30:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1236551ms till timeout)
2022-03-30 21:30:48 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (756563ms till timeout)
2022-03-30 21:30:49 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (757585ms till timeout)
2022-03-30 21:30:49 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (755564ms till timeout)
2022-03-30 21:30:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1235556ms till timeout)
2022-03-30 21:30:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1235547ms till timeout)
2022-03-30 21:30:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (755560ms till timeout)
2022-03-30 21:30:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (756571ms till timeout)
2022-03-30 21:30:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (754560ms till timeout)
2022-03-30 21:30:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1234553ms till timeout)
2022-03-30 21:30:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1234543ms till timeout)
2022-03-30 21:30:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (754554ms till timeout)
2022-03-30 21:30:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (755558ms till timeout)
2022-03-30 21:30:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (753554ms till timeout)
2022-03-30 21:30:51 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1233549ms till timeout)
2022-03-30 21:30:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1233536ms till timeout)
2022-03-30 21:30:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (753547ms till timeout)
2022-03-30 21:30:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:30:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:30:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (754553ms till timeout)
2022-03-30 21:30:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (752549ms till timeout)
2022-03-30 21:30:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1232544ms till timeout)
2022-03-30 21:30:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1232531ms till timeout)
2022-03-30 21:30:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (752527ms till timeout)
2022-03-30 21:30:53 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (753548ms till timeout)
2022-03-30 21:30:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (751537ms till timeout)
2022-03-30 21:30:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1231532ms till timeout)
2022-03-30 21:30:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1231524ms till timeout)
2022-03-30 21:30:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (751513ms till timeout)
2022-03-30 21:30:54 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (752543ms till timeout)
2022-03-30 21:30:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (750530ms till timeout)
2022-03-30 21:30:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1230526ms till timeout)
2022-03-30 21:30:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1230519ms till timeout)
2022-03-30 21:30:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (750502ms till timeout)
2022-03-30 21:30:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (751533ms till timeout)
2022-03-30 21:30:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (749525ms till timeout)
2022-03-30 21:30:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1229514ms till timeout)
2022-03-30 21:30:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1229521ms till timeout)
2022-03-30 21:30:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (749498ms till timeout)
2022-03-30 21:30:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (750528ms till timeout)
2022-03-30 21:30:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (748520ms till timeout)
2022-03-30 21:30:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1228505ms till timeout)
2022-03-30 21:30:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1228509ms till timeout)
2022-03-30 21:30:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (748490ms till timeout)
2022-03-30 21:30:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:30:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:30:57 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (749516ms till timeout)
2022-03-30 21:30:57 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (747515ms till timeout)
2022-03-30 21:30:57 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1227502ms till timeout)
2022-03-30 21:30:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1227505ms till timeout)
2022-03-30 21:30:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (747486ms till timeout)
2022-03-30 21:30:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (748474ms till timeout)
2022-03-30 21:30:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (746507ms till timeout)
2022-03-30 21:30:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1226462ms till timeout)
2022-03-30 21:30:58 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1226469ms till timeout)
2022-03-30 21:30:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (746473ms till timeout)
2022-03-30 21:30:59 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (747469ms till timeout)
2022-03-30 21:30:59 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (745502ms till timeout)
2022-03-30 21:30:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1225458ms till timeout)
2022-03-30 21:30:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1225462ms till timeout)
2022-03-30 21:30:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (745468ms till timeout)
2022-03-30 21:31:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (746456ms till timeout)
2022-03-30 21:31:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (744498ms till timeout)
2022-03-30 21:31:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1224454ms till timeout)
2022-03-30 21:31:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (744464ms till timeout)
2022-03-30 21:31:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1224458ms till timeout)
2022-03-30 21:31:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (745449ms till timeout)
2022-03-30 21:31:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (743491ms till timeout)
2022-03-30 21:31:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1223450ms till timeout)
2022-03-30 21:31:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (743458ms till timeout)
2022-03-30 21:31:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1223449ms till timeout)
2022-03-30 21:31:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:31:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:31:02 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (744445ms till timeout)
2022-03-30 21:31:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (742487ms till timeout)
2022-03-30 21:31:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1222439ms till timeout)
2022-03-30 21:31:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1222446ms till timeout)
2022-03-30 21:31:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (742452ms till timeout)
2022-03-30 21:31:03 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (743440ms till timeout)
2022-03-30 21:31:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (741482ms till timeout)
2022-03-30 21:31:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1221423ms till timeout)
2022-03-30 21:31:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1221428ms till timeout)
2022-03-30 21:31:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (741434ms till timeout)
2022-03-30 21:31:04 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (742436ms till timeout)
2022-03-30 21:31:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (740462ms till timeout)
2022-03-30 21:31:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1220418ms till timeout)
2022-03-30 21:31:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (740430ms till timeout)
2022-03-30 21:31:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1220423ms till timeout)
2022-03-30 21:31:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (741420ms till timeout)
2022-03-30 21:31:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (739457ms till timeout)
2022-03-30 21:31:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1219412ms till timeout)
2022-03-30 21:31:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (739412ms till timeout)
2022-03-30 21:31:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1219403ms till timeout)
2022-03-30 21:31:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (740415ms till timeout)
2022-03-30 21:31:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (738453ms till timeout)
2022-03-30 21:31:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1218408ms till timeout)
2022-03-30 21:31:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (738407ms till timeout)
2022-03-30 21:31:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1218400ms till timeout)
2022-03-30 21:31:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:31:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:31:07 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (739410ms till timeout)
2022-03-30 21:31:07 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (737448ms till timeout)
2022-03-30 21:31:07 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1217403ms till timeout)
2022-03-30 21:31:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (737401ms till timeout)
2022-03-30 21:31:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1217393ms till timeout)
2022-03-30 21:31:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (738405ms till timeout)
2022-03-30 21:31:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (736445ms till timeout)
2022-03-30 21:31:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1216398ms till timeout)
2022-03-30 21:31:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (736395ms till timeout)
2022-03-30 21:31:08 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1216388ms till timeout)
2022-03-30 21:31:09 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (737400ms till timeout)
2022-03-30 21:31:09 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (735439ms till timeout)
2022-03-30 21:31:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1215394ms till timeout)
2022-03-30 21:31:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (735390ms till timeout)
2022-03-30 21:31:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1215383ms till timeout)
2022-03-30 21:31:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (736396ms till timeout)
2022-03-30 21:31:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (734435ms till timeout)
2022-03-30 21:31:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1214389ms till timeout)
2022-03-30 21:31:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (734386ms till timeout)
2022-03-30 21:31:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1214379ms till timeout)
2022-03-30 21:31:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (735392ms till timeout)
2022-03-30 21:31:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (733429ms till timeout)
2022-03-30 21:31:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1213385ms till timeout)
2022-03-30 21:31:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (733382ms till timeout)
2022-03-30 21:31:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1213374ms till timeout)
2022-03-30 21:31:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:31:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:31:12 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (734387ms till timeout)
2022-03-30 21:31:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (732426ms till timeout)
2022-03-30 21:31:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1212381ms till timeout)
2022-03-30 21:31:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (732376ms till timeout)
2022-03-30 21:31:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1212370ms till timeout)
2022-03-30 21:31:13 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (733383ms till timeout)
2022-03-30 21:31:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (731422ms till timeout)
2022-03-30 21:31:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1211378ms till timeout)
2022-03-30 21:31:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (731373ms till timeout)
2022-03-30 21:31:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1211366ms till timeout)
2022-03-30 21:31:14 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (732379ms till timeout)
2022-03-30 21:31:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (730419ms till timeout)
2022-03-30 21:31:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1210372ms till timeout)
2022-03-30 21:31:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (730368ms till timeout)
2022-03-30 21:31:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1210359ms till timeout)
2022-03-30 21:31:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (731372ms till timeout)
2022-03-30 21:31:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: http-bridge-tls-cluster-name is in desired state: Ready
2022-03-30 21:31:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1638214753-683095272 in namespace http-bridge-tls-st
2022-03-30 21:31:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1209364ms till timeout)
2022-03-30 21:31:15 [ForkJoinPool-3-worker-7] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkausers' with unstable version 'v1beta2'
2022-03-30 21:31:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (729362ms till timeout)
2022-03-30 21:31:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1209355ms till timeout)
2022-03-30 21:31:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1638214753-683095272
2022-03-30 21:31:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1638214753-683095272 will have desired state: Ready
2022-03-30 21:31:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-1638214753-683095272 will have desired state: Ready
2022-03-30 21:31:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-1638214753-683095272 will have desired state: Ready not ready, will try again in 1000 ms (179994ms till timeout)
2022-03-30 21:31:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (730352ms till timeout)
2022-03-30 21:31:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1208355ms till timeout)
2022-03-30 21:31:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (728358ms till timeout)
2022-03-30 21:31:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1208351ms till timeout)
2022-03-30 21:31:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-1638214753-683095272 will have desired state: Ready not ready, will try again in 1000 ms (178990ms till timeout)
2022-03-30 21:31:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:31:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:31:17 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (729347ms till timeout)
2022-03-30 21:31:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1207349ms till timeout)
2022-03-30 21:31:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (727354ms till timeout)
2022-03-30 21:31:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1207346ms till timeout)
2022-03-30 21:31:17 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1638214753-683095272 is in desired state: Ready
2022-03-30 21:31:17 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment http-bridge-tls-st-kafka-clients in namespace http-bridge-tls-st
2022-03-30 21:31:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients
2022-03-30 21:31:17 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready
2022-03-30 21:31:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready
2022-03-30 21:31:17 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready not ready, will try again in 1000 ms (479982ms till timeout)
2022-03-30 21:31:18 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (728342ms till timeout)
2022-03-30 21:31:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1206345ms till timeout)
2022-03-30 21:31:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (726349ms till timeout)
2022-03-30 21:31:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1206341ms till timeout)
2022-03-30 21:31:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready not ready, will try again in 1000 ms (478976ms till timeout)
2022-03-30 21:31:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (727333ms till timeout)
2022-03-30 21:31:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1205341ms till timeout)
2022-03-30 21:31:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (725342ms till timeout)
2022-03-30 21:31:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1205334ms till timeout)
2022-03-30 21:31:19 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: http-bridge-tls-st-kafka-clients is ready
2022-03-30 21:31:20 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-30 21:31:20 [ForkJoinPool-3-worker-7] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkabridges' with unstable version 'v1beta2'
2022-03-30 21:31:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaBridge:http-bridge-tls-cluster-name
2022-03-30 21:31:20 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-30 21:31:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-30 21:31:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (479969ms till timeout)
2022-03-30 21:31:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (726328ms till timeout)
2022-03-30 21:31:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1204330ms till timeout)
2022-03-30 21:31:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (724337ms till timeout)
2022-03-30 21:31:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1204328ms till timeout)
2022-03-30 21:31:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (478965ms till timeout)
2022-03-30 21:31:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (725322ms till timeout)
2022-03-30 21:31:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1203326ms till timeout)
2022-03-30 21:31:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (723332ms till timeout)
2022-03-30 21:31:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1203324ms till timeout)
2022-03-30 21:31:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (477960ms till timeout)
2022-03-30 21:31:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:31:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:31:22 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (724316ms till timeout)
2022-03-30 21:31:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1202321ms till timeout)
2022-03-30 21:31:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (722327ms till timeout)
2022-03-30 21:31:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1202318ms till timeout)
2022-03-30 21:31:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (476956ms till timeout)
2022-03-30 21:31:23 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (723311ms till timeout)
2022-03-30 21:31:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1201316ms till timeout)
2022-03-30 21:31:23 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (721320ms till timeout)
2022-03-30 21:31:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1201312ms till timeout)
2022-03-30 21:31:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (475951ms till timeout)
2022-03-30 21:31:24 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (722299ms till timeout)
2022-03-30 21:31:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1200306ms till timeout)
2022-03-30 21:31:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (720316ms till timeout)
2022-03-30 21:31:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1200307ms till timeout)
2022-03-30 21:31:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (474947ms till timeout)
2022-03-30 21:31:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (721290ms till timeout)
2022-03-30 21:31:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1199299ms till timeout)
2022-03-30 21:31:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (719309ms till timeout)
2022-03-30 21:31:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1199302ms till timeout)
2022-03-30 21:31:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (473942ms till timeout)
2022-03-30 21:31:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (720286ms till timeout)
2022-03-30 21:31:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1198295ms till timeout)
2022-03-30 21:31:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (718304ms till timeout)
2022-03-30 21:31:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1198293ms till timeout)
2022-03-30 21:31:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (472936ms till timeout)
2022-03-30 21:31:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:31:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:31:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (719281ms till timeout)
2022-03-30 21:31:27 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1197290ms till timeout)
2022-03-30 21:31:27 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: user-cluster-name is in desired state: Ready
2022-03-30 21:31:27 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 21:31:27 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testUpdateUser-STARTED
2022-03-30 21:31:27 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 21:31:27 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-30 21:31:27 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testUpdateUser
2022-03-30 21:31:27 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 4
2022-03-30 21:31:27 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:230] testUpdateUser test now can proceed its execution
2022-03-30 21:31:27 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 21:31:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendMessagesTlsScramSha=my-cluster-380e6586, testUpdateUser=my-cluster-9b2bd187, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09}
2022-03-30 21:31:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendMessagesTlsScramSha=my-user-2039401547-533835955, testUpdateUser=my-user-595127408-1604948726, testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659}
2022-03-30 21:31:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendMessagesTlsScramSha=my-topic-988094002-1400465950, testUpdateUser=my-topic-1875736194-1340884809, testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393}
2022-03-30 21:31:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendMessagesTlsScramSha=my-cluster-380e6586-kafka-clients, testUpdateUser=my-cluster-9b2bd187-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients}
2022-03-30 21:31:27 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-595127408-1604948726 in namespace user-st
2022-03-30 21:31:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1197286ms till timeout)
2022-03-30 21:31:27 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-595127408-1604948726
2022-03-30 21:31:27 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-595127408-1604948726 will have desired state: Ready
2022-03-30 21:31:27 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-595127408-1604948726 will have desired state: Ready
2022-03-30 21:31:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-595127408-1604948726 will have desired state: Ready not ready, will try again in 1000 ms (179997ms till timeout)
2022-03-30 21:31:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (471929ms till timeout)
2022-03-30 21:31:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (718277ms till timeout)
2022-03-30 21:31:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1196282ms till timeout)
2022-03-30 21:31:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1196275ms till timeout)
2022-03-30 21:31:28 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-595127408-1604948726 will have desired state: Ready not ready, will try again in 1000 ms (178992ms till timeout)
2022-03-30 21:31:29 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (470917ms till timeout)
2022-03-30 21:31:29 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (717273ms till timeout)
2022-03-30 21:31:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1195277ms till timeout)
2022-03-30 21:31:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1195272ms till timeout)
2022-03-30 21:31:29 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-595127408-1604948726 is in desired state: Ready
2022-03-30 21:31:29 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['data']['ca.crt']
2022-03-30 21:31:29 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['data']['user.crt']
2022-03-30 21:31:29 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['data']['user.key']
2022-03-30 21:31:29 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['metadata']['name']
2022-03-30 21:31:29 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['metadata']['namespace']
2022-03-30 21:31:29 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['metadata']['name']
2022-03-30 21:31:29 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['metadata']['namespace']
2022-03-30 21:31:29 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['spec']['authentication']['type']
2022-03-30 21:31:30 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for increase observation generation from 1 for user my-user-595127408-1604948726
2022-03-30 21:31:30 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] increase observation generation from 1 for user my-user-595127408-1604948726 not ready, will try again in 1000 ms (179995ms till timeout)
2022-03-30 21:31:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (469907ms till timeout)
2022-03-30 21:31:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (716269ms till timeout)
2022-03-30 21:31:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1194273ms till timeout)
2022-03-30 21:31:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1194268ms till timeout)
2022-03-30 21:31:31 [ForkJoinPool-3-worker-5] [32mINFO [m [SecretUtils:46] Waiting for Secret my-user-595127408-1604948726
2022-03-30 21:31:31 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Expected secret my-user-595127408-1604948726 exists
2022-03-30 21:31:31 [ForkJoinPool-3-worker-5] [32mINFO [m [SecretUtils:50] Secret my-user-595127408-1604948726 created
2022-03-30 21:31:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-595127408-1604948726 will have desired state: Ready
2022-03-30 21:31:31 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-595127408-1604948726 will have desired state: Ready
2022-03-30 21:31:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-595127408-1604948726 is in desired state: Ready
2022-03-30 21:31:31 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['data']['password']
2022-03-30 21:31:31 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['metadata']['name']
2022-03-30 21:31:31 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['metadata']['namespace']
2022-03-30 21:31:31 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['spec']['authentication']['type']
2022-03-30 21:31:31 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaUserUtils:62] Waiting for KafkaUser deletion my-user-595127408-1604948726
2022-03-30 21:31:31 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser deletion my-user-595127408-1604948726
2022-03-30 21:31:31 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaUserUtils:75] KafkaUser my-user-595127408-1604948726 deleted
2022-03-30 21:31:31 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 21:31:31 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-30 21:31:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 21:31:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testUpdateUser
2022-03-30 21:31:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-595127408-1604948726 in namespace user-st
2022-03-30 21:31:31 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-595127408-1604948726
2022-03-30 21:31:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 21:31:31 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:267] testUpdateUser - Notifies waiting test cases:[testCruiseControlBasicAPIRequests, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testUpdateUser] to and randomly select one to start execution
2022-03-30 21:31:31 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testUpdateUser
2022-03-30 21:31:31 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 3
2022-03-30 21:31:31 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testUpdateUser-FINISHED
2022-03-30 21:31:31 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 21:31:31 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 21:31:31 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:690] [operators.user.UserST - After All] - Clean up after test suite
2022-03-30 21:31:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 21:31:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for UserST
2022-03-30 21:31:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka user-cluster-name in namespace user-st
2022-03-30 21:31:31 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:user-cluster-name
2022-03-30 21:31:31 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:user-cluster-name not ready, will try again in 10000 ms (839997ms till timeout)
2022-03-30 21:31:31 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (468898ms till timeout)
2022-03-30 21:31:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (715262ms till timeout)
2022-03-30 21:31:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1193269ms till timeout)
2022-03-30 21:31:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1193257ms till timeout)
2022-03-30 21:31:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (467893ms till timeout)
2022-03-30 21:31:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:31:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:31:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (714259ms till timeout)
2022-03-30 21:31:32 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1192266ms till timeout)
2022-03-30 21:31:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1192254ms till timeout)
2022-03-30 21:31:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (466889ms till timeout)
2022-03-30 21:31:33 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (713255ms till timeout)
2022-03-30 21:31:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1191262ms till timeout)
2022-03-30 21:31:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1191251ms till timeout)
2022-03-30 21:31:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (465885ms till timeout)
2022-03-30 21:31:34 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (712252ms till timeout)
2022-03-30 21:31:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1190259ms till timeout)
2022-03-30 21:31:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1190248ms till timeout)
2022-03-30 21:31:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (464882ms till timeout)
2022-03-30 21:31:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (711248ms till timeout)
2022-03-30 21:31:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1189256ms till timeout)
2022-03-30 21:31:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1189245ms till timeout)
2022-03-30 21:31:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (463878ms till timeout)
2022-03-30 21:31:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (710245ms till timeout)
2022-03-30 21:31:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1188253ms till timeout)
2022-03-30 21:31:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1188243ms till timeout)
2022-03-30 21:31:37 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (462875ms till timeout)
2022-03-30 21:31:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:31:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:31:37 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (709239ms till timeout)
2022-03-30 21:31:37 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1187243ms till timeout)
2022-03-30 21:31:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1187239ms till timeout)
2022-03-30 21:31:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (461870ms till timeout)
2022-03-30 21:31:38 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (708235ms till timeout)
2022-03-30 21:31:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1186237ms till timeout)
2022-03-30 21:31:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1186235ms till timeout)
2022-03-30 21:31:39 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaBridge: http-bridge-tls-cluster-name is in desired state: Ready
2022-03-30 21:31:39 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-30 21:31:39 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 21:31:39 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testSendSimpleMessageTls-STARTED
2022-03-30 21:31:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 21:31:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:659] [bridge.HttpBridgeTlsST - Before Each] - Setup test case environment
2022-03-30 21:31:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:77] [bridge.HttpBridgeTlsST] - Adding parallel test: testSendSimpleMessageTls
2022-03-30 21:31:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:81] [bridge.HttpBridgeTlsST] - Parallel test count: 4
2022-03-30 21:31:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:230] testSendSimpleMessageTls test now can proceed its execution
2022-03-30 21:31:39 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 21:31:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-b70e5342, testSendMessagesTlsScramSha=my-cluster-380e6586, testUpdateUser=my-cluster-9b2bd187, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09}
2022-03-30 21:31:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-602433797-900694548, testSendMessagesTlsScramSha=my-user-2039401547-533835955, testUpdateUser=my-user-595127408-1604948726, testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659}
2022-03-30 21:31:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-1299629356-597168683, testSendMessagesTlsScramSha=my-topic-988094002-1400465950, testUpdateUser=my-topic-1875736194-1340884809, testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393}
2022-03-30 21:31:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-b70e5342-kafka-clients, testSendMessagesTlsScramSha=my-cluster-380e6586-kafka-clients, testUpdateUser=my-cluster-9b2bd187-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients}
2022-03-30 21:31:39 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1922413090-69870416 in namespace http-bridge-tls-st
2022-03-30 21:31:39 [ForkJoinPool-3-worker-7] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkatopics' with unstable version 'v1beta2'
2022-03-30 21:31:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1922413090-69870416
2022-03-30 21:31:39 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1922413090-69870416 will have desired state: Ready
2022-03-30 21:31:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1922413090-69870416 will have desired state: Ready
2022-03-30 21:31:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1922413090-69870416 will have desired state: Ready not ready, will try again in 1000 ms (179997ms till timeout)
2022-03-30 21:31:39 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (707229ms till timeout)
2022-03-30 21:31:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1185231ms till timeout)
2022-03-30 21:31:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1185228ms till timeout)
2022-03-30 21:31:40 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1922413090-69870416 is in desired state: Ready
2022-03-30 21:31:40 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job producer-337125814 in namespace http-bridge-tls-st
2022-03-30 21:31:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:producer-337125814
2022-03-30 21:31:40 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: producer-337125814 will be in active state
2022-03-30 21:31:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for job active
2022-03-30 21:31:40 [ForkJoinPool-3-worker-7] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:producer-337125814 to finished
2022-03-30 21:31:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for job finished
2022-03-30 21:31:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job producer-337125814 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T21:31:40Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 21:31:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (219963ms till timeout)
2022-03-30 21:31:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (706225ms till timeout)
2022-03-30 21:31:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1184226ms till timeout)
2022-03-30 21:31:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1184224ms till timeout)
2022-03-30 21:31:41 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 21:31:41 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Namespace user-st removal
2022-03-30 21:31:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace user-st -o yaml
2022-03-30 21:31:41 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace user-st -o yaml
2022-03-30 21:31:41 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:31:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (479896ms till timeout)
2022-03-30 21:31:41 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job producer-337125814 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T21:31:40Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 21:31:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (218956ms till timeout)
2022-03-30 21:31:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (705218ms till timeout)
2022-03-30 21:31:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1183209ms till timeout)
2022-03-30 21:31:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1183161ms till timeout)
2022-03-30 21:31:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace user-st -o yaml
2022-03-30 21:31:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job producer-337125814 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T21:31:40Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 21:31:42 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace user-st -o yaml
2022-03-30 21:31:42 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:31:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (478779ms till timeout)
2022-03-30 21:31:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (217943ms till timeout)
2022-03-30 21:31:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:31:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:31:42 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (704210ms till timeout)
2022-03-30 21:31:42 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1182200ms till timeout)
2022-03-30 21:31:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1182149ms till timeout)
2022-03-30 21:31:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace user-st -o yaml
2022-03-30 21:31:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job producer-337125814 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T21:31:40Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 21:31:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (216924ms till timeout)
2022-03-30 21:31:43 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace user-st -o yaml
2022-03-30 21:31:43 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:31:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (477671ms till timeout)
2022-03-30 21:31:43 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (703203ms till timeout)
2022-03-30 21:31:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1181196ms till timeout)
2022-03-30 21:31:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1181143ms till timeout)
2022-03-30 21:31:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job producer-337125814 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T21:31:40Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 21:31:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (215914ms till timeout)
2022-03-30 21:31:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace user-st -o yaml
2022-03-30 21:31:44 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace user-st -o yaml
2022-03-30 21:31:44 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:31:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (476552ms till timeout)
2022-03-30 21:31:44 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (702194ms till timeout)
2022-03-30 21:31:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1180171ms till timeout)
2022-03-30 21:31:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1180139ms till timeout)
2022-03-30 21:31:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job producer-337125814 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T21:31:40Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 21:31:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (214896ms till timeout)
2022-03-30 21:31:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace user-st -o yaml
2022-03-30 21:31:45 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace user-st -o yaml
2022-03-30 21:31:45 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:31:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (475437ms till timeout)
2022-03-30 21:31:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (701189ms till timeout)
2022-03-30 21:31:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1179166ms till timeout)
2022-03-30 21:31:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1179135ms till timeout)
2022-03-30 21:31:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job producer-337125814 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T21:31:40Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 21:31:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (213887ms till timeout)
2022-03-30 21:31:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace user-st -o yaml
2022-03-30 21:31:46 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace user-st -o yaml
2022-03-30 21:31:46 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:31:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (474336ms till timeout)
2022-03-30 21:31:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (700186ms till timeout)
2022-03-30 21:31:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1178163ms till timeout)
2022-03-30 21:31:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1178130ms till timeout)
2022-03-30 21:31:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:31:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:31:47 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job producer-337125814 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T21:31:40Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 21:31:47 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (212879ms till timeout)
2022-03-30 21:31:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace user-st -o yaml
2022-03-30 21:31:47 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (699180ms till timeout)
2022-03-30 21:31:47 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace user-st -o yaml
2022-03-30 21:31:47 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:31:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (473152ms till timeout)
2022-03-30 21:31:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1177158ms till timeout)
2022-03-30 21:31:48 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1177124ms till timeout)
2022-03-30 21:31:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job producer-337125814 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T21:31:40Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 21:31:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (211868ms till timeout)
2022-03-30 21:31:48 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (698176ms till timeout)
2022-03-30 21:31:48 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace user-st -o yaml
2022-03-30 21:31:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1176151ms till timeout)
2022-03-30 21:31:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1176120ms till timeout)
2022-03-30 21:31:49 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace user-st -o yaml
2022-03-30 21:31:49 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:31:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (472047ms till timeout)
2022-03-30 21:31:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job producer-337125814 in namespace http-bridge-tls-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-30T21:31:48Z, conditions=[JobCondition(lastProbeTime=2022-03-30T21:31:48Z, lastTransitionTime=2022-03-30T21:31:48Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-30T21:31:40Z, succeeded=1, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 21:31:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [JobUtils:37] Waiting for ReplicaSet of Deployment producer-337125814 deletion
2022-03-30 21:31:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for ReplicaSet producer-337125814 to be deleted
2022-03-30 21:31:49 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] ReplicaSet producer-337125814 to be deleted not ready, will try again in 5000 ms (179993ms till timeout)
2022-03-30 21:31:49 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (697172ms till timeout)
2022-03-30 21:31:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1175147ms till timeout)
2022-03-30 21:31:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1175115ms till timeout)
2022-03-30 21:31:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace user-st -o yaml
2022-03-30 21:31:50 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace user-st -o yaml
2022-03-30 21:31:50 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:31:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (470956ms till timeout)
2022-03-30 21:31:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (696168ms till timeout)
2022-03-30 21:31:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1174144ms till timeout)
2022-03-30 21:31:51 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1174112ms till timeout)
2022-03-30 21:31:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace user-st -o yaml
2022-03-30 21:31:51 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace user-st -o yaml
2022-03-30 21:31:51 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:31:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (469869ms till timeout)
2022-03-30 21:31:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (695164ms till timeout)
2022-03-30 21:31:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1173140ms till timeout)
2022-03-30 21:31:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1173108ms till timeout)
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace user-st -o yaml
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-2 get Namespace user-st -o yaml
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "user-st" not found
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 21:31:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:31:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[namespace-2], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@12282135=[namespace-1], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-0]}
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:254] UserST - Notifies waiting test suites:[CruiseControlST, HttpBridgeTlsST, UserST, CruiseControlApiST, ListenersST] to and randomly select one to start execution
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:85] [operators.user.UserST] - Removing parallel suite: UserST
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:89] [operators.user.UserST] - Parallel suites count: 4
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 196.064 s - in io.strimzi.systemtest.operators.user.UserST
[[1;34mINFO[m] Running io.strimzi.systemtest.security.SecurityST
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:667] [security.SecurityST - Before All] - Setup test suite environment
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:69] [security.SecurityST] - Adding parallel suite: SecurityST
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:73] [security.SecurityST] - Parallel suites count: 5
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:184] SecurityST suite now can proceed its execution
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestSuiteNamespaceManager:129] Test suite `SecurityST` creates these additional namespaces:[security-st]
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: security-st
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Namespace security-st
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace security-st -o json
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace security-st -o json
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T21:31:52Z",
        "name": "security-st",
        "resourceVersion": "99014",
        "selfLink": "/api/v1/namespaces/security-st",
        "uid": "b6687257-cd1c-468c-8b74-6a70de30a2b5"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[namespace-2], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@12282135=[namespace-1], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-0], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: security-st
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=security-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: security-st
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testAutoRenewAllCaCertsTriggeredByAnno-STARTED
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:659] [security.SecurityST - Before Each] - Setup test case environment
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:77] [security.SecurityST] - Adding parallel test: testAutoRenewAllCaCertsTriggeredByAnno
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:81] [security.SecurityST] - Parallel test count: 5
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:230] testAutoRenewAllCaCertsTriggeredByAnno test now can proceed its execution
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349, testSendSimpleMessageTls=my-cluster-b70e5342, testSendMessagesTlsScramSha=my-cluster-380e6586, testUpdateUser=my-cluster-9b2bd187, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09}
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testAutoRenewAllCaCertsTriggeredByAnno=my-user-1590558165-46977776, testSendSimpleMessageTls=my-user-602433797-900694548, testSendMessagesTlsScramSha=my-user-2039401547-533835955, testUpdateUser=my-user-595127408-1604948726, testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659}
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1210910585-1669754462, testSendSimpleMessageTls=my-topic-1299629356-597168683, testSendMessagesTlsScramSha=my-topic-988094002-1400465950, testUpdateUser=my-topic-1875736194-1340884809, testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393}
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349-kafka-clients, testSendSimpleMessageTls=my-cluster-b70e5342-kafka-clients, testSendMessagesTlsScramSha=my-cluster-380e6586-kafka-clients, testUpdateUser=my-cluster-9b2bd187-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients}
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-3 for test case:testAutoRenewAllCaCertsTriggeredByAnno
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-3
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-3
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace security-st get Namespace namespace-3 -o json
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace security-st get Namespace namespace-3 -o json
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T21:31:52Z",
        "name": "namespace-3",
        "resourceVersion": "99018",
        "selfLink": "/api/v1/namespaces/namespace-3",
        "uid": "e764e11c-0414-48d6-8967-75d022a1d9db"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[namespace-2], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-3], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@12282135=[namespace-1], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-0], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-3
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-3, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-3
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:596] Creating a cluster
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-58833349 in namespace namespace-3
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-3
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-58833349
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-58833349 will have desired state: Ready
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-58833349 will have desired state: Ready
2022-03-30 21:31:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1799990ms till timeout)
2022-03-30 21:31:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (694154ms till timeout)
2022-03-30 21:31:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1172129ms till timeout)
2022-03-30 21:31:53 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] Kafka: cruise-control-api-cluster-name is in desired state: Ready
2022-03-30 21:31:53 [ForkJoinPool-3-worker-9] [32mINFO [m [CruiseControlApiST:153] ----> CRUISE CONTROL DEPLOYMENT STATE ENDPOINT <----
2022-03-30 21:31:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec cruise-control-api-cluster-name-cruise-control-5fbb4447f5-xdf8g -c cruise-control -- /bin/bash -c curl -XGET -k  HTTP://localhost:9090/kafkacruisecontrol/state
2022-03-30 21:31:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec cruise-control-api-cluster-name-cruise-control-5fbb4447f5-xdf8g -c cruise-control -- /bin/bash -c curl -XGET -k  HTTP://localhost:9090/kafkacruisecontrol/state
2022-03-30 21:31:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:31:53 [ForkJoinPool-3-worker-9] [32mINFO [m [CruiseControlApiST:157] Verifying that Cruise Control REST API is available using HTTP request without credentials
2022-03-30 21:31:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 21:31:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:675] [cruisecontrol.CruiseControlApiST - After Each] - Clean up after test
2022-03-30 21:31:53 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 21:31:53 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for testCruiseControlBasicAPIRequestsWithSecurityDisabled
2022-03-30 21:31:53 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Kafka cruise-control-api-cluster-name in namespace namespace-1
2022-03-30 21:31:53 [ForkJoinPool-3-worker-9] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-1, for cruise control Kafka cluster cruise-control-api-cluster-name
2022-03-30 21:31:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:cruise-control-api-cluster-name
2022-03-30 21:31:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:cruise-control-api-cluster-name not ready, will try again in 10000 ms (839988ms till timeout)
2022-03-30 21:31:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1798979ms till timeout)
2022-03-30 21:31:53 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (693147ms till timeout)
2022-03-30 21:31:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1171122ms till timeout)
2022-03-30 21:31:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [JobUtils:40] Job producer-337125814 was deleted
2022-03-30 21:31:54 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-30 21:31:54 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job consumer-753605083 in namespace http-bridge-tls-st
2022-03-30 21:31:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:consumer-753605083
2022-03-30 21:31:54 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: consumer-753605083 will be in active state
2022-03-30 21:31:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for job active
2022-03-30 21:31:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job active not ready, will try again in 1000 ms (179963ms till timeout)
2022-03-30 21:31:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1797975ms till timeout)
2022-03-30 21:31:54 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (692143ms till timeout)
2022-03-30 21:31:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1170116ms till timeout)
2022-03-30 21:31:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:consumer-753605083 to finished
2022-03-30 21:31:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for job finished
2022-03-30 21:31:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job consumer-753605083 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T21:31:54Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 21:31:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (219984ms till timeout)
2022-03-30 21:31:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1796967ms till timeout)
2022-03-30 21:31:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (691138ms till timeout)
2022-03-30 21:31:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1169110ms till timeout)
2022-03-30 21:31:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job consumer-753605083 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T21:31:54Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 21:31:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (218972ms till timeout)
2022-03-30 21:31:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1795946ms till timeout)
2022-03-30 21:31:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (690124ms till timeout)
2022-03-30 21:31:57 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1168105ms till timeout)
2022-03-30 21:31:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:31:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:31:57 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job consumer-753605083 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T21:31:54Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 21:31:57 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (217966ms till timeout)
2022-03-30 21:31:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1794941ms till timeout)
2022-03-30 21:31:57 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (689113ms till timeout)
2022-03-30 21:31:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1167100ms till timeout)
2022-03-30 21:31:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job consumer-753605083 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T21:31:54Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 21:31:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (216950ms till timeout)
2022-03-30 21:31:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1793937ms till timeout)
2022-03-30 21:31:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (688109ms till timeout)
2022-03-30 21:31:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1166086ms till timeout)
2022-03-30 21:31:59 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job consumer-753605083 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T21:31:54Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 21:31:59 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (215939ms till timeout)
2022-03-30 21:31:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1792933ms till timeout)
2022-03-30 21:31:59 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (687103ms till timeout)
2022-03-30 21:32:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1165081ms till timeout)
2022-03-30 21:32:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job consumer-753605083 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T21:31:54Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 21:32:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (214926ms till timeout)
2022-03-30 21:32:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1791927ms till timeout)
2022-03-30 21:32:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (686100ms till timeout)
2022-03-30 21:32:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1164070ms till timeout)
2022-03-30 21:32:01 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job consumer-753605083 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T21:31:54Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 21:32:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (213920ms till timeout)
2022-03-30 21:32:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1790921ms till timeout)
2022-03-30 21:32:02 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (685095ms till timeout)
2022-03-30 21:32:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1163065ms till timeout)
2022-03-30 21:32:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:32:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:32:02 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job consumer-753605083 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T21:31:54Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 21:32:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (212914ms till timeout)
2022-03-30 21:32:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1789915ms till timeout)
2022-03-30 21:32:03 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (684089ms till timeout)
2022-03-30 21:32:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1162061ms till timeout)
2022-03-30 21:32:03 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 21:32:03 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-1 for test case:testCruiseControlBasicAPIRequestsWithSecurityDisabled
2022-03-30 21:32:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-1 removal
2022-03-30 21:32:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (479912ms till timeout)
2022-03-30 21:32:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job consumer-753605083 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T21:31:54Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 21:32:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (211904ms till timeout)
2022-03-30 21:32:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1788911ms till timeout)
2022-03-30 21:32:04 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (683085ms till timeout)
2022-03-30 21:32:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1161052ms till timeout)
2022-03-30 21:32:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (478823ms till timeout)
2022-03-30 21:32:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job consumer-753605083 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T21:31:54Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 21:32:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (210898ms till timeout)
2022-03-30 21:32:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1787907ms till timeout)
2022-03-30 21:32:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (682077ms till timeout)
2022-03-30 21:32:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1160048ms till timeout)
2022-03-30 21:32:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job consumer-753605083 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T21:31:54Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 21:32:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (209887ms till timeout)
2022-03-30 21:32:05 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:05 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (477745ms till timeout)
2022-03-30 21:32:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1786904ms till timeout)
2022-03-30 21:32:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (681073ms till timeout)
2022-03-30 21:32:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1159045ms till timeout)
2022-03-30 21:32:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job consumer-753605083 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T21:31:54Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 21:32:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (208881ms till timeout)
2022-03-30 21:32:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1785900ms till timeout)
2022-03-30 21:32:06 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:06 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (476665ms till timeout)
2022-03-30 21:32:07 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (680070ms till timeout)
2022-03-30 21:32:07 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1158042ms till timeout)
2022-03-30 21:32:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:32:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:32:07 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job consumer-753605083 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T21:31:54Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 21:32:07 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (207876ms till timeout)
2022-03-30 21:32:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1784896ms till timeout)
2022-03-30 21:32:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:07 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:07 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (475591ms till timeout)
2022-03-30 21:32:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (679066ms till timeout)
2022-03-30 21:32:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1157039ms till timeout)
2022-03-30 21:32:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job consumer-753605083 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T21:31:54Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 21:32:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (206869ms till timeout)
2022-03-30 21:32:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1783892ms till timeout)
2022-03-30 21:32:08 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:08 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:08 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:08 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (474504ms till timeout)
2022-03-30 21:32:09 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (678061ms till timeout)
2022-03-30 21:32:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1156035ms till timeout)
2022-03-30 21:32:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job consumer-753605083 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T21:31:54Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 21:32:09 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (205860ms till timeout)
2022-03-30 21:32:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1782889ms till timeout)
2022-03-30 21:32:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (473405ms till timeout)
2022-03-30 21:32:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (677056ms till timeout)
2022-03-30 21:32:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1155031ms till timeout)
2022-03-30 21:32:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job consumer-753605083 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T21:31:54Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 21:32:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (204854ms till timeout)
2022-03-30 21:32:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1781885ms till timeout)
2022-03-30 21:32:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-380e6586 will have desired state: Ready not ready, will try again in 1000 ms (676050ms till timeout)
2022-03-30 21:32:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1154026ms till timeout)
2022-03-30 21:32:11 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:11 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (472299ms till timeout)
2022-03-30 21:32:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job consumer-753605083 in namespace http-bridge-tls-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-30T21:32:10Z, conditions=[JobCondition(lastProbeTime=2022-03-30T21:32:10Z, lastTransitionTime=2022-03-30T21:32:10Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-30T21:31:54Z, succeeded=1, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 21:32:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [JobUtils:37] Waiting for ReplicaSet of Deployment consumer-753605083 deletion
2022-03-30 21:32:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for ReplicaSet consumer-753605083 to be deleted
2022-03-30 21:32:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] ReplicaSet consumer-753605083 to be deleted not ready, will try again in 5000 ms (179996ms till timeout)
2022-03-30 21:32:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1780881ms till timeout)
2022-03-30 21:32:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-380e6586 is in desired state: Ready
2022-03-30 21:32:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-988094002-1400465950 in namespace namespace-3
2022-03-30 21:32:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-2
2022-03-30 21:32:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-988094002-1400465950
2022-03-30 21:32:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-988094002-1400465950 will have desired state: Ready
2022-03-30 21:32:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-988094002-1400465950 will have desired state: Ready
2022-03-30 21:32:12 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-988094002-1400465950 will have desired state: Ready not ready, will try again in 1000 ms (179996ms till timeout)
2022-03-30 21:32:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1153022ms till timeout)
2022-03-30 21:32:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:12 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:12 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (471201ms till timeout)
2022-03-30 21:32:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:32:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:32:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1779878ms till timeout)
2022-03-30 21:32:13 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-988094002-1400465950 is in desired state: Ready
2022-03-30 21:32:13 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-2039401547-533835955 in namespace namespace-3
2022-03-30 21:32:13 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-2
2022-03-30 21:32:13 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-2039401547-533835955
2022-03-30 21:32:13 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-2039401547-533835955 will have desired state: Ready
2022-03-30 21:32:13 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-2039401547-533835955 will have desired state: Ready
2022-03-30 21:32:13 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-2039401547-533835955 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 21:32:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1152018ms till timeout)
2022-03-30 21:32:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (470103ms till timeout)
2022-03-30 21:32:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1778875ms till timeout)
2022-03-30 21:32:14 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-2039401547-533835955 is in desired state: Ready
2022-03-30 21:32:14 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-380e6586-kafka-clients in namespace namespace-2
2022-03-30 21:32:14 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-2
2022-03-30 21:32:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e1c9825e will have desired state: Ready not ready, will try again in 1000 ms (1151016ms till timeout)
2022-03-30 21:32:14 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-380e6586-kafka-clients
2022-03-30 21:32:14 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-380e6586-kafka-clients will be ready
2022-03-30 21:32:14 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-380e6586-kafka-clients will be ready
2022-03-30 21:32:14 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-380e6586-kafka-clients will be ready not ready, will try again in 1000 ms (479990ms till timeout)
2022-03-30 21:32:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:14 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:14 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (468995ms till timeout)
2022-03-30 21:32:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1777871ms till timeout)
2022-03-30 21:32:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-e1c9825e is in desired state: Ready
2022-03-30 21:32:15 [ForkJoinPool-3-worker-13] [32mINFO [m [CruiseControlApiST:48] ----> CRUISE CONTROL DEPLOYMENT STATE ENDPOINT <----
2022-03-30 21:32:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/state
2022-03-30 21:32:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-380e6586-kafka-clients will be ready not ready, will try again in 1000 ms (478987ms till timeout)
2022-03-30 21:32:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/state
2022-03-30 21:32:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XGET --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/state
2022-03-30 21:32:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:15 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:15 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (467914ms till timeout)
2022-03-30 21:32:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XGET --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/state
2022-03-30 21:32:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:15 [ForkJoinPool-3-worker-13] [32mINFO [m [CruiseControlApiST:58] Verifying that Cruise Control REST API is available
2022-03-30 21:32:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-30 21:32:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-30 21:32:15 [ForkJoinPool-3-worker-13] [32mINFO [m [CruiseControlApiST:66] ----> KAFKA REBALANCE <----
2022-03-30 21:32:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XGET --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:32:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1776868ms till timeout)
2022-03-30 21:32:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XGET --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:32:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:15 [ForkJoinPool-3-worker-13] [32mINFO [m [CruiseControlApiST:73] Waiting for CC will have for enough metrics to be recorded to make a proposal 
2022-03-30 21:32:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Wait for rebalance endpoint is ready
2022-03-30 21:32:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:32:16 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:32:16 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:16 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648675936098] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-30 21:32:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (599713ms till timeout)
2022-03-30 21:32:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-380e6586-kafka-clients will be ready not ready, will try again in 1000 ms (477984ms till timeout)
2022-03-30 21:32:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:16 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:16 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (466827ms till timeout)
2022-03-30 21:32:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [JobUtils:40] Job consumer-753605083 was deleted
2022-03-30 21:32:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 21:32:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:675] [bridge.HttpBridgeTlsST - After Each] - Clean up after test
2022-03-30 21:32:16 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 21:32:16 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testSendSimpleMessageTls
2022-03-30 21:32:16 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job producer-337125814 in namespace http-bridge-tls-st
2022-03-30 21:32:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:producer-337125814
2022-03-30 21:32:16 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job consumer-753605083 in namespace http-bridge-tls-st
2022-03-30 21:32:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:consumer-753605083
2022-03-30 21:32:16 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1922413090-69870416 in namespace http-bridge-tls-st
2022-03-30 21:32:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1922413090-69870416
2022-03-30 21:32:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1922413090-69870416 not ready, will try again in 10000 ms (179993ms till timeout)
2022-03-30 21:32:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1775863ms till timeout)
2022-03-30 21:32:17 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-380e6586-kafka-clients is ready
2022-03-30 21:32:17 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-30 21:32:17 [ForkJoinPool-3-worker-15] [32mINFO [m [ListenersST:370] Checking produced and consumed messages to pod:my-cluster-380e6586-kafka-clients-6bc4444cb4-zlx76
2022-03-30 21:32:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@40820ebe, which are set.
2022-03-30 21:32:17 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@659a0bf8, messages=[], arguments=[--topic, my-topic-988094002-1400465950, --bootstrap-server, my-cluster-380e6586-kafka-bootstrap.namespace-2.svc:9096, --max-messages, 100, USER=my_user_2039401547_533835955], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-380e6586-kafka-clients-6bc4444cb4-zlx76', podNamespace='namespace-2', bootstrapServer='my-cluster-380e6586-kafka-bootstrap.namespace-2.svc:9096', topicName='my-topic-988094002-1400465950', maxMessages=100, kafkaUsername='my-user-2039401547-533835955', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@40820ebe}
2022-03-30 21:32:17 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-380e6586-kafka-bootstrap.namespace-2.svc:9096:my-topic-988094002-1400465950 from pod my-cluster-380e6586-kafka-clients-6bc4444cb4-zlx76
2022-03-30 21:32:17 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-380e6586-kafka-clients-6bc4444cb4-zlx76 -n namespace-2 -- /opt/kafka/producer.sh --topic my-topic-988094002-1400465950 --bootstrap-server my-cluster-380e6586-kafka-bootstrap.namespace-2.svc:9096 --max-messages 100 USER=my_user_2039401547_533835955
2022-03-30 21:32:17 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-380e6586-kafka-clients-6bc4444cb4-zlx76 -n namespace-2 -- /opt/kafka/producer.sh --topic my-topic-988094002-1400465950 --bootstrap-server my-cluster-380e6586-kafka-bootstrap.namespace-2.svc:9096 --max-messages 100 USER=my_user_2039401547_533835955
2022-03-30 21:32:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:32:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:32:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (465739ms till timeout)
2022-03-30 21:32:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1774859ms till timeout)
2022-03-30 21:32:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:18 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:18 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (464659ms till timeout)
2022-03-30 21:32:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1773856ms till timeout)
2022-03-30 21:32:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1772852ms till timeout)
2022-03-30 21:32:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (463578ms till timeout)
2022-03-30 21:32:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1771848ms till timeout)
2022-03-30 21:32:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:20 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:20 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (462494ms till timeout)
2022-03-30 21:32:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:32:21 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-30 21:32:21 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-03-30 21:32:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4bda233, which are set.
2022-03-30 21:32:21 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@6388ee48, messages=[], arguments=[--topic, my-topic-988094002-1400465950, --bootstrap-server, my-cluster-380e6586-kafka-bootstrap.namespace-2.svc:9096, --max-messages, 100, USER=my_user_2039401547_533835955, --group-id, my-consumer-group-1165360355, --group-instance-id, instance1892068374], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-380e6586-kafka-clients-6bc4444cb4-zlx76', podNamespace='namespace-2', bootstrapServer='my-cluster-380e6586-kafka-bootstrap.namespace-2.svc:9096', topicName='my-topic-988094002-1400465950', maxMessages=100, kafkaUsername='my-user-2039401547-533835955', consumerGroupName='my-consumer-group-1165360355', consumerInstanceId='instance1892068374', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4bda233}
2022-03-30 21:32:21 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-380e6586-kafka-bootstrap.namespace-2.svc:9096:my-topic-988094002-1400465950 from pod my-cluster-380e6586-kafka-clients-6bc4444cb4-zlx76
2022-03-30 21:32:21 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-380e6586-kafka-clients-6bc4444cb4-zlx76 -n namespace-2 -- /opt/kafka/consumer.sh --topic my-topic-988094002-1400465950 --bootstrap-server my-cluster-380e6586-kafka-bootstrap.namespace-2.svc:9096 --max-messages 100 USER=my_user_2039401547_533835955 --group-id my-consumer-group-1165360355 --group-instance-id instance1892068374
2022-03-30 21:32:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-380e6586-kafka-clients-6bc4444cb4-zlx76 -n namespace-2 -- /opt/kafka/consumer.sh --topic my-topic-988094002-1400465950 --bootstrap-server my-cluster-380e6586-kafka-bootstrap.namespace-2.svc:9096 --max-messages 100 USER=my_user_2039401547_533835955 --group-id my-consumer-group-1165360355 --group-instance-id instance1892068374
2022-03-30 21:32:21 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:32:21 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:21 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648675941408] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-30 21:32:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (594412ms till timeout)
2022-03-30 21:32:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1770845ms till timeout)
2022-03-30 21:32:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (461413ms till timeout)
2022-03-30 21:32:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:32:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:32:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1769841ms till timeout)
2022-03-30 21:32:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (460307ms till timeout)
2022-03-30 21:32:23 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1768837ms till timeout)
2022-03-30 21:32:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (459227ms till timeout)
2022-03-30 21:32:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1767834ms till timeout)
2022-03-30 21:32:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (458145ms till timeout)
2022-03-30 21:32:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1766830ms till timeout)
2022-03-30 21:32:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:26 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:26 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (457075ms till timeout)
2022-03-30 21:32:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:32:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:32:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648675946687] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-30 21:32:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (589134ms till timeout)
2022-03-30 21:32:26 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 21:32:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:267] testSendSimpleMessageTls - Notifies waiting test cases:[testCruiseControlBasicAPIRequests, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testUpdateUser, testSendSimpleMessageTls, testAutoRenewAllCaCertsTriggeredByAnno] to and randomly select one to start execution
2022-03-30 21:32:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:93] [bridge.HttpBridgeTlsST] - Removing parallel test: testSendSimpleMessageTls
2022-03-30 21:32:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:97] [bridge.HttpBridgeTlsST] - Parallel test count: 4
2022-03-30 21:32:26 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testSendSimpleMessageTls-FINISHED
2022-03-30 21:32:26 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 21:32:26 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 21:32:26 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testReceiveSimpleMessageTls-STARTED
2022-03-30 21:32:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 21:32:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:659] [bridge.HttpBridgeTlsST - Before Each] - Setup test case environment
2022-03-30 21:32:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:77] [bridge.HttpBridgeTlsST] - Adding parallel test: testReceiveSimpleMessageTls
2022-03-30 21:32:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:81] [bridge.HttpBridgeTlsST] - Parallel test count: 5
2022-03-30 21:32:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:230] testReceiveSimpleMessageTls test now can proceed its execution
2022-03-30 21:32:26 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 21:32:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349, testSendSimpleMessageTls=my-cluster-b70e5342, testSendMessagesTlsScramSha=my-cluster-380e6586, testUpdateUser=my-cluster-9b2bd187, testReceiveSimpleMessageTls=my-cluster-e8a518cd, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09}
2022-03-30 21:32:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testAutoRenewAllCaCertsTriggeredByAnno=my-user-1590558165-46977776, testSendSimpleMessageTls=my-user-602433797-900694548, testSendMessagesTlsScramSha=my-user-2039401547-533835955, testUpdateUser=my-user-595127408-1604948726, testReceiveSimpleMessageTls=my-user-761922164-2019863647, testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659}
2022-03-30 21:32:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1210910585-1669754462, testSendSimpleMessageTls=my-topic-1299629356-597168683, testSendMessagesTlsScramSha=my-topic-988094002-1400465950, testUpdateUser=my-topic-1875736194-1340884809, testReceiveSimpleMessageTls=my-topic-1007790874-1032272408, testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393}
2022-03-30 21:32:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349-kafka-clients, testSendSimpleMessageTls=my-cluster-b70e5342-kafka-clients, testSendMessagesTlsScramSha=my-cluster-380e6586-kafka-clients, testUpdateUser=my-cluster-9b2bd187-kafka-clients, testReceiveSimpleMessageTls=my-cluster-e8a518cd-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients}
2022-03-30 21:32:26 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1176175932-1191555846 in namespace http-bridge-tls-st
2022-03-30 21:32:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1176175932-1191555846
2022-03-30 21:32:26 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1176175932-1191555846 will have desired state: Ready
2022-03-30 21:32:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1176175932-1191555846 will have desired state: Ready
2022-03-30 21:32:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1176175932-1191555846 will have desired state: Ready not ready, will try again in 1000 ms (179991ms till timeout)
2022-03-30 21:32:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1765826ms till timeout)
2022-03-30 21:32:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:32:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:32:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:27 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:27 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (455983ms till timeout)
2022-03-30 21:32:27 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1176175932-1191555846 is in desired state: Ready
2022-03-30 21:32:27 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job consumer-792932762 in namespace http-bridge-tls-st
2022-03-30 21:32:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:consumer-792932762
2022-03-30 21:32:27 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: consumer-792932762 will be in active state
2022-03-30 21:32:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for job active
2022-03-30 21:32:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job active not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 21:32:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1764822ms till timeout)
2022-03-30 21:32:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:28 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:28 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (454893ms till timeout)
2022-03-30 21:32:28 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 21:32:28 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-03-30 21:32:28 [ForkJoinPool-3-worker-15] [32mINFO [m [ListenersST:377] Checking if generated password has 25 characters
2022-03-30 21:32:28 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 21:32:28 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:675] [kafka.listeners.ListenersST - After Each] - Clean up after test
2022-03-30 21:32:28 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 21:32:28 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testSendMessagesTlsScramSha
2022-03-30 21:32:28 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-2039401547-533835955 in namespace namespace-2
2022-03-30 21:32:28 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-2039401547-533835955
2022-03-30 21:32:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-2039401547-533835955 not ready, will try again in 10000 ms (179982ms till timeout)
2022-03-30 21:32:28 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-30 21:32:28 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job producer-283332790 in namespace http-bridge-tls-st
2022-03-30 21:32:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:producer-283332790
2022-03-30 21:32:28 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: producer-283332790 will be in active state
2022-03-30 21:32:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for job active
2022-03-30 21:32:28 [ForkJoinPool-3-worker-7] [32mINFO [m [ClientUtils:61] Waiting till producer producer-283332790 and consumer consumer-792932762 finish
2022-03-30 21:32:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for clients finished
2022-03-30 21:32:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (219998ms till timeout)
2022-03-30 21:32:28 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1763819ms till timeout)
2022-03-30 21:32:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:29 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:29 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (453793ms till timeout)
2022-03-30 21:32:29 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (218993ms till timeout)
2022-03-30 21:32:29 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1762812ms till timeout)
2022-03-30 21:32:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (452660ms till timeout)
2022-03-30 21:32:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (217989ms till timeout)
2022-03-30 21:32:30 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1761808ms till timeout)
2022-03-30 21:32:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:32:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:31 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (216986ms till timeout)
2022-03-30 21:32:31 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1760803ms till timeout)
2022-03-30 21:32:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (451571ms till timeout)
2022-03-30 21:32:31 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:32:31 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:31 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648675951968] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-30 21:32:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (583844ms till timeout)
2022-03-30 21:32:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:32:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:32:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (215983ms till timeout)
2022-03-30 21:32:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1759800ms till timeout)
2022-03-30 21:32:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (450499ms till timeout)
2022-03-30 21:32:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (214975ms till timeout)
2022-03-30 21:32:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1758795ms till timeout)
2022-03-30 21:32:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (449420ms till timeout)
2022-03-30 21:32:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (213969ms till timeout)
2022-03-30 21:32:34 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1757792ms till timeout)
2022-03-30 21:32:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (448340ms till timeout)
2022-03-30 21:32:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [JobUtils:37] Waiting for ReplicaSet of Deployment producer-283332790 deletion
2022-03-30 21:32:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for ReplicaSet producer-283332790 to be deleted
2022-03-30 21:32:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] ReplicaSet producer-283332790 to be deleted not ready, will try again in 5000 ms (179997ms till timeout)
2022-03-30 21:32:35 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1756790ms till timeout)
2022-03-30 21:32:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:36 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:36 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (447268ms till timeout)
2022-03-30 21:32:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1755786ms till timeout)
2022-03-30 21:32:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:32:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:37 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:32:37 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:37 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648675957233] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-30 21:32:37 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (578581ms till timeout)
2022-03-30 21:32:37 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:37 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (446183ms till timeout)
2022-03-30 21:32:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:32:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:32:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1754783ms till timeout)
2022-03-30 21:32:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:38 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:38 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (445101ms till timeout)
2022-03-30 21:32:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-380e6586-kafka-clients in namespace namespace-2
2022-03-30 21:32:38 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-380e6586-kafka-clients
2022-03-30 21:32:38 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-380e6586-kafka-clients not ready, will try again in 10000 ms (479951ms till timeout)
2022-03-30 21:32:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1753779ms till timeout)
2022-03-30 21:32:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:39 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:39 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (444006ms till timeout)
2022-03-30 21:32:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1752775ms till timeout)
2022-03-30 21:32:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (442921ms till timeout)
2022-03-30 21:32:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [JobUtils:40] Job producer-283332790 was deleted
2022-03-30 21:32:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [JobUtils:37] Waiting for ReplicaSet of Deployment consumer-792932762 deletion
2022-03-30 21:32:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for ReplicaSet consumer-792932762 to be deleted
2022-03-30 21:32:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1751753ms till timeout)
2022-03-30 21:32:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [JobUtils:40] Job consumer-792932762 was deleted
2022-03-30 21:32:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 21:32:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:675] [bridge.HttpBridgeTlsST - After Each] - Clean up after test
2022-03-30 21:32:40 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 21:32:40 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testReceiveSimpleMessageTls
2022-03-30 21:32:40 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job consumer-792932762 in namespace http-bridge-tls-st
2022-03-30 21:32:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:consumer-792932762
2022-03-30 21:32:40 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job producer-283332790 in namespace http-bridge-tls-st
2022-03-30 21:32:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:producer-283332790
2022-03-30 21:32:40 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1176175932-1191555846 in namespace http-bridge-tls-st
2022-03-30 21:32:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1176175932-1191555846
2022-03-30 21:32:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1176175932-1191555846 not ready, will try again in 10000 ms (179994ms till timeout)
2022-03-30 21:32:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (441841ms till timeout)
2022-03-30 21:32:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1750749ms till timeout)
2022-03-30 21:32:42 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:32:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:32:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:32:42 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:32:42 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:42 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648675962506] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-30 21:32:42 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (573316ms till timeout)
2022-03-30 21:32:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (440765ms till timeout)
2022-03-30 21:32:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1749745ms till timeout)
2022-03-30 21:32:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:43 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:43 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (439686ms till timeout)
2022-03-30 21:32:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1748742ms till timeout)
2022-03-30 21:32:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:44 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:44 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (438605ms till timeout)
2022-03-30 21:32:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1747738ms till timeout)
2022-03-30 21:32:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1746734ms till timeout)
2022-03-30 21:32:45 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:45 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (437530ms till timeout)
2022-03-30 21:32:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1745731ms till timeout)
2022-03-30 21:32:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:46 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-3 get Namespace namespace-1 -o yaml
2022-03-30 21:32:46 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 21:32:46 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 21:32:46 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-1" not found
2022-03-30 21:32:46 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 21:32:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[namespace-2], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-3], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-0], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-30 21:32:46 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:267] testCruiseControlBasicAPIRequestsWithSecurityDisabled - Notifies waiting test cases:[testCruiseControlBasicAPIRequests, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testUpdateUser, testSendSimpleMessageTls, testAutoRenewAllCaCertsTriggeredByAnno, testReceiveSimpleMessageTls] to and randomly select one to start execution
2022-03-30 21:32:46 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:93] [cruisecontrol.CruiseControlApiST] - Removing parallel test: testCruiseControlBasicAPIRequestsWithSecurityDisabled
2022-03-30 21:32:46 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:97] [cruisecontrol.CruiseControlApiST] - Parallel test count: 4
2022-03-30 21:32:46 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlApiST.testCruiseControlBasicAPIRequestsWithSecurityDisabled-FINISHED
2022-03-30 21:32:46 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 21:32:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:32:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:32:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:32:47 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:32:47 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:47 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648675967755] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-30 21:32:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (568061ms till timeout)
2022-03-30 21:32:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1744728ms till timeout)
2022-03-30 21:32:48 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-380e6586-kafka-clients not ready, will try again in 10000 ms (469939ms till timeout)
2022-03-30 21:32:48 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1743725ms till timeout)
2022-03-30 21:32:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1742721ms till timeout)
2022-03-30 21:32:50 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 21:32:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:267] testReceiveSimpleMessageTls - Notifies waiting test cases:[testCruiseControlBasicAPIRequests, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testUpdateUser, testSendSimpleMessageTls, testAutoRenewAllCaCertsTriggeredByAnno, testReceiveSimpleMessageTls] to and randomly select one to start execution
2022-03-30 21:32:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:93] [bridge.HttpBridgeTlsST] - Removing parallel test: testReceiveSimpleMessageTls
2022-03-30 21:32:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:97] [bridge.HttpBridgeTlsST] - Parallel test count: 3
2022-03-30 21:32:50 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testReceiveSimpleMessageTls-FINISHED
2022-03-30 21:32:50 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 21:32:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 21:32:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:690] [bridge.HttpBridgeTlsST - After All] - Clean up after test suite
2022-03-30 21:32:50 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 21:32:50 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for HttpBridgeTlsST
2022-03-30 21:32:50 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment http-bridge-tls-st-kafka-clients in namespace http-bridge-tls-st
2022-03-30 21:32:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients
2022-03-30 21:32:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1741717ms till timeout)
2022-03-30 21:32:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (479991ms till timeout)
2022-03-30 21:32:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1740713ms till timeout)
2022-03-30 21:32:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:32:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:32:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:32:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1739710ms till timeout)
2022-03-30 21:32:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:32:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648675973012] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-30 21:32:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (562808ms till timeout)
2022-03-30 21:32:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1738706ms till timeout)
2022-03-30 21:32:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1737702ms till timeout)
2022-03-30 21:32:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1736698ms till timeout)
2022-03-30 21:32:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1735695ms till timeout)
2022-03-30 21:32:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:32:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:32:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1734691ms till timeout)
2022-03-30 21:32:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:32:58 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:32:58 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:32:58 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648675978257] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:32:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (557566ms till timeout)
2022-03-30 21:32:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-380e6586-kafka-clients not ready, will try again in 10000 ms (459930ms till timeout)
2022-03-30 21:32:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1733688ms till timeout)
2022-03-30 21:32:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1732684ms till timeout)
2022-03-30 21:33:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (469981ms till timeout)
2022-03-30 21:33:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1731658ms till timeout)
2022-03-30 21:33:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1730654ms till timeout)
2022-03-30 21:33:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:33:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:33:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1729650ms till timeout)
2022-03-30 21:33:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:33:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:33:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:33:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648675983513] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:33:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (552308ms till timeout)
2022-03-30 21:33:03 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 21:33:03 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesCustomListenerTlsScramSha-STARTED
2022-03-30 21:33:03 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 21:33:03 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:659] [kafka.listeners.ListenersST - Before Each] - Setup test case environment
2022-03-30 21:33:03 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:77] [kafka.listeners.ListenersST] - Adding parallel test: testSendMessagesCustomListenerTlsScramSha
2022-03-30 21:33:03 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:81] [kafka.listeners.ListenersST] - Parallel test count: 4
2022-03-30 21:33:03 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:230] testSendMessagesCustomListenerTlsScramSha test now can proceed its execution
2022-03-30 21:33:03 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 21:33:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349, testSendSimpleMessageTls=my-cluster-b70e5342, testSendMessagesTlsScramSha=my-cluster-380e6586, testUpdateUser=my-cluster-9b2bd187, testReceiveSimpleMessageTls=my-cluster-e8a518cd, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09}
2022-03-30 21:33:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testAutoRenewAllCaCertsTriggeredByAnno=my-user-1590558165-46977776, testSendSimpleMessageTls=my-user-602433797-900694548, testSendMessagesTlsScramSha=my-user-2039401547-533835955, testUpdateUser=my-user-595127408-1604948726, testReceiveSimpleMessageTls=my-user-761922164-2019863647, testSendMessagesCustomListenerTlsScramSha=my-user-445988529-51741427, testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659}
2022-03-30 21:33:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1210910585-1669754462, testSendSimpleMessageTls=my-topic-1299629356-597168683, testSendMessagesTlsScramSha=my-topic-988094002-1400465950, testUpdateUser=my-topic-1875736194-1340884809, testReceiveSimpleMessageTls=my-topic-1007790874-1032272408, testSendMessagesCustomListenerTlsScramSha=my-topic-1039904883-1044305687, testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393}
2022-03-30 21:33:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349-kafka-clients, testSendSimpleMessageTls=my-cluster-b70e5342-kafka-clients, testSendMessagesTlsScramSha=my-cluster-380e6586-kafka-clients, testUpdateUser=my-cluster-9b2bd187-kafka-clients, testReceiveSimpleMessageTls=my-cluster-e8a518cd-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients}
2022-03-30 21:33:03 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-4 for test case:testSendMessagesCustomListenerTlsScramSha
2022-03-30 21:33:03 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-4
2022-03-30 21:33:03 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-4
2022-03-30 21:33:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-4 -o json
2022-03-30 21:33:03 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-4 -o json
2022-03-30 21:33:03 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:33:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T21:33:03Z",
        "name": "namespace-4",
        "resourceVersion": "99577",
        "selfLink": "/api/v1/namespaces/namespace-4",
        "uid": "2d7a5868-0370-46fc-a14f-58a8867e2e60"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 21:33:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[namespace-4], io.strimzi.test.logs.CollectorElement@6e85bf86=[namespace-2], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-3], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-0], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-30 21:33:03 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-4
2022-03-30 21:33:03 [ForkJoinPool-3-worker-1] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-4, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 21:33:03 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-4
2022-03-30 21:33:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-98f0c2b1 in namespace namespace-4
2022-03-30 21:33:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-03-30 21:33:03 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-98f0c2b1
2022-03-30 21:33:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-98f0c2b1 will have desired state: Ready
2022-03-30 21:33:03 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-98f0c2b1 will have desired state: Ready
2022-03-30 21:33:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (839997ms till timeout)
2022-03-30 21:33:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1728646ms till timeout)
2022-03-30 21:33:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (838994ms till timeout)
2022-03-30 21:33:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1727642ms till timeout)
2022-03-30 21:33:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (837990ms till timeout)
2022-03-30 21:33:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1726639ms till timeout)
2022-03-30 21:33:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (836984ms till timeout)
2022-03-30 21:33:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1725635ms till timeout)
2022-03-30 21:33:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:33:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:33:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (835977ms till timeout)
2022-03-30 21:33:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1724631ms till timeout)
2022-03-30 21:33:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:33:08 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (834974ms till timeout)
2022-03-30 21:33:08 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:33:08 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:33:08 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648675988801] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:33:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (547012ms till timeout)
2022-03-30 21:33:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-380e6586-kafka-clients not ready, will try again in 10000 ms (449910ms till timeout)
2022-03-30 21:33:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1723625ms till timeout)
2022-03-30 21:33:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (833969ms till timeout)
2022-03-30 21:33:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1722621ms till timeout)
2022-03-30 21:33:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (832965ms till timeout)
2022-03-30 21:33:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (459965ms till timeout)
2022-03-30 21:33:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1721614ms till timeout)
2022-03-30 21:33:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (831961ms till timeout)
2022-03-30 21:33:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1720611ms till timeout)
2022-03-30 21:33:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:33:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:33:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (830958ms till timeout)
2022-03-30 21:33:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1719607ms till timeout)
2022-03-30 21:33:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (829954ms till timeout)
2022-03-30 21:33:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:33:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1718603ms till timeout)
2022-03-30 21:33:14 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:33:14 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:33:14 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648675994065] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:33:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (541755ms till timeout)
2022-03-30 21:33:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (828951ms till timeout)
2022-03-30 21:33:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1717600ms till timeout)
2022-03-30 21:33:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (827947ms till timeout)
2022-03-30 21:33:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1716597ms till timeout)
2022-03-30 21:33:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (826944ms till timeout)
2022-03-30 21:33:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1715593ms till timeout)
2022-03-30 21:33:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:33:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:33:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (825941ms till timeout)
2022-03-30 21:33:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1714590ms till timeout)
2022-03-30 21:33:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (824937ms till timeout)
2022-03-30 21:33:18 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-380e6586-kafka-clients not ready, will try again in 10000 ms (439901ms till timeout)
2022-03-30 21:33:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1713586ms till timeout)
2022-03-30 21:33:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:33:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:33:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:33:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648675999344] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:33:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (536474ms till timeout)
2022-03-30 21:33:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (823933ms till timeout)
2022-03-30 21:33:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1712582ms till timeout)
2022-03-30 21:33:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (822930ms till timeout)
2022-03-30 21:33:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (449956ms till timeout)
2022-03-30 21:33:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1711576ms till timeout)
2022-03-30 21:33:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (821926ms till timeout)
2022-03-30 21:33:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1710573ms till timeout)
2022-03-30 21:33:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:33:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:33:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (820923ms till timeout)
2022-03-30 21:33:23 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1709569ms till timeout)
2022-03-30 21:33:23 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (819919ms till timeout)
2022-03-30 21:33:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1708565ms till timeout)
2022-03-30 21:33:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:33:24 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:33:24 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:33:24 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648676004642] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:33:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (531177ms till timeout)
2022-03-30 21:33:24 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (818914ms till timeout)
2022-03-30 21:33:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1707560ms till timeout)
2022-03-30 21:33:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (817910ms till timeout)
2022-03-30 21:33:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1706557ms till timeout)
2022-03-30 21:33:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (816906ms till timeout)
2022-03-30 21:33:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1705553ms till timeout)
2022-03-30 21:33:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:33:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:33:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (815902ms till timeout)
2022-03-30 21:33:28 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1704548ms till timeout)
2022-03-30 21:33:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (814898ms till timeout)
2022-03-30 21:33:28 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-988094002-1400465950 in namespace namespace-2
2022-03-30 21:33:28 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-988094002-1400465950
2022-03-30 21:33:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-988094002-1400465950 not ready, will try again in 10000 ms (179988ms till timeout)
2022-03-30 21:33:29 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1703545ms till timeout)
2022-03-30 21:33:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:33:29 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (813891ms till timeout)
2022-03-30 21:33:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:33:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:33:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648676009993] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:33:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (525826ms till timeout)
2022-03-30 21:33:30 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1702540ms till timeout)
2022-03-30 21:33:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (812886ms till timeout)
2022-03-30 21:33:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (439943ms till timeout)
2022-03-30 21:33:31 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1701527ms till timeout)
2022-03-30 21:33:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (811881ms till timeout)
2022-03-30 21:33:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1700523ms till timeout)
2022-03-30 21:33:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:33:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:33:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (810868ms till timeout)
2022-03-30 21:33:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1699519ms till timeout)
2022-03-30 21:33:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (809864ms till timeout)
2022-03-30 21:33:34 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1698515ms till timeout)
2022-03-30 21:33:34 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (808850ms till timeout)
2022-03-30 21:33:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:33:35 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1697511ms till timeout)
2022-03-30 21:33:35 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:33:35 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:33:35 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648676015274] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:33:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (520544ms till timeout)
2022-03-30 21:33:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (807845ms till timeout)
2022-03-30 21:33:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1696502ms till timeout)
2022-03-30 21:33:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (806841ms till timeout)
2022-03-30 21:33:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1695498ms till timeout)
2022-03-30 21:33:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:33:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:33:37 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (805834ms till timeout)
2022-03-30 21:33:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1694493ms till timeout)
2022-03-30 21:33:38 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (804830ms till timeout)
2022-03-30 21:33:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-380e6586 in namespace namespace-2
2022-03-30 21:33:38 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-380e6586
2022-03-30 21:33:38 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-380e6586 not ready, will try again in 10000 ms (839955ms till timeout)
2022-03-30 21:33:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1693488ms till timeout)
2022-03-30 21:33:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (803815ms till timeout)
2022-03-30 21:33:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1692483ms till timeout)
2022-03-30 21:33:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:33:40 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:33:40 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:33:40 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648676020640] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:33:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (515166ms till timeout)
2022-03-30 21:33:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (802809ms till timeout)
2022-03-30 21:33:40 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-30 21:33:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaBridge:http-bridge-tls-cluster-name
2022-03-30 21:33:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaBridge:http-bridge-tls-cluster-name not ready, will try again in 10000 ms (479987ms till timeout)
2022-03-30 21:33:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1691479ms till timeout)
2022-03-30 21:33:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (801804ms till timeout)
2022-03-30 21:33:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1690474ms till timeout)
2022-03-30 21:33:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:33:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:33:42 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (800796ms till timeout)
2022-03-30 21:33:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1689468ms till timeout)
2022-03-30 21:33:43 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (799791ms till timeout)
2022-03-30 21:33:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1688457ms till timeout)
2022-03-30 21:33:44 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (798788ms till timeout)
2022-03-30 21:33:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1687454ms till timeout)
2022-03-30 21:33:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:33:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (797784ms till timeout)
2022-03-30 21:33:45 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:33:45 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:33:45 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648676025906] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:33:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (509915ms till timeout)
2022-03-30 21:33:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1686451ms till timeout)
2022-03-30 21:33:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (796781ms till timeout)
2022-03-30 21:33:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1685447ms till timeout)
2022-03-30 21:33:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:33:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:33:47 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (795777ms till timeout)
2022-03-30 21:33:48 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1684444ms till timeout)
2022-03-30 21:33:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (794773ms till timeout)
2022-03-30 21:33:48 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 21:33:48 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-2 for test case:testSendMessagesTlsScramSha
2022-03-30 21:33:48 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-2 removal
2022-03-30 21:33:48 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:33:49 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:33:49 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:33:49 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (479907ms till timeout)
2022-03-30 21:33:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1683440ms till timeout)
2022-03-30 21:33:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (793770ms till timeout)
2022-03-30 21:33:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:33:50 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:33:50 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:33:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (478830ms till timeout)
2022-03-30 21:33:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1682436ms till timeout)
2022-03-30 21:33:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (792766ms till timeout)
2022-03-30 21:33:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:33:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1638214753-683095272 in namespace http-bridge-tls-st
2022-03-30 21:33:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1638214753-683095272
2022-03-30 21:33:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1638214753-683095272 not ready, will try again in 10000 ms (179986ms till timeout)
2022-03-30 21:33:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:33:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:33:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:33:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (477724ms till timeout)
2022-03-30 21:33:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1681433ms till timeout)
2022-03-30 21:33:51 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:33:51 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:33:51 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648676031236] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:33:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (504559ms till timeout)
2022-03-30 21:33:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (791763ms till timeout)
2022-03-30 21:33:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:33:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1680429ms till timeout)
2022-03-30 21:33:52 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:33:52 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:33:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (476638ms till timeout)
2022-03-30 21:33:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:33:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:33:52 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (790759ms till timeout)
2022-03-30 21:33:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1679425ms till timeout)
2022-03-30 21:33:53 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:33:53 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:33:53 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:33:53 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (475562ms till timeout)
2022-03-30 21:33:53 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (789754ms till timeout)
2022-03-30 21:33:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1678420ms till timeout)
2022-03-30 21:33:54 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:33:54 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:33:54 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:33:54 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (474475ms till timeout)
2022-03-30 21:33:54 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (788750ms till timeout)
2022-03-30 21:33:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1677416ms till timeout)
2022-03-30 21:33:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:33:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:33:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:33:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (473400ms till timeout)
2022-03-30 21:33:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (787745ms till timeout)
2022-03-30 21:33:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1676413ms till timeout)
2022-03-30 21:33:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:33:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:33:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:33:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:33:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (472303ms till timeout)
2022-03-30 21:33:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:33:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:33:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648676036598] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:33:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (499205ms till timeout)
2022-03-30 21:33:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (786742ms till timeout)
2022-03-30 21:33:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1675409ms till timeout)
2022-03-30 21:33:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:33:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:33:57 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:33:57 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:33:57 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:33:57 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (471229ms till timeout)
2022-03-30 21:33:57 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (785738ms till timeout)
2022-03-30 21:33:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1674406ms till timeout)
2022-03-30 21:33:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:33:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:33:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:33:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (470147ms till timeout)
2022-03-30 21:33:58 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (784735ms till timeout)
2022-03-30 21:33:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1673402ms till timeout)
2022-03-30 21:33:59 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:33:59 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:33:59 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:33:59 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (469063ms till timeout)
2022-03-30 21:33:59 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (783731ms till timeout)
2022-03-30 21:34:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1672398ms till timeout)
2022-03-30 21:34:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:34:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (782728ms till timeout)
2022-03-30 21:34:00 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:34:00 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (467952ms till timeout)
2022-03-30 21:34:01 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-30 21:34:01 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:http-bridge-tls-cluster-name
2022-03-30 21:34:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:http-bridge-tls-cluster-name not ready, will try again in 10000 ms (839996ms till timeout)
2022-03-30 21:34:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1671394ms till timeout)
2022-03-30 21:34:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:34:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (781722ms till timeout)
2022-03-30 21:34:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:34:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648676041901] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:34:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (493879ms till timeout)
2022-03-30 21:34:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:34:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:34:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:02 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (466864ms till timeout)
2022-03-30 21:34:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1670391ms till timeout)
2022-03-30 21:34:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:34:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:34:02 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (780719ms till timeout)
2022-03-30 21:34:03 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:34:03 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:34:03 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:03 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (465790ms till timeout)
2022-03-30 21:34:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1669387ms till timeout)
2022-03-30 21:34:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (779714ms till timeout)
2022-03-30 21:34:04 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:34:04 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:34:04 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:04 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (464702ms till timeout)
2022-03-30 21:34:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1668383ms till timeout)
2022-03-30 21:34:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (778707ms till timeout)
2022-03-30 21:34:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:34:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1667378ms till timeout)
2022-03-30 21:34:05 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:34:05 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (463608ms till timeout)
2022-03-30 21:34:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (777694ms till timeout)
2022-03-30 21:34:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1666374ms till timeout)
2022-03-30 21:34:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:34:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:34:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (462500ms till timeout)
2022-03-30 21:34:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:34:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (776691ms till timeout)
2022-03-30 21:34:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1665371ms till timeout)
2022-03-30 21:34:07 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:34:07 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:07 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648676047294] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:34:07 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (488520ms till timeout)
2022-03-30 21:34:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:34:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:34:07 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:34:07 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:34:07 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:07 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (461380ms till timeout)
2022-03-30 21:34:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (775688ms till timeout)
2022-03-30 21:34:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1664364ms till timeout)
2022-03-30 21:34:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:34:08 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:34:08 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (460283ms till timeout)
2022-03-30 21:34:08 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (774683ms till timeout)
2022-03-30 21:34:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1663360ms till timeout)
2022-03-30 21:34:09 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:34:09 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:34:09 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:09 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (459191ms till timeout)
2022-03-30 21:34:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (773680ms till timeout)
2022-03-30 21:34:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1662356ms till timeout)
2022-03-30 21:34:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:34:10 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:34:10 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (458117ms till timeout)
2022-03-30 21:34:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (772676ms till timeout)
2022-03-30 21:34:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 21:34:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Namespace http-bridge-tls-st removal
2022-03-30 21:34:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace http-bridge-tls-st -o yaml
2022-03-30 21:34:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace http-bridge-tls-st -o yaml
2022-03-30 21:34:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (479916ms till timeout)
2022-03-30 21:34:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1661353ms till timeout)
2022-03-30 21:34:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:34:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:34:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (457044ms till timeout)
2022-03-30 21:34:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (771672ms till timeout)
2022-03-30 21:34:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace http-bridge-tls-st -o yaml
2022-03-30 21:34:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace http-bridge-tls-st -o yaml
2022-03-30 21:34:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (478842ms till timeout)
2022-03-30 21:34:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1660349ms till timeout)
2022-03-30 21:34:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:34:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:34:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:34:12 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:34:12 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:12 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648676052539] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:34:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (483278ms till timeout)
2022-03-30 21:34:12 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:34:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:34:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:12 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (455951ms till timeout)
2022-03-30 21:34:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (770668ms till timeout)
2022-03-30 21:34:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace http-bridge-tls-st -o yaml
2022-03-30 21:34:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1659343ms till timeout)
2022-03-30 21:34:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace http-bridge-tls-st -o yaml
2022-03-30 21:34:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (477755ms till timeout)
2022-03-30 21:34:13 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:34:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (769664ms till timeout)
2022-03-30 21:34:14 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:34:14 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:14 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (454872ms till timeout)
2022-03-30 21:34:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1658340ms till timeout)
2022-03-30 21:34:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace http-bridge-tls-st -o yaml
2022-03-30 21:34:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace http-bridge-tls-st -o yaml
2022-03-30 21:34:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (476623ms till timeout)
2022-03-30 21:34:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (768661ms till timeout)
2022-03-30 21:34:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:34:15 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:34:15 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (453798ms till timeout)
2022-03-30 21:34:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1657336ms till timeout)
2022-03-30 21:34:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace http-bridge-tls-st -o yaml
2022-03-30 21:34:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace http-bridge-tls-st -o yaml
2022-03-30 21:34:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (475548ms till timeout)
2022-03-30 21:34:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (767657ms till timeout)
2022-03-30 21:34:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:34:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 21:34:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 21:34:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 21:34:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-2" not found
2022-03-30 21:34:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 21:34:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[namespace-4], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-3], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-0], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-30 21:34:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:267] testSendMessagesTlsScramSha - Notifies waiting test cases:[testCruiseControlBasicAPIRequests, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testUpdateUser, testSendSimpleMessageTls, testAutoRenewAllCaCertsTriggeredByAnno, testReceiveSimpleMessageTls, testSendMessagesCustomListenerTlsScramSha] to and randomly select one to start execution
2022-03-30 21:34:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:93] [kafka.listeners.ListenersST] - Removing parallel test: testSendMessagesTlsScramSha
2022-03-30 21:34:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:97] [kafka.listeners.ListenersST] - Parallel test count: 3
2022-03-30 21:34:16 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesTlsScramSha-FINISHED
2022-03-30 21:34:16 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 21:34:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1656333ms till timeout)
2022-03-30 21:34:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace http-bridge-tls-st -o yaml
2022-03-30 21:34:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace http-bridge-tls-st -o yaml
2022-03-30 21:34:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (474466ms till timeout)
2022-03-30 21:34:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (766654ms till timeout)
2022-03-30 21:34:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1655329ms till timeout)
2022-03-30 21:34:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:34:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:34:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:34:17 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace http-bridge-tls-st -o yaml
2022-03-30 21:34:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace http-bridge-tls-st -o yaml
2022-03-30 21:34:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:17 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (473390ms till timeout)
2022-03-30 21:34:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:34:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648676057805] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:34:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (478014ms till timeout)
2022-03-30 21:34:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (765650ms till timeout)
2022-03-30 21:34:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1654326ms till timeout)
2022-03-30 21:34:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace http-bridge-tls-st -o yaml
2022-03-30 21:34:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace http-bridge-tls-st -o yaml
2022-03-30 21:34:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (472305ms till timeout)
2022-03-30 21:34:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (764647ms till timeout)
2022-03-30 21:34:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1653322ms till timeout)
2022-03-30 21:34:19 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace http-bridge-tls-st -o yaml
2022-03-30 21:34:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace http-bridge-tls-st -o yaml
2022-03-30 21:34:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:19 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (471224ms till timeout)
2022-03-30 21:34:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (763643ms till timeout)
2022-03-30 21:34:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1652319ms till timeout)
2022-03-30 21:34:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace http-bridge-tls-st -o yaml
2022-03-30 21:34:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace http-bridge-tls-st -o yaml
2022-03-30 21:34:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (470140ms till timeout)
2022-03-30 21:34:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (762640ms till timeout)
2022-03-30 21:34:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1651315ms till timeout)
2022-03-30 21:34:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace http-bridge-tls-st -o yaml
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-4 get Namespace http-bridge-tls-st -o yaml
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "http-bridge-tls-st" not found
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[namespace-4], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-3], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-0], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:254] HttpBridgeTlsST - Notifies waiting test suites:[CruiseControlST, HttpBridgeTlsST, UserST, CruiseControlApiST, ListenersST, SecurityST] to and randomly select one to start execution
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:85] [bridge.HttpBridgeTlsST] - Removing parallel suite: HttpBridgeTlsST
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:89] [bridge.HttpBridgeTlsST] - Parallel suites count: 4
[[1;34mINFO[m] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 345.715 s - in io.strimzi.systemtest.bridge.HttpBridgeTlsST
[[1;34mINFO[m] Running io.strimzi.systemtest.rollingupdate.RollingUpdateST
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:667] [rollingupdate.RollingUpdateST - Before All] - Setup test suite environment
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:69] [rollingupdate.RollingUpdateST] - Adding parallel suite: RollingUpdateST
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:73] [rollingupdate.RollingUpdateST] - Parallel suites count: 5
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:184] RollingUpdateST suite now can proceed its execution
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestSuiteNamespaceManager:129] Test suite `RollingUpdateST` creates these additional namespaces:[rolling-update-st]
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: rolling-update-st
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Namespace rolling-update-st
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace rolling-update-st -o json
2022-03-30 21:34:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (761637ms till timeout)
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace rolling-update-st -o json
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T21:34:22Z",
        "name": "rolling-update-st",
        "resourceVersion": "100500",
        "selfLink": "/api/v1/namespaces/rolling-update-st",
        "uid": "325beb67-8b65-4705-9dd4-87b029f14e2d"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[namespace-4], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@dce7372d=[rolling-update-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-3], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-0], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: rolling-update-st
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=rolling-update-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: rolling-update-st
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testKafkaAndZookeeperScaleUpScaleDown-STARTED
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:659] [rollingupdate.RollingUpdateST - Before Each] - Setup test case environment
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:77] [rollingupdate.RollingUpdateST] - Adding parallel test: testKafkaAndZookeeperScaleUpScaleDown
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:81] [rollingupdate.RollingUpdateST] - Parallel test count: 4
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:230] testKafkaAndZookeeperScaleUpScaleDown test now can proceed its execution
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349, testSendSimpleMessageTls=my-cluster-b70e5342, testSendMessagesTlsScramSha=my-cluster-380e6586, testUpdateUser=my-cluster-9b2bd187, testReceiveSimpleMessageTls=my-cluster-e8a518cd, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09}
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1001559761-691587912, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1590558165-46977776, testSendSimpleMessageTls=my-user-602433797-900694548, testSendMessagesTlsScramSha=my-user-2039401547-533835955, testUpdateUser=my-user-595127408-1604948726, testReceiveSimpleMessageTls=my-user-761922164-2019863647, testSendMessagesCustomListenerTlsScramSha=my-user-445988529-51741427, testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659}
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-741363275-285355018, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1210910585-1669754462, testSendSimpleMessageTls=my-topic-1299629356-597168683, testSendMessagesTlsScramSha=my-topic-988094002-1400465950, testUpdateUser=my-topic-1875736194-1340884809, testReceiveSimpleMessageTls=my-topic-1007790874-1032272408, testSendMessagesCustomListenerTlsScramSha=my-topic-1039904883-1044305687, testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393}
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349-kafka-clients, testSendSimpleMessageTls=my-cluster-b70e5342-kafka-clients, testSendMessagesTlsScramSha=my-cluster-380e6586-kafka-clients, testUpdateUser=my-cluster-9b2bd187-kafka-clients, testReceiveSimpleMessageTls=my-cluster-e8a518cd-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients}
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-5 for test case:testKafkaAndZookeeperScaleUpScaleDown
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-5
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-5
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace rolling-update-st get Namespace namespace-5 -o json
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace rolling-update-st get Namespace namespace-5 -o json
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T21:34:22Z",
        "name": "namespace-5",
        "resourceVersion": "100504",
        "selfLink": "/api/v1/namespaces/namespace-5",
        "uid": "3ca8c316-c5d3-407e-99b1-58bd79b54efc"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[namespace-4], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[namespace-5], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@dce7372d=[rolling-update-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-3], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-0], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-5
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-5, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-5
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-94dd1070 in namespace namespace-5
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-94dd1070
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-94dd1070 will have desired state: Ready
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-94dd1070 will have desired state: Ready
2022-03-30 21:34:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (839997ms till timeout)
2022-03-30 21:34:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1650312ms till timeout)
2022-03-30 21:34:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:34:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:34:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:34:23 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (760633ms till timeout)
2022-03-30 21:34:23 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:34:23 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:23 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648676063048] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:34:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (472767ms till timeout)
2022-03-30 21:34:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (838994ms till timeout)
2022-03-30 21:34:23 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1649308ms till timeout)
2022-03-30 21:34:24 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (759629ms till timeout)
2022-03-30 21:34:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (837990ms till timeout)
2022-03-30 21:34:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1648296ms till timeout)
2022-03-30 21:34:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (758626ms till timeout)
2022-03-30 21:34:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (836987ms till timeout)
2022-03-30 21:34:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1647293ms till timeout)
2022-03-30 21:34:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (757622ms till timeout)
2022-03-30 21:34:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (835982ms till timeout)
2022-03-30 21:34:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1646289ms till timeout)
2022-03-30 21:34:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-98f0c2b1 will have desired state: Ready not ready, will try again in 1000 ms (756617ms till timeout)
2022-03-30 21:34:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (834978ms till timeout)
2022-03-30 21:34:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1645283ms till timeout)
2022-03-30 21:34:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:34:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:34:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-98f0c2b1 is in desired state: Ready
2022-03-30 21:34:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1039904883-1044305687 in namespace namespace-5
2022-03-30 21:34:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-03-30 21:34:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1039904883-1044305687
2022-03-30 21:34:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1039904883-1044305687 will have desired state: Ready
2022-03-30 21:34:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1039904883-1044305687 will have desired state: Ready
2022-03-30 21:34:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:34:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1039904883-1044305687 will have desired state: Ready not ready, will try again in 1000 ms (179997ms till timeout)
2022-03-30 21:34:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (833969ms till timeout)
2022-03-30 21:34:28 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1644276ms till timeout)
2022-03-30 21:34:28 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:34:28 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:28 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648676068521] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:34:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (467258ms till timeout)
2022-03-30 21:34:29 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1039904883-1044305687 is in desired state: Ready
2022-03-30 21:34:29 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-445988529-51741427 in namespace namespace-5
2022-03-30 21:34:29 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-03-30 21:34:29 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-445988529-51741427
2022-03-30 21:34:29 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-445988529-51741427 will have desired state: Ready
2022-03-30 21:34:29 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-445988529-51741427 will have desired state: Ready
2022-03-30 21:34:29 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-445988529-51741427 will have desired state: Ready not ready, will try again in 1000 ms (179996ms till timeout)
2022-03-30 21:34:29 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (832964ms till timeout)
2022-03-30 21:34:29 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1643271ms till timeout)
2022-03-30 21:34:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-445988529-51741427 is in desired state: Ready
2022-03-30 21:34:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-98f0c2b1-kafka-clients in namespace namespace-4
2022-03-30 21:34:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-03-30 21:34:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-98f0c2b1-kafka-clients
2022-03-30 21:34:30 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-98f0c2b1-kafka-clients will be ready
2022-03-30 21:34:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-98f0c2b1-kafka-clients will be ready
2022-03-30 21:34:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-98f0c2b1-kafka-clients will be ready not ready, will try again in 1000 ms (479993ms till timeout)
2022-03-30 21:34:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (831959ms till timeout)
2022-03-30 21:34:30 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1642267ms till timeout)
2022-03-30 21:34:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-98f0c2b1-kafka-clients will be ready not ready, will try again in 1000 ms (478988ms till timeout)
2022-03-30 21:34:31 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (830956ms till timeout)
2022-03-30 21:34:31 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1641262ms till timeout)
2022-03-30 21:34:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-98f0c2b1-kafka-clients will be ready not ready, will try again in 1000 ms (477984ms till timeout)
2022-03-30 21:34:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (829950ms till timeout)
2022-03-30 21:34:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:34:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:34:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1640257ms till timeout)
2022-03-30 21:34:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-98f0c2b1-kafka-clients will be ready not ready, will try again in 1000 ms (476980ms till timeout)
2022-03-30 21:34:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (828947ms till timeout)
2022-03-30 21:34:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1639253ms till timeout)
2022-03-30 21:34:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:34:33 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:34:33 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:33 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648676073836] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:34:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (461979ms till timeout)
2022-03-30 21:34:34 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-98f0c2b1-kafka-clients is ready
2022-03-30 21:34:34 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-30 21:34:34 [ForkJoinPool-3-worker-1] [32mINFO [m [ListenersST:442] Checking produced and consumed messages to pod:my-cluster-98f0c2b1-kafka-clients-df65469b9-2h48z
2022-03-30 21:34:34 [ForkJoinPool-3-worker-1] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@118b973a, which are set.
2022-03-30 21:34:34 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@755831a7, messages=[], arguments=[--topic, my-topic-1039904883-1044305687, --bootstrap-server, my-cluster-98f0c2b1-kafka-bootstrap.namespace-4.svc:9122, --max-messages, 100, USER=my_user_445988529_51741427], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-98f0c2b1-kafka-clients-df65469b9-2h48z', podNamespace='namespace-4', bootstrapServer='my-cluster-98f0c2b1-kafka-bootstrap.namespace-4.svc:9122', topicName='my-topic-1039904883-1044305687', maxMessages=100, kafkaUsername='my-user-445988529-51741427', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@118b973a}
2022-03-30 21:34:34 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-98f0c2b1-kafka-bootstrap.namespace-4.svc:9122:my-topic-1039904883-1044305687 from pod my-cluster-98f0c2b1-kafka-clients-df65469b9-2h48z
2022-03-30 21:34:34 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-98f0c2b1-kafka-clients-df65469b9-2h48z -n namespace-4 -- /opt/kafka/producer.sh --topic my-topic-1039904883-1044305687 --bootstrap-server my-cluster-98f0c2b1-kafka-bootstrap.namespace-4.svc:9122 --max-messages 100 USER=my_user_445988529_51741427
2022-03-30 21:34:34 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-98f0c2b1-kafka-clients-df65469b9-2h48z -n namespace-4 -- /opt/kafka/producer.sh --topic my-topic-1039904883-1044305687 --bootstrap-server my-cluster-98f0c2b1-kafka-bootstrap.namespace-4.svc:9122 --max-messages 100 USER=my_user_445988529_51741427
2022-03-30 21:34:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (827943ms till timeout)
2022-03-30 21:34:34 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1638246ms till timeout)
2022-03-30 21:34:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (826940ms till timeout)
2022-03-30 21:34:35 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1637243ms till timeout)
2022-03-30 21:34:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (825936ms till timeout)
2022-03-30 21:34:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1636239ms till timeout)
2022-03-30 21:34:37 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (824933ms till timeout)
2022-03-30 21:34:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:34:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:34:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1635235ms till timeout)
2022-03-30 21:34:37 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-30 21:34:37 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-03-30 21:34:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2c0fb7fe, which are set.
2022-03-30 21:34:37 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@5a14420d, messages=[], arguments=[--topic, my-topic-1039904883-1044305687, --bootstrap-server, my-cluster-98f0c2b1-kafka-bootstrap.namespace-4.svc:9122, --max-messages, 100, USER=my_user_445988529_51741427, --group-id, my-consumer-group-413127858, --group-instance-id, instance224704858], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-98f0c2b1-kafka-clients-df65469b9-2h48z', podNamespace='namespace-4', bootstrapServer='my-cluster-98f0c2b1-kafka-bootstrap.namespace-4.svc:9122', topicName='my-topic-1039904883-1044305687', maxMessages=100, kafkaUsername='my-user-445988529-51741427', consumerGroupName='my-consumer-group-413127858', consumerInstanceId='instance224704858', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2c0fb7fe}
2022-03-30 21:34:37 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-98f0c2b1-kafka-bootstrap.namespace-4.svc:9122:my-topic-1039904883-1044305687 from pod my-cluster-98f0c2b1-kafka-clients-df65469b9-2h48z
2022-03-30 21:34:37 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-98f0c2b1-kafka-clients-df65469b9-2h48z -n namespace-4 -- /opt/kafka/consumer.sh --topic my-topic-1039904883-1044305687 --bootstrap-server my-cluster-98f0c2b1-kafka-bootstrap.namespace-4.svc:9122 --max-messages 100 USER=my_user_445988529_51741427 --group-id my-consumer-group-413127858 --group-instance-id instance224704858
2022-03-30 21:34:37 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-98f0c2b1-kafka-clients-df65469b9-2h48z -n namespace-4 -- /opt/kafka/consumer.sh --topic my-topic-1039904883-1044305687 --bootstrap-server my-cluster-98f0c2b1-kafka-bootstrap.namespace-4.svc:9122 --max-messages 100 USER=my_user_445988529_51741427 --group-id my-consumer-group-413127858 --group-instance-id instance224704858
2022-03-30 21:34:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (823929ms till timeout)
2022-03-30 21:34:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1634232ms till timeout)
2022-03-30 21:34:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:34:39 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:34:39 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:39 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648676079094] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:34:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (456728ms till timeout)
2022-03-30 21:34:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (822926ms till timeout)
2022-03-30 21:34:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1633228ms till timeout)
2022-03-30 21:34:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (821922ms till timeout)
2022-03-30 21:34:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1632225ms till timeout)
2022-03-30 21:34:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (820918ms till timeout)
2022-03-30 21:34:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1631221ms till timeout)
2022-03-30 21:34:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (819914ms till timeout)
2022-03-30 21:34:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:34:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:34:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1630218ms till timeout)
2022-03-30 21:34:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (818911ms till timeout)
2022-03-30 21:34:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1629214ms till timeout)
2022-03-30 21:34:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:34:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (817907ms till timeout)
2022-03-30 21:34:44 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:34:44 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:44 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648676084335] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:34:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (451439ms till timeout)
2022-03-30 21:34:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1628211ms till timeout)
2022-03-30 21:34:45 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 21:34:45 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-03-30 21:34:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 21:34:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:675] [kafka.listeners.ListenersST - After Each] - Clean up after test
2022-03-30 21:34:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 21:34:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testSendMessagesCustomListenerTlsScramSha
2022-03-30 21:34:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-445988529-51741427 in namespace namespace-4
2022-03-30 21:34:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-445988529-51741427
2022-03-30 21:34:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-445988529-51741427 not ready, will try again in 10000 ms (179993ms till timeout)
2022-03-30 21:34:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (816904ms till timeout)
2022-03-30 21:34:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1627203ms till timeout)
2022-03-30 21:34:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (815900ms till timeout)
2022-03-30 21:34:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1626200ms till timeout)
2022-03-30 21:34:47 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (814897ms till timeout)
2022-03-30 21:34:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:34:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:34:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1625196ms till timeout)
2022-03-30 21:34:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (813894ms till timeout)
2022-03-30 21:34:48 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1624193ms till timeout)
2022-03-30 21:34:49 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (812890ms till timeout)
2022-03-30 21:34:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:34:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1623189ms till timeout)
2022-03-30 21:34:49 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:34:49 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:49 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648676089655] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:34:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (446166ms till timeout)
2022-03-30 21:34:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (811887ms till timeout)
2022-03-30 21:34:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1622185ms till timeout)
2022-03-30 21:34:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (810883ms till timeout)
2022-03-30 21:34:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1621182ms till timeout)
2022-03-30 21:34:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (809880ms till timeout)
2022-03-30 21:34:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:34:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:34:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1620178ms till timeout)
2022-03-30 21:34:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (808877ms till timeout)
2022-03-30 21:34:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1619173ms till timeout)
2022-03-30 21:34:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (807873ms till timeout)
2022-03-30 21:34:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1618169ms till timeout)
2022-03-30 21:34:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:34:54 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:34:54 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:34:54 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648676094924] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:34:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (440893ms till timeout)
2022-03-30 21:34:55 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-98f0c2b1-kafka-clients in namespace namespace-4
2022-03-30 21:34:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-98f0c2b1-kafka-clients
2022-03-30 21:34:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-98f0c2b1-kafka-clients not ready, will try again in 10000 ms (479949ms till timeout)
2022-03-30 21:34:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (806870ms till timeout)
2022-03-30 21:34:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1617164ms till timeout)
2022-03-30 21:34:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (805865ms till timeout)
2022-03-30 21:34:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1616160ms till timeout)
2022-03-30 21:34:57 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (804862ms till timeout)
2022-03-30 21:34:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:34:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:34:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1615152ms till timeout)
2022-03-30 21:34:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (803844ms till timeout)
2022-03-30 21:34:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1614147ms till timeout)
2022-03-30 21:34:59 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (802839ms till timeout)
2022-03-30 21:34:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1613143ms till timeout)
2022-03-30 21:34:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:35:00 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:35:00 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:35:00 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648676100276] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:35:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (435522ms till timeout)
2022-03-30 21:35:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (801833ms till timeout)
2022-03-30 21:35:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1612137ms till timeout)
2022-03-30 21:35:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (800830ms till timeout)
2022-03-30 21:35:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1611133ms till timeout)
2022-03-30 21:35:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:35:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:35:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (799826ms till timeout)
2022-03-30 21:35:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1610126ms till timeout)
2022-03-30 21:35:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (798816ms till timeout)
2022-03-30 21:35:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1609123ms till timeout)
2022-03-30 21:35:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (797813ms till timeout)
2022-03-30 21:35:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1608119ms till timeout)
2022-03-30 21:35:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:35:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-98f0c2b1-kafka-clients not ready, will try again in 10000 ms (469940ms till timeout)
2022-03-30 21:35:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (796807ms till timeout)
2022-03-30 21:35:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1607116ms till timeout)
2022-03-30 21:35:05 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:35:05 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:35:05 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648676105544] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:35:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (430274ms till timeout)
2022-03-30 21:35:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (795803ms till timeout)
2022-03-30 21:35:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1606112ms till timeout)
2022-03-30 21:35:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:35:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:35:07 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (794800ms till timeout)
2022-03-30 21:35:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1605109ms till timeout)
2022-03-30 21:35:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (793796ms till timeout)
2022-03-30 21:35:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1604105ms till timeout)
2022-03-30 21:35:09 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (792793ms till timeout)
2022-03-30 21:35:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1603102ms till timeout)
2022-03-30 21:35:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (791789ms till timeout)
2022-03-30 21:35:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1602098ms till timeout)
2022-03-30 21:35:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:35:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:35:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:35:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648676110795] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:35:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (425026ms till timeout)
2022-03-30 21:35:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (790786ms till timeout)
2022-03-30 21:35:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1601094ms till timeout)
2022-03-30 21:35:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:35:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:35:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (789782ms till timeout)
2022-03-30 21:35:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (1600091ms till timeout)
2022-03-30 21:35:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (788779ms till timeout)
2022-03-30 21:35:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-58833349 is in desired state: Ready
2022-03-30 21:35:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1590558165-46977776 in namespace namespace-5
2022-03-30 21:35:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-3
2022-03-30 21:35:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1590558165-46977776
2022-03-30 21:35:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1590558165-46977776 will have desired state: Ready
2022-03-30 21:35:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-1590558165-46977776 will have desired state: Ready
2022-03-30 21:35:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-1590558165-46977776 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 21:35:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (787775ms till timeout)
2022-03-30 21:35:14 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1590558165-46977776 is in desired state: Ready
2022-03-30 21:35:14 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1210910585-1669754462 in namespace namespace-5
2022-03-30 21:35:14 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-3
2022-03-30 21:35:14 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1210910585-1669754462
2022-03-30 21:35:14 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1210910585-1669754462 will have desired state: Ready
2022-03-30 21:35:14 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1210910585-1669754462 will have desired state: Ready
2022-03-30 21:35:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1210910585-1669754462 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 21:35:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-98f0c2b1-kafka-clients not ready, will try again in 10000 ms (459931ms till timeout)
2022-03-30 21:35:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (786772ms till timeout)
2022-03-30 21:35:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1210910585-1669754462 is in desired state: Ready
2022-03-30 21:35:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-58833349-kafka-clients in namespace namespace-5
2022-03-30 21:35:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-3
2022-03-30 21:35:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-58833349-kafka-clients
2022-03-30 21:35:15 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-58833349-kafka-clients will be ready
2022-03-30 21:35:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-58833349-kafka-clients will be ready
2022-03-30 21:35:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-kafka-clients will be ready not ready, will try again in 1000 ms (479994ms till timeout)
2022-03-30 21:35:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:35:16 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:35:16 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:35:16 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648676116054] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:35:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (419758ms till timeout)
2022-03-30 21:35:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (785769ms till timeout)
2022-03-30 21:35:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-kafka-clients will be ready not ready, will try again in 1000 ms (478991ms till timeout)
2022-03-30 21:35:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:35:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:35:17 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (784765ms till timeout)
2022-03-30 21:35:17 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-58833349-kafka-clients is ready
2022-03-30 21:35:17 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-30 21:35:17 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:269] Checking produced and consumed messages to pod:my-cluster-58833349-kafka-clients-84f7bb746-62dqm
2022-03-30 21:35:17 [ForkJoinPool-3-worker-5] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@49f273ab, which are set.
2022-03-30 21:35:17 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@6e8b2642, messages=[], arguments=[--topic, my-topic-1210910585-1669754462, --bootstrap-server, my-cluster-58833349-kafka-bootstrap.namespace-3.svc:9092, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-58833349-kafka-clients-84f7bb746-62dqm', podNamespace='namespace-3', bootstrapServer='my-cluster-58833349-kafka-bootstrap.namespace-3.svc:9092', topicName='my-topic-1210910585-1669754462', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@49f273ab}
2022-03-30 21:35:17 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-58833349-kafka-bootstrap.namespace-3.svc:9092:my-topic-1210910585-1669754462 from pod my-cluster-58833349-kafka-clients-84f7bb746-62dqm
2022-03-30 21:35:17 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-58833349-kafka-clients-84f7bb746-62dqm -n namespace-3 -- /opt/kafka/producer.sh --topic my-topic-1210910585-1669754462 --bootstrap-server my-cluster-58833349-kafka-bootstrap.namespace-3.svc:9092 --max-messages 100
2022-03-30 21:35:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-58833349-kafka-clients-84f7bb746-62dqm -n namespace-3 -- /opt/kafka/producer.sh --topic my-topic-1210910585-1669754462 --bootstrap-server my-cluster-58833349-kafka-bootstrap.namespace-3.svc:9092 --max-messages 100
2022-03-30 21:35:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (783762ms till timeout)
2022-03-30 21:35:19 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (782756ms till timeout)
2022-03-30 21:35:20 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-03-30 21:35:20 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-03-30 21:35:20 [ForkJoinPool-3-worker-5] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@121a6a96, which are set.
2022-03-30 21:35:20 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@265b7a5d, messages=[], arguments=[--topic, my-topic-1210910585-1669754462, --bootstrap-server, my-cluster-58833349-kafka-bootstrap.namespace-3.svc:9092, --max-messages, 100, --group-id, my-consumer-group-1704807394, --group-instance-id, instance1338194642], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-58833349-kafka-clients-84f7bb746-62dqm', podNamespace='namespace-3', bootstrapServer='my-cluster-58833349-kafka-bootstrap.namespace-3.svc:9092', topicName='my-topic-1210910585-1669754462', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1704807394', consumerInstanceId='instance1338194642', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@121a6a96}
2022-03-30 21:35:20 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-58833349-kafka-bootstrap.namespace-3.svc:9092#my-topic-1210910585-1669754462 from pod my-cluster-58833349-kafka-clients-84f7bb746-62dqm
2022-03-30 21:35:20 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-58833349-kafka-clients-84f7bb746-62dqm -n namespace-3 -- /opt/kafka/consumer.sh --topic my-topic-1210910585-1669754462 --bootstrap-server my-cluster-58833349-kafka-bootstrap.namespace-3.svc:9092 --max-messages 100 --group-id my-consumer-group-1704807394 --group-instance-id instance1338194642
2022-03-30 21:35:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-58833349-kafka-clients-84f7bb746-62dqm -n namespace-3 -- /opt/kafka/consumer.sh --topic my-topic-1210910585-1669754462 --bootstrap-server my-cluster-58833349-kafka-bootstrap.namespace-3.svc:9092 --max-messages 100 --group-id my-consumer-group-1704807394 --group-instance-id instance1338194642
2022-03-30 21:35:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (781752ms till timeout)
2022-03-30 21:35:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:35:21 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:35:21 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:35:21 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648676121338] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:35:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (414483ms till timeout)
2022-03-30 21:35:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (780736ms till timeout)
2022-03-30 21:35:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:35:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:35:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (779733ms till timeout)
2022-03-30 21:35:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (778730ms till timeout)
2022-03-30 21:35:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (777727ms till timeout)
2022-03-30 21:35:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-98f0c2b1-kafka-clients not ready, will try again in 10000 ms (449905ms till timeout)
2022-03-30 21:35:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (776723ms till timeout)
2022-03-30 21:35:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:35:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (775719ms till timeout)
2022-03-30 21:35:26 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-30 21:35:26 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-03-30 21:35:26 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:283] Triggering CA cert renewal by adding the annotation
2022-03-30 21:35:26 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:295] Patching secret my-cluster-58833349-cluster-ca-cert with strimzi.io/force-renew
2022-03-30 21:35:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:35:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:35:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648676126681] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:35:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (409140ms till timeout)
2022-03-30 21:35:26 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:295] Patching secret my-cluster-58833349-clients-ca-cert with strimzi.io/force-renew
2022-03-30 21:35:26 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:300] Wait for zk to rolling restart ...
2022-03-30 21:35:26 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-58833349-zookeeper rolling update
2022-03-30 21:35:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:74] Waiting for rolling update of component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})
2022-03-30 21:35:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for component with name my-cluster-58833349-zookeeper rolling update
2022-03-30 21:35:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:35:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:35:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:35:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-zookeeper-1 hasn't rolled
2022-03-30 21:35:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-zookeeper rolling update not ready, will try again in 5000 ms (1799994ms till timeout)
2022-03-30 21:35:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:35:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:35:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (774716ms till timeout)
2022-03-30 21:35:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (773710ms till timeout)
2022-03-30 21:35:29 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (772706ms till timeout)
2022-03-30 21:35:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (771703ms till timeout)
2022-03-30 21:35:31 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (770699ms till timeout)
2022-03-30 21:35:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:35:31 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:35:31 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:35:31 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:35:31 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-zookeeper-1 hasn't rolled
2022-03-30 21:35:31 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-zookeeper rolling update not ready, will try again in 5000 ms (1794988ms till timeout)
2022-03-30 21:35:31 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:35:31 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:35:31 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648676131965] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:35:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (403850ms till timeout)
2022-03-30 21:35:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:35:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:35:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (769696ms till timeout)
2022-03-30 21:35:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (768692ms till timeout)
2022-03-30 21:35:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (767689ms till timeout)
2022-03-30 21:35:35 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1039904883-1044305687 in namespace namespace-4
2022-03-30 21:35:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1039904883-1044305687
2022-03-30 21:35:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1039904883-1044305687 not ready, will try again in 10000 ms (179993ms till timeout)
2022-03-30 21:35:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (766686ms till timeout)
2022-03-30 21:35:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (765680ms till timeout)
2022-03-30 21:35:36 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:35:36 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:35:36 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:35:36 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-zookeeper-1 hasn't rolled
2022-03-30 21:35:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-zookeeper rolling update not ready, will try again in 5000 ms (1789982ms till timeout)
2022-03-30 21:35:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:35:37 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:35:37 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:35:37 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648676137258] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:35:37 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (398562ms till timeout)
2022-03-30 21:35:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:35:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:35:37 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (764677ms till timeout)
2022-03-30 21:35:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (763674ms till timeout)
2022-03-30 21:35:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-94dd1070 will have desired state: Ready not ready, will try again in 1000 ms (762671ms till timeout)
2022-03-30 21:35:40 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-94dd1070 is in desired state: Ready
2022-03-30 21:35:40 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1001559761-691587912 in namespace namespace-5
2022-03-30 21:35:40 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-03-30 21:35:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1001559761-691587912
2022-03-30 21:35:40 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1001559761-691587912 will have desired state: Ready
2022-03-30 21:35:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-1001559761-691587912 will have desired state: Ready
2022-03-30 21:35:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-1001559761-691587912 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 21:35:41 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1001559761-691587912 is in desired state: Ready
2022-03-30 21:35:41 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:489] Verifying docker image names
2022-03-30 21:35:41 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:172] strimzi-cluster-operator
2022-03-30 21:35:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get pod -l strimzi.io/name=my-cluster-94dd1070-entity-operator -o jsonpath={range .items[*]}{.metadata.name} 
2022-03-30 21:35:41 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get pod -l strimzi.io/name=my-cluster-94dd1070-entity-operator -o jsonpath={range .items[*]}{.metadata.name} 
2022-03-30 21:35:41 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:35:41 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:525] Docker images verified
2022-03-30 21:35:41 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateST:292] Running kafkaScaleUpScaleDown my-cluster-94dd1070
2022-03-30 21:35:41 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-741363275-285355018 in namespace namespace-5
2022-03-30 21:35:41 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-03-30 21:35:41 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-741363275-285355018
2022-03-30 21:35:41 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-741363275-285355018 will have desired state: Ready
2022-03-30 21:35:41 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-741363275-285355018 will have desired state: Ready
2022-03-30 21:35:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-741363275-285355018 will have desired state: Ready not ready, will try again in 1000 ms (179999ms till timeout)
2022-03-30 21:35:41 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:35:41 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:35:41 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:35:41 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-zookeeper-1 hasn't rolled
2022-03-30 21:35:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-zookeeper rolling update not ready, will try again in 5000 ms (1784975ms till timeout)
2022-03-30 21:35:42 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:35:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:35:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:35:42 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:35:42 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:35:42 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648676142515] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:35:42 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (393306ms till timeout)
2022-03-30 21:35:42 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-741363275-285355018 is in desired state: Ready
2022-03-30 21:35:42 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-94dd1070-kafka-clients in namespace namespace-5
2022-03-30 21:35:42 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-03-30 21:35:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for pod with prefixmy-cluster-94dd1070-kafka-clients is present.
2022-03-30 21:35:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] pod with prefixmy-cluster-94dd1070-kafka-clients is present. not ready, will try again in 10000 ms (299993ms till timeout)
2022-03-30 21:35:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-98f0c2b1 in namespace namespace-4
2022-03-30 21:35:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-98f0c2b1
2022-03-30 21:35:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-98f0c2b1 not ready, will try again in 10000 ms (839995ms till timeout)
2022-03-30 21:35:46 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:35:46 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:35:46 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:35:46 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-zookeeper-1 hasn't rolled
2022-03-30 21:35:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-zookeeper rolling update not ready, will try again in 5000 ms (1779970ms till timeout)
2022-03-30 21:35:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:35:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:35:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:35:47 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:35:47 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:35:47 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648676147799] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:35:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (388017ms till timeout)
2022-03-30 21:35:51 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:35:51 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:35:51 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:35:51 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-zookeeper-1 hasn't rolled
2022-03-30 21:35:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-zookeeper rolling update not ready, will try again in 5000 ms (1774964ms till timeout)
2022-03-30 21:35:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:35:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:35:52 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-30 21:35:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Producer and consumer will successfully send and receive messages.
2022-03-30 21:35:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4a0840d8, which are set.
2022-03-30 21:35:52 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@4db9716c, messages=[], arguments=[--topic, my-topic-741363275-285355018, --bootstrap-server, my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093, --max-messages, 100, USER=my_user_1001559761_691587912], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-94dd1070-kafka-clients-65ccf55595-d4d87', podNamespace='namespace-5', bootstrapServer='my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093', topicName='my-topic-741363275-285355018', maxMessages=100, kafkaUsername='my-user-1001559761-691587912', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4a0840d8}
2022-03-30 21:35:52 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093:my-topic-741363275-285355018 from pod my-cluster-94dd1070-kafka-clients-65ccf55595-d4d87
2022-03-30 21:35:52 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-94dd1070-kafka-clients-65ccf55595-d4d87 -n namespace-5 -- /opt/kafka/producer.sh --topic my-topic-741363275-285355018 --bootstrap-server my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093 --max-messages 100 USER=my_user_1001559761_691587912
2022-03-30 21:35:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-94dd1070-kafka-clients-65ccf55595-d4d87 -n namespace-5 -- /opt/kafka/producer.sh --topic my-topic-741363275-285355018 --bootstrap-server my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093 --max-messages 100 USER=my_user_1001559761_691587912
2022-03-30 21:35:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:35:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:35:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:35:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648676153067] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 21:35:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (382737ms till timeout)
2022-03-30 21:35:55 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 21:35:55 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-4 for test case:testSendMessagesCustomListenerTlsScramSha
2022-03-30 21:35:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-4 removal
2022-03-30 21:35:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:35:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:35:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:35:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (479923ms till timeout)
2022-03-30 21:35:56 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-30 21:35:56 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-03-30 21:35:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5dd8f91, which are set.
2022-03-30 21:35:56 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@3eeb72b5, messages=[], arguments=[--topic, my-topic-741363275-285355018, --bootstrap-server, my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093, --max-messages, 100, USER=my_user_1001559761_691587912, --group-id, my-consumer-group-1673030102, --group-instance-id, instance140892140], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-94dd1070-kafka-clients-65ccf55595-d4d87', podNamespace='namespace-5', bootstrapServer='my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093', topicName='my-topic-741363275-285355018', maxMessages=100, kafkaUsername='my-user-1001559761-691587912', consumerGroupName='my-consumer-group-1673030102', consumerInstanceId='instance140892140', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5dd8f91}
2022-03-30 21:35:56 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093:my-topic-741363275-285355018 from pod my-cluster-94dd1070-kafka-clients-65ccf55595-d4d87
2022-03-30 21:35:56 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-94dd1070-kafka-clients-65ccf55595-d4d87 -n namespace-5 -- /opt/kafka/consumer.sh --topic my-topic-741363275-285355018 --bootstrap-server my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093 --max-messages 100 USER=my_user_1001559761_691587912 --group-id my-consumer-group-1673030102 --group-instance-id instance140892140
2022-03-30 21:35:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-94dd1070-kafka-clients-65ccf55595-d4d87 -n namespace-5 -- /opt/kafka/consumer.sh --topic my-topic-741363275-285355018 --bootstrap-server my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093 --max-messages 100 USER=my_user_1001559761_691587912 --group-id my-consumer-group-1673030102 --group-instance-id instance140892140
2022-03-30 21:35:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:35:56 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:35:56 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:35:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (478845ms till timeout)
2022-03-30 21:35:56 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:35:56 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:35:56 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:35:56 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-zookeeper-1 hasn't rolled
2022-03-30 21:35:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-zookeeper rolling update not ready, will try again in 5000 ms (1769959ms till timeout)
2022-03-30 21:35:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:35:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:35:57 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:35:57 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:35:57 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:35:57 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (477767ms till timeout)
2022-03-30 21:35:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:35:58 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:35:58 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:35:58 [ForkJoinPool-3-worker-13] [36mDEBUG[m [CruiseControlUtils:175] API response 

Optimization has 18 inter-broker replica(0 MB) moves, 0 intra-broker replica(0 MB) moves and 1 leadership moves with a cluster model of 1 recent windows and 100.000% of the partitions covered.
Excluded Topics: [].
Excluded Brokers For Leadership: [].
Excluded Brokers For Replica Move: [].
Counts: 3 brokers 287 replicas 6 topics.
On-demand Balancedness Score Before (87.919) After(87.919).
Provision Status: RIGHT_SIZED.

[     2 ms] Stats for RackAwareGoal(NO-ACTION):
AVG:{cpu:       1.921 networkInbound:       0.428 networkOutbound:       0.365 disk:       0.020 potentialNwOut:       1.078 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       3.430 networkInbound:       0.437 networkOutbound:       0.619 disk:       0.020 potentialNwOut:       1.086 replicas:97 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.581 networkInbound:       0.419 networkOutbound:       0.109 disk:       0.020 potentialNwOut:       1.068 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       1.169 networkInbound:       0.008 networkOutbound:       0.208 disk:       0.000 potentialNwOut:       0.008 replicas:0.9428090415820634 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[     1 ms] Stats for MinTopicLeadersPerBrokerGoal(NO-ACTION):
AVG:{cpu:       1.921 networkInbound:       0.428 networkOutbound:       0.365 disk:       0.020 potentialNwOut:       1.078 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       3.430 networkInbound:       0.437 networkOutbound:       0.619 disk:       0.020 potentialNwOut:       1.086 replicas:97 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.581 networkInbound:       0.419 networkOutbound:       0.109 disk:       0.020 potentialNwOut:       1.068 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       1.169 networkInbound:       0.008 networkOutbound:       0.208 disk:       0.000 potentialNwOut:       0.008 replicas:0.9428090415820634 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[     0 ms] Stats for ReplicaCapacityGoal(NO-ACTION):
AVG:{cpu:       1.921 networkInbound:       0.428 networkOutbound:       0.365 disk:       0.020 potentialNwOut:       1.078 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       3.430 networkInbound:       0.437 networkOutbound:       0.619 disk:       0.020 potentialNwOut:       1.086 replicas:97 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.581 networkInbound:       0.419 networkOutbound:       0.109 disk:       0.020 potentialNwOut:       1.068 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       1.169 networkInbound:       0.008 networkOutbound:       0.208 disk:       0.000 potentialNwOut:       0.008 replicas:0.9428090415820634 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[     1 ms] Stats for DiskCapacityGoal(NO-ACTION):
AVG:{cpu:       1.921 networkInbound:       0.428 networkOutbound:       0.365 disk:       0.020 potentialNwOut:       1.078 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       3.430 networkInbound:       0.437 networkOutbound:       0.619 disk:       0.020 potentialNwOut:       1.086 replicas:97 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.581 networkInbound:       0.419 networkOutbound:       0.109 disk:       0.020 potentialNwOut:       1.068 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       1.169 networkInbound:       0.008 networkOutbound:       0.208 disk:       0.000 potentialNwOut:       0.008 replicas:0.9428090415820634 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[    13 ms] Stats for NetworkInboundCapacityGoal(NO-ACTION):
AVG:{cpu:       1.921 networkInbound:       0.428 networkOutbound:       0.365 disk:       0.020 potentialNwOut:       1.078 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       3.430 networkInbound:       0.437 networkOutbound:       0.619 disk:       0.020 potentialNwOut:       1.086 replicas:97 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.581 networkInbound:       0.419 networkOutbound:       0.109 disk:       0.020 potentialNwOut:       1.068 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       1.169 networkInbound:       0.008 networkOutbound:       0.208 disk:       0.000 potentialNwOut:       0.008 replicas:0.9428090415820634 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[     1 ms] Stats for NetworkOutboundCapacityGoal(NO-ACTION):
AVG:{cpu:       1.921 networkInbound:       0.428 networkOutbound:       0.365 disk:       0.020 potentialNwOut:       1.078 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       3.430 networkInbound:       0.437 networkOutbound:       0.619 disk:       0.020 potentialNwOut:       1.086 replicas:97 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.581 networkInbound:       0.419 networkOutbound:       0.109 disk:       0.020 potentialNwOut:       1.068 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       1.169 networkInbound:       0.008 networkOutbound:       0.208 disk:       0.000 potentialNwOut:       0.008 replicas:0.9428090415820634 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[     3 ms] Stats for CpuCapacityGoal(NO-ACTION):
AVG:{cpu:       1.921 networkInbound:       0.428 networkOutbound:       0.365 disk:       0.020 potentialNwOut:       1.078 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       3.430 networkInbound:       0.437 networkOutbound:       0.619 disk:       0.020 potentialNwOut:       1.086 replicas:97 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.581 networkInbound:       0.419 networkOutbound:       0.109 disk:       0.020 potentialNwOut:       1.068 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       1.169 networkInbound:       0.008 networkOutbound:       0.208 disk:       0.000 potentialNwOut:       0.008 replicas:0.9428090415820634 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[     1 ms] Stats for ReplicaDistributionGoal(NO-ACTION):
AVG:{cpu:       1.921 networkInbound:       0.428 networkOutbound:       0.365 disk:       0.020 potentialNwOut:       1.078 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       3.430 networkInbound:       0.437 networkOutbound:       0.619 disk:       0.020 potentialNwOut:       1.086 replicas:97 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.581 networkInbound:       0.419 networkOutbound:       0.109 disk:       0.020 potentialNwOut:       1.068 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       1.169 networkInbound:       0.008 networkOutbound:       0.208 disk:       0.000 potentialNwOut:       0.008 replicas:0.9428090415820634 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[     1 ms] Stats for PotentialNwOutGoal(NO-ACTION):
AVG:{cpu:       1.921 networkInbound:       0.428 networkOutbound:       0.365 disk:       0.020 potentialNwOut:       1.078 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       3.430 networkInbound:       0.437 networkOutbound:       0.619 disk:       0.020 potentialNwOut:       1.086 replicas:97 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.581 networkInbound:       0.419 networkOutbound:       0.109 disk:       0.020 potentialNwOut:       1.068 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       1.169 networkInbound:       0.008 networkOutbound:       0.208 disk:       0.000 potentialNwOut:       0.008 replicas:0.9428090415820634 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[     1 ms] Stats for DiskUsageDistributionGoal(NO-ACTION):
AVG:{cpu:       1.921 networkInbound:       0.428 networkOutbound:       0.365 disk:       0.020 potentialNwOut:       1.078 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       3.430 networkInbound:       0.437 networkOutbound:       0.619 disk:       0.020 potentialNwOut:       1.086 replicas:97 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.581 networkInbound:       0.419 networkOutbound:       0.109 disk:       0.020 potentialNwOut:       1.068 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       1.169 networkInbound:       0.008 networkOutbound:       0.208 disk:       0.000 potentialNwOut:       0.008 replicas:0.9428090415820634 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[     1 ms] Stats for NetworkInboundUsageDistributionGoal(NO-ACTION):
AVG:{cpu:       1.921 networkInbound:       0.428 networkOutbound:       0.365 disk:       0.020 potentialNwOut:       1.078 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       3.430 networkInbound:       0.437 networkOutbound:       0.619 disk:       0.020 potentialNwOut:       1.086 replicas:97 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.581 networkInbound:       0.419 networkOutbound:       0.109 disk:       0.020 potentialNwOut:       1.068 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       1.169 networkInbound:       0.008 networkOutbound:       0.208 disk:       0.000 potentialNwOut:       0.008 replicas:0.9428090415820634 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[    14 ms] Stats for NetworkOutboundUsageDistributionGoal(VIOLATED):
AVG:{cpu:       1.921 networkInbound:       0.428 networkOutbound:       0.365 disk:       0.020 potentialNwOut:       1.078 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       2.688 networkInbound:       0.445 networkOutbound:       0.498 disk:       0.021 potentialNwOut:       1.094 replicas:101 leaderReplicas:49 topicReplicas:50}
MIN:{cpu:       1.508 networkInbound:       0.419 networkOutbound:       0.265 disk:       0.018 potentialNwOut:       1.068 replicas:91 leaderReplicas:30 topicReplicas:1}
STD:{cpu:       0.543 networkInbound:       0.012 networkOutbound:       0.098 disk:       0.001 potentialNwOut:       0.012 replicas:4.109609335312651 leaderReplicas:7.788880963698615 topicReplicas:0.8280528403113964

[    11 ms] Stats for CpuUsageDistributionGoal(VIOLATED):
AVG:{cpu:       1.921 networkInbound:       0.428 networkOutbound:       0.365 disk:       0.020 potentialNwOut:       1.078 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       2.609 networkInbound:       0.445 networkOutbound:       0.498 disk:       0.021 potentialNwOut:       1.094 replicas:101 leaderReplicas:49 topicReplicas:50}
MIN:{cpu:       1.508 networkInbound:       0.403 networkOutbound:       0.265 disk:       0.018 potentialNwOut:       1.053 replicas:91 leaderReplicas:33 topicReplicas:1}
STD:{cpu:       0.490 networkInbound:       0.018 networkOutbound:       0.098 disk:       0.001 potentialNwOut:       0.018 replicas:4.109609335312651 leaderReplicas:7.118052168020874 topicReplicas:1.480988630683912

[     1 ms] Stats for TopicReplicaDistributionGoal(NO-ACTION):
AVG:{cpu:       1.921 networkInbound:       0.428 networkOutbound:       0.365 disk:       0.020 potentialNwOut:       1.078 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       2.609 networkInbound:       0.445 networkOutbound:       0.498 disk:       0.021 potentialNwOut:       1.094 replicas:101 leaderReplicas:49 topicReplicas:50}
MIN:{cpu:       1.508 networkInbound:       0.403 networkOutbound:       0.265 disk:       0.018 potentialNwOut:       1.053 replicas:91 leaderReplicas:33 topicReplicas:1}
STD:{cpu:       0.490 networkInbound:       0.018 networkOutbound:       0.098 disk:       0.001 potentialNwOut:       0.018 replicas:4.109609335312651 leaderReplicas:7.118052168020874 topicReplicas:1.480988630683912

[    13 ms] Stats for LeaderReplicaDistributionGoal(VIOLATED):
AVG:{cpu:       1.921 networkInbound:       0.428 networkOutbound:       0.365 disk:       0.020 potentialNwOut:       1.078 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       2.609 networkInbound:       0.445 networkOutbound:       0.498 disk:       0.021 potentialNwOut:       1.094 replicas:101 leaderReplicas:49 topicReplicas:50}
MIN:{cpu:       1.508 networkInbound:       0.403 networkOutbound:       0.265 disk:       0.018 potentialNwOut:       1.053 replicas:91 leaderReplicas:33 topicReplicas:1}
STD:{cpu:       0.490 networkInbound:       0.018 networkOutbound:       0.098 disk:       0.001 potentialNwOut:       0.018 replicas:4.109609335312651 leaderReplicas:7.118052168020874 topicReplicas:1.480988630683912

[     3 ms] Stats for LeaderBytesInDistributionGoal(VIOLATED):
AVG:{cpu:       1.921 networkInbound:       0.428 networkOutbound:       0.365 disk:       0.020 potentialNwOut:       1.078 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       2.609 networkInbound:       0.445 networkOutbound:       0.498 disk:       0.021 potentialNwOut:       1.094 replicas:101 leaderReplicas:49 topicReplicas:50}
MIN:{cpu:       1.508 networkInbound:       0.403 networkOutbound:       0.265 disk:       0.018 potentialNwOut:       1.053 replicas:91 leaderReplicas:33 topicReplicas:1}
STD:{cpu:       0.490 networkInbound:       0.018 networkOutbound:       0.098 disk:       0.001 potentialNwOut:       0.018 replicas:4.109609335312651 leaderReplicas:7.118052168020874 topicReplicas:1.480988630683912

[    20 ms] Stats for PreferredLeaderElectionGoal(VIOLATED):
AVG:{cpu:       1.921 networkInbound:       0.428 networkOutbound:       0.365 disk:       0.020 potentialNwOut:       1.078 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       2.609 networkInbound:       0.445 networkOutbound:       0.498 disk:       0.021 potentialNwOut:       1.094 replicas:101 leaderReplicas:49 topicReplicas:50}
MIN:{cpu:       1.508 networkInbound:       0.403 networkOutbound:       0.265 disk:       0.018 potentialNwOut:       1.053 replicas:91 leaderReplicas:33 topicReplicas:1}
STD:{cpu:       0.490 networkInbound:       0.018 networkOutbound:       0.098 disk:       0.001 potentialNwOut:       0.018 replicas:4.109609335312651 leaderReplicas:7.118052168020874 topicReplicas:1.480988630683912

Cluster load after rebalance:


                                                                         HOST         BROKER                                                                         RACK         DISK_CAP(MB)            DISK(MB)/_(%)_            CORE_NUM         CPU(%)          NW_IN_CAP(KB/s)       LEADER_NW_IN(KB/s)     FOLLOWER_NW_IN(KB/s)         NW_OUT_CAP(KB/s)        NW_OUT(KB/s)       PNW_OUT(KB/s)    LEADERS/REPLICAS
my-cluster-e1c9825e-kafka-0.my-cluster-e1c9825e-kafka-brokers.namespace-0.svc,             0,my-cluster-e1c9825e-kafka-0.my-cluster-e1c9825e-kafka-brokers.namespace-0.svc,          100000.000,              0.020/00.00,                  1,         1.646,               10000.000,                   0.176,                   0.261,               10000.000,              0.498,              1.086,            35/95
my-cluster-e1c9825e-kafka-1.my-cluster-e1c9825e-kafka-brokers.namespace-0.svc,             1,my-cluster-e1c9825e-kafka-1.my-cluster-e1c9825e-kafka-brokers.namespace-0.svc,          100000.000,              0.018/00.00,                  1,         2.609,               10000.000,                   0.112,                   0.291,               10000.000,              0.333,              1.053,            33/91
my-cluster-e1c9825e-kafka-2.my-cluster-e1c9825e-kafka-brokers.namespace-0.svc,             2,my-cluster-e1c9825e-kafka-2.my-cluster-e1c9825e-kafka-brokers.namespace-0.svc,          100000.000,              0.021/00.00,                  1,         1.508,               10000.000,                   0.159,                   0.286,               10000.000,              0.265,              1.094,            49/101

2022-03-30 21:35:58 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:35:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:35:58 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:35:58 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:35:58 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (476680ms till timeout)
2022-03-30 21:35:58 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 21:35:58 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:35:58 [ForkJoinPool-3-worker-13] [32mINFO [m [CruiseControlApiST:97] ----> EXECUTION OF STOP PROPOSAL <----
2022-03-30 21:35:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XGET --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/stop_proposal_execution
2022-03-30 21:35:59 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XGET --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/stop_proposal_execution
2022-03-30 21:35:59 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:35:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/stop_proposal_execution
2022-03-30 21:35:59 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/stop_proposal_execution
2022-03-30 21:35:59 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:35:59 [ForkJoinPool-3-worker-13] [32mINFO [m [CruiseControlApiST:108] ----> USER TASKS <----
2022-03-30 21:35:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/user_tasks
2022-03-30 21:35:59 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:35:59 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/user_tasks
2022-03-30 21:35:59 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:35:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XGET --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/user_tasks
2022-03-30 21:35:59 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:35:59 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:35:59 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (475597ms till timeout)
2022-03-30 21:36:00 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XGET --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/user_tasks
2022-03-30 21:36:00 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:00 [ForkJoinPool-3-worker-13] [32mINFO [m [CruiseControlApiST:126] Verifying that Cruise Control REST API doesn't allow HTTP requests
2022-03-30 21:36:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XGET -k  HTTP://localhost:9090/kafkacruisecontrol/state
2022-03-30 21:36:00 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XGET -k  HTTP://localhost:9090/kafkacruisecontrol/state
2022-03-30 21:36:00 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:00 [ForkJoinPool-3-worker-13] [32mINFO [m [CruiseControlApiST:132] Verifying that Cruise Control REST API doesn't allow unauthenticated requests
2022-03-30 21:36:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XGET -k  HTTPS://localhost:9090/kafkacruisecontrol/state
2022-03-30 21:36:00 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec my-cluster-e1c9825e-cruise-control-6b4764948f-4b7g9 -c cruise-control -- /bin/bash -c curl -XGET -k  HTTPS://localhost:9090/kafkacruisecontrol/state
2022-03-30 21:36:00 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:00 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 21:36:00 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:675] [cruisecontrol.CruiseControlApiST - After Each] - Clean up after test
2022-03-30 21:36:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 21:36:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testCruiseControlBasicAPIRequests
2022-03-30 21:36:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-e1c9825e in namespace namespace-0
2022-03-30 21:36:00 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-0, for cruise control Kafka cluster my-cluster-e1c9825e
2022-03-30 21:36:00 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-e1c9825e
2022-03-30 21:36:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-e1c9825e not ready, will try again in 10000 ms (839993ms till timeout)
2022-03-30 21:36:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:36:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:36:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (474527ms till timeout)
2022-03-30 21:36:01 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:36:01 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=8739d439-0656-4462-acf7-6c4cd1de5698}
2022-03-30 21:36:01 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=8739d439-0656-4462-acf7-6c4cd1de5698}
2022-03-30 21:36:01 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-zookeeper-1 hasn't rolled
2022-03-30 21:36:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-zookeeper rolling update not ready, will try again in 5000 ms (1764953ms till timeout)
2022-03-30 21:36:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:36:01 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:36:01 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (473419ms till timeout)
2022-03-30 21:36:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:36:02 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:36:03 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:36:03 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (472294ms till timeout)
[[1;34mINFO[m] Running io.strimzi.systemtest.specific.SpecificIsolatedST
2022-03-30 21:36:03 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 21:36:03 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractST:667] [specific.SpecificIsolatedST - Before All] - Setup test suite environment
2022-03-30 21:36:03 [ForkJoinPool-3-worker-17] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 21:36:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-30 21:36:03 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 21:36:03 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-03-30 21:36:03 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateST:317] Scale up Kafka to 7
2022-03-30 21:36:03 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-94dd1070-kafka rolling update
2022-03-30 21:36:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:74] Waiting for rolling update of component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})
2022-03-30 21:36:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for component with name my-cluster-94dd1070-kafka rolling update
2022-03-30 21:36:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=7db81283-b62b-400d-8103-f4bdc33b13fe, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=7db81283-b62b-400d-8103-f4bdc33b13fe, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=7db81283-b62b-400d-8103-f4bdc33b13fe, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-1 hasn't rolled
2022-03-30 21:36:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (1799995ms till timeout)
2022-03-30 21:36:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:36:04 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:36:04 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (471203ms till timeout)
2022-03-30 21:36:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:36:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:36:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (470110ms till timeout)
2022-03-30 21:36:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:36:06 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:36:06 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (469015ms till timeout)
2022-03-30 21:36:06 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:36:06 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=8739d439-0656-4462-acf7-6c4cd1de5698}
2022-03-30 21:36:06 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=8739d439-0656-4462-acf7-6c4cd1de5698}
2022-03-30 21:36:06 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-zookeeper-1 hasn't rolled
2022-03-30 21:36:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-zookeeper rolling update not ready, will try again in 5000 ms (1759945ms till timeout)
2022-03-30 21:36:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:36:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:36:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:36:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (467934ms till timeout)
2022-03-30 21:36:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-30 21:36:08 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:36:08 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:36:08 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:08 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (466855ms till timeout)
2022-03-30 21:36:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=7db81283-b62b-400d-8103-f4bdc33b13fe, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=7db81283-b62b-400d-8103-f4bdc33b13fe, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=7db81283-b62b-400d-8103-f4bdc33b13fe, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-1 hasn't rolled
2022-03-30 21:36:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (1794989ms till timeout)
2022-03-30 21:36:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:36:09 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:36:09 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (465740ms till timeout)
2022-03-30 21:36:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 21:36:10 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-0 for test case:testCruiseControlBasicAPIRequests
2022-03-30 21:36:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-0 removal
2022-03-30 21:36:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:36:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (479918ms till timeout)
2022-03-30 21:36:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:36:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (464665ms till timeout)
2022-03-30 21:36:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:36:11 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:36:11 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=8739d439-0656-4462-acf7-6c4cd1de5698}
2022-03-30 21:36:11 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=8739d439-0656-4462-acf7-6c4cd1de5698}
2022-03-30 21:36:11 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-zookeeper-1 hasn't rolled
2022-03-30 21:36:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-zookeeper rolling update not ready, will try again in 5000 ms (1754939ms till timeout)
2022-03-30 21:36:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (478833ms till timeout)
2022-03-30 21:36:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:36:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (463588ms till timeout)
2022-03-30 21:36:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:36:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:36:12 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:12 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (477756ms till timeout)
2022-03-30 21:36:12 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:36:12 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (462514ms till timeout)
2022-03-30 21:36:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-30 21:36:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=7db81283-b62b-400d-8103-f4bdc33b13fe, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=7db81283-b62b-400d-8103-f4bdc33b13fe, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=7db81283-b62b-400d-8103-f4bdc33b13fe, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-1 hasn't rolled
2022-03-30 21:36:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (1789983ms till timeout)
2022-03-30 21:36:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:36:13 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:13 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (476678ms till timeout)
2022-03-30 21:36:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:36:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (461436ms till timeout)
2022-03-30 21:36:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:36:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:36:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (460365ms till timeout)
2022-03-30 21:36:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (475602ms till timeout)
2022-03-30 21:36:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:36:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-5 get Namespace namespace-4 -o yaml
2022-03-30 21:36:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 21:36:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 21:36:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-4" not found
2022-03-30 21:36:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 21:36:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[namespace-5], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@dce7372d=[rolling-update-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-3], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-0], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-30 21:36:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:267] testSendMessagesCustomListenerTlsScramSha - Notifies waiting test cases:[testCruiseControlBasicAPIRequests, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testUpdateUser, testSendSimpleMessageTls, testAutoRenewAllCaCertsTriggeredByAnno, testReceiveSimpleMessageTls, testSendMessagesCustomListenerTlsScramSha, testKafkaAndZookeeperScaleUpScaleDown] to and randomly select one to start execution
2022-03-30 21:36:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:93] [kafka.listeners.ListenersST] - Removing parallel test: testSendMessagesCustomListenerTlsScramSha
2022-03-30 21:36:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:97] [kafka.listeners.ListenersST] - Parallel test count: 3
2022-03-30 21:36:16 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesCustomListenerTlsScramSha-FINISHED
2022-03-30 21:36:16 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 21:36:16 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:16 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (474527ms till timeout)
2022-03-30 21:36:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 21:36:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:690] [kafka.listeners.ListenersST - After All] - Clean up after test suite
2022-03-30 21:36:16 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 21:36:16 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:346] In context ListenersST is everything deleted.
2022-03-30 21:36:16 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Running io.strimzi.systemtest.watcher.AllNamespaceIsolatedST
2022-03-30 21:36:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 21:36:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:667] [watcher.AllNamespaceIsolatedST - Before All] - Setup test suite environment
2022-03-30 21:36:16 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 21:36:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-30 21:36:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Namespace listeners-st removal
2022-03-30 21:36:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-30 21:36:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-30 21:36:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace listeners-st removal not ready, will try again in 1000 ms (479922ms till timeout)
2022-03-30 21:36:16 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:36:16 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=8739d439-0656-4462-acf7-6c4cd1de5698}
2022-03-30 21:36:16 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=8739d439-0656-4462-acf7-6c4cd1de5698}
2022-03-30 21:36:16 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-zookeeper-1 hasn't rolled
2022-03-30 21:36:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-zookeeper rolling update not ready, will try again in 5000 ms (1749934ms till timeout)
2022-03-30 21:36:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (473447ms till timeout)
2022-03-30 21:36:17 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-30 21:36:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-30 21:36:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:17 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace listeners-st removal not ready, will try again in 1000 ms (478849ms till timeout)
2022-03-30 21:36:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 21:36:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-30 21:36:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:18 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-30 21:36:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (472363ms till timeout)
2022-03-30 21:36:18 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-30 21:36:18 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:18 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace listeners-st removal not ready, will try again in 1000 ms (477772ms till timeout)
2022-03-30 21:36:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=7db81283-b62b-400d-8103-f4bdc33b13fe, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-1 hasn't rolled
2022-03-30 21:36:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (1784978ms till timeout)
2022-03-30 21:36:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-30 21:36:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (471286ms till timeout)
2022-03-30 21:36:19 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-30 21:36:19 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace listeners-st removal not ready, will try again in 1000 ms (476688ms till timeout)
2022-03-30 21:36:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:20 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:20 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (470211ms till timeout)
2022-03-30 21:36:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-30 21:36:20 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-30 21:36:20 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace listeners-st removal not ready, will try again in 1000 ms (475605ms till timeout)
2022-03-30 21:36:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 5
2022-03-30 21:36:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-30 21:36:21 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:21 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (469114ms till timeout)
2022-03-30 21:36:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-30 21:36:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 21:36:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 21:36:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "listeners-st" not found
2022-03-30 21:36:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 21:36:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[namespace-5], io.strimzi.test.logs.CollectorElement@3d7a59f3=[], io.strimzi.test.logs.CollectorElement@dce7372d=[rolling-update-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-3], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-0], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-30 21:36:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:254] ListenersST - Notifies waiting test suites:[CruiseControlST, HttpBridgeTlsST, UserST, CruiseControlApiST, ListenersST, SecurityST, RollingUpdateST] to and randomly select one to start execution
2022-03-30 21:36:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:85] [kafka.listeners.ListenersST] - Removing parallel suite: ListenersST
2022-03-30 21:36:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:89] [kafka.listeners.ListenersST] - Parallel suites count: 4
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 416.947 s - in io.strimzi.systemtest.kafka.listeners.ListenersST
[[1;34mINFO[m] Running io.strimzi.systemtest.connect.ConnectIsolatedST
2022-03-30 21:36:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 21:36:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:667] [connect.ConnectIsolatedST - Before All] - Setup test suite environment
2022-03-30 21:36:21 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 21:36:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-30 21:36:21 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:36:21 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=8739d439-0656-4462-acf7-6c4cd1de5698}
2022-03-30 21:36:21 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=8739d439-0656-4462-acf7-6c4cd1de5698}
2022-03-30 21:36:21 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-zookeeper-1 hasn't rolled
2022-03-30 21:36:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-zookeeper rolling update not ready, will try again in 5000 ms (1744928ms till timeout)
2022-03-30 21:36:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 21:36:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (468018ms till timeout)
2022-03-30 21:36:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 21:36:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=7db81283-b62b-400d-8103-f4bdc33b13fe, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-1 hasn't rolled
2022-03-30 21:36:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (1779972ms till timeout)
2022-03-30 21:36:23 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:23 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (466931ms till timeout)
2022-03-30 21:36:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:24 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:24 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (465851ms till timeout)
2022-03-30 21:36:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:25 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:25 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (464782ms till timeout)
2022-03-30 21:36:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 21:36:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-30 21:36:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:36:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=8739d439-0656-4462-acf7-6c4cd1de5698}
2022-03-30 21:36:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=8739d439-0656-4462-acf7-6c4cd1de5698}
2022-03-30 21:36:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-zookeeper-1 hasn't rolled
2022-03-30 21:36:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-zookeeper rolling update not ready, will try again in 5000 ms (1739923ms till timeout)
2022-03-30 21:36:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (463712ms till timeout)
2022-03-30 21:36:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 21:36:27 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:27 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:27 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:27 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (462636ms till timeout)
2022-03-30 21:36:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 21:36:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=7db81283-b62b-400d-8103-f4bdc33b13fe, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-1 hasn't rolled
2022-03-30 21:36:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (1774967ms till timeout)
2022-03-30 21:36:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:29 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:29 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (461550ms till timeout)
2022-03-30 21:36:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (460472ms till timeout)
2022-03-30 21:36:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 21:36:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:31 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:31 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (459384ms till timeout)
2022-03-30 21:36:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-30 21:36:31 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:36:31 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-zookeeper-0=bc4644f2-5d85-46a5-9332-9f459e74e22e, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=8739d439-0656-4462-acf7-6c4cd1de5698}
2022-03-30 21:36:31 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-zookeeper-0=bc4644f2-5d85-46a5-9332-9f459e74e22e, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=8739d439-0656-4462-acf7-6c4cd1de5698}
2022-03-30 21:36:31 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-zookeeper-1 hasn't rolled
2022-03-30 21:36:31 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-zookeeper rolling update not ready, will try again in 5000 ms (1734917ms till timeout)
2022-03-30 21:36:32 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:32 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:32 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:32 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (458309ms till timeout)
2022-03-30 21:36:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 21:36:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 21:36:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:33 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:33 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (457227ms till timeout)
2022-03-30 21:36:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=7db81283-b62b-400d-8103-f4bdc33b13fe, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-1 hasn't rolled
2022-03-30 21:36:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (1769961ms till timeout)
2022-03-30 21:36:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:34 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:34 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (456123ms till timeout)
2022-03-30 21:36:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:35 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:35 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (455053ms till timeout)
2022-03-30 21:36:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 21:36:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-30 21:36:36 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:36 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (453980ms till timeout)
2022-03-30 21:36:36 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:36:36 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-zookeeper-0=bc4644f2-5d85-46a5-9332-9f459e74e22e, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=8739d439-0656-4462-acf7-6c4cd1de5698}
2022-03-30 21:36:36 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-zookeeper-0=bc4644f2-5d85-46a5-9332-9f459e74e22e, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=8739d439-0656-4462-acf7-6c4cd1de5698}
2022-03-30 21:36:36 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-zookeeper-1 hasn't rolled
2022-03-30 21:36:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-zookeeper rolling update not ready, will try again in 5000 ms (1729912ms till timeout)
2022-03-30 21:36:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 21:36:37 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:37 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:37 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:37 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (452903ms till timeout)
2022-03-30 21:36:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 21:36:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=7db81283-b62b-400d-8103-f4bdc33b13fe, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-1 hasn't rolled
2022-03-30 21:36:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (1764956ms till timeout)
2022-03-30 21:36:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:38 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:38 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (451822ms till timeout)
2022-03-30 21:36:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:39 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:39 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (450743ms till timeout)
2022-03-30 21:36:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:40 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:40 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (449656ms till timeout)
2022-03-30 21:36:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 21:36:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-30 21:36:41 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:36:41 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-zookeeper-0=bc4644f2-5d85-46a5-9332-9f459e74e22e, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=8739d439-0656-4462-acf7-6c4cd1de5698}
2022-03-30 21:36:41 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-zookeeper-0=bc4644f2-5d85-46a5-9332-9f459e74e22e, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=8739d439-0656-4462-acf7-6c4cd1de5698}
2022-03-30 21:36:41 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-zookeeper-1 hasn't rolled
2022-03-30 21:36:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-zookeeper rolling update not ready, will try again in 5000 ms (1724907ms till timeout)
2022-03-30 21:36:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:42 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:42 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:42 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (448581ms till timeout)
2022-03-30 21:36:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 21:36:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 21:36:43 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:43 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (447492ms till timeout)
2022-03-30 21:36:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=7db81283-b62b-400d-8103-f4bdc33b13fe, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-1 hasn't rolled
2022-03-30 21:36:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (1759950ms till timeout)
2022-03-30 21:36:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:44 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:44 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (446416ms till timeout)
2022-03-30 21:36:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:45 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:45 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (445343ms till timeout)
2022-03-30 21:36:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 21:36:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:46 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:46 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (444270ms till timeout)
2022-03-30 21:36:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-30 21:36:46 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:36:46 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-zookeeper-0=bc4644f2-5d85-46a5-9332-9f459e74e22e, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=8739d439-0656-4462-acf7-6c4cd1de5698}
2022-03-30 21:36:46 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-zookeeper-0=bc4644f2-5d85-46a5-9332-9f459e74e22e, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=8739d439-0656-4462-acf7-6c4cd1de5698}
2022-03-30 21:36:46 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-zookeeper-1 hasn't rolled
2022-03-30 21:36:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-zookeeper rolling update not ready, will try again in 5000 ms (1719901ms till timeout)
2022-03-30 21:36:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 21:36:47 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:47 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (443191ms till timeout)
2022-03-30 21:36:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 21:36:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:48 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:48 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (442111ms till timeout)
2022-03-30 21:36:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=7db81283-b62b-400d-8103-f4bdc33b13fe, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-1 hasn't rolled
2022-03-30 21:36:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (1754945ms till timeout)
2022-03-30 21:36:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:49 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:49 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (441030ms till timeout)
2022-03-30 21:36:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:50 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:50 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (439949ms till timeout)
2022-03-30 21:36:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 21:36:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-30 21:36:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:51 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:51 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (438877ms till timeout)
2022-03-30 21:36:51 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:36:51 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-zookeeper-0=bc4644f2-5d85-46a5-9332-9f459e74e22e, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=8739d439-0656-4462-acf7-6c4cd1de5698}
2022-03-30 21:36:51 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-zookeeper-0=bc4644f2-5d85-46a5-9332-9f459e74e22e, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=8739d439-0656-4462-acf7-6c4cd1de5698}
2022-03-30 21:36:51 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-zookeeper-1 hasn't rolled
2022-03-30 21:36:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-zookeeper rolling update not ready, will try again in 5000 ms (1714896ms till timeout)
2022-03-30 21:36:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 21:36:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (437805ms till timeout)
2022-03-30 21:36:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 21:36:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=7db81283-b62b-400d-8103-f4bdc33b13fe, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-1 hasn't rolled
2022-03-30 21:36:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (1749939ms till timeout)
2022-03-30 21:36:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace infra-namespace get Namespace namespace-0 -o yaml
2022-03-30 21:36:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 21:36:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 21:36:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-0" not found
2022-03-30 21:36:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 21:36:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[namespace-5], io.strimzi.test.logs.CollectorElement@3d7a59f3=[], io.strimzi.test.logs.CollectorElement@dce7372d=[rolling-update-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-3], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-30 21:36:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:267] testCruiseControlBasicAPIRequests - Notifies waiting test cases:[testCruiseControlBasicAPIRequests, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testUpdateUser, testSendSimpleMessageTls, testAutoRenewAllCaCertsTriggeredByAnno, testReceiveSimpleMessageTls, testSendMessagesCustomListenerTlsScramSha, testKafkaAndZookeeperScaleUpScaleDown] to and randomly select one to start execution
2022-03-30 21:36:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:93] [cruisecontrol.CruiseControlApiST] - Removing parallel test: testCruiseControlBasicAPIRequests
2022-03-30 21:36:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:97] [cruisecontrol.CruiseControlApiST] - Parallel test count: 2
2022-03-30 21:36:53 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlApiST.testCruiseControlBasicAPIRequests-FINISHED
2022-03-30 21:36:53 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 21:36:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 21:36:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:690] [cruisecontrol.CruiseControlApiST - After All] - Clean up after test suite
2022-03-30 21:36:53 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 21:36:53 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:346] In context CruiseControlApiST is everything deleted.
2022-03-30 21:36:53 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Running io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST
2022-03-30 21:36:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 21:36:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:667] [mirrormaker.MirrorMaker2IsolatedST - Before All] - Setup test suite environment
2022-03-30 21:36:53 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 21:36:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-30 21:36:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Namespace cruise-control-api-st removal
2022-03-30 21:36:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-30 21:36:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-30 21:36:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace cruise-control-api-st removal not ready, will try again in 1000 ms (479923ms till timeout)
2022-03-30 21:36:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-30 21:36:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-30 21:36:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace cruise-control-api-st removal not ready, will try again in 1000 ms (478844ms till timeout)
2022-03-30 21:36:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-30 21:36:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 21:36:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-30 21:36:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace cruise-control-api-st removal not ready, will try again in 1000 ms (477771ms till timeout)
2022-03-30 21:36:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 4
2022-03-30 21:36:56 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:36:56 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-zookeeper-0=bc4644f2-5d85-46a5-9332-9f459e74e22e, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=8739d439-0656-4462-acf7-6c4cd1de5698}
2022-03-30 21:36:56 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-zookeeper-0=bc4644f2-5d85-46a5-9332-9f459e74e22e, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=8739d439-0656-4462-acf7-6c4cd1de5698}
2022-03-30 21:36:56 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-zookeeper-1 hasn't rolled
2022-03-30 21:36:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-zookeeper rolling update not ready, will try again in 5000 ms (1709891ms till timeout)
2022-03-30 21:36:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-30 21:36:57 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-30 21:36:57 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace cruise-control-api-st removal not ready, will try again in 1000 ms (476696ms till timeout)
2022-03-30 21:36:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 21:36:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 21:36:58 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-30 21:36:58 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-30 21:36:58 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:36:58 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace cruise-control-api-st removal not ready, will try again in 1000 ms (475621ms till timeout)
2022-03-30 21:36:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=7db81283-b62b-400d-8103-f4bdc33b13fe, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:36:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-1 hasn't rolled
2022-03-30 21:36:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (1744934ms till timeout)
2022-03-30 21:36:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:36:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 4
2022-03-30 21:36:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-30 21:36:59 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-30 21:36:59 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 21:36:59 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 21:36:59 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "cruise-control-api-st" not found
2022-03-30 21:36:59 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 21:36:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[namespace-5], io.strimzi.test.logs.CollectorElement@3d7a59f3=[], io.strimzi.test.logs.CollectorElement@dce7372d=[rolling-update-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-3], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-30 21:36:59 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:254] CruiseControlApiST - Notifies waiting test suites:[CruiseControlST, HttpBridgeTlsST, UserST, CruiseControlApiST, ListenersST, SecurityST, RollingUpdateST] to and randomly select one to start execution
2022-03-30 21:36:59 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:85] [cruisecontrol.CruiseControlApiST] - Removing parallel suite: CruiseControlApiST
2022-03-30 21:36:59 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:89] [cruisecontrol.CruiseControlApiST] - Parallel suites count: 3
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 503.068 s - in io.strimzi.systemtest.cruisecontrol.CruiseControlApiST
[[1;34mINFO[m] Running io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST
2022-03-30 21:36:59 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 21:36:59 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:667] [mirrormaker.MirrorMakerIsolatedST - Before All] - Setup test suite environment
2022-03-30 21:36:59 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 21:36:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:01 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-zookeeper-0=b629af33-a157-48b7-bdb8-1340f9c4939f, my-cluster-58833349-zookeeper-1=65d1d5e5-be66-428f-8b98-6969907d7465, my-cluster-58833349-zookeeper-2=7c93e517-b29e-4af7-8eb3-2af102c84f0b}
2022-03-30 21:37:01 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-zookeeper-0=bc4644f2-5d85-46a5-9332-9f459e74e22e, my-cluster-58833349-zookeeper-1=00be6324-89e4-4358-a1e0-47a5ea7d25bd, my-cluster-58833349-zookeeper-2=8739d439-0656-4462-acf7-6c4cd1de5698}
2022-03-30 21:37:01 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-zookeeper-0=bc4644f2-5d85-46a5-9332-9f459e74e22e, my-cluster-58833349-zookeeper-1=00be6324-89e4-4358-a1e0-47a5ea7d25bd, my-cluster-58833349-zookeeper-2=8739d439-0656-4462-acf7-6c4cd1de5698}
2022-03-30 21:37:01 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:55] All pods seem to have rolled
2022-03-30 21:37:01 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-58833349-zookeeper has been successfully rolled
2022-03-30 21:37:01 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:87] Component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={}) successfully rolled
2022-03-30 21:37:01 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-58833349-zookeeper to be ready
2022-03-30 21:37:01 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-30 21:37:01 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:01 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1799996ms till timeout)
2022-03-30 21:37:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:02 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:02 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1798991ms till timeout)
2022-03-30 21:37:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=7db81283-b62b-400d-8103-f4bdc33b13fe, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-1 hasn't rolled
2022-03-30 21:37:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (1739928ms till timeout)
2022-03-30 21:37:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1797987ms till timeout)
2022-03-30 21:37:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:37:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:04 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:04 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1796982ms till timeout)
2022-03-30 21:37:05 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:05 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1795974ms till timeout)
2022-03-30 21:37:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:06 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:06 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1794964ms till timeout)
2022-03-30 21:37:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:07 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:07 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1793955ms till timeout)
2022-03-30 21:37:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=7db81283-b62b-400d-8103-f4bdc33b13fe, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-2 hasn't rolled
2022-03-30 21:37:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (1734923ms till timeout)
2022-03-30 21:37:08 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:08 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1792951ms till timeout)
2022-03-30 21:37:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:37:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:09 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:09 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1791946ms till timeout)
2022-03-30 21:37:10 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:10 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1790941ms till timeout)
2022-03-30 21:37:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:11 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:11 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1789935ms till timeout)
2022-03-30 21:37:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:12 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:12 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1788931ms till timeout)
2022-03-30 21:37:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=7db81283-b62b-400d-8103-f4bdc33b13fe, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-2 hasn't rolled
2022-03-30 21:37:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (1729917ms till timeout)
2022-03-30 21:37:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1787926ms till timeout)
2022-03-30 21:37:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:37:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:14 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:14 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1786921ms till timeout)
2022-03-30 21:37:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1785915ms till timeout)
2022-03-30 21:37:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:16 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:16 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1784910ms till timeout)
2022-03-30 21:37:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:17 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:17 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1783904ms till timeout)
2022-03-30 21:37:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=7db81283-b62b-400d-8103-f4bdc33b13fe, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-2 hasn't rolled
2022-03-30 21:37:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (1724912ms till timeout)
2022-03-30 21:37:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:37:18 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:18 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1782899ms till timeout)
2022-03-30 21:37:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1781894ms till timeout)
2022-03-30 21:37:20 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:20 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1780889ms till timeout)
2022-03-30 21:37:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:21 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:21 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1779884ms till timeout)
2022-03-30 21:37:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:22 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:22 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1778879ms till timeout)
2022-03-30 21:37:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=7db81283-b62b-400d-8103-f4bdc33b13fe, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-2 hasn't rolled
2022-03-30 21:37:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (1719906ms till timeout)
2022-03-30 21:37:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:37:23 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:23 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:23 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1777874ms till timeout)
2022-03-30 21:37:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1776868ms till timeout)
2022-03-30 21:37:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1775863ms till timeout)
2022-03-30 21:37:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1774858ms till timeout)
2022-03-30 21:37:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:27 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:27 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1773853ms till timeout)
2022-03-30 21:37:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=7db81283-b62b-400d-8103-f4bdc33b13fe, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-2 hasn't rolled
2022-03-30 21:37:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (1714901ms till timeout)
2022-03-30 21:37:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:37:28 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:28 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:28 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1772848ms till timeout)
2022-03-30 21:37:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:29 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:29 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:29 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1771843ms till timeout)
2022-03-30 21:37:30 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:30 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:30 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1770838ms till timeout)
2022-03-30 21:37:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:31 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:31 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:31 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1769832ms till timeout)
2022-03-30 21:37:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:32 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:32 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1768827ms till timeout)
2022-03-30 21:37:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=7db81283-b62b-400d-8103-f4bdc33b13fe, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-2 hasn't rolled
2022-03-30 21:37:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (1709895ms till timeout)
2022-03-30 21:37:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:37:34 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:34 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:34 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1767822ms till timeout)
2022-03-30 21:37:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:35 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:35 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:35 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1766817ms till timeout)
2022-03-30 21:37:36 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:36 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1765811ms till timeout)
2022-03-30 21:37:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:37 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:37 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1764806ms till timeout)
2022-03-30 21:37:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:38 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:38 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-zookeeper-1)
2022-03-30 21:37:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1763800ms till timeout)
2022-03-30 21:37:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=7db81283-b62b-400d-8103-f4bdc33b13fe, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-2 hasn't rolled
2022-03-30 21:37:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (1704890ms till timeout)
2022-03-30 21:37:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:37:39 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:39 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-1 not ready: zookeeper)
2022-03-30 21:37:39 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-2 not ready: zookeeper)
2022-03-30 21:37:39 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-zookeeper-0, my-cluster-58833349-zookeeper-1, my-cluster-58833349-zookeeper-2 are ready
2022-03-30 21:37:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1762795ms till timeout)
2022-03-30 21:37:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:40 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:40 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-1 not ready: zookeeper)
2022-03-30 21:37:40 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-2 not ready: zookeeper)
2022-03-30 21:37:40 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-zookeeper-0, my-cluster-58833349-zookeeper-1, my-cluster-58833349-zookeeper-2 are ready
2022-03-30 21:37:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1761790ms till timeout)
2022-03-30 21:37:41 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:41 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-1 not ready: zookeeper)
2022-03-30 21:37:41 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-2 not ready: zookeeper)
2022-03-30 21:37:41 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-zookeeper-0, my-cluster-58833349-zookeeper-1, my-cluster-58833349-zookeeper-2 are ready
2022-03-30 21:37:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1760785ms till timeout)
2022-03-30 21:37:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:42 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:42 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-1 not ready: zookeeper)
2022-03-30 21:37:42 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-2 not ready: zookeeper)
2022-03-30 21:37:42 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-zookeeper-0, my-cluster-58833349-zookeeper-1, my-cluster-58833349-zookeeper-2 are ready
2022-03-30 21:37:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1759780ms till timeout)
2022-03-30 21:37:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:43 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:43 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-1 not ready: zookeeper)
2022-03-30 21:37:43 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-2 not ready: zookeeper)
2022-03-30 21:37:43 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-zookeeper-0, my-cluster-58833349-zookeeper-1, my-cluster-58833349-zookeeper-2 are ready
2022-03-30 21:37:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1758772ms till timeout)
2022-03-30 21:37:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=7db81283-b62b-400d-8103-f4bdc33b13fe, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-2 hasn't rolled
2022-03-30 21:37:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (1699885ms till timeout)
2022-03-30 21:37:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:37:44 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:44 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-1 not ready: zookeeper)
2022-03-30 21:37:44 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-2 not ready: zookeeper)
2022-03-30 21:37:44 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-zookeeper-0, my-cluster-58833349-zookeeper-1, my-cluster-58833349-zookeeper-2 are ready
2022-03-30 21:37:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1757766ms till timeout)
2022-03-30 21:37:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:45 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:45 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-1 not ready: zookeeper)
2022-03-30 21:37:45 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-2 not ready: zookeeper)
2022-03-30 21:37:45 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-zookeeper-0, my-cluster-58833349-zookeeper-1, my-cluster-58833349-zookeeper-2 are ready
2022-03-30 21:37:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1756761ms till timeout)
2022-03-30 21:37:46 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:46 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-1 not ready: zookeeper)
2022-03-30 21:37:46 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-2 not ready: zookeeper)
2022-03-30 21:37:46 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-zookeeper-0, my-cluster-58833349-zookeeper-1, my-cluster-58833349-zookeeper-2 are ready
2022-03-30 21:37:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1755755ms till timeout)
2022-03-30 21:37:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:47 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:47 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-1 not ready: zookeeper)
2022-03-30 21:37:47 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-2 not ready: zookeeper)
2022-03-30 21:37:47 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-zookeeper-0, my-cluster-58833349-zookeeper-1, my-cluster-58833349-zookeeper-2 are ready
2022-03-30 21:37:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1754750ms till timeout)
2022-03-30 21:37:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:48 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:48 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-1 not ready: zookeeper)
2022-03-30 21:37:48 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-2 not ready: zookeeper)
2022-03-30 21:37:48 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-zookeeper-0, my-cluster-58833349-zookeeper-1, my-cluster-58833349-zookeeper-2 are ready
2022-03-30 21:37:48 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-zookeeper, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1753745ms till timeout)
2022-03-30 21:37:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=7db81283-b62b-400d-8103-f4bdc33b13fe, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-2 hasn't rolled
2022-03-30 21:37:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (1694879ms till timeout)
2022-03-30 21:37:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:37:49 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-0 not ready: zookeeper)
2022-03-30 21:37:49 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-1 not ready: zookeeper)
2022-03-30 21:37:49 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-zookeeper-2 not ready: zookeeper)
2022-03-30 21:37:49 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-zookeeper-0, my-cluster-58833349-zookeeper-1, my-cluster-58833349-zookeeper-2 are ready
2022-03-30 21:37:49 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:304] Wait for kafka to rolling restart ...
2022-03-30 21:37:49 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-58833349-kafka rolling update
2022-03-30 21:37:49 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:74] Waiting for rolling update of component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-kafka, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})
2022-03-30 21:37:49 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for component with name my-cluster-58833349-kafka rolling update
2022-03-30 21:37:49 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-kafka-0=06b6601d-7396-4887-8081-dab74ae6685c, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:37:49 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-kafka-0=06b6601d-7396-4887-8081-dab74ae6685c, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:37:49 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-kafka-0=06b6601d-7396-4887-8081-dab74ae6685c, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:37:49 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-kafka-1 hasn't rolled
2022-03-30 21:37:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-kafka rolling update not ready, will try again in 5000 ms (1799996ms till timeout)
2022-03-30 21:37:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=7db81283-b62b-400d-8103-f4bdc33b13fe, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-2 hasn't rolled
2022-03-30 21:37:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (1689874ms till timeout)
2022-03-30 21:37:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:37:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-kafka-0=06b6601d-7396-4887-8081-dab74ae6685c, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:37:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-kafka-0=397a1b3f-874d-427d-8487-49228cb1c4b3, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:37:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-kafka-0=397a1b3f-874d-427d-8487-49228cb1c4b3, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:37:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-kafka-1 hasn't rolled
2022-03-30 21:37:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-kafka rolling update not ready, will try again in 5000 ms (1794990ms till timeout)
2022-03-30 21:37:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=7db81283-b62b-400d-8103-f4bdc33b13fe, my-cluster-94dd1070-kafka-1=5a29f6e4-b93f-4855-81c9-621da665ff79, my-cluster-94dd1070-kafka-2=bb8e19a2-95d7-47b1-97c3-5edc7fe6bbd2}
2022-03-30 21:37:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7}
2022-03-30 21:37:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7}
2022-03-30 21:37:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:55] All pods seem to have rolled
2022-03-30 21:37:58 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-94dd1070-kafka has been successfully rolled
2022-03-30 21:37:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:87] Component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) successfully rolled
2022-03-30 21:37:58 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:127] Waiting for 7 Pod(s) of my-cluster-94dd1070-kafka to be ready
2022-03-30 21:37:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-30 21:37:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:37:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4199996ms till timeout)
2022-03-30 21:37:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:37:59 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-kafka-0=06b6601d-7396-4887-8081-dab74ae6685c, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:37:59 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-kafka-0=397a1b3f-874d-427d-8487-49228cb1c4b3, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:37:59 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-kafka-0=397a1b3f-874d-427d-8487-49228cb1c4b3, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:37:59 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-kafka-1 hasn't rolled
2022-03-30 21:37:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-kafka rolling update not ready, will try again in 5000 ms (1789983ms till timeout)
2022-03-30 21:37:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:37:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:37:59 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:37:59 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4198990ms till timeout)
2022-03-30 21:38:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:38:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4197985ms till timeout)
2022-03-30 21:38:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:01 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:38:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4196980ms till timeout)
2022-03-30 21:38:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:02 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:38:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4195974ms till timeout)
2022-03-30 21:38:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:38:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4194969ms till timeout)
2022-03-30 21:38:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:38:04 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-kafka-0=06b6601d-7396-4887-8081-dab74ae6685c, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:38:04 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-kafka-0=397a1b3f-874d-427d-8487-49228cb1c4b3, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:38:04 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-kafka-0=397a1b3f-874d-427d-8487-49228cb1c4b3, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:38:04 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-kafka-1 hasn't rolled
2022-03-30 21:38:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-kafka rolling update not ready, will try again in 5000 ms (1784978ms till timeout)
2022-03-30 21:38:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:38:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4193964ms till timeout)
2022-03-30 21:38:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:38:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4192959ms till timeout)
2022-03-30 21:38:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:38:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4191954ms till timeout)
2022-03-30 21:38:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:07 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:38:07 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4190949ms till timeout)
2022-03-30 21:38:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:38:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4189944ms till timeout)
2022-03-30 21:38:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:38:09 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-kafka-0=06b6601d-7396-4887-8081-dab74ae6685c, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:38:09 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-kafka-0=397a1b3f-874d-427d-8487-49228cb1c4b3, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:38:09 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-kafka-0=397a1b3f-874d-427d-8487-49228cb1c4b3, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:38:09 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-kafka-1 hasn't rolled
2022-03-30 21:38:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-kafka rolling update not ready, will try again in 5000 ms (1779972ms till timeout)
2022-03-30 21:38:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:38:09 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4188939ms till timeout)
2022-03-30 21:38:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:38:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4187934ms till timeout)
2022-03-30 21:38:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:38:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4186929ms till timeout)
2022-03-30 21:38:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:38:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4185924ms till timeout)
2022-03-30 21:38:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:38:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4184919ms till timeout)
2022-03-30 21:38:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:38:14 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-kafka-0=06b6601d-7396-4887-8081-dab74ae6685c, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:38:14 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-kafka-0=397a1b3f-874d-427d-8487-49228cb1c4b3, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:38:14 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-kafka-0=397a1b3f-874d-427d-8487-49228cb1c4b3, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:38:14 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-kafka-1 hasn't rolled
2022-03-30 21:38:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-kafka rolling update not ready, will try again in 5000 ms (1774967ms till timeout)
2022-03-30 21:38:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:38:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4183914ms till timeout)
2022-03-30 21:38:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:38:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4182908ms till timeout)
2022-03-30 21:38:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:38:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4181903ms till timeout)
2022-03-30 21:38:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:38:17 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4180898ms till timeout)
2022-03-30 21:38:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-3)
2022-03-30 21:38:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4179892ms till timeout)
2022-03-30 21:38:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:38:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-kafka-0=06b6601d-7396-4887-8081-dab74ae6685c, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:38:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-kafka-0=397a1b3f-874d-427d-8487-49228cb1c4b3, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:38:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-kafka-0=397a1b3f-874d-427d-8487-49228cb1c4b3, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:38:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-kafka-1 hasn't rolled
2022-03-30 21:38:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-kafka rolling update not ready, will try again in 5000 ms (1769961ms till timeout)
2022-03-30 21:38:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-3)
2022-03-30 21:38:19 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4178885ms till timeout)
2022-03-30 21:38:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-3)
2022-03-30 21:38:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4177875ms till timeout)
2022-03-30 21:38:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-3)
2022-03-30 21:38:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4176868ms till timeout)
2022-03-30 21:38:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-3)
2022-03-30 21:38:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4175861ms till timeout)
2022-03-30 21:38:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-3)
2022-03-30 21:38:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4174852ms till timeout)
2022-03-30 21:38:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:38:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-kafka-0=06b6601d-7396-4887-8081-dab74ae6685c, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:38:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-kafka-0=397a1b3f-874d-427d-8487-49228cb1c4b3, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:38:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-kafka-0=397a1b3f-874d-427d-8487-49228cb1c4b3, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:38:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-kafka-1 hasn't rolled
2022-03-30 21:38:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-kafka rolling update not ready, will try again in 5000 ms (1764954ms till timeout)
2022-03-30 21:38:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-3)
2022-03-30 21:38:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4173845ms till timeout)
2022-03-30 21:38:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-3)
2022-03-30 21:38:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4172837ms till timeout)
2022-03-30 21:38:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-3)
2022-03-30 21:38:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4171826ms till timeout)
2022-03-30 21:38:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-3)
2022-03-30 21:38:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4170820ms till timeout)
2022-03-30 21:38:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:38:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-3)
2022-03-30 21:38:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4169811ms till timeout)
2022-03-30 21:38:29 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-kafka-0=06b6601d-7396-4887-8081-dab74ae6685c, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:38:29 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-kafka-0=397a1b3f-874d-427d-8487-49228cb1c4b3, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:38:29 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-kafka-0=397a1b3f-874d-427d-8487-49228cb1c4b3, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:38:29 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-kafka-1 hasn't rolled
2022-03-30 21:38:29 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-kafka rolling update not ready, will try again in 5000 ms (1759949ms till timeout)
2022-03-30 21:38:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:29 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:29 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:29 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:29 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-3)
2022-03-30 21:38:29 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4168805ms till timeout)
2022-03-30 21:38:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-3)
2022-03-30 21:38:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4167799ms till timeout)
2022-03-30 21:38:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:31 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:31 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:31 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:31 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-3)
2022-03-30 21:38:31 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4166793ms till timeout)
2022-03-30 21:38:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:32 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:32 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:32 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:32 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-3)
2022-03-30 21:38:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4165787ms till timeout)
2022-03-30 21:38:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:38:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-3)
2022-03-30 21:38:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4164780ms till timeout)
2022-03-30 21:38:34 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-kafka-0=06b6601d-7396-4887-8081-dab74ae6685c, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:38:34 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-kafka-0=397a1b3f-874d-427d-8487-49228cb1c4b3, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:38:34 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-kafka-0=397a1b3f-874d-427d-8487-49228cb1c4b3, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:38:34 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-kafka-1 hasn't rolled
2022-03-30 21:38:34 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-kafka rolling update not ready, will try again in 5000 ms (1754943ms till timeout)
2022-03-30 21:38:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:34 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:34 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:34 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:34 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-3)
2022-03-30 21:38:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4163774ms till timeout)
2022-03-30 21:38:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-3)
2022-03-30 21:38:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4162758ms till timeout)
2022-03-30 21:38:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-3)
2022-03-30 21:38:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4161752ms till timeout)
2022-03-30 21:38:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:37 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:37 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:37 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:37 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-3)
2022-03-30 21:38:37 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4160746ms till timeout)
2022-03-30 21:38:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:38:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-3)
2022-03-30 21:38:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4159740ms till timeout)
2022-03-30 21:38:39 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-kafka-0=06b6601d-7396-4887-8081-dab74ae6685c, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:38:39 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-kafka-0=397a1b3f-874d-427d-8487-49228cb1c4b3, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:38:39 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-kafka-0=397a1b3f-874d-427d-8487-49228cb1c4b3, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:38:39 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-kafka-1 hasn't rolled
2022-03-30 21:38:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-kafka rolling update not ready, will try again in 5000 ms (1749938ms till timeout)
2022-03-30 21:38:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-3)
2022-03-30 21:38:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4158735ms till timeout)
2022-03-30 21:38:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-3)
2022-03-30 21:38:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4157729ms till timeout)
2022-03-30 21:38:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:41 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:41 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:41 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:41 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-3)
2022-03-30 21:38:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4156724ms till timeout)
2022-03-30 21:38:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-3 not ready: kafka)
2022-03-30 21:38:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-4 not ready: kafka)
2022-03-30 21:38:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-5 not ready: kafka)
2022-03-30 21:38:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-6 not ready: kafka)
2022-03-30 21:38:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-94dd1070-kafka-0, my-cluster-94dd1070-kafka-1, my-cluster-94dd1070-kafka-2, my-cluster-94dd1070-kafka-3, my-cluster-94dd1070-kafka-4, my-cluster-94dd1070-kafka-5, my-cluster-94dd1070-kafka-6 are ready
2022-03-30 21:38:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4155718ms till timeout)
2022-03-30 21:38:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:38:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-3 not ready: kafka)
2022-03-30 21:38:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-4 not ready: kafka)
2022-03-30 21:38:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-5 not ready: kafka)
2022-03-30 21:38:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-6 not ready: kafka)
2022-03-30 21:38:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-94dd1070-kafka-0, my-cluster-94dd1070-kafka-1, my-cluster-94dd1070-kafka-2, my-cluster-94dd1070-kafka-3, my-cluster-94dd1070-kafka-4, my-cluster-94dd1070-kafka-5, my-cluster-94dd1070-kafka-6 are ready
2022-03-30 21:38:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4154712ms till timeout)
2022-03-30 21:38:44 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-kafka-0=06b6601d-7396-4887-8081-dab74ae6685c, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:38:44 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-kafka-0=397a1b3f-874d-427d-8487-49228cb1c4b3, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=ecfe5ada-27e9-43da-9de4-72a06d3b2dfd}
2022-03-30 21:38:44 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-kafka-0=397a1b3f-874d-427d-8487-49228cb1c4b3, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=ecfe5ada-27e9-43da-9de4-72a06d3b2dfd}
2022-03-30 21:38:44 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-kafka-1 hasn't rolled
2022-03-30 21:38:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-kafka rolling update not ready, will try again in 5000 ms (1744932ms till timeout)
2022-03-30 21:38:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-3 not ready: kafka)
2022-03-30 21:38:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-4 not ready: kafka)
2022-03-30 21:38:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-5 not ready: kafka)
2022-03-30 21:38:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-6 not ready: kafka)
2022-03-30 21:38:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-94dd1070-kafka-0, my-cluster-94dd1070-kafka-1, my-cluster-94dd1070-kafka-2, my-cluster-94dd1070-kafka-3, my-cluster-94dd1070-kafka-4, my-cluster-94dd1070-kafka-5, my-cluster-94dd1070-kafka-6 are ready
2022-03-30 21:38:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4153705ms till timeout)
2022-03-30 21:38:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-3 not ready: kafka)
2022-03-30 21:38:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-4 not ready: kafka)
2022-03-30 21:38:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-5 not ready: kafka)
2022-03-30 21:38:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-6 not ready: kafka)
2022-03-30 21:38:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-94dd1070-kafka-0, my-cluster-94dd1070-kafka-1, my-cluster-94dd1070-kafka-2, my-cluster-94dd1070-kafka-3, my-cluster-94dd1070-kafka-4, my-cluster-94dd1070-kafka-5, my-cluster-94dd1070-kafka-6 are ready
2022-03-30 21:38:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4152693ms till timeout)
2022-03-30 21:38:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:47 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:47 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:47 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:47 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-3 not ready: kafka)
2022-03-30 21:38:47 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-4 not ready: kafka)
2022-03-30 21:38:47 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-5 not ready: kafka)
2022-03-30 21:38:47 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-6 not ready: kafka)
2022-03-30 21:38:47 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-94dd1070-kafka-0, my-cluster-94dd1070-kafka-1, my-cluster-94dd1070-kafka-2, my-cluster-94dd1070-kafka-3, my-cluster-94dd1070-kafka-4, my-cluster-94dd1070-kafka-5, my-cluster-94dd1070-kafka-6 are ready
2022-03-30 21:38:47 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4151687ms till timeout)
2022-03-30 21:38:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-3 not ready: kafka)
2022-03-30 21:38:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-4 not ready: kafka)
2022-03-30 21:38:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-5 not ready: kafka)
2022-03-30 21:38:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-6 not ready: kafka)
2022-03-30 21:38:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-94dd1070-kafka-0, my-cluster-94dd1070-kafka-1, my-cluster-94dd1070-kafka-2, my-cluster-94dd1070-kafka-3, my-cluster-94dd1070-kafka-4, my-cluster-94dd1070-kafka-5, my-cluster-94dd1070-kafka-6 are ready
2022-03-30 21:38:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4150680ms till timeout)
2022-03-30 21:38:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:38:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-3 not ready: kafka)
2022-03-30 21:38:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-4 not ready: kafka)
2022-03-30 21:38:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-5 not ready: kafka)
2022-03-30 21:38:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-6 not ready: kafka)
2022-03-30 21:38:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-94dd1070-kafka-0, my-cluster-94dd1070-kafka-1, my-cluster-94dd1070-kafka-2, my-cluster-94dd1070-kafka-3, my-cluster-94dd1070-kafka-4, my-cluster-94dd1070-kafka-5, my-cluster-94dd1070-kafka-6 are ready
2022-03-30 21:38:49 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4149674ms till timeout)
2022-03-30 21:38:49 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-kafka-0=06b6601d-7396-4887-8081-dab74ae6685c, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:38:49 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-kafka-0=397a1b3f-874d-427d-8487-49228cb1c4b3, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=ecfe5ada-27e9-43da-9de4-72a06d3b2dfd}
2022-03-30 21:38:49 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-kafka-0=397a1b3f-874d-427d-8487-49228cb1c4b3, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=ecfe5ada-27e9-43da-9de4-72a06d3b2dfd}
2022-03-30 21:38:49 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-kafka-1 hasn't rolled
2022-03-30 21:38:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-kafka rolling update not ready, will try again in 5000 ms (1739927ms till timeout)
2022-03-30 21:38:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-3 not ready: kafka)
2022-03-30 21:38:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-4 not ready: kafka)
2022-03-30 21:38:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-5 not ready: kafka)
2022-03-30 21:38:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-6 not ready: kafka)
2022-03-30 21:38:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-94dd1070-kafka-0, my-cluster-94dd1070-kafka-1, my-cluster-94dd1070-kafka-2, my-cluster-94dd1070-kafka-3, my-cluster-94dd1070-kafka-4, my-cluster-94dd1070-kafka-5, my-cluster-94dd1070-kafka-6 are ready
2022-03-30 21:38:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4148668ms till timeout)
2022-03-30 21:38:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-3 not ready: kafka)
2022-03-30 21:38:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-4 not ready: kafka)
2022-03-30 21:38:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-5 not ready: kafka)
2022-03-30 21:38:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-6 not ready: kafka)
2022-03-30 21:38:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-94dd1070-kafka-0, my-cluster-94dd1070-kafka-1, my-cluster-94dd1070-kafka-2, my-cluster-94dd1070-kafka-3, my-cluster-94dd1070-kafka-4, my-cluster-94dd1070-kafka-5, my-cluster-94dd1070-kafka-6 are ready
2022-03-30 21:38:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4147662ms till timeout)
2022-03-30 21:38:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-3 not ready: kafka)
2022-03-30 21:38:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-4 not ready: kafka)
2022-03-30 21:38:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-5 not ready: kafka)
2022-03-30 21:38:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-6 not ready: kafka)
2022-03-30 21:38:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-94dd1070-kafka-0, my-cluster-94dd1070-kafka-1, my-cluster-94dd1070-kafka-2, my-cluster-94dd1070-kafka-3, my-cluster-94dd1070-kafka-4, my-cluster-94dd1070-kafka-5, my-cluster-94dd1070-kafka-6 are ready
2022-03-30 21:38:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4146655ms till timeout)
2022-03-30 21:38:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:38:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:38:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:38:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-3 not ready: kafka)
2022-03-30 21:38:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-4 not ready: kafka)
2022-03-30 21:38:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-5 not ready: kafka)
2022-03-30 21:38:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-6 not ready: kafka)
2022-03-30 21:38:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-94dd1070-kafka-0, my-cluster-94dd1070-kafka-1, my-cluster-94dd1070-kafka-2, my-cluster-94dd1070-kafka-3, my-cluster-94dd1070-kafka-4, my-cluster-94dd1070-kafka-5, my-cluster-94dd1070-kafka-6 are ready
2022-03-30 21:38:53 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-94dd1070 will have desired state: Ready
2022-03-30 21:38:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-94dd1070 will have desired state: Ready
2022-03-30 21:38:53 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-94dd1070 is in desired state: Ready
2022-03-30 21:38:53 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-94dd1070 is ready
2022-03-30 21:38:53 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateST:327] Kafka scale up to 7 finished
2022-03-30 21:38:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Consumer will successfully receive messages.
2022-03-30 21:38:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@29b883ee, which are set.
2022-03-30 21:38:53 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@6ff86edb, messages=[], arguments=[--topic, my-topic-741363275-285355018, --bootstrap-server, my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093, --max-messages, 100, USER=my_user_1001559761_691587912, --group-id, my-consumer-group-498626604, --group-instance-id, instance1853376993], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-94dd1070-kafka-clients-65ccf55595-d4d87', podNamespace='namespace-5', bootstrapServer='my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093', topicName='my-topic-741363275-285355018', maxMessages=100, kafkaUsername='my-user-1001559761-691587912', consumerGroupName='my-consumer-group-498626604', consumerInstanceId='instance1853376993', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@29b883ee}
2022-03-30 21:38:53 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093:my-topic-741363275-285355018 from pod my-cluster-94dd1070-kafka-clients-65ccf55595-d4d87
2022-03-30 21:38:53 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-94dd1070-kafka-clients-65ccf55595-d4d87 -n namespace-5 -- /opt/kafka/consumer.sh --topic my-topic-741363275-285355018 --bootstrap-server my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093 --max-messages 100 USER=my_user_1001559761_691587912 --group-id my-consumer-group-498626604 --group-instance-id instance1853376993
2022-03-30 21:38:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-94dd1070-kafka-clients-65ccf55595-d4d87 -n namespace-5 -- /opt/kafka/consumer.sh --topic my-topic-741363275-285355018 --bootstrap-server my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093 --max-messages 100 USER=my_user_1001559761_691587912 --group-id my-consumer-group-498626604 --group-instance-id instance1853376993
2022-03-30 21:38:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:38:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-kafka-0=06b6601d-7396-4887-8081-dab74ae6685c, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:38:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-kafka-0=397a1b3f-874d-427d-8487-49228cb1c4b3, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=ecfe5ada-27e9-43da-9de4-72a06d3b2dfd}
2022-03-30 21:38:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-kafka-0=397a1b3f-874d-427d-8487-49228cb1c4b3, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=ecfe5ada-27e9-43da-9de4-72a06d3b2dfd}
2022-03-30 21:38:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-kafka-1 hasn't rolled
2022-03-30 21:38:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-kafka rolling update not ready, will try again in 5000 ms (1734921ms till timeout)
2022-03-30 21:38:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:38:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:38:59 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-kafka-0=06b6601d-7396-4887-8081-dab74ae6685c, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:38:59 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-kafka-0=397a1b3f-874d-427d-8487-49228cb1c4b3, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=ecfe5ada-27e9-43da-9de4-72a06d3b2dfd}
2022-03-30 21:38:59 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-kafka-0=397a1b3f-874d-427d-8487-49228cb1c4b3, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=ecfe5ada-27e9-43da-9de4-72a06d3b2dfd}
2022-03-30 21:38:59 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-58833349-kafka-1 hasn't rolled
2022-03-30 21:38:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] component with name my-cluster-58833349-kafka rolling update not ready, will try again in 5000 ms (1729916ms till timeout)
2022-03-30 21:38:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:38:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:00 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 21:39:00 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-03-30 21:39:00 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateST:339] Scale up Zookeeper to 5
2022-03-30 21:39:00 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:127] Waiting for 5 Pod(s) of my-cluster-94dd1070-zookeeper to be ready
2022-03-30 21:39:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-30 21:39:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:39:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2999996ms till timeout)
2022-03-30 21:39:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:01 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:39:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2998990ms till timeout)
2022-03-30 21:39:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:02 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:39:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2997985ms till timeout)
2022-03-30 21:39:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:39:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2996979ms till timeout)
2022-03-30 21:39:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:39:04 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-58833349-kafka-0=06b6601d-7396-4887-8081-dab74ae6685c, my-cluster-58833349-kafka-1=a048545e-f6f4-45c2-b1ac-78cfa876d307, my-cluster-58833349-kafka-2=5022be88-c57c-426d-aa27-64543434e3d0}
2022-03-30 21:39:04 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-58833349-kafka-0=397a1b3f-874d-427d-8487-49228cb1c4b3, my-cluster-58833349-kafka-1=b79a6094-12a1-4aeb-b791-8fbebe66eb74, my-cluster-58833349-kafka-2=ecfe5ada-27e9-43da-9de4-72a06d3b2dfd}
2022-03-30 21:39:04 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-58833349-kafka-0=397a1b3f-874d-427d-8487-49228cb1c4b3, my-cluster-58833349-kafka-1=b79a6094-12a1-4aeb-b791-8fbebe66eb74, my-cluster-58833349-kafka-2=ecfe5ada-27e9-43da-9de4-72a06d3b2dfd}
2022-03-30 21:39:04 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:55] All pods seem to have rolled
2022-03-30 21:39:04 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-58833349-kafka has been successfully rolled
2022-03-30 21:39:04 [ForkJoinPool-3-worker-5] [36mDEBUG[m [RollingUpdateUtils:87] Component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-kafka, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={}) successfully rolled
2022-03-30 21:39:04 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-58833349-kafka to be ready
2022-03-30 21:39:04 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-kafka, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-30 21:39:04 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-0 not ready: kafka)
2022-03-30 21:39:04 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-kafka-1)
2022-03-30 21:39:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-kafka, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1799996ms till timeout)
2022-03-30 21:39:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:39:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2995974ms till timeout)
2022-03-30 21:39:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:05 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-0 not ready: kafka)
2022-03-30 21:39:05 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-kafka-1)
2022-03-30 21:39:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-kafka, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1798991ms till timeout)
2022-03-30 21:39:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:39:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2994969ms till timeout)
2022-03-30 21:39:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:06 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-0 not ready: kafka)
2022-03-30 21:39:06 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-kafka-1)
2022-03-30 21:39:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-kafka, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1797985ms till timeout)
2022-03-30 21:39:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:39:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2993963ms till timeout)
2022-03-30 21:39:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:07 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-0 not ready: kafka)
2022-03-30 21:39:07 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-kafka-1)
2022-03-30 21:39:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-kafka, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1796981ms till timeout)
2022-03-30 21:39:07 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:39:07 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2992958ms till timeout)
2022-03-30 21:39:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:08 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-0 not ready: kafka)
2022-03-30 21:39:08 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-kafka-1)
2022-03-30 21:39:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-kafka, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1795976ms till timeout)
2022-03-30 21:39:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:39:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2991953ms till timeout)
2022-03-30 21:39:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:39:09 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-0 not ready: kafka)
2022-03-30 21:39:09 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-kafka-1)
2022-03-30 21:39:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-kafka, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1794971ms till timeout)
2022-03-30 21:39:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:39:09 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2990948ms till timeout)
2022-03-30 21:39:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:10 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-0 not ready: kafka)
2022-03-30 21:39:10 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-kafka-1)
2022-03-30 21:39:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-kafka, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1793966ms till timeout)
2022-03-30 21:39:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:39:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2989941ms till timeout)
2022-03-30 21:39:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:11 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-0 not ready: kafka)
2022-03-30 21:39:11 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-kafka-1)
2022-03-30 21:39:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-kafka, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1792961ms till timeout)
2022-03-30 21:39:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:39:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2988936ms till timeout)
2022-03-30 21:39:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:12 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-0 not ready: kafka)
2022-03-30 21:39:12 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-kafka-1)
2022-03-30 21:39:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-kafka, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1791956ms till timeout)
2022-03-30 21:39:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:39:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2987931ms till timeout)
2022-03-30 21:39:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-0 not ready: kafka)
2022-03-30 21:39:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-kafka-1)
2022-03-30 21:39:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-kafka, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1790949ms till timeout)
2022-03-30 21:39:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:39:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2986926ms till timeout)
2022-03-30 21:39:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:39:14 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-0 not ready: kafka)
2022-03-30 21:39:14 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-kafka-1)
2022-03-30 21:39:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-kafka, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1789944ms till timeout)
2022-03-30 21:39:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:39:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2985921ms till timeout)
2022-03-30 21:39:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-0 not ready: kafka)
2022-03-30 21:39:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-kafka-1)
2022-03-30 21:39:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-kafka, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1788939ms till timeout)
2022-03-30 21:39:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:39:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2984915ms till timeout)
2022-03-30 21:39:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:16 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-0 not ready: kafka)
2022-03-30 21:39:16 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-kafka-1)
2022-03-30 21:39:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-kafka, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1787934ms till timeout)
2022-03-30 21:39:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:39:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2983910ms till timeout)
2022-03-30 21:39:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:17 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-0 not ready: kafka)
2022-03-30 21:39:17 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-kafka-1)
2022-03-30 21:39:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-kafka, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1786930ms till timeout)
2022-03-30 21:39:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:39:17 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2982906ms till timeout)
2022-03-30 21:39:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:18 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-0 not ready: kafka)
2022-03-30 21:39:18 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-kafka-1)
2022-03-30 21:39:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-kafka, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1785925ms till timeout)
2022-03-30 21:39:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:39:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2981901ms till timeout)
2022-03-30 21:39:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:39:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-0 not ready: kafka)
2022-03-30 21:39:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-kafka-1)
2022-03-30 21:39:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-kafka, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1784920ms till timeout)
2022-03-30 21:39:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:39:19 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2980897ms till timeout)
2022-03-30 21:39:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:20 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-0 not ready: kafka)
2022-03-30 21:39:20 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-kafka-1)
2022-03-30 21:39:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-kafka, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1783915ms till timeout)
2022-03-30 21:39:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:39:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2979892ms till timeout)
2022-03-30 21:39:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:21 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-0 not ready: kafka)
2022-03-30 21:39:21 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-58833349-kafka-1)
2022-03-30 21:39:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-kafka, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1782909ms till timeout)
2022-03-30 21:39:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 21:39:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2978886ms till timeout)
2022-03-30 21:39:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:22 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-0 not ready: kafka)
2022-03-30 21:39:22 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-1 not ready: kafka)
2022-03-30 21:39:22 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-2 not ready: kafka)
2022-03-30 21:39:22 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-kafka-0, my-cluster-58833349-kafka-1, my-cluster-58833349-kafka-2 are ready
2022-03-30 21:39:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-kafka, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1781905ms till timeout)
2022-03-30 21:39:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2977882ms till timeout)
2022-03-30 21:39:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:23 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-0 not ready: kafka)
2022-03-30 21:39:23 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-1 not ready: kafka)
2022-03-30 21:39:23 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-2 not ready: kafka)
2022-03-30 21:39:23 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-kafka-0, my-cluster-58833349-kafka-1, my-cluster-58833349-kafka-2 are ready
2022-03-30 21:39:23 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-kafka, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1780898ms till timeout)
2022-03-30 21:39:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2976876ms till timeout)
2022-03-30 21:39:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:39:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-0 not ready: kafka)
2022-03-30 21:39:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-1 not ready: kafka)
2022-03-30 21:39:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-2 not ready: kafka)
2022-03-30 21:39:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-kafka-0, my-cluster-58833349-kafka-1, my-cluster-58833349-kafka-2 are ready
2022-03-30 21:39:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-kafka, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1779893ms till timeout)
2022-03-30 21:39:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2975871ms till timeout)
2022-03-30 21:39:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-0 not ready: kafka)
2022-03-30 21:39:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-1 not ready: kafka)
2022-03-30 21:39:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-2 not ready: kafka)
2022-03-30 21:39:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-kafka-0, my-cluster-58833349-kafka-1, my-cluster-58833349-kafka-2 are ready
2022-03-30 21:39:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-kafka, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1778888ms till timeout)
2022-03-30 21:39:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2974866ms till timeout)
2022-03-30 21:39:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-0 not ready: kafka)
2022-03-30 21:39:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-1 not ready: kafka)
2022-03-30 21:39:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-2 not ready: kafka)
2022-03-30 21:39:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-kafka-0, my-cluster-58833349-kafka-1, my-cluster-58833349-kafka-2 are ready
2022-03-30 21:39:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-kafka, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1777880ms till timeout)
2022-03-30 21:39:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2973861ms till timeout)
2022-03-30 21:39:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:27 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-0 not ready: kafka)
2022-03-30 21:39:27 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-1 not ready: kafka)
2022-03-30 21:39:27 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-2 not ready: kafka)
2022-03-30 21:39:27 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-kafka-0, my-cluster-58833349-kafka-1, my-cluster-58833349-kafka-2 are ready
2022-03-30 21:39:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-kafka, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1776876ms till timeout)
2022-03-30 21:39:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2972855ms till timeout)
2022-03-30 21:39:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:28 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-0 not ready: kafka)
2022-03-30 21:39:28 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-1 not ready: kafka)
2022-03-30 21:39:28 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-2 not ready: kafka)
2022-03-30 21:39:28 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-kafka-0, my-cluster-58833349-kafka-1, my-cluster-58833349-kafka-2 are ready
2022-03-30 21:39:28 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-kafka, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1775871ms till timeout)
2022-03-30 21:39:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2971850ms till timeout)
2022-03-30 21:39:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:39:29 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-0 not ready: kafka)
2022-03-30 21:39:29 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-1 not ready: kafka)
2022-03-30 21:39:29 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-2 not ready: kafka)
2022-03-30 21:39:29 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-kafka-0, my-cluster-58833349-kafka-1, my-cluster-58833349-kafka-2 are ready
2022-03-30 21:39:29 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-kafka, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1774865ms till timeout)
2022-03-30 21:39:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:29 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:29 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:29 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:29 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:29 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:29 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2970844ms till timeout)
2022-03-30 21:39:30 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-0 not ready: kafka)
2022-03-30 21:39:30 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-1 not ready: kafka)
2022-03-30 21:39:30 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-2 not ready: kafka)
2022-03-30 21:39:30 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-kafka-0, my-cluster-58833349-kafka-1, my-cluster-58833349-kafka-2 are ready
2022-03-30 21:39:30 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-kafka, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1773860ms till timeout)
2022-03-30 21:39:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2969834ms till timeout)
2022-03-30 21:39:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:31 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-0 not ready: kafka)
2022-03-30 21:39:31 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-1 not ready: kafka)
2022-03-30 21:39:31 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-2 not ready: kafka)
2022-03-30 21:39:31 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-kafka-0, my-cluster-58833349-kafka-1, my-cluster-58833349-kafka-2 are ready
2022-03-30 21:39:31 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-58833349-kafka, strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1772854ms till timeout)
2022-03-30 21:39:31 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:31 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:31 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:31 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:31 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:31 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2968827ms till timeout)
2022-03-30 21:39:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:32 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-0 not ready: kafka)
2022-03-30 21:39:32 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-1 not ready: kafka)
2022-03-30 21:39:32 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-2 not ready: kafka)
2022-03-30 21:39:32 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-kafka-0, my-cluster-58833349-kafka-1, my-cluster-58833349-kafka-2 are ready
2022-03-30 21:39:32 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:308] Wait for EO to rolling restart ...
2022-03-30 21:39:32 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-58833349-entity-operator rolling update
2022-03-30 21:39:32 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Deployment my-cluster-58833349-entity-operator rolling update in namespace:namespace-3
2022-03-30 21:39:32 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-58833349-entity-operator-5ccc44f8fb-t5n9k=9009f4a5-ad59-4b4d-b841-b5c246bfa0d6}
2022-03-30 21:39:32 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-58833349-entity-operator-7c4b54df69-qjnww=28445096-3501-4571-b7e6-f7bd0d522b97}
2022-03-30 21:39:32 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:119] All pods seem to have rolled
2022-03-30 21:39:32 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-58833349-entity-operator will be ready
2022-03-30 21:39:32 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-58833349-entity-operator will be ready
2022-03-30 21:39:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (479991ms till timeout)
2022-03-30 21:39:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:32 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:32 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:32 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:32 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:32 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2967820ms till timeout)
2022-03-30 21:39:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (478987ms till timeout)
2022-03-30 21:39:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2966811ms till timeout)
2022-03-30 21:39:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:39:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:34 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (477984ms till timeout)
2022-03-30 21:39:34 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:34 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:34 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:34 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:34 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2965805ms till timeout)
2022-03-30 21:39:35 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (476976ms till timeout)
2022-03-30 21:39:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2964799ms till timeout)
2022-03-30 21:39:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (475973ms till timeout)
2022-03-30 21:39:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2963794ms till timeout)
2022-03-30 21:39:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (474969ms till timeout)
2022-03-30 21:39:37 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:37 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:37 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:37 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:37 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:37 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2962789ms till timeout)
2022-03-30 21:39:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (473966ms till timeout)
2022-03-30 21:39:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2961784ms till timeout)
2022-03-30 21:39:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:39:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (472963ms till timeout)
2022-03-30 21:39:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2960778ms till timeout)
2022-03-30 21:39:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (471960ms till timeout)
2022-03-30 21:39:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2959772ms till timeout)
2022-03-30 21:39:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (470957ms till timeout)
2022-03-30 21:39:41 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:41 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:41 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:41 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:41 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2958767ms till timeout)
2022-03-30 21:39:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (469954ms till timeout)
2022-03-30 21:39:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2957762ms till timeout)
2022-03-30 21:39:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (468951ms till timeout)
2022-03-30 21:39:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2956756ms till timeout)
2022-03-30 21:39:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:39:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (467948ms till timeout)
2022-03-30 21:39:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2955751ms till timeout)
2022-03-30 21:39:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (466945ms till timeout)
2022-03-30 21:39:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2954745ms till timeout)
2022-03-30 21:39:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (465942ms till timeout)
2022-03-30 21:39:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2953740ms till timeout)
2022-03-30 21:39:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (464939ms till timeout)
2022-03-30 21:39:47 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:47 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:47 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:47 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:47 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:47 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2952734ms till timeout)
2022-03-30 21:39:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:48 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (463935ms till timeout)
2022-03-30 21:39:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2951729ms till timeout)
2022-03-30 21:39:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:39:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (462931ms till timeout)
2022-03-30 21:39:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:49 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2950722ms till timeout)
2022-03-30 21:39:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (461927ms till timeout)
2022-03-30 21:39:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2949716ms till timeout)
2022-03-30 21:39:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (460924ms till timeout)
2022-03-30 21:39:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2948710ms till timeout)
2022-03-30 21:39:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (459921ms till timeout)
2022-03-30 21:39:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2947705ms till timeout)
2022-03-30 21:39:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (458917ms till timeout)
2022-03-30 21:39:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2946700ms till timeout)
2022-03-30 21:39:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:39:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (457913ms till timeout)
2022-03-30 21:39:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2945694ms till timeout)
2022-03-30 21:39:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (456910ms till timeout)
2022-03-30 21:39:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2944688ms till timeout)
2022-03-30 21:39:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (455907ms till timeout)
2022-03-30 21:39:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2943683ms till timeout)
2022-03-30 21:39:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (454903ms till timeout)
2022-03-30 21:39:57 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:57 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:57 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:57 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:57 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:57 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2942678ms till timeout)
2022-03-30 21:39:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (453899ms till timeout)
2022-03-30 21:39:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2941672ms till timeout)
2022-03-30 21:39:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:39:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:39:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:39:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (452894ms till timeout)
2022-03-30 21:39:59 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:39:59 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:39:59 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:39:59 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:39:59 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:39:59 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2940666ms till timeout)
2022-03-30 21:40:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (451890ms till timeout)
2022-03-30 21:40:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:40:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:40:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:40:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:40:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:40:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2939661ms till timeout)
2022-03-30 21:40:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (450886ms till timeout)
2022-03-30 21:40:01 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:40:01 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:40:01 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:40:01 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:40:01 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:40:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2938655ms till timeout)
2022-03-30 21:40:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (449883ms till timeout)
2022-03-30 21:40:02 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:40:02 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:40:02 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:40:02 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:40:02 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:40:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2937650ms till timeout)
2022-03-30 21:40:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (448879ms till timeout)
2022-03-30 21:40:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:40:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:40:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:40:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:40:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:40:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2936644ms till timeout)
2022-03-30 21:40:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:40:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (447875ms till timeout)
2022-03-30 21:40:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:40:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:40:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:40:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:40:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:40:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2935639ms till timeout)
2022-03-30 21:40:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (446872ms till timeout)
2022-03-30 21:40:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:40:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:40:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:40:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:40:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:40:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2934633ms till timeout)
2022-03-30 21:40:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (445869ms till timeout)
2022-03-30 21:40:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:40:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:40:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:40:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:40:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:40:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2933628ms till timeout)
2022-03-30 21:40:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (444865ms till timeout)
2022-03-30 21:40:07 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:40:07 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:40:07 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:40:07 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:40:07 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:40:07 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2932623ms till timeout)
2022-03-30 21:40:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (443861ms till timeout)
2022-03-30 21:40:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:40:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:40:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:40:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:40:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:40:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2931618ms till timeout)
2022-03-30 21:40:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:40:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (442857ms till timeout)
2022-03-30 21:40:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:40:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:40:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:40:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:40:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:40:09 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2930612ms till timeout)
2022-03-30 21:40:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (441854ms till timeout)
2022-03-30 21:40:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:40:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:40:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:40:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:40:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:40:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2929607ms till timeout)
2022-03-30 21:40:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (440850ms till timeout)
2022-03-30 21:40:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:40:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:40:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:40:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:40:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:40:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2928601ms till timeout)
2022-03-30 21:40:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (439846ms till timeout)
2022-03-30 21:40:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:40:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:40:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:40:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:40:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:40:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2927595ms till timeout)
2022-03-30 21:40:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (438843ms till timeout)
2022-03-30 21:40:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:40:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:40:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:40:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:40:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:40:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2926590ms till timeout)
2022-03-30 21:40:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:40:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (437839ms till timeout)
2022-03-30 21:40:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:40:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:40:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:40:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:40:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:40:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2925584ms till timeout)
2022-03-30 21:40:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (436836ms till timeout)
2022-03-30 21:40:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:40:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:40:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:40:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:40:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:40:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2924579ms till timeout)
2022-03-30 21:40:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (435832ms till timeout)
2022-03-30 21:40:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:40:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:40:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:40:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:40:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:40:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2923573ms till timeout)
2022-03-30 21:40:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (434829ms till timeout)
2022-03-30 21:40:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:40:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:40:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:40:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:40:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-zookeeper-4)
2022-03-30 21:40:17 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2922567ms till timeout)
2022-03-30 21:40:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (433825ms till timeout)
2022-03-30 21:40:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:40:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:40:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:40:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:40:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-4 not ready: zookeeper)
2022-03-30 21:40:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-94dd1070-zookeeper-0, my-cluster-94dd1070-zookeeper-1, my-cluster-94dd1070-zookeeper-2, my-cluster-94dd1070-zookeeper-3, my-cluster-94dd1070-zookeeper-4 are ready
2022-03-30 21:40:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2921562ms till timeout)
2022-03-30 21:40:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:40:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (432822ms till timeout)
2022-03-30 21:40:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:40:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:40:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:40:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:40:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-4 not ready: zookeeper)
2022-03-30 21:40:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-94dd1070-zookeeper-0, my-cluster-94dd1070-zookeeper-1, my-cluster-94dd1070-zookeeper-2, my-cluster-94dd1070-zookeeper-3, my-cluster-94dd1070-zookeeper-4 are ready
2022-03-30 21:40:19 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2920556ms till timeout)
2022-03-30 21:40:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (431818ms till timeout)
2022-03-30 21:40:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:40:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:40:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:40:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:40:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-4 not ready: zookeeper)
2022-03-30 21:40:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-94dd1070-zookeeper-0, my-cluster-94dd1070-zookeeper-1, my-cluster-94dd1070-zookeeper-2, my-cluster-94dd1070-zookeeper-3, my-cluster-94dd1070-zookeeper-4 are ready
2022-03-30 21:40:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2919550ms till timeout)
2022-03-30 21:40:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (430815ms till timeout)
2022-03-30 21:40:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:40:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:40:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:40:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:40:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-4 not ready: zookeeper)
2022-03-30 21:40:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-94dd1070-zookeeper-0, my-cluster-94dd1070-zookeeper-1, my-cluster-94dd1070-zookeeper-2, my-cluster-94dd1070-zookeeper-3, my-cluster-94dd1070-zookeeper-4 are ready
2022-03-30 21:40:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2918545ms till timeout)
2022-03-30 21:40:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (429811ms till timeout)
2022-03-30 21:40:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:40:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:40:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:40:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:40:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-4 not ready: zookeeper)
2022-03-30 21:40:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-94dd1070-zookeeper-0, my-cluster-94dd1070-zookeeper-1, my-cluster-94dd1070-zookeeper-2, my-cluster-94dd1070-zookeeper-3, my-cluster-94dd1070-zookeeper-4 are ready
2022-03-30 21:40:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2917535ms till timeout)
2022-03-30 21:40:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:23 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (428809ms till timeout)
2022-03-30 21:40:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:40:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:40:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:40:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:40:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-4 not ready: zookeeper)
2022-03-30 21:40:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-94dd1070-zookeeper-0, my-cluster-94dd1070-zookeeper-1, my-cluster-94dd1070-zookeeper-2, my-cluster-94dd1070-zookeeper-3, my-cluster-94dd1070-zookeeper-4 are ready
2022-03-30 21:40:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2916530ms till timeout)
2022-03-30 21:40:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:40:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (427805ms till timeout)
2022-03-30 21:40:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:40:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:40:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:40:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:40:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-4 not ready: zookeeper)
2022-03-30 21:40:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-94dd1070-zookeeper-0, my-cluster-94dd1070-zookeeper-1, my-cluster-94dd1070-zookeeper-2, my-cluster-94dd1070-zookeeper-3, my-cluster-94dd1070-zookeeper-4 are ready
2022-03-30 21:40:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2915525ms till timeout)
2022-03-30 21:40:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (426802ms till timeout)
2022-03-30 21:40:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:40:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:40:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:40:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:40:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-4 not ready: zookeeper)
2022-03-30 21:40:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-94dd1070-zookeeper-0, my-cluster-94dd1070-zookeeper-1, my-cluster-94dd1070-zookeeper-2, my-cluster-94dd1070-zookeeper-3, my-cluster-94dd1070-zookeeper-4 are ready
2022-03-30 21:40:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2914519ms till timeout)
2022-03-30 21:40:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (425798ms till timeout)
2022-03-30 21:40:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:40:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:40:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:40:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:40:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-4 not ready: zookeeper)
2022-03-30 21:40:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-94dd1070-zookeeper-0, my-cluster-94dd1070-zookeeper-1, my-cluster-94dd1070-zookeeper-2, my-cluster-94dd1070-zookeeper-3, my-cluster-94dd1070-zookeeper-4 are ready
2022-03-30 21:40:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2913512ms till timeout)
2022-03-30 21:40:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (424794ms till timeout)
2022-03-30 21:40:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:40:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:40:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:40:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:40:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-4 not ready: zookeeper)
2022-03-30 21:40:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-94dd1070-zookeeper-0, my-cluster-94dd1070-zookeeper-1, my-cluster-94dd1070-zookeeper-2, my-cluster-94dd1070-zookeeper-3, my-cluster-94dd1070-zookeeper-4 are ready
2022-03-30 21:40:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-zookeeper, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2912507ms till timeout)
2022-03-30 21:40:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:28 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (423791ms till timeout)
2022-03-30 21:40:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-0 not ready: zookeeper)
2022-03-30 21:40:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-1 not ready: zookeeper)
2022-03-30 21:40:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-2 not ready: zookeeper)
2022-03-30 21:40:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-3 not ready: zookeeper)
2022-03-30 21:40:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-zookeeper-4 not ready: zookeeper)
2022-03-30 21:40:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-94dd1070-zookeeper-0, my-cluster-94dd1070-zookeeper-1, my-cluster-94dd1070-zookeeper-2, my-cluster-94dd1070-zookeeper-3, my-cluster-94dd1070-zookeeper-4 are ready
2022-03-30 21:40:28 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-94dd1070 will have desired state: Ready
2022-03-30 21:40:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-94dd1070 will have desired state: Ready
2022-03-30 21:40:28 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-94dd1070 is in desired state: Ready
2022-03-30 21:40:28 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-94dd1070 is ready
2022-03-30 21:40:28 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateST:342] Kafka scale up to 5 finished
2022-03-30 21:40:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Consumer will successfully receive messages.
2022-03-30 21:40:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@28b461c2, which are set.
2022-03-30 21:40:28 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@4e0382a1, messages=[], arguments=[--topic, my-topic-741363275-285355018, --bootstrap-server, my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093, --max-messages, 100, USER=my_user_1001559761_691587912, --group-id, my-consumer-group-1396610460, --group-instance-id, instance806584872], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-94dd1070-kafka-clients-65ccf55595-d4d87', podNamespace='namespace-5', bootstrapServer='my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093', topicName='my-topic-741363275-285355018', maxMessages=100, kafkaUsername='my-user-1001559761-691587912', consumerGroupName='my-consumer-group-1396610460', consumerInstanceId='instance806584872', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@28b461c2}
2022-03-30 21:40:28 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093:my-topic-741363275-285355018 from pod my-cluster-94dd1070-kafka-clients-65ccf55595-d4d87
2022-03-30 21:40:28 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-94dd1070-kafka-clients-65ccf55595-d4d87 -n namespace-5 -- /opt/kafka/consumer.sh --topic my-topic-741363275-285355018 --bootstrap-server my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093 --max-messages 100 USER=my_user_1001559761_691587912 --group-id my-consumer-group-1396610460 --group-instance-id instance806584872
2022-03-30 21:40:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-94dd1070-kafka-clients-65ccf55595-d4d87 -n namespace-5 -- /opt/kafka/consumer.sh --topic my-topic-741363275-285355018 --bootstrap-server my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093 --max-messages 100 USER=my_user_1001559761_691587912 --group-id my-consumer-group-1396610460 --group-instance-id instance806584872
2022-03-30 21:40:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:40:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:29 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (422787ms till timeout)
2022-03-30 21:40:30 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (421784ms till timeout)
2022-03-30 21:40:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:31 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (420780ms till timeout)
2022-03-30 21:40:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (419777ms till timeout)
2022-03-30 21:40:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (418773ms till timeout)
2022-03-30 21:40:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:40:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:34 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (417769ms till timeout)
2022-03-30 21:40:35 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (416766ms till timeout)
2022-03-30 21:40:35 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 21:40:35 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-03-30 21:40:35 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateST:351] Scale down Kafka to 3
2022-03-30 21:40:35 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-94dd1070-kafka rolling update
2022-03-30 21:40:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:74] Waiting for rolling update of component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})
2022-03-30 21:40:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for component with name my-cluster-94dd1070-kafka rolling update
2022-03-30 21:40:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259, my-cluster-94dd1070-kafka-4=c8f4700c-9768-4dab-8927-aa6cc4defc2e, my-cluster-94dd1070-kafka-5=a97534bc-0ab0-4459-abc1-beab1b793583, my-cluster-94dd1070-kafka-6=bbca259c-5bfd-47f1-88c2-c8dac21f781c}
2022-03-30 21:40:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259, my-cluster-94dd1070-kafka-4=c8f4700c-9768-4dab-8927-aa6cc4defc2e, my-cluster-94dd1070-kafka-5=a97534bc-0ab0-4459-abc1-beab1b793583, my-cluster-94dd1070-kafka-6=bbca259c-5bfd-47f1-88c2-c8dac21f781c}
2022-03-30 21:40:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259, my-cluster-94dd1070-kafka-4=c8f4700c-9768-4dab-8927-aa6cc4defc2e, my-cluster-94dd1070-kafka-5=a97534bc-0ab0-4459-abc1-beab1b793583, my-cluster-94dd1070-kafka-6=bbca259c-5bfd-47f1-88c2-c8dac21f781c}
2022-03-30 21:40:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-3 hasn't rolled
2022-03-30 21:40:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (4199995ms till timeout)
2022-03-30 21:40:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (415762ms till timeout)
2022-03-30 21:40:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (414759ms till timeout)
2022-03-30 21:40:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (413756ms till timeout)
2022-03-30 21:40:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:40:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (412752ms till timeout)
2022-03-30 21:40:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (411748ms till timeout)
2022-03-30 21:40:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259, my-cluster-94dd1070-kafka-4=c8f4700c-9768-4dab-8927-aa6cc4defc2e, my-cluster-94dd1070-kafka-5=a97534bc-0ab0-4459-abc1-beab1b793583, my-cluster-94dd1070-kafka-6=bbca259c-5bfd-47f1-88c2-c8dac21f781c}
2022-03-30 21:40:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259, my-cluster-94dd1070-kafka-4=c8f4700c-9768-4dab-8927-aa6cc4defc2e, my-cluster-94dd1070-kafka-5=a97534bc-0ab0-4459-abc1-beab1b793583, my-cluster-94dd1070-kafka-6=bbca259c-5bfd-47f1-88c2-c8dac21f781c}
2022-03-30 21:40:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259, my-cluster-94dd1070-kafka-4=c8f4700c-9768-4dab-8927-aa6cc4defc2e, my-cluster-94dd1070-kafka-5=a97534bc-0ab0-4459-abc1-beab1b793583, my-cluster-94dd1070-kafka-6=bbca259c-5bfd-47f1-88c2-c8dac21f781c}
2022-03-30 21:40:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-3 hasn't rolled
2022-03-30 21:40:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (4194988ms till timeout)
2022-03-30 21:40:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (410745ms till timeout)
2022-03-30 21:40:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (409741ms till timeout)
2022-03-30 21:40:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (408737ms till timeout)
2022-03-30 21:40:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:40:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (407734ms till timeout)
2022-03-30 21:40:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (406730ms till timeout)
2022-03-30 21:40:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259, my-cluster-94dd1070-kafka-4=c8f4700c-9768-4dab-8927-aa6cc4defc2e, my-cluster-94dd1070-kafka-5=a97534bc-0ab0-4459-abc1-beab1b793583, my-cluster-94dd1070-kafka-6=bbca259c-5bfd-47f1-88c2-c8dac21f781c}
2022-03-30 21:40:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259, my-cluster-94dd1070-kafka-4=c8f4700c-9768-4dab-8927-aa6cc4defc2e, my-cluster-94dd1070-kafka-5=a97534bc-0ab0-4459-abc1-beab1b793583}
2022-03-30 21:40:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259, my-cluster-94dd1070-kafka-4=c8f4700c-9768-4dab-8927-aa6cc4defc2e, my-cluster-94dd1070-kafka-5=a97534bc-0ab0-4459-abc1-beab1b793583}
2022-03-30 21:40:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-3 hasn't rolled
2022-03-30 21:40:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (4189983ms till timeout)
2022-03-30 21:40:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (405727ms till timeout)
2022-03-30 21:40:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (404723ms till timeout)
2022-03-30 21:40:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:48 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (403719ms till timeout)
2022-03-30 21:40:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:40:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (402716ms till timeout)
2022-03-30 21:40:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (401712ms till timeout)
2022-03-30 21:40:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259, my-cluster-94dd1070-kafka-4=c8f4700c-9768-4dab-8927-aa6cc4defc2e, my-cluster-94dd1070-kafka-5=a97534bc-0ab0-4459-abc1-beab1b793583, my-cluster-94dd1070-kafka-6=bbca259c-5bfd-47f1-88c2-c8dac21f781c}
2022-03-30 21:40:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259, my-cluster-94dd1070-kafka-4=c8f4700c-9768-4dab-8927-aa6cc4defc2e, my-cluster-94dd1070-kafka-5=a97534bc-0ab0-4459-abc1-beab1b793583}
2022-03-30 21:40:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259, my-cluster-94dd1070-kafka-4=c8f4700c-9768-4dab-8927-aa6cc4defc2e, my-cluster-94dd1070-kafka-5=a97534bc-0ab0-4459-abc1-beab1b793583}
2022-03-30 21:40:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-3 hasn't rolled
2022-03-30 21:40:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (4184977ms till timeout)
2022-03-30 21:40:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (400709ms till timeout)
2022-03-30 21:40:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (399705ms till timeout)
2022-03-30 21:40:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (398702ms till timeout)
2022-03-30 21:40:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:40:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (397699ms till timeout)
2022-03-30 21:40:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (396695ms till timeout)
2022-03-30 21:40:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259, my-cluster-94dd1070-kafka-4=c8f4700c-9768-4dab-8927-aa6cc4defc2e, my-cluster-94dd1070-kafka-5=a97534bc-0ab0-4459-abc1-beab1b793583, my-cluster-94dd1070-kafka-6=bbca259c-5bfd-47f1-88c2-c8dac21f781c}
2022-03-30 21:40:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259}
2022-03-30 21:40:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259}
2022-03-30 21:40:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-3 hasn't rolled
2022-03-30 21:40:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (4179971ms till timeout)
2022-03-30 21:40:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (395691ms till timeout)
2022-03-30 21:40:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (394687ms till timeout)
2022-03-30 21:40:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (393683ms till timeout)
2022-03-30 21:40:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:40:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:40:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:40:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (392680ms till timeout)
2022-03-30 21:41:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (391677ms till timeout)
2022-03-30 21:41:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259, my-cluster-94dd1070-kafka-4=c8f4700c-9768-4dab-8927-aa6cc4defc2e, my-cluster-94dd1070-kafka-5=a97534bc-0ab0-4459-abc1-beab1b793583, my-cluster-94dd1070-kafka-6=bbca259c-5bfd-47f1-88c2-c8dac21f781c}
2022-03-30 21:41:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259}
2022-03-30 21:41:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259}
2022-03-30 21:41:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-3 hasn't rolled
2022-03-30 21:41:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (4174965ms till timeout)
2022-03-30 21:41:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (390673ms till timeout)
2022-03-30 21:41:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (389669ms till timeout)
2022-03-30 21:41:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (388666ms till timeout)
2022-03-30 21:41:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:41:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (387663ms till timeout)
2022-03-30 21:41:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (386660ms till timeout)
2022-03-30 21:41:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259, my-cluster-94dd1070-kafka-4=c8f4700c-9768-4dab-8927-aa6cc4defc2e, my-cluster-94dd1070-kafka-5=a97534bc-0ab0-4459-abc1-beab1b793583, my-cluster-94dd1070-kafka-6=bbca259c-5bfd-47f1-88c2-c8dac21f781c}
2022-03-30 21:41:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259}
2022-03-30 21:41:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259}
2022-03-30 21:41:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-3 hasn't rolled
2022-03-30 21:41:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (4169960ms till timeout)
2022-03-30 21:41:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (385656ms till timeout)
2022-03-30 21:41:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (384653ms till timeout)
2022-03-30 21:41:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (383649ms till timeout)
2022-03-30 21:41:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:41:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (382646ms till timeout)
2022-03-30 21:41:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (381642ms till timeout)
2022-03-30 21:41:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259, my-cluster-94dd1070-kafka-4=c8f4700c-9768-4dab-8927-aa6cc4defc2e, my-cluster-94dd1070-kafka-5=a97534bc-0ab0-4459-abc1-beab1b793583, my-cluster-94dd1070-kafka-6=bbca259c-5bfd-47f1-88c2-c8dac21f781c}
2022-03-30 21:41:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259}
2022-03-30 21:41:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259}
2022-03-30 21:41:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-3 hasn't rolled
2022-03-30 21:41:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (4164954ms till timeout)
2022-03-30 21:41:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (380637ms till timeout)
2022-03-30 21:41:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (379633ms till timeout)
2022-03-30 21:41:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (378630ms till timeout)
2022-03-30 21:41:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:41:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (377627ms till timeout)
2022-03-30 21:41:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (376623ms till timeout)
2022-03-30 21:41:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259, my-cluster-94dd1070-kafka-4=c8f4700c-9768-4dab-8927-aa6cc4defc2e, my-cluster-94dd1070-kafka-5=a97534bc-0ab0-4459-abc1-beab1b793583, my-cluster-94dd1070-kafka-6=bbca259c-5bfd-47f1-88c2-c8dac21f781c}
2022-03-30 21:41:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7}
2022-03-30 21:41:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7}
2022-03-30 21:41:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-1 hasn't rolled
2022-03-30 21:41:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (4159949ms till timeout)
2022-03-30 21:41:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (375619ms till timeout)
2022-03-30 21:41:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (374616ms till timeout)
2022-03-30 21:41:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (373612ms till timeout)
2022-03-30 21:41:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:41:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (372609ms till timeout)
2022-03-30 21:41:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (371605ms till timeout)
2022-03-30 21:41:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259, my-cluster-94dd1070-kafka-4=c8f4700c-9768-4dab-8927-aa6cc4defc2e, my-cluster-94dd1070-kafka-5=a97534bc-0ab0-4459-abc1-beab1b793583, my-cluster-94dd1070-kafka-6=bbca259c-5bfd-47f1-88c2-c8dac21f781c}
2022-03-30 21:41:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7}
2022-03-30 21:41:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7}
2022-03-30 21:41:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-1 hasn't rolled
2022-03-30 21:41:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (4154943ms till timeout)
2022-03-30 21:41:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (370602ms till timeout)
2022-03-30 21:41:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (369598ms till timeout)
2022-03-30 21:41:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:23 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (368595ms till timeout)
2022-03-30 21:41:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:41:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (367591ms till timeout)
2022-03-30 21:41:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (366588ms till timeout)
2022-03-30 21:41:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259, my-cluster-94dd1070-kafka-4=c8f4700c-9768-4dab-8927-aa6cc4defc2e, my-cluster-94dd1070-kafka-5=a97534bc-0ab0-4459-abc1-beab1b793583, my-cluster-94dd1070-kafka-6=bbca259c-5bfd-47f1-88c2-c8dac21f781c}
2022-03-30 21:41:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=caefd92b-36ec-4cc7-9fc6-175244e97118, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7}
2022-03-30 21:41:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=caefd92b-36ec-4cc7-9fc6-175244e97118, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7}
2022-03-30 21:41:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-2 hasn't rolled
2022-03-30 21:41:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (4149938ms till timeout)
2022-03-30 21:41:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (365584ms till timeout)
2022-03-30 21:41:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (364576ms till timeout)
2022-03-30 21:41:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:28 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (363571ms till timeout)
2022-03-30 21:41:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:41:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:29 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (362567ms till timeout)
2022-03-30 21:41:30 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (361564ms till timeout)
2022-03-30 21:41:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259, my-cluster-94dd1070-kafka-4=c8f4700c-9768-4dab-8927-aa6cc4defc2e, my-cluster-94dd1070-kafka-5=a97534bc-0ab0-4459-abc1-beab1b793583, my-cluster-94dd1070-kafka-6=bbca259c-5bfd-47f1-88c2-c8dac21f781c}
2022-03-30 21:41:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=caefd92b-36ec-4cc7-9fc6-175244e97118, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7}
2022-03-30 21:41:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=caefd92b-36ec-4cc7-9fc6-175244e97118, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7}
2022-03-30 21:41:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-2 hasn't rolled
2022-03-30 21:41:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (4144933ms till timeout)
2022-03-30 21:41:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:31 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (360560ms till timeout)
2022-03-30 21:41:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (359557ms till timeout)
2022-03-30 21:41:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (358553ms till timeout)
2022-03-30 21:41:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:41:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:34 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (357550ms till timeout)
2022-03-30 21:41:35 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (356546ms till timeout)
2022-03-30 21:41:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259, my-cluster-94dd1070-kafka-4=c8f4700c-9768-4dab-8927-aa6cc4defc2e, my-cluster-94dd1070-kafka-5=a97534bc-0ab0-4459-abc1-beab1b793583, my-cluster-94dd1070-kafka-6=bbca259c-5bfd-47f1-88c2-c8dac21f781c}
2022-03-30 21:41:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=caefd92b-36ec-4cc7-9fc6-175244e97118, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7}
2022-03-30 21:41:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=caefd92b-36ec-4cc7-9fc6-175244e97118, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7}
2022-03-30 21:41:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-2 hasn't rolled
2022-03-30 21:41:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (4139928ms till timeout)
2022-03-30 21:41:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (355542ms till timeout)
2022-03-30 21:41:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (354538ms till timeout)
2022-03-30 21:41:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (353535ms till timeout)
2022-03-30 21:41:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:41:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (352531ms till timeout)
2022-03-30 21:41:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (351528ms till timeout)
2022-03-30 21:41:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259, my-cluster-94dd1070-kafka-4=c8f4700c-9768-4dab-8927-aa6cc4defc2e, my-cluster-94dd1070-kafka-5=a97534bc-0ab0-4459-abc1-beab1b793583, my-cluster-94dd1070-kafka-6=bbca259c-5bfd-47f1-88c2-c8dac21f781c}
2022-03-30 21:41:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=caefd92b-36ec-4cc7-9fc6-175244e97118, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7}
2022-03-30 21:41:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=caefd92b-36ec-4cc7-9fc6-175244e97118, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7}
2022-03-30 21:41:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-2 hasn't rolled
2022-03-30 21:41:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (4134923ms till timeout)
2022-03-30 21:41:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (350524ms till timeout)
2022-03-30 21:41:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (349521ms till timeout)
2022-03-30 21:41:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (348518ms till timeout)
2022-03-30 21:41:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:41:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (347514ms till timeout)
2022-03-30 21:41:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (346510ms till timeout)
2022-03-30 21:41:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259, my-cluster-94dd1070-kafka-4=c8f4700c-9768-4dab-8927-aa6cc4defc2e, my-cluster-94dd1070-kafka-5=a97534bc-0ab0-4459-abc1-beab1b793583, my-cluster-94dd1070-kafka-6=bbca259c-5bfd-47f1-88c2-c8dac21f781c}
2022-03-30 21:41:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=caefd92b-36ec-4cc7-9fc6-175244e97118, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7}
2022-03-30 21:41:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=caefd92b-36ec-4cc7-9fc6-175244e97118, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7}
2022-03-30 21:41:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-2 hasn't rolled
2022-03-30 21:41:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (4129918ms till timeout)
2022-03-30 21:41:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (345507ms till timeout)
2022-03-30 21:41:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (344503ms till timeout)
2022-03-30 21:41:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:48 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (343500ms till timeout)
2022-03-30 21:41:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:41:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (342497ms till timeout)
2022-03-30 21:41:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (341493ms till timeout)
2022-03-30 21:41:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259, my-cluster-94dd1070-kafka-4=c8f4700c-9768-4dab-8927-aa6cc4defc2e, my-cluster-94dd1070-kafka-5=a97534bc-0ab0-4459-abc1-beab1b793583, my-cluster-94dd1070-kafka-6=bbca259c-5bfd-47f1-88c2-c8dac21f781c}
2022-03-30 21:41:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=caefd92b-36ec-4cc7-9fc6-175244e97118, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7}
2022-03-30 21:41:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=caefd92b-36ec-4cc7-9fc6-175244e97118, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7}
2022-03-30 21:41:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-2 hasn't rolled
2022-03-30 21:41:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (4124913ms till timeout)
2022-03-30 21:41:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (340486ms till timeout)
2022-03-30 21:41:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (339482ms till timeout)
2022-03-30 21:41:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (338479ms till timeout)
2022-03-30 21:41:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:41:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (337475ms till timeout)
2022-03-30 21:41:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (336471ms till timeout)
2022-03-30 21:41:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259, my-cluster-94dd1070-kafka-4=c8f4700c-9768-4dab-8927-aa6cc4defc2e, my-cluster-94dd1070-kafka-5=a97534bc-0ab0-4459-abc1-beab1b793583, my-cluster-94dd1070-kafka-6=bbca259c-5bfd-47f1-88c2-c8dac21f781c}
2022-03-30 21:41:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=caefd92b-36ec-4cc7-9fc6-175244e97118, my-cluster-94dd1070-kafka-2=13026648-2347-4aed-b16a-454483d77117}
2022-03-30 21:41:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=caefd92b-36ec-4cc7-9fc6-175244e97118, my-cluster-94dd1070-kafka-2=13026648-2347-4aed-b16a-454483d77117}
2022-03-30 21:41:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-0 hasn't rolled
2022-03-30 21:41:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (4119909ms till timeout)
2022-03-30 21:41:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (335468ms till timeout)
2022-03-30 21:41:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (334464ms till timeout)
2022-03-30 21:41:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (333461ms till timeout)
2022-03-30 21:41:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:41:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:41:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:41:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (332457ms till timeout)
2022-03-30 21:42:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259, my-cluster-94dd1070-kafka-4=c8f4700c-9768-4dab-8927-aa6cc4defc2e, my-cluster-94dd1070-kafka-5=a97534bc-0ab0-4459-abc1-beab1b793583, my-cluster-94dd1070-kafka-6=bbca259c-5bfd-47f1-88c2-c8dac21f781c}
2022-03-30 21:42:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=caefd92b-36ec-4cc7-9fc6-175244e97118, my-cluster-94dd1070-kafka-2=13026648-2347-4aed-b16a-454483d77117}
2022-03-30 21:42:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=caefd92b-36ec-4cc7-9fc6-175244e97118, my-cluster-94dd1070-kafka-2=13026648-2347-4aed-b16a-454483d77117}
2022-03-30 21:42:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-0 hasn't rolled
2022-03-30 21:42:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (4114903ms till timeout)
2022-03-30 21:42:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (331452ms till timeout)
2022-03-30 21:42:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (330448ms till timeout)
2022-03-30 21:42:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (329444ms till timeout)
2022-03-30 21:42:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:42:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (328441ms till timeout)
2022-03-30 21:42:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (327438ms till timeout)
2022-03-30 21:42:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259, my-cluster-94dd1070-kafka-4=c8f4700c-9768-4dab-8927-aa6cc4defc2e, my-cluster-94dd1070-kafka-5=a97534bc-0ab0-4459-abc1-beab1b793583, my-cluster-94dd1070-kafka-6=bbca259c-5bfd-47f1-88c2-c8dac21f781c}
2022-03-30 21:42:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=caefd92b-36ec-4cc7-9fc6-175244e97118, my-cluster-94dd1070-kafka-2=13026648-2347-4aed-b16a-454483d77117}
2022-03-30 21:42:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=caefd92b-36ec-4cc7-9fc6-175244e97118, my-cluster-94dd1070-kafka-2=13026648-2347-4aed-b16a-454483d77117}
2022-03-30 21:42:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-0 hasn't rolled
2022-03-30 21:42:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (4109898ms till timeout)
2022-03-30 21:42:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (326435ms till timeout)
2022-03-30 21:42:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (325431ms till timeout)
2022-03-30 21:42:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (324428ms till timeout)
2022-03-30 21:42:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:42:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (323424ms till timeout)
2022-03-30 21:42:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (322421ms till timeout)
2022-03-30 21:42:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259, my-cluster-94dd1070-kafka-4=c8f4700c-9768-4dab-8927-aa6cc4defc2e, my-cluster-94dd1070-kafka-5=a97534bc-0ab0-4459-abc1-beab1b793583, my-cluster-94dd1070-kafka-6=bbca259c-5bfd-47f1-88c2-c8dac21f781c}
2022-03-30 21:42:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=caefd92b-36ec-4cc7-9fc6-175244e97118, my-cluster-94dd1070-kafka-2=13026648-2347-4aed-b16a-454483d77117}
2022-03-30 21:42:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=caefd92b-36ec-4cc7-9fc6-175244e97118, my-cluster-94dd1070-kafka-2=13026648-2347-4aed-b16a-454483d77117}
2022-03-30 21:42:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-0 hasn't rolled
2022-03-30 21:42:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (4104893ms till timeout)
2022-03-30 21:42:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (321418ms till timeout)
2022-03-30 21:42:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (320415ms till timeout)
2022-03-30 21:42:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (319411ms till timeout)
2022-03-30 21:42:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:42:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (318408ms till timeout)
2022-03-30 21:42:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (317404ms till timeout)
2022-03-30 21:42:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259, my-cluster-94dd1070-kafka-4=c8f4700c-9768-4dab-8927-aa6cc4defc2e, my-cluster-94dd1070-kafka-5=a97534bc-0ab0-4459-abc1-beab1b793583, my-cluster-94dd1070-kafka-6=bbca259c-5bfd-47f1-88c2-c8dac21f781c}
2022-03-30 21:42:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=caefd92b-36ec-4cc7-9fc6-175244e97118, my-cluster-94dd1070-kafka-2=13026648-2347-4aed-b16a-454483d77117}
2022-03-30 21:42:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=caefd92b-36ec-4cc7-9fc6-175244e97118, my-cluster-94dd1070-kafka-2=13026648-2347-4aed-b16a-454483d77117}
2022-03-30 21:42:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-0 hasn't rolled
2022-03-30 21:42:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (4099888ms till timeout)
2022-03-30 21:42:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (316401ms till timeout)
2022-03-30 21:42:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (315398ms till timeout)
2022-03-30 21:42:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (314394ms till timeout)
2022-03-30 21:42:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:42:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (313391ms till timeout)
2022-03-30 21:42:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (312387ms till timeout)
2022-03-30 21:42:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259, my-cluster-94dd1070-kafka-4=c8f4700c-9768-4dab-8927-aa6cc4defc2e, my-cluster-94dd1070-kafka-5=a97534bc-0ab0-4459-abc1-beab1b793583, my-cluster-94dd1070-kafka-6=bbca259c-5bfd-47f1-88c2-c8dac21f781c}
2022-03-30 21:42:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=caefd92b-36ec-4cc7-9fc6-175244e97118, my-cluster-94dd1070-kafka-2=13026648-2347-4aed-b16a-454483d77117}
2022-03-30 21:42:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=caefd92b-36ec-4cc7-9fc6-175244e97118, my-cluster-94dd1070-kafka-2=13026648-2347-4aed-b16a-454483d77117}
2022-03-30 21:42:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-94dd1070-kafka-0 hasn't rolled
2022-03-30 21:42:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-94dd1070-kafka rolling update not ready, will try again in 5000 ms (4094882ms till timeout)
2022-03-30 21:42:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (311384ms till timeout)
2022-03-30 21:42:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (310381ms till timeout)
2022-03-30 21:42:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:23 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (309377ms till timeout)
2022-03-30 21:42:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:42:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (308374ms till timeout)
2022-03-30 21:42:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (307371ms till timeout)
2022-03-30 21:42:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-94dd1070-kafka-0=aedfa627-b533-48c4-8c29-10a3a373e2c4, my-cluster-94dd1070-kafka-1=f73753a3-61d5-4a72-a304-e918f1337f13, my-cluster-94dd1070-kafka-2=6b508913-5b47-4c7b-b7fe-f6ad80ea30e7, my-cluster-94dd1070-kafka-3=7acab859-a590-4782-bc44-be74f4a7c259, my-cluster-94dd1070-kafka-4=c8f4700c-9768-4dab-8927-aa6cc4defc2e, my-cluster-94dd1070-kafka-5=a97534bc-0ab0-4459-abc1-beab1b793583, my-cluster-94dd1070-kafka-6=bbca259c-5bfd-47f1-88c2-c8dac21f781c}
2022-03-30 21:42:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-94dd1070-kafka-0=cf947541-f620-4472-8239-7dced6ee93ce, my-cluster-94dd1070-kafka-1=caefd92b-36ec-4cc7-9fc6-175244e97118, my-cluster-94dd1070-kafka-2=13026648-2347-4aed-b16a-454483d77117}
2022-03-30 21:42:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-94dd1070-kafka-0=cf947541-f620-4472-8239-7dced6ee93ce, my-cluster-94dd1070-kafka-1=caefd92b-36ec-4cc7-9fc6-175244e97118, my-cluster-94dd1070-kafka-2=13026648-2347-4aed-b16a-454483d77117}
2022-03-30 21:42:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:55] All pods seem to have rolled
2022-03-30 21:42:25 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-94dd1070-kafka has been successfully rolled
2022-03-30 21:42:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:87] Component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={}) successfully rolled
2022-03-30 21:42:25 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-94dd1070-kafka to be ready
2022-03-30 21:42:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-30 21:42:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-0)
2022-03-30 21:42:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1799996ms till timeout)
2022-03-30 21:42:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (306368ms till timeout)
2022-03-30 21:42:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-0)
2022-03-30 21:42:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1798991ms till timeout)
2022-03-30 21:42:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (305364ms till timeout)
2022-03-30 21:42:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-0)
2022-03-30 21:42:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1797986ms till timeout)
2022-03-30 21:42:28 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (304361ms till timeout)
2022-03-30 21:42:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:42:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-0)
2022-03-30 21:42:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1796981ms till timeout)
2022-03-30 21:42:29 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (303358ms till timeout)
2022-03-30 21:42:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:29 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-0)
2022-03-30 21:42:29 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1795973ms till timeout)
2022-03-30 21:42:30 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (302354ms till timeout)
2022-03-30 21:42:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-0)
2022-03-30 21:42:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1794969ms till timeout)
2022-03-30 21:42:31 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (301351ms till timeout)
2022-03-30 21:42:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:32 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-0)
2022-03-30 21:42:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1793963ms till timeout)
2022-03-30 21:42:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (300348ms till timeout)
2022-03-30 21:42:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-0)
2022-03-30 21:42:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1792958ms till timeout)
2022-03-30 21:42:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (299346ms till timeout)
2022-03-30 21:42:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:42:34 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-0)
2022-03-30 21:42:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1791954ms till timeout)
2022-03-30 21:42:34 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (298343ms till timeout)
2022-03-30 21:42:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-0)
2022-03-30 21:42:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1790949ms till timeout)
2022-03-30 21:42:35 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (297340ms till timeout)
2022-03-30 21:42:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-0)
2022-03-30 21:42:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1789944ms till timeout)
2022-03-30 21:42:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (296338ms till timeout)
2022-03-30 21:42:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:37 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-0)
2022-03-30 21:42:37 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1788939ms till timeout)
2022-03-30 21:42:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (295335ms till timeout)
2022-03-30 21:42:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-0)
2022-03-30 21:42:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1787934ms till timeout)
2022-03-30 21:42:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (294332ms till timeout)
2022-03-30 21:42:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:42:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-0)
2022-03-30 21:42:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1786929ms till timeout)
2022-03-30 21:42:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (293330ms till timeout)
2022-03-30 21:42:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-0)
2022-03-30 21:42:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1785924ms till timeout)
2022-03-30 21:42:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (292327ms till timeout)
2022-03-30 21:42:41 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-94dd1070-kafka-0)
2022-03-30 21:42:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1784919ms till timeout)
2022-03-30 21:42:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (291324ms till timeout)
2022-03-30 21:42:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:42:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:42:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:42:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-94dd1070-kafka-0, my-cluster-94dd1070-kafka-1, my-cluster-94dd1070-kafka-2 are ready
2022-03-30 21:42:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1783914ms till timeout)
2022-03-30 21:42:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (290321ms till timeout)
2022-03-30 21:42:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:42:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:42:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:42:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-94dd1070-kafka-0, my-cluster-94dd1070-kafka-1, my-cluster-94dd1070-kafka-2 are ready
2022-03-30 21:42:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1782909ms till timeout)
2022-03-30 21:42:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (289318ms till timeout)
2022-03-30 21:42:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:42:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:42:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:42:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:42:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-94dd1070-kafka-0, my-cluster-94dd1070-kafka-1, my-cluster-94dd1070-kafka-2 are ready
2022-03-30 21:42:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1781904ms till timeout)
2022-03-30 21:42:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (288315ms till timeout)
2022-03-30 21:42:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:42:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:42:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:42:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-94dd1070-kafka-0, my-cluster-94dd1070-kafka-1, my-cluster-94dd1070-kafka-2 are ready
2022-03-30 21:42:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1780900ms till timeout)
2022-03-30 21:42:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (287310ms till timeout)
2022-03-30 21:42:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:42:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:42:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:42:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-94dd1070-kafka-0, my-cluster-94dd1070-kafka-1, my-cluster-94dd1070-kafka-2 are ready
2022-03-30 21:42:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1779895ms till timeout)
2022-03-30 21:42:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (286274ms till timeout)
2022-03-30 21:42:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:47 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:42:47 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:42:47 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:42:47 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-94dd1070-kafka-0, my-cluster-94dd1070-kafka-1, my-cluster-94dd1070-kafka-2 are ready
2022-03-30 21:42:47 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1778890ms till timeout)
2022-03-30 21:42:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (285270ms till timeout)
2022-03-30 21:42:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:42:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:42:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:42:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-94dd1070-kafka-0, my-cluster-94dd1070-kafka-1, my-cluster-94dd1070-kafka-2 are ready
2022-03-30 21:42:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1777883ms till timeout)
2022-03-30 21:42:48 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (284266ms till timeout)
2022-03-30 21:42:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:42:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:42:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:42:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:42:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-94dd1070-kafka-0, my-cluster-94dd1070-kafka-1, my-cluster-94dd1070-kafka-2 are ready
2022-03-30 21:42:49 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1776878ms till timeout)
2022-03-30 21:42:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (283263ms till timeout)
2022-03-30 21:42:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:42:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:42:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:42:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-94dd1070-kafka-0, my-cluster-94dd1070-kafka-1, my-cluster-94dd1070-kafka-2 are ready
2022-03-30 21:42:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1775873ms till timeout)
2022-03-30 21:42:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (282259ms till timeout)
2022-03-30 21:42:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:42:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:42:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:42:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-94dd1070-kafka-0, my-cluster-94dd1070-kafka-1, my-cluster-94dd1070-kafka-2 are ready
2022-03-30 21:42:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-94dd1070-kafka, strimzi.io/cluster=my-cluster-94dd1070, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1774868ms till timeout)
2022-03-30 21:42:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (281256ms till timeout)
2022-03-30 21:42:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-0 not ready: kafka)
2022-03-30 21:42:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-1 not ready: kafka)
2022-03-30 21:42:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-94dd1070-kafka-2 not ready: kafka)
2022-03-30 21:42:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-94dd1070-kafka-0, my-cluster-94dd1070-kafka-1, my-cluster-94dd1070-kafka-2 are ready
2022-03-30 21:42:52 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-94dd1070 will have desired state: Ready
2022-03-30 21:42:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-94dd1070 will have desired state: Ready
2022-03-30 21:42:52 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-94dd1070 is in desired state: Ready
2022-03-30 21:42:52 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-94dd1070 is ready
2022-03-30 21:42:52 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateST:356] Kafka scale down to 3 finished
2022-03-30 21:42:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Consumer will successfully receive messages.
2022-03-30 21:42:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3cc4786a, which are set.
2022-03-30 21:42:52 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@7607761b, messages=[], arguments=[--topic, my-topic-741363275-285355018, --bootstrap-server, my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093, --max-messages, 100, USER=my_user_1001559761_691587912, --group-id, my-consumer-group-29672203, --group-instance-id, instance687430316], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-94dd1070-kafka-clients-65ccf55595-d4d87', podNamespace='namespace-5', bootstrapServer='my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093', topicName='my-topic-741363275-285355018', maxMessages=100, kafkaUsername='my-user-1001559761-691587912', consumerGroupName='my-consumer-group-29672203', consumerInstanceId='instance687430316', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3cc4786a}
2022-03-30 21:42:52 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093:my-topic-741363275-285355018 from pod my-cluster-94dd1070-kafka-clients-65ccf55595-d4d87
2022-03-30 21:42:52 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-94dd1070-kafka-clients-65ccf55595-d4d87 -n namespace-5 -- /opt/kafka/consumer.sh --topic my-topic-741363275-285355018 --bootstrap-server my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093 --max-messages 100 USER=my_user_1001559761_691587912 --group-id my-consumer-group-29672203 --group-instance-id instance687430316
2022-03-30 21:42:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-94dd1070-kafka-clients-65ccf55595-d4d87 -n namespace-5 -- /opt/kafka/consumer.sh --topic my-topic-741363275-285355018 --bootstrap-server my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093 --max-messages 100 USER=my_user_1001559761_691587912 --group-id my-consumer-group-29672203 --group-instance-id instance687430316
2022-03-30 21:42:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (280253ms till timeout)
2022-03-30 21:42:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (279249ms till timeout)
2022-03-30 21:42:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:42:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (278246ms till timeout)
2022-03-30 21:42:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (277243ms till timeout)
2022-03-30 21:42:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (276240ms till timeout)
2022-03-30 21:42:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (275236ms till timeout)
2022-03-30 21:42:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (274233ms till timeout)
2022-03-30 21:42:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:42:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (273229ms till timeout)
2022-03-30 21:42:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:42:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:42:59 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 21:42:59 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-03-30 21:42:59 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-741363275-285355018-new in namespace infra-namespace
2022-03-30 21:42:59 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-03-30 21:42:59 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-741363275-285355018-new
2022-03-30 21:42:59 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-741363275-285355018-new will have desired state: Ready
2022-03-30 21:42:59 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-741363275-285355018-new will have desired state: Ready
2022-03-30 21:42:59 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-741363275-285355018-new will have desired state: Ready not ready, will try again in 1000 ms (179999ms till timeout)
2022-03-30 21:43:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (272226ms till timeout)
2022-03-30 21:43:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-741363275-285355018-new will have desired state: Ready not ready, will try again in 1000 ms (178995ms till timeout)
2022-03-30 21:43:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (271222ms till timeout)
2022-03-30 21:43:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-741363275-285355018-new will have desired state: Ready not ready, will try again in 1000 ms (177992ms till timeout)
2022-03-30 21:43:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (270219ms till timeout)
2022-03-30 21:43:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-741363275-285355018-new will have desired state: Ready not ready, will try again in 1000 ms (176989ms till timeout)
2022-03-30 21:43:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (269215ms till timeout)
2022-03-30 21:43:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-741363275-285355018-new will have desired state: Ready not ready, will try again in 1000 ms (175986ms till timeout)
2022-03-30 21:43:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:43:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (268212ms till timeout)
2022-03-30 21:43:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:04 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-741363275-285355018-new is in desired state: Ready
2022-03-30 21:43:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Producer and consumer will successfully send and receive messages.
2022-03-30 21:43:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5430d921, which are set.
2022-03-30 21:43:04 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@417d031e, messages=[], arguments=[--topic, my-topic-741363275-285355018-new, --bootstrap-server, my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093, --max-messages, 100, USER=my_user_1001559761_691587912], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-94dd1070-kafka-clients-65ccf55595-d4d87', podNamespace='namespace-5', bootstrapServer='my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093', topicName='my-topic-741363275-285355018-new', maxMessages=100, kafkaUsername='my-user-1001559761-691587912', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5430d921}
2022-03-30 21:43:04 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093:my-topic-741363275-285355018-new from pod my-cluster-94dd1070-kafka-clients-65ccf55595-d4d87
2022-03-30 21:43:04 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-94dd1070-kafka-clients-65ccf55595-d4d87 -n namespace-5 -- /opt/kafka/producer.sh --topic my-topic-741363275-285355018-new --bootstrap-server my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093 --max-messages 100 USER=my_user_1001559761_691587912
2022-03-30 21:43:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-94dd1070-kafka-clients-65ccf55595-d4d87 -n namespace-5 -- /opt/kafka/producer.sh --topic my-topic-741363275-285355018-new --bootstrap-server my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093 --max-messages 100 USER=my_user_1001559761_691587912
2022-03-30 21:43:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (267209ms till timeout)
2022-03-30 21:43:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (266205ms till timeout)
2022-03-30 21:43:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (265202ms till timeout)
2022-03-30 21:43:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:08 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-30 21:43:08 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-03-30 21:43:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@218d7feb, which are set.
2022-03-30 21:43:08 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@1fe80c57, messages=[], arguments=[--topic, my-topic-741363275-285355018-new, --bootstrap-server, my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093, --max-messages, 100, USER=my_user_1001559761_691587912, --group-id, my-consumer-group-2050645639, --group-instance-id, instance654425291], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-94dd1070-kafka-clients-65ccf55595-d4d87', podNamespace='namespace-5', bootstrapServer='my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093', topicName='my-topic-741363275-285355018-new', maxMessages=100, kafkaUsername='my-user-1001559761-691587912', consumerGroupName='my-consumer-group-2050645639', consumerInstanceId='instance654425291', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@218d7feb}
2022-03-30 21:43:08 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093:my-topic-741363275-285355018-new from pod my-cluster-94dd1070-kafka-clients-65ccf55595-d4d87
2022-03-30 21:43:08 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-94dd1070-kafka-clients-65ccf55595-d4d87 -n namespace-5 -- /opt/kafka/consumer.sh --topic my-topic-741363275-285355018-new --bootstrap-server my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093 --max-messages 100 USER=my_user_1001559761_691587912 --group-id my-consumer-group-2050645639 --group-instance-id instance654425291
2022-03-30 21:43:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-94dd1070-kafka-clients-65ccf55595-d4d87 -n namespace-5 -- /opt/kafka/consumer.sh --topic my-topic-741363275-285355018-new --bootstrap-server my-cluster-94dd1070-kafka-bootstrap.namespace-5.svc:9093 --max-messages 100 USER=my_user_1001559761_691587912 --group-id my-consumer-group-2050645639 --group-instance-id instance654425291
2022-03-30 21:43:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (264198ms till timeout)
2022-03-30 21:43:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:43:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (263195ms till timeout)
2022-03-30 21:43:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (262192ms till timeout)
2022-03-30 21:43:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (261188ms till timeout)
2022-03-30 21:43:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (260185ms till timeout)
2022-03-30 21:43:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (259180ms till timeout)
2022-03-30 21:43:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:43:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (258177ms till timeout)
2022-03-30 21:43:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:14 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 21:43:14 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-03-30 21:43:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 21:43:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:675] [rollingupdate.RollingUpdateST - After Each] - Clean up after test
2022-03-30 21:43:14 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 21:43:14 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaAndZookeeperScaleUpScaleDown
2022-03-30 21:43:14 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-741363275-285355018 in namespace namespace-5
2022-03-30 21:43:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-741363275-285355018
2022-03-30 21:43:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-741363275-285355018 not ready, will try again in 10000 ms (179988ms till timeout)
2022-03-30 21:43:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (257174ms till timeout)
2022-03-30 21:43:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (256170ms till timeout)
2022-03-30 21:43:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (255167ms till timeout)
2022-03-30 21:43:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (254164ms till timeout)
2022-03-30 21:43:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:43:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (253161ms till timeout)
2022-03-30 21:43:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (252157ms till timeout)
2022-03-30 21:43:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (251154ms till timeout)
2022-03-30 21:43:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (250150ms till timeout)
2022-03-30 21:43:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:23 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (249147ms till timeout)
2022-03-30 21:43:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:43:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (248143ms till timeout)
2022-03-30 21:43:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:24 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-741363275-285355018-new in namespace namespace-5
2022-03-30 21:43:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-741363275-285355018-new
2022-03-30 21:43:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-741363275-285355018-new not ready, will try again in 10000 ms (179995ms till timeout)
2022-03-30 21:43:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (247139ms till timeout)
2022-03-30 21:43:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (246135ms till timeout)
2022-03-30 21:43:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (245132ms till timeout)
2022-03-30 21:43:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:28 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (244128ms till timeout)
2022-03-30 21:43:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:43:29 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (243125ms till timeout)
2022-03-30 21:43:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:30 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (242122ms till timeout)
2022-03-30 21:43:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:31 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (241119ms till timeout)
2022-03-30 21:43:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (240115ms till timeout)
2022-03-30 21:43:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (239111ms till timeout)
2022-03-30 21:43:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:43:34 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (238108ms till timeout)
2022-03-30 21:43:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:34 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-94dd1070-kafka-clients in namespace namespace-5
2022-03-30 21:43:34 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-94dd1070-kafka-clients
2022-03-30 21:43:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-94dd1070-kafka-clients not ready, will try again in 10000 ms (479952ms till timeout)
2022-03-30 21:43:35 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (237105ms till timeout)
2022-03-30 21:43:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (236101ms till timeout)
2022-03-30 21:43:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (235098ms till timeout)
2022-03-30 21:43:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (234095ms till timeout)
2022-03-30 21:43:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:43:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (233091ms till timeout)
2022-03-30 21:43:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (232088ms till timeout)
2022-03-30 21:43:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (231084ms till timeout)
2022-03-30 21:43:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (230081ms till timeout)
2022-03-30 21:43:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (229078ms till timeout)
2022-03-30 21:43:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:43:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (228074ms till timeout)
2022-03-30 21:43:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-94dd1070-kafka-clients not ready, will try again in 10000 ms (469943ms till timeout)
2022-03-30 21:43:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (227071ms till timeout)
2022-03-30 21:43:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (226067ms till timeout)
2022-03-30 21:43:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (225063ms till timeout)
2022-03-30 21:43:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:48 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (224061ms till timeout)
2022-03-30 21:43:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:43:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (223057ms till timeout)
2022-03-30 21:43:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (222053ms till timeout)
2022-03-30 21:43:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (221050ms till timeout)
2022-03-30 21:43:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (220047ms till timeout)
2022-03-30 21:43:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (219043ms till timeout)
2022-03-30 21:43:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:43:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (218040ms till timeout)
2022-03-30 21:43:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-94dd1070-kafka-clients not ready, will try again in 10000 ms (459933ms till timeout)
2022-03-30 21:43:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (217037ms till timeout)
2022-03-30 21:43:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (216034ms till timeout)
2022-03-30 21:43:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (215030ms till timeout)
2022-03-30 21:43:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:43:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (214027ms till timeout)
2022-03-30 21:43:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:43:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (213024ms till timeout)
2022-03-30 21:43:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:43:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (212020ms till timeout)
2022-03-30 21:44:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (211017ms till timeout)
2022-03-30 21:44:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (210014ms till timeout)
2022-03-30 21:44:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (209010ms till timeout)
2022-03-30 21:44:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:44:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (208007ms till timeout)
2022-03-30 21:44:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-94dd1070-kafka-clients not ready, will try again in 10000 ms (449924ms till timeout)
2022-03-30 21:44:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (207004ms till timeout)
2022-03-30 21:44:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (206001ms till timeout)
2022-03-30 21:44:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (204998ms till timeout)
2022-03-30 21:44:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (203994ms till timeout)
2022-03-30 21:44:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:44:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (202991ms till timeout)
2022-03-30 21:44:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (201988ms till timeout)
2022-03-30 21:44:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (200984ms till timeout)
2022-03-30 21:44:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (199981ms till timeout)
2022-03-30 21:44:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (198977ms till timeout)
2022-03-30 21:44:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:44:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (197974ms till timeout)
2022-03-30 21:44:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1001559761-691587912 in namespace namespace-5
2022-03-30 21:44:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1001559761-691587912
2022-03-30 21:44:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1001559761-691587912 not ready, will try again in 10000 ms (179994ms till timeout)
2022-03-30 21:44:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (196972ms till timeout)
2022-03-30 21:44:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (195969ms till timeout)
2022-03-30 21:44:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (194966ms till timeout)
2022-03-30 21:44:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (193963ms till timeout)
2022-03-30 21:44:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:44:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (192959ms till timeout)
2022-03-30 21:44:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (191956ms till timeout)
2022-03-30 21:44:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (190953ms till timeout)
2022-03-30 21:44:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (189949ms till timeout)
2022-03-30 21:44:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:23 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (188946ms till timeout)
2022-03-30 21:44:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:44:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (187943ms till timeout)
2022-03-30 21:44:25 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-94dd1070 in namespace namespace-5
2022-03-30 21:44:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-94dd1070
2022-03-30 21:44:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-94dd1070 not ready, will try again in 10000 ms (839997ms till timeout)
2022-03-30 21:44:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (186940ms till timeout)
2022-03-30 21:44:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (185937ms till timeout)
2022-03-30 21:44:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (184934ms till timeout)
2022-03-30 21:44:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:28 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (183930ms till timeout)
2022-03-30 21:44:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:44:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:29 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (182927ms till timeout)
2022-03-30 21:44:30 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (181922ms till timeout)
2022-03-30 21:44:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:31 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (180919ms till timeout)
2022-03-30 21:44:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (179915ms till timeout)
2022-03-30 21:44:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (178912ms till timeout)
2022-03-30 21:44:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:44:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:34 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (177909ms till timeout)
2022-03-30 21:44:35 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 21:44:35 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-5 for test case:testKafkaAndZookeeperScaleUpScaleDown
2022-03-30 21:44:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-5 removal
2022-03-30 21:44:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:44:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (479924ms till timeout)
2022-03-30 21:44:35 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (176906ms till timeout)
2022-03-30 21:44:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:44:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (478847ms till timeout)
2022-03-30 21:44:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (175902ms till timeout)
2022-03-30 21:44:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:37 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:37 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:37 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:44:37 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (477775ms till timeout)
2022-03-30 21:44:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (174897ms till timeout)
2022-03-30 21:44:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:44:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (476697ms till timeout)
2022-03-30 21:44:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (173895ms till timeout)
2022-03-30 21:44:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:44:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:44:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (475629ms till timeout)
2022-03-30 21:44:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (172892ms till timeout)
2022-03-30 21:44:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (171889ms till timeout)
2022-03-30 21:44:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:44:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (474555ms till timeout)
2022-03-30 21:44:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (170886ms till timeout)
2022-03-30 21:44:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:41 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:41 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:44:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (473482ms till timeout)
2022-03-30 21:44:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (169883ms till timeout)
2022-03-30 21:44:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:44:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (472409ms till timeout)
2022-03-30 21:44:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (168880ms till timeout)
2022-03-30 21:44:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:44:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (471333ms till timeout)
2022-03-30 21:44:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:44:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (167877ms till timeout)
2022-03-30 21:44:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:44:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (470254ms till timeout)
2022-03-30 21:44:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (166873ms till timeout)
2022-03-30 21:44:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:44:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (469174ms till timeout)
2022-03-30 21:44:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (165870ms till timeout)
2022-03-30 21:44:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:44:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (468100ms till timeout)
2022-03-30 21:44:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (164867ms till timeout)
2022-03-30 21:44:47 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:44:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (467032ms till timeout)
2022-03-30 21:44:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:48 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (163864ms till timeout)
2022-03-30 21:44:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:44:49 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:44:49 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (465954ms till timeout)
2022-03-30 21:44:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (162860ms till timeout)
2022-03-30 21:44:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:44:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (464881ms till timeout)
2022-03-30 21:44:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (161857ms till timeout)
2022-03-30 21:44:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:44:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (463802ms till timeout)
2022-03-30 21:44:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (160854ms till timeout)
2022-03-30 21:44:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:44:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (462724ms till timeout)
2022-03-30 21:44:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (159850ms till timeout)
2022-03-30 21:44:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:44:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (461650ms till timeout)
2022-03-30 21:44:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (158847ms till timeout)
2022-03-30 21:44:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:44:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:44:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (460580ms till timeout)
2022-03-30 21:44:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (157844ms till timeout)
2022-03-30 21:44:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (156838ms till timeout)
2022-03-30 21:44:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:44:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (459486ms till timeout)
2022-03-30 21:44:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (155835ms till timeout)
2022-03-30 21:44:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:44:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (458414ms till timeout)
2022-03-30 21:44:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (154826ms till timeout)
2022-03-30 21:44:57 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:57 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:57 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:44:57 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (457336ms till timeout)
2022-03-30 21:44:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (153823ms till timeout)
2022-03-30 21:44:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:44:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (456259ms till timeout)
2022-03-30 21:44:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:44:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:44:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:44:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (152819ms till timeout)
2022-03-30 21:44:59 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:59 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:44:59 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:44:59 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (455185ms till timeout)
2022-03-30 21:45:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (151816ms till timeout)
2022-03-30 21:45:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:45:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (454111ms till timeout)
2022-03-30 21:45:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:45:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (150812ms till timeout)
2022-03-30 21:45:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:45:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:02 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:02 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:45:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (453016ms till timeout)
2022-03-30 21:45:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:45:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (149809ms till timeout)
2022-03-30 21:45:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:45:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (451938ms till timeout)
2022-03-30 21:45:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:45:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (148806ms till timeout)
2022-03-30 21:45:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:45:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:45:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (450860ms till timeout)
2022-03-30 21:45:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:45:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-entity-operator will be ready not ready, will try again in 1000 ms (147802ms till timeout)
2022-03-30 21:45:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:45:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (449782ms till timeout)
2022-03-30 21:45:05 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-58833349-entity-operator is ready
2022-03-30 21:45:05 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-58833349-entity-operator}, additionalProperties={})to be ready
2022-03-30 21:45:05 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-entity-operator-7c4b54df69-qjnww not ready: tls-sidecar)
2022-03-30 21:45:05 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-entity-operator-7c4b54df69-qjnww not ready: topic-operator)
2022-03-30 21:45:05 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-entity-operator-7c4b54df69-qjnww not ready: user-operator)
2022-03-30 21:45:05 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-entity-operator-7c4b54df69-qjnww are ready
2022-03-30 21:45:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-58833349-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599996ms till timeout)
2022-03-30 21:45:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:45:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:45:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (448690ms till timeout)
2022-03-30 21:45:06 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-entity-operator-7c4b54df69-qjnww not ready: tls-sidecar)
2022-03-30 21:45:06 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-entity-operator-7c4b54df69-qjnww not ready: topic-operator)
2022-03-30 21:45:06 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-entity-operator-7c4b54df69-qjnww not ready: user-operator)
2022-03-30 21:45:06 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-entity-operator-7c4b54df69-qjnww are ready
2022-03-30 21:45:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-58833349-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598992ms till timeout)
2022-03-30 21:45:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:45:07 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:45:07 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:07 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:45:07 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (447608ms till timeout)
2022-03-30 21:45:07 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-entity-operator-7c4b54df69-qjnww not ready: tls-sidecar)
2022-03-30 21:45:07 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-entity-operator-7c4b54df69-qjnww not ready: topic-operator)
2022-03-30 21:45:07 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-entity-operator-7c4b54df69-qjnww not ready: user-operator)
2022-03-30 21:45:07 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-entity-operator-7c4b54df69-qjnww are ready
2022-03-30 21:45:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-58833349-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597987ms till timeout)
2022-03-30 21:45:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:45:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:45:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (446521ms till timeout)
2022-03-30 21:45:08 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-entity-operator-7c4b54df69-qjnww not ready: tls-sidecar)
2022-03-30 21:45:08 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-entity-operator-7c4b54df69-qjnww not ready: topic-operator)
2022-03-30 21:45:08 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-entity-operator-7c4b54df69-qjnww not ready: user-operator)
2022-03-30 21:45:08 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-entity-operator-7c4b54df69-qjnww are ready
2022-03-30 21:45:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-58833349-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596982ms till timeout)
2022-03-30 21:45:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:45:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:45:09 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:09 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-entity-operator-7c4b54df69-qjnww not ready: tls-sidecar)
2022-03-30 21:45:09 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-entity-operator-7c4b54df69-qjnww not ready: topic-operator)
2022-03-30 21:45:09 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-entity-operator-7c4b54df69-qjnww not ready: user-operator)
2022-03-30 21:45:09 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-entity-operator-7c4b54df69-qjnww are ready
2022-03-30 21:45:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-58833349-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595977ms till timeout)
2022-03-30 21:45:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:45:09 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (445437ms till timeout)
2022-03-30 21:45:10 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-entity-operator-7c4b54df69-qjnww not ready: tls-sidecar)
2022-03-30 21:45:10 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-entity-operator-7c4b54df69-qjnww not ready: topic-operator)
2022-03-30 21:45:10 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-entity-operator-7c4b54df69-qjnww not ready: user-operator)
2022-03-30 21:45:10 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-entity-operator-7c4b54df69-qjnww are ready
2022-03-30 21:45:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-58833349-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594972ms till timeout)
2022-03-30 21:45:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:45:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (444364ms till timeout)
2022-03-30 21:45:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:45:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:45:11 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-entity-operator-7c4b54df69-qjnww not ready: tls-sidecar)
2022-03-30 21:45:11 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-entity-operator-7c4b54df69-qjnww not ready: topic-operator)
2022-03-30 21:45:11 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-entity-operator-7c4b54df69-qjnww not ready: user-operator)
2022-03-30 21:45:11 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-entity-operator-7c4b54df69-qjnww are ready
2022-03-30 21:45:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-58833349-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593967ms till timeout)
2022-03-30 21:45:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:45:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (443283ms till timeout)
2022-03-30 21:45:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:45:12 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-entity-operator-7c4b54df69-qjnww not ready: tls-sidecar)
2022-03-30 21:45:12 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-entity-operator-7c4b54df69-qjnww not ready: topic-operator)
2022-03-30 21:45:12 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-entity-operator-7c4b54df69-qjnww not ready: user-operator)
2022-03-30 21:45:12 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-entity-operator-7c4b54df69-qjnww are ready
2022-03-30 21:45:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-58833349-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592963ms till timeout)
2022-03-30 21:45:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:45:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (442212ms till timeout)
2022-03-30 21:45:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:45:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-entity-operator-7c4b54df69-qjnww not ready: tls-sidecar)
2022-03-30 21:45:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-entity-operator-7c4b54df69-qjnww not ready: topic-operator)
2022-03-30 21:45:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-entity-operator-7c4b54df69-qjnww not ready: user-operator)
2022-03-30 21:45:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-entity-operator-7c4b54df69-qjnww are ready
2022-03-30 21:45:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-58833349-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591958ms till timeout)
2022-03-30 21:45:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:45:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (441139ms till timeout)
2022-03-30 21:45:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:45:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:45:14 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-entity-operator-7c4b54df69-qjnww not ready: tls-sidecar)
2022-03-30 21:45:14 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-entity-operator-7c4b54df69-qjnww not ready: topic-operator)
2022-03-30 21:45:14 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-entity-operator-7c4b54df69-qjnww not ready: user-operator)
2022-03-30 21:45:14 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-entity-operator-7c4b54df69-qjnww are ready
2022-03-30 21:45:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-58833349-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590953ms till timeout)
2022-03-30 21:45:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:45:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (440066ms till timeout)
2022-03-30 21:45:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-entity-operator-7c4b54df69-qjnww not ready: tls-sidecar)
2022-03-30 21:45:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-entity-operator-7c4b54df69-qjnww not ready: topic-operator)
2022-03-30 21:45:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-entity-operator-7c4b54df69-qjnww not ready: user-operator)
2022-03-30 21:45:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-entity-operator-7c4b54df69-qjnww are ready
2022-03-30 21:45:15 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-58833349-entity-operator rolling update finished
2022-03-30 21:45:15 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:312] Wait for CC and KE to rolling restart ...
2022-03-30 21:45:15 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-58833349-kafka-exporter rolling update
2022-03-30 21:45:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Deployment my-cluster-58833349-kafka-exporter rolling update in namespace:namespace-3
2022-03-30 21:45:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:45:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:45:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:45:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Deployment my-cluster-58833349-kafka-exporter rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (599994ms till timeout)
2022-03-30 21:45:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:45:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (438987ms till timeout)
2022-03-30 21:45:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:45:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:45:17 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:45:17 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (437911ms till timeout)
2022-03-30 21:45:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:45:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:45:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 21:45:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 21:45:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 21:45:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-5" not found
2022-03-30 21:45:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 21:45:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[], io.strimzi.test.logs.CollectorElement@3d7a59f3=[], io.strimzi.test.logs.CollectorElement@dce7372d=[rolling-update-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-3], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-30 21:45:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:267] testKafkaAndZookeeperScaleUpScaleDown - Notifies waiting test cases:[testCruiseControlBasicAPIRequests, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testUpdateUser, testSendSimpleMessageTls, testAutoRenewAllCaCertsTriggeredByAnno, testReceiveSimpleMessageTls, testSendMessagesCustomListenerTlsScramSha, testKafkaAndZookeeperScaleUpScaleDown] to and randomly select one to start execution
2022-03-30 21:45:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:93] [rollingupdate.RollingUpdateST] - Removing parallel test: testKafkaAndZookeeperScaleUpScaleDown
2022-03-30 21:45:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:97] [rollingupdate.RollingUpdateST] - Parallel test count: 1
2022-03-30 21:45:18 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testKafkaAndZookeeperScaleUpScaleDown-FINISHED
2022-03-30 21:45:18 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 21:45:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 21:45:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:690] [rollingupdate.RollingUpdateST - After All] - Clean up after test suite
2022-03-30 21:45:18 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 21:45:18 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:346] In context RollingUpdateST is everything deleted.
2022-03-30 21:45:18 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 21:45:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Namespace rolling-update-st removal
2022-03-30 21:45:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-30 21:45:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-30 21:45:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:45:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace rolling-update-st removal not ready, will try again in 1000 ms (479928ms till timeout)
2022-03-30 21:45:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 3
2022-03-30 21:45:19 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-30 21:45:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-30 21:45:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:45:19 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace rolling-update-st removal not ready, will try again in 1000 ms (478850ms till timeout)
2022-03-30 21:45:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 3
2022-03-30 21:45:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-30 21:45:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-30 21:45:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:45:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace rolling-update-st removal not ready, will try again in 1000 ms (477771ms till timeout)
2022-03-30 21:45:20 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:45:20 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:45:20 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:45:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Deployment my-cluster-58833349-kafka-exporter rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (594987ms till timeout)
2022-03-30 21:45:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 21:45:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-30 21:45:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-30 21:45:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:45:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace rolling-update-st removal not ready, will try again in 1000 ms (476691ms till timeout)
2022-03-30 21:45:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 21:45:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 21:45:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-30 21:45:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-30 21:45:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:45:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace rolling-update-st removal not ready, will try again in 1000 ms (475619ms till timeout)
2022-03-30 21:45:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 21:45:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-30 21:45:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-30 21:45:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 21:45:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 21:45:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "rolling-update-st" not found
2022-03-30 21:45:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 21:45:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[], io.strimzi.test.logs.CollectorElement@3d7a59f3=[], io.strimzi.test.logs.CollectorElement@dce7372d=[], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-3], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-30 21:45:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:254] RollingUpdateST - Notifies waiting test suites:[CruiseControlST, HttpBridgeTlsST, UserST, CruiseControlApiST, ListenersST, SecurityST, RollingUpdateST] to and randomly select one to start execution
2022-03-30 21:45:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:85] [rollingupdate.RollingUpdateST] - Removing parallel suite: RollingUpdateST
2022-03-30 21:45:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:89] [rollingupdate.RollingUpdateST] - Parallel suites count: 2
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 661.676 s - in io.strimzi.systemtest.rollingupdate.RollingUpdateST
2022-03-30 21:45:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:45:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:45:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:45:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:45:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Deployment my-cluster-58833349-kafka-exporter rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (589981ms till timeout)
2022-03-30 21:45:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:45:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:30 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:45:30 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-58833349-kafka-exporter-5c766d57c8-d5nng=eef61860-c57b-4c1b-8daa-ef8d11d220cf, my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:45:30 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:45:30 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Deployment my-cluster-58833349-kafka-exporter rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (584974ms till timeout)
2022-03-30 21:45:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:45:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:35 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:45:35 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-58833349-kafka-exporter-5c766d57c8-d5nng=eef61860-c57b-4c1b-8daa-ef8d11d220cf, my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:45:35 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:45:35 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Deployment my-cluster-58833349-kafka-exporter rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (579967ms till timeout)
2022-03-30 21:45:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:45:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:40 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:45:40 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-58833349-kafka-exporter-5c766d57c8-d5nng=eef61860-c57b-4c1b-8daa-ef8d11d220cf, my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:45:40 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:45:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Deployment my-cluster-58833349-kafka-exporter rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (574960ms till timeout)
2022-03-30 21:45:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:45:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:45 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:45:45 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-58833349-kafka-exporter-5c766d57c8-d5nng=eef61860-c57b-4c1b-8daa-ef8d11d220cf, my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:45:45 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:45:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Deployment my-cluster-58833349-kafka-exporter rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (569953ms till timeout)
2022-03-30 21:45:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:45:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:50 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:45:50 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-58833349-kafka-exporter-5c766d57c8-d5nng=eef61860-c57b-4c1b-8daa-ef8d11d220cf, my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:45:50 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:45:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Deployment my-cluster-58833349-kafka-exporter rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (564947ms till timeout)
2022-03-30 21:45:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:45:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:55 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:45:55 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-58833349-kafka-exporter-5c766d57c8-d5nng=eef61860-c57b-4c1b-8daa-ef8d11d220cf, my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:45:55 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:45:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Deployment my-cluster-58833349-kafka-exporter rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (559941ms till timeout)
2022-03-30 21:45:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:45:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:45:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:45:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:00 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:46:00 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-58833349-kafka-exporter-5c766d57c8-d5nng=eef61860-c57b-4c1b-8daa-ef8d11d220cf, my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:46:00 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:46:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Deployment my-cluster-58833349-kafka-exporter rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (554933ms till timeout)
2022-03-30 21:46:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:46:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:05 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:46:05 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-58833349-kafka-exporter-5c766d57c8-d5nng=eef61860-c57b-4c1b-8daa-ef8d11d220cf, my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:46:05 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:46:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Deployment my-cluster-58833349-kafka-exporter rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (549926ms till timeout)
2022-03-30 21:46:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:46:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:10 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-58833349-kafka-exporter-8549f4db77-9xxrd=965c03b6-20b3-44f4-aec9-87b9d690de6f}
2022-03-30 21:46:10 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-58833349-kafka-exporter-5c766d57c8-d5nng=eef61860-c57b-4c1b-8daa-ef8d11d220cf}
2022-03-30 21:46:10 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DeploymentUtils:119] All pods seem to have rolled
2022-03-30 21:46:10 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-58833349-kafka-exporter will be ready
2022-03-30 21:46:10 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-58833349-kafka-exporter will be ready
2022-03-30 21:46:10 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-58833349-kafka-exporter is ready
2022-03-30 21:46:10 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-58833349-kafka-exporter}, additionalProperties={})to be ready
2022-03-30 21:46:10 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-exporter-5c766d57c8-d5nng not ready: my-cluster-58833349-kafka-exporter)
2022-03-30 21:46:10 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-kafka-exporter-5c766d57c8-d5nng are ready
2022-03-30 21:46:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-58833349-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-30 21:46:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:11 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-exporter-5c766d57c8-d5nng not ready: my-cluster-58833349-kafka-exporter)
2022-03-30 21:46:11 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-kafka-exporter-5c766d57c8-d5nng are ready
2022-03-30 21:46:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-58833349-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598993ms till timeout)
2022-03-30 21:46:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:12 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-exporter-5c766d57c8-d5nng not ready: my-cluster-58833349-kafka-exporter)
2022-03-30 21:46:12 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-kafka-exporter-5c766d57c8-d5nng are ready
2022-03-30 21:46:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-58833349-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597988ms till timeout)
2022-03-30 21:46:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-exporter-5c766d57c8-d5nng not ready: my-cluster-58833349-kafka-exporter)
2022-03-30 21:46:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-kafka-exporter-5c766d57c8-d5nng are ready
2022-03-30 21:46:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-58833349-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596984ms till timeout)
2022-03-30 21:46:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:46:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:14 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-exporter-5c766d57c8-d5nng not ready: my-cluster-58833349-kafka-exporter)
2022-03-30 21:46:14 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-kafka-exporter-5c766d57c8-d5nng are ready
2022-03-30 21:46:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-58833349-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595979ms till timeout)
2022-03-30 21:46:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-exporter-5c766d57c8-d5nng not ready: my-cluster-58833349-kafka-exporter)
2022-03-30 21:46:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-kafka-exporter-5c766d57c8-d5nng are ready
2022-03-30 21:46:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-58833349-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594975ms till timeout)
2022-03-30 21:46:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:16 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-exporter-5c766d57c8-d5nng not ready: my-cluster-58833349-kafka-exporter)
2022-03-30 21:46:16 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-kafka-exporter-5c766d57c8-d5nng are ready
2022-03-30 21:46:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-58833349-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593970ms till timeout)
2022-03-30 21:46:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:17 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-exporter-5c766d57c8-d5nng not ready: my-cluster-58833349-kafka-exporter)
2022-03-30 21:46:17 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-kafka-exporter-5c766d57c8-d5nng are ready
2022-03-30 21:46:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-58833349-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592966ms till timeout)
2022-03-30 21:46:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:18 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-exporter-5c766d57c8-d5nng not ready: my-cluster-58833349-kafka-exporter)
2022-03-30 21:46:18 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-kafka-exporter-5c766d57c8-d5nng are ready
2022-03-30 21:46:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-58833349-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591962ms till timeout)
2022-03-30 21:46:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:46:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-exporter-5c766d57c8-d5nng not ready: my-cluster-58833349-kafka-exporter)
2022-03-30 21:46:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-kafka-exporter-5c766d57c8-d5nng are ready
2022-03-30 21:46:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-58833349, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-58833349-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590957ms till timeout)
2022-03-30 21:46:20 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-58833349-kafka-exporter-5c766d57c8-d5nng not ready: my-cluster-58833349-kafka-exporter)
2022-03-30 21:46:20 [ForkJoinPool-3-worker-5] [36mDEBUG[m [PodUtils:106] Pods my-cluster-58833349-kafka-exporter-5c766d57c8-d5nng are ready
2022-03-30 21:46:20 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-58833349-kafka-exporter rolling update finished
2022-03-30 21:46:20 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:317] Checking the certificates have been replaced
2022-03-30 21:46:20 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:331] Checking consumed messages to pod:my-cluster-58833349-kafka-clients-84f7bb746-62dqm
2022-03-30 21:46:20 [ForkJoinPool-3-worker-5] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3acca096, which are set.
2022-03-30 21:46:20 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@5781b468, messages=[], arguments=[--topic, my-topic-1210910585-1669754462, --bootstrap-server, my-cluster-58833349-kafka-bootstrap.namespace-3.svc:9092, --max-messages, 100, --group-id, my-consumer-group-516464903, --group-instance-id, instance1800041306], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-58833349-kafka-clients-84f7bb746-62dqm', podNamespace='namespace-3', bootstrapServer='my-cluster-58833349-kafka-bootstrap.namespace-3.svc:9092', topicName='my-topic-1210910585-1669754462', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-516464903', consumerInstanceId='instance1800041306', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3acca096}
2022-03-30 21:46:20 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-58833349-kafka-bootstrap.namespace-3.svc:9092#my-topic-1210910585-1669754462 from pod my-cluster-58833349-kafka-clients-84f7bb746-62dqm
2022-03-30 21:46:20 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-58833349-kafka-clients-84f7bb746-62dqm -n namespace-3 -- /opt/kafka/consumer.sh --topic my-topic-1210910585-1669754462 --bootstrap-server my-cluster-58833349-kafka-bootstrap.namespace-3.svc:9092 --max-messages 100 --group-id my-consumer-group-516464903 --group-instance-id instance1800041306
2022-03-30 21:46:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-58833349-kafka-clients-84f7bb746-62dqm -n namespace-3 -- /opt/kafka/consumer.sh --topic my-topic-1210910585-1669754462 --bootstrap-server my-cluster-58833349-kafka-bootstrap.namespace-3.svc:9092 --max-messages 100 --group-id my-consumer-group-516464903 --group-instance-id instance1800041306
2022-03-30 21:46:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:46:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:26 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-30 21:46:26 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-03-30 21:46:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser bob-my-cluster-58833349 in namespace infra-namespace
2022-03-30 21:46:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-3
2022-03-30 21:46:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:bob-my-cluster-58833349
2022-03-30 21:46:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: bob-my-cluster-58833349 will have desired state: Ready
2022-03-30 21:46:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: bob-my-cluster-58833349 will have desired state: Ready
2022-03-30 21:46:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaUser: bob-my-cluster-58833349 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 21:46:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:27 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaUser: bob-my-cluster-58833349 is in desired state: Ready
2022-03-30 21:46:27 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-58833349-kafka-clients-tls in namespace namespace-3
2022-03-30 21:46:27 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-3
2022-03-30 21:46:27 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-58833349-kafka-clients-tls
2022-03-30 21:46:27 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-58833349-kafka-clients-tls will be ready
2022-03-30 21:46:27 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-58833349-kafka-clients-tls will be ready
2022-03-30 21:46:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-kafka-clients-tls will be ready not ready, will try again in 1000 ms (479998ms till timeout)
2022-03-30 21:46:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:28 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-58833349-kafka-clients-tls will be ready not ready, will try again in 1000 ms (478995ms till timeout)
2022-03-30 21:46:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:46:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:29 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-58833349-kafka-clients-tls is ready
2022-03-30 21:46:29 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:355] Checking consumed messages to pod:my-cluster-58833349-kafka-clients-tls-64fddccbd5-z86sk
2022-03-30 21:46:29 [ForkJoinPool-3-worker-5] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@24b4f96, which are set.
2022-03-30 21:46:29 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@2b86db7d, messages=[], arguments=[--topic, my-topic-1210910585-1669754462, --bootstrap-server, my-cluster-58833349-kafka-bootstrap.namespace-3.svc:9093, --max-messages, 100, USER=bob_my_cluster_58833349, --group-id, my-consumer-group-37717791, --group-instance-id, instance1624425291], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-58833349-kafka-clients-tls-64fddccbd5-z86sk', podNamespace='namespace-3', bootstrapServer='my-cluster-58833349-kafka-bootstrap.namespace-3.svc:9093', topicName='my-topic-1210910585-1669754462', maxMessages=100, kafkaUsername='bob-my-cluster-58833349', consumerGroupName='my-consumer-group-37717791', consumerInstanceId='instance1624425291', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@24b4f96}
2022-03-30 21:46:29 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-58833349-kafka-bootstrap.namespace-3.svc:9093#my-topic-1210910585-1669754462 from pod my-cluster-58833349-kafka-clients-tls-64fddccbd5-z86sk
2022-03-30 21:46:29 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-58833349-kafka-clients-tls-64fddccbd5-z86sk -n namespace-3 -- /opt/kafka/consumer.sh --topic my-topic-1210910585-1669754462 --bootstrap-server my-cluster-58833349-kafka-bootstrap.namespace-3.svc:9093 --max-messages 100 USER=bob_my_cluster_58833349 --group-id my-consumer-group-37717791 --group-instance-id instance1624425291
2022-03-30 21:46:29 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-58833349-kafka-clients-tls-64fddccbd5-z86sk -n namespace-3 -- /opt/kafka/consumer.sh --topic my-topic-1210910585-1669754462 --bootstrap-server my-cluster-58833349-kafka-bootstrap.namespace-3.svc:9093 --max-messages 100 USER=bob_my_cluster_58833349 --group-id my-consumer-group-37717791 --group-instance-id instance1624425291
2022-03-30 21:46:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:46:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:36 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-30 21:46:36 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-03-30 21:46:36 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 21:46:36 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:675] [security.SecurityST - After Each] - Clean up after test
2022-03-30 21:46:36 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 21:46:36 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testAutoRenewAllCaCertsTriggeredByAnno
2022-03-30 21:46:36 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-58833349-kafka-clients in namespace namespace-3
2022-03-30 21:46:36 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-58833349-kafka-clients
2022-03-30 21:46:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-58833349-kafka-clients not ready, will try again in 10000 ms (479992ms till timeout)
2022-03-30 21:46:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:46:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:46:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-58833349-kafka-clients not ready, will try again in 10000 ms (469984ms till timeout)
2022-03-30 21:46:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:46:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:46:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-58833349-kafka-clients not ready, will try again in 10000 ms (459975ms till timeout)
2022-03-30 21:46:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:46:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:46:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:46:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:47:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-58833349-kafka-clients not ready, will try again in 10000 ms (449965ms till timeout)
2022-03-30 21:47:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:47:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:47:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-58833349-kafka-clients not ready, will try again in 10000 ms (439956ms till timeout)
2022-03-30 21:47:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:47:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:47:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-58833349-kafka-clients not ready, will try again in 10000 ms (429947ms till timeout)
2022-03-30 21:47:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:47:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:47:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-58833349-kafka-clients not ready, will try again in 10000 ms (419937ms till timeout)
2022-03-30 21:47:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:47:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:47:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-58833349-kafka-clients not ready, will try again in 10000 ms (409928ms till timeout)
2022-03-30 21:47:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:47:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:47:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-58833349-kafka-clients-tls in namespace namespace-3
2022-03-30 21:47:56 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-58833349-kafka-clients-tls
2022-03-30 21:47:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser bob-my-cluster-58833349 in namespace namespace-3
2022-03-30 21:47:56 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:bob-my-cluster-58833349
2022-03-30 21:47:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:bob-my-cluster-58833349 not ready, will try again in 10000 ms (179994ms till timeout)
2022-03-30 21:47:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:47:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:47:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:47:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:48:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:06 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1590558165-46977776 in namespace namespace-3
2022-03-30 21:48:06 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1590558165-46977776
2022-03-30 21:48:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1590558165-46977776 not ready, will try again in 10000 ms (179993ms till timeout)
2022-03-30 21:48:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:48:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:48:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:16 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1210910585-1669754462 in namespace namespace-3
2022-03-30 21:48:16 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1210910585-1669754462
2022-03-30 21:48:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1210910585-1669754462 not ready, will try again in 10000 ms (179995ms till timeout)
2022-03-30 21:48:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:48:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:48:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-58833349 in namespace namespace-3
2022-03-30 21:48:26 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-3, for cruise control Kafka cluster my-cluster-58833349
2022-03-30 21:48:26 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-58833349
2022-03-30 21:48:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-58833349 not ready, will try again in 10000 ms (839958ms till timeout)
2022-03-30 21:48:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:48:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:48:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:36 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 21:48:36 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-3 for test case:testAutoRenewAllCaCertsTriggeredByAnno
2022-03-30 21:48:36 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-3 removal
2022-03-30 21:48:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:36 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:36 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:48:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (479928ms till timeout)
2022-03-30 21:48:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:37 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:37 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:48:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (478857ms till timeout)
2022-03-30 21:48:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:38 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:38 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:48:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (477789ms till timeout)
2022-03-30 21:48:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:48:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:40 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:40 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:48:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (476708ms till timeout)
2022-03-30 21:48:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:41 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:41 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:48:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (475631ms till timeout)
2022-03-30 21:48:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:42 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:42 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:48:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (474557ms till timeout)
2022-03-30 21:48:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:43 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:43 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:48:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (473484ms till timeout)
2022-03-30 21:48:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:48:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:44 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:44 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:48:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (472411ms till timeout)
2022-03-30 21:48:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:45 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:45 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:48:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (471339ms till timeout)
2022-03-30 21:48:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:46 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:46 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:48:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (470266ms till timeout)
2022-03-30 21:48:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:47 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:47 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:48:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (469176ms till timeout)
2022-03-30 21:48:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:48 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:48 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:48 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:48:48 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (468104ms till timeout)
2022-03-30 21:48:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:48:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:49 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:49 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:48:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (467032ms till timeout)
2022-03-30 21:48:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:50 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:50 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:48:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (465952ms till timeout)
2022-03-30 21:48:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:51 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:51 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:48:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (464875ms till timeout)
2022-03-30 21:48:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:48:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (463796ms till timeout)
2022-03-30 21:48:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:48:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:48:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (462564ms till timeout)
2022-03-30 21:48:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:55 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:55 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:48:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (461492ms till timeout)
2022-03-30 21:48:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:56 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:56 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:48:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (460414ms till timeout)
2022-03-30 21:48:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:57 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:57 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:48:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (459334ms till timeout)
2022-03-30 21:48:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:58 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:58 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:48:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (458251ms till timeout)
2022-03-30 21:48:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:48:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:48:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:48:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:59 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:48:59 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:48:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (457179ms till timeout)
2022-03-30 21:49:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:00 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:00 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:49:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (456103ms till timeout)
2022-03-30 21:49:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:49:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:49:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:01 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:01 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:49:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (455029ms till timeout)
2022-03-30 21:49:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:49:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:02 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:02 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:49:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (453952ms till timeout)
2022-03-30 21:49:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:49:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:49:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (452879ms till timeout)
2022-03-30 21:49:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:49:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:49:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:04 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:04 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:49:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (451806ms till timeout)
2022-03-30 21:49:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:06 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:06 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:49:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (450727ms till timeout)
2022-03-30 21:49:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:49:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:49:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:07 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:07 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:49:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (449660ms till timeout)
2022-03-30 21:49:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:49:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:08 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:08 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:49:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (448586ms till timeout)
2022-03-30 21:49:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:49:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:49:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:09 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:09 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:49:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (447507ms till timeout)
2022-03-30 21:49:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:49:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:10 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:10 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:49:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (446434ms till timeout)
2022-03-30 21:49:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:49:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:11 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:11 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:49:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (445361ms till timeout)
2022-03-30 21:49:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:49:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:49:12 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:12 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:49:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (444282ms till timeout)
2022-03-30 21:49:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:49:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:49:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (443210ms till timeout)
2022-03-30 21:49:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:49:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:49:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:14 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:14 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:49:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (442143ms till timeout)
2022-03-30 21:49:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:49:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (441070ms till timeout)
2022-03-30 21:49:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:49:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:49:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:16 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:16 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:49:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (439997ms till timeout)
2022-03-30 21:49:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:49:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:17 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:17 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:49:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (438919ms till timeout)
2022-03-30 21:49:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:49:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:18 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:18 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:49:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (437847ms till timeout)
2022-03-30 21:49:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:49:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:49:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 21:49:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 21:49:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 21:49:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-3" not found
2022-03-30 21:49:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 21:49:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[], io.strimzi.test.logs.CollectorElement@3d7a59f3=[], io.strimzi.test.logs.CollectorElement@dce7372d=[], io.strimzi.test.logs.CollectorElement@7386938d=[], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-30 21:49:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:267] testAutoRenewAllCaCertsTriggeredByAnno - Notifies waiting test cases:[testCruiseControlBasicAPIRequests, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testUpdateUser, testSendSimpleMessageTls, testAutoRenewAllCaCertsTriggeredByAnno, testReceiveSimpleMessageTls, testSendMessagesCustomListenerTlsScramSha, testKafkaAndZookeeperScaleUpScaleDown] to and randomly select one to start execution
2022-03-30 21:49:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:93] [security.SecurityST] - Removing parallel test: testAutoRenewAllCaCertsTriggeredByAnno
2022-03-30 21:49:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:97] [security.SecurityST] - Parallel test count: 0
2022-03-30 21:49:19 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testAutoRenewAllCaCertsTriggeredByAnno-FINISHED
2022-03-30 21:49:19 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 21:49:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 21:49:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:690] [security.SecurityST - After All] - Clean up after test suite
2022-03-30 21:49:19 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 21:49:19 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:346] In context SecurityST is everything deleted.
2022-03-30 21:49:19 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 21:49:19 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 21:49:19 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlWithRebalanceResourceAndRefreshAnnotation-STARTED
2022-03-30 21:49:19 [ForkJoinPool-3-worker-11] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 21:49:19 [ForkJoinPool-3-worker-11] [36mDEBUG[m [AbstractST:659] [cruisecontrol.CruiseControlST - Before Each] - Setup test case environment
2022-03-30 21:49:19 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 21:49:19 [ForkJoinPool-3-worker-11] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349, testSendSimpleMessageTls=my-cluster-b70e5342, testSendMessagesTlsScramSha=my-cluster-380e6586, testUpdateUser=my-cluster-9b2bd187, testReceiveSimpleMessageTls=my-cluster-e8a518cd, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09}
2022-03-30 21:49:19 [ForkJoinPool-3-worker-11] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1001559761-691587912, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1590558165-46977776, testSendSimpleMessageTls=my-user-602433797-900694548, testSendMessagesTlsScramSha=my-user-2039401547-533835955, testUpdateUser=my-user-595127408-1604948726, testReceiveSimpleMessageTls=my-user-761922164-2019863647, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1606351157-1815623087, testSendMessagesCustomListenerTlsScramSha=my-user-445988529-51741427, testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659}
2022-03-30 21:49:19 [ForkJoinPool-3-worker-11] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-741363275-285355018, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1210910585-1669754462, testSendSimpleMessageTls=my-topic-1299629356-597168683, testSendMessagesTlsScramSha=my-topic-988094002-1400465950, testUpdateUser=my-topic-1875736194-1340884809, testReceiveSimpleMessageTls=my-topic-1007790874-1032272408, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1931531910-629202741, testSendMessagesCustomListenerTlsScramSha=my-topic-1039904883-1044305687, testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393}
2022-03-30 21:49:19 [ForkJoinPool-3-worker-11] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349-kafka-clients, testSendSimpleMessageTls=my-cluster-b70e5342-kafka-clients, testSendMessagesTlsScramSha=my-cluster-380e6586-kafka-clients, testUpdateUser=my-cluster-9b2bd187-kafka-clients, testReceiveSimpleMessageTls=my-cluster-e8a518cd-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients}
2022-03-30 21:49:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Namespace security-st removal
2022-03-30 21:49:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-30 21:49:19 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-b2680d3c in namespace cruise-control-st
2022-03-30 21:49:19 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-b2680d3c
2022-03-30 21:49:20 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-b2680d3c will have desired state: Ready
2022-03-30 21:49:20 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-b2680d3c will have desired state: Ready
2022-03-30 21:49:20 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1319998ms till timeout)
2022-03-30 21:49:20 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-30 21:49:20 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:49:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace security-st removal not ready, will try again in 1000 ms (479919ms till timeout)
2022-03-30 21:49:21 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1318995ms till timeout)
2022-03-30 21:49:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-30 21:49:21 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-30 21:49:21 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:49:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace security-st removal not ready, will try again in 1000 ms (478851ms till timeout)
2022-03-30 21:49:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 21:49:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 21:49:22 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1317992ms till timeout)
2022-03-30 21:49:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-30 21:49:22 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-30 21:49:22 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:49:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace security-st removal not ready, will try again in 1000 ms (477777ms till timeout)
2022-03-30 21:49:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 21:49:23 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1316988ms till timeout)
2022-03-30 21:49:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 21:49:23 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-30 21:49:23 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-30 21:49:23 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:49:23 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace security-st removal not ready, will try again in 1000 ms (476692ms till timeout)
2022-03-30 21:49:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 21:49:24 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1315985ms till timeout)
2022-03-30 21:49:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-30 21:49:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-30 21:49:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:49:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace security-st removal not ready, will try again in 1000 ms (475610ms till timeout)
2022-03-30 21:49:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 2
2022-03-30 21:49:25 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1314979ms till timeout)
2022-03-30 21:49:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-30 21:49:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-30 21:49:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 21:49:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 21:49:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "security-st" not found
2022-03-30 21:49:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 21:49:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[], io.strimzi.test.logs.CollectorElement@3d7a59f3=[], io.strimzi.test.logs.CollectorElement@dce7372d=[], io.strimzi.test.logs.CollectorElement@7386938d=[], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@bfb2174d=[]}
2022-03-30 21:49:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:254] SecurityST - Notifies waiting test suites:[CruiseControlST, HttpBridgeTlsST, UserST, CruiseControlApiST, ListenersST, SecurityST, RollingUpdateST] to and randomly select one to start execution
2022-03-30 21:49:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:85] [security.SecurityST] - Removing parallel suite: SecurityST
2022-03-30 21:49:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:89] [security.SecurityST] - Parallel suites count: 1
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,053.076 s - in io.strimzi.systemtest.security.SecurityST
2022-03-30 21:49:26 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1313971ms till timeout)
2022-03-30 21:49:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:27 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1312967ms till timeout)
2022-03-30 21:49:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:28 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1311964ms till timeout)
2022-03-30 21:49:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:49:29 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1310961ms till timeout)
2022-03-30 21:49:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:30 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1309958ms till timeout)
2022-03-30 21:49:31 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1308955ms till timeout)
2022-03-30 21:49:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:32 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1307951ms till timeout)
2022-03-30 21:49:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:33 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1306948ms till timeout)
2022-03-30 21:49:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:49:34 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1305945ms till timeout)
2022-03-30 21:49:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:35 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1304942ms till timeout)
2022-03-30 21:49:36 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1303939ms till timeout)
2022-03-30 21:49:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:37 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1302936ms till timeout)
2022-03-30 21:49:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:38 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1301933ms till timeout)
2022-03-30 21:49:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:49:39 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1300930ms till timeout)
2022-03-30 21:49:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:40 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1299927ms till timeout)
2022-03-30 21:49:41 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1298924ms till timeout)
2022-03-30 21:49:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:42 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1297920ms till timeout)
2022-03-30 21:49:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:43 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1296917ms till timeout)
2022-03-30 21:49:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:49:44 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1295914ms till timeout)
2022-03-30 21:49:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:45 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1294911ms till timeout)
2022-03-30 21:49:46 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1293908ms till timeout)
2022-03-30 21:49:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:47 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1292904ms till timeout)
2022-03-30 21:49:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:48 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1291901ms till timeout)
2022-03-30 21:49:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:49:49 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1290898ms till timeout)
2022-03-30 21:49:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:50 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1289895ms till timeout)
2022-03-30 21:49:51 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1288890ms till timeout)
2022-03-30 21:49:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:52 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1287886ms till timeout)
2022-03-30 21:49:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:53 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1286882ms till timeout)
2022-03-30 21:49:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:49:54 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1285879ms till timeout)
2022-03-30 21:49:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:55 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1284875ms till timeout)
2022-03-30 21:49:56 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1283872ms till timeout)
2022-03-30 21:49:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:57 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1282868ms till timeout)
2022-03-30 21:49:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:58 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1281865ms till timeout)
2022-03-30 21:49:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:49:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:49:59 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1280861ms till timeout)
2022-03-30 21:49:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:49:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:00 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1279858ms till timeout)
2022-03-30 21:50:01 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1278854ms till timeout)
2022-03-30 21:50:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:02 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1277851ms till timeout)
2022-03-30 21:50:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:03 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1276848ms till timeout)
2022-03-30 21:50:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:50:04 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1275844ms till timeout)
2022-03-30 21:50:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:05 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1274841ms till timeout)
2022-03-30 21:50:06 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1273838ms till timeout)
2022-03-30 21:50:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:07 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1272835ms till timeout)
2022-03-30 21:50:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:08 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1271831ms till timeout)
2022-03-30 21:50:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:50:09 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1270829ms till timeout)
2022-03-30 21:50:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:10 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1269825ms till timeout)
2022-03-30 21:50:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:11 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1268822ms till timeout)
2022-03-30 21:50:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:12 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1267819ms till timeout)
2022-03-30 21:50:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:13 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1266816ms till timeout)
2022-03-30 21:50:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:50:14 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1265813ms till timeout)
2022-03-30 21:50:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:15 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1264807ms till timeout)
2022-03-30 21:50:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:16 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1263804ms till timeout)
2022-03-30 21:50:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:17 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1262802ms till timeout)
2022-03-30 21:50:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:18 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1261799ms till timeout)
2022-03-30 21:50:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:50:19 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1260795ms till timeout)
2022-03-30 21:50:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:20 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1259792ms till timeout)
2022-03-30 21:50:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:21 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1258789ms till timeout)
2022-03-30 21:50:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:22 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1257785ms till timeout)
2022-03-30 21:50:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:23 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1256781ms till timeout)
2022-03-30 21:50:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:50:24 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1255778ms till timeout)
2022-03-30 21:50:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:25 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1254775ms till timeout)
2022-03-30 21:50:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:26 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1253771ms till timeout)
2022-03-30 21:50:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:27 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1252768ms till timeout)
2022-03-30 21:50:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:28 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1251765ms till timeout)
2022-03-30 21:50:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:50:29 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1250761ms till timeout)
2022-03-30 21:50:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:30 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1249758ms till timeout)
2022-03-30 21:50:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:31 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1248754ms till timeout)
2022-03-30 21:50:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:32 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1247751ms till timeout)
2022-03-30 21:50:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:33 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1246748ms till timeout)
2022-03-30 21:50:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:50:34 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1245745ms till timeout)
2022-03-30 21:50:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:35 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1244741ms till timeout)
2022-03-30 21:50:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:36 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1243738ms till timeout)
2022-03-30 21:50:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:37 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1242735ms till timeout)
2022-03-30 21:50:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:38 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1241731ms till timeout)
2022-03-30 21:50:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:50:39 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1240728ms till timeout)
2022-03-30 21:50:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:40 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1239725ms till timeout)
2022-03-30 21:50:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:41 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1238721ms till timeout)
2022-03-30 21:50:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:42 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1237718ms till timeout)
2022-03-30 21:50:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:43 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1236715ms till timeout)
2022-03-30 21:50:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:50:44 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1235712ms till timeout)
2022-03-30 21:50:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:45 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1234709ms till timeout)
2022-03-30 21:50:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:46 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1233706ms till timeout)
2022-03-30 21:50:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:47 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1232702ms till timeout)
2022-03-30 21:50:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:48 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1231699ms till timeout)
2022-03-30 21:50:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:50:49 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1230693ms till timeout)
2022-03-30 21:50:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:50 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1229690ms till timeout)
2022-03-30 21:50:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:51 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1228686ms till timeout)
2022-03-30 21:50:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:52 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1227683ms till timeout)
2022-03-30 21:50:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:53 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1226680ms till timeout)
2022-03-30 21:50:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:50:54 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1225677ms till timeout)
2022-03-30 21:50:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:55 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1224673ms till timeout)
2022-03-30 21:50:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:56 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1223670ms till timeout)
2022-03-30 21:50:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:57 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1222667ms till timeout)
2022-03-30 21:50:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:50:58 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1221663ms till timeout)
2022-03-30 21:50:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:50:59 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1220660ms till timeout)
2022-03-30 21:50:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:50:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:00 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1219656ms till timeout)
2022-03-30 21:51:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:01 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (1218653ms till timeout)
2022-03-30 21:51:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:02 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-b2680d3c is in desired state: Ready
2022-03-30 21:51:02 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaRebalance my-cluster-b2680d3c in namespace cruise-control-st
2022-03-30 21:51:02 [ForkJoinPool-3-worker-11] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkarebalances' with unstable version 'v1beta2'
2022-03-30 21:51:02 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaRebalance:my-cluster-b2680d3c
2022-03-30 21:51:02 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-b2680d3c will have desired state: PendingProposal
2022-03-30 21:51:02 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for KafkaRebalance: my-cluster-b2680d3c will have desired state: PendingProposal
2022-03-30 21:51:02 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: PendingProposal not ready, will try again in 1000 ms (359998ms till timeout)
2022-03-30 21:51:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:03 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-b2680d3c is in desired state: PendingProposal
2022-03-30 21:51:03 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:75] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-b2680d3c): ============================================================================
2022-03-30 21:51:03 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:76] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-b2680d3c): PendingProposal
2022-03-30 21:51:03 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:77] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-b2680d3c): ============================================================================
2022-03-30 21:51:03 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:81] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-b2680d3c): Verifying that KafkaRebalance resource is in PendingProposal state
2022-03-30 21:51:03 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-b2680d3c will have desired state: PendingProposal
2022-03-30 21:51:03 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for KafkaRebalance: my-cluster-b2680d3c will have desired state: PendingProposal
2022-03-30 21:51:03 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-b2680d3c is in desired state: PendingProposal
2022-03-30 21:51:03 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:85] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-b2680d3c): Verifying that KafkaRebalance resource is in ProposalReady state
2022-03-30 21:51:03 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady
2022-03-30 21:51:03 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady
2022-03-30 21:51:03 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 21:51:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:51:04 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (598995ms till timeout)
2022-03-30 21:51:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:05 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (597992ms till timeout)
2022-03-30 21:51:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:06 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (596988ms till timeout)
2022-03-30 21:51:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:07 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (595984ms till timeout)
2022-03-30 21:51:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:08 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (594981ms till timeout)
2022-03-30 21:51:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:51:09 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (593978ms till timeout)
2022-03-30 21:51:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:10 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (592973ms till timeout)
2022-03-30 21:51:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:11 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (591970ms till timeout)
2022-03-30 21:51:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:12 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (590966ms till timeout)
2022-03-30 21:51:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:13 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (589963ms till timeout)
2022-03-30 21:51:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:51:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:14 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (588959ms till timeout)
2022-03-30 21:51:15 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (587956ms till timeout)
2022-03-30 21:51:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:16 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (586954ms till timeout)
2022-03-30 21:51:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:17 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (585950ms till timeout)
2022-03-30 21:51:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:18 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (584948ms till timeout)
2022-03-30 21:51:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:51:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:19 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (583945ms till timeout)
2022-03-30 21:51:20 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (582941ms till timeout)
2022-03-30 21:51:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:21 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (581938ms till timeout)
2022-03-30 21:51:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:22 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (580935ms till timeout)
2022-03-30 21:51:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:23 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (579932ms till timeout)
2022-03-30 21:51:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:51:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:24 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (578929ms till timeout)
2022-03-30 21:51:25 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (577926ms till timeout)
2022-03-30 21:51:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:26 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (576922ms till timeout)
2022-03-30 21:51:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:27 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (575919ms till timeout)
2022-03-30 21:51:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:28 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (574916ms till timeout)
2022-03-30 21:51:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:51:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:29 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (573913ms till timeout)
2022-03-30 21:51:30 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (572910ms till timeout)
2022-03-30 21:51:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:31 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (571907ms till timeout)
2022-03-30 21:51:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:32 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (570904ms till timeout)
2022-03-30 21:51:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:33 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (569901ms till timeout)
2022-03-30 21:51:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:51:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:34 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (568898ms till timeout)
2022-03-30 21:51:35 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (567895ms till timeout)
2022-03-30 21:51:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:36 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (566891ms till timeout)
2022-03-30 21:51:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:37 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (565889ms till timeout)
2022-03-30 21:51:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:38 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (564885ms till timeout)
2022-03-30 21:51:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:51:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:39 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (563882ms till timeout)
2022-03-30 21:51:40 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (562879ms till timeout)
2022-03-30 21:51:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:41 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (561876ms till timeout)
2022-03-30 21:51:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:42 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (560873ms till timeout)
2022-03-30 21:51:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:43 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (559870ms till timeout)
2022-03-30 21:51:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:51:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:44 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (558867ms till timeout)
2022-03-30 21:51:45 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (557864ms till timeout)
2022-03-30 21:51:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:46 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (556862ms till timeout)
2022-03-30 21:51:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:47 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (555859ms till timeout)
2022-03-30 21:51:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:48 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (554856ms till timeout)
2022-03-30 21:51:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:51:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:49 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (553853ms till timeout)
2022-03-30 21:51:50 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (552849ms till timeout)
2022-03-30 21:51:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:51 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (551847ms till timeout)
2022-03-30 21:51:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:52 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (550843ms till timeout)
2022-03-30 21:51:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:53 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (549840ms till timeout)
2022-03-30 21:51:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:51:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:54 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (548838ms till timeout)
2022-03-30 21:51:55 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (547835ms till timeout)
2022-03-30 21:51:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:56 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (546832ms till timeout)
2022-03-30 21:51:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:57 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (545829ms till timeout)
2022-03-30 21:51:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:58 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (544826ms till timeout)
2022-03-30 21:51:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:51:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:51:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:51:59 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (543823ms till timeout)
2022-03-30 21:52:00 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (542819ms till timeout)
2022-03-30 21:52:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:01 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (541817ms till timeout)
2022-03-30 21:52:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:02 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (540813ms till timeout)
2022-03-30 21:52:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:03 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (539811ms till timeout)
2022-03-30 21:52:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:52:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:04 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (538808ms till timeout)
2022-03-30 21:52:05 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (537805ms till timeout)
2022-03-30 21:52:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:06 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (536802ms till timeout)
2022-03-30 21:52:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:07 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (535799ms till timeout)
2022-03-30 21:52:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:08 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (534796ms till timeout)
2022-03-30 21:52:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:52:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:09 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (533792ms till timeout)
2022-03-30 21:52:10 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (532790ms till timeout)
2022-03-30 21:52:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:11 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (531787ms till timeout)
2022-03-30 21:52:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:12 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (530784ms till timeout)
2022-03-30 21:52:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:13 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (529781ms till timeout)
2022-03-30 21:52:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:52:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:14 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (528778ms till timeout)
2022-03-30 21:52:15 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (527775ms till timeout)
2022-03-30 21:52:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:16 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (526772ms till timeout)
2022-03-30 21:52:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:17 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (525769ms till timeout)
2022-03-30 21:52:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:18 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (524766ms till timeout)
2022-03-30 21:52:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:52:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:19 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (523763ms till timeout)
2022-03-30 21:52:20 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (522760ms till timeout)
2022-03-30 21:52:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:21 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (521757ms till timeout)
2022-03-30 21:52:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:22 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (520754ms till timeout)
2022-03-30 21:52:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:23 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (519751ms till timeout)
2022-03-30 21:52:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:52:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:24 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (518748ms till timeout)
2022-03-30 21:52:25 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (517745ms till timeout)
2022-03-30 21:52:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:26 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (516741ms till timeout)
2022-03-30 21:52:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:27 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (515738ms till timeout)
2022-03-30 21:52:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:28 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (514735ms till timeout)
2022-03-30 21:52:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:52:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:29 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (513732ms till timeout)
2022-03-30 21:52:30 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (512729ms till timeout)
2022-03-30 21:52:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:31 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (511726ms till timeout)
2022-03-30 21:52:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:32 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (510723ms till timeout)
2022-03-30 21:52:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:33 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (509720ms till timeout)
2022-03-30 21:52:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:52:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:34 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (508717ms till timeout)
2022-03-30 21:52:35 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (507714ms till timeout)
2022-03-30 21:52:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:36 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (506711ms till timeout)
2022-03-30 21:52:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:37 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (505708ms till timeout)
2022-03-30 21:52:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:38 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (504705ms till timeout)
2022-03-30 21:52:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:52:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:39 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (503702ms till timeout)
2022-03-30 21:52:40 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (502698ms till timeout)
2022-03-30 21:52:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:41 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (501695ms till timeout)
2022-03-30 21:52:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:42 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (500692ms till timeout)
2022-03-30 21:52:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:43 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (499689ms till timeout)
2022-03-30 21:52:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:52:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:44 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (498686ms till timeout)
2022-03-30 21:52:45 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (497683ms till timeout)
2022-03-30 21:52:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:46 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (496680ms till timeout)
2022-03-30 21:52:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:47 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (495678ms till timeout)
2022-03-30 21:52:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:48 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (494674ms till timeout)
2022-03-30 21:52:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:52:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:49 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (493671ms till timeout)
2022-03-30 21:52:50 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (492668ms till timeout)
2022-03-30 21:52:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:51 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (491665ms till timeout)
2022-03-30 21:52:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:52 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (490661ms till timeout)
2022-03-30 21:52:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:53 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (489658ms till timeout)
2022-03-30 21:52:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:52:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:54 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (488655ms till timeout)
2022-03-30 21:52:55 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (487652ms till timeout)
2022-03-30 21:52:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:56 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (486649ms till timeout)
2022-03-30 21:52:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:57 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (485646ms till timeout)
2022-03-30 21:52:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:58 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (484643ms till timeout)
2022-03-30 21:52:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:52:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:52:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:52:59 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (483639ms till timeout)
2022-03-30 21:53:00 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (482637ms till timeout)
2022-03-30 21:53:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:01 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (481633ms till timeout)
2022-03-30 21:53:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:02 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (480630ms till timeout)
2022-03-30 21:53:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:03 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (479627ms till timeout)
2022-03-30 21:53:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:53:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:04 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (478624ms till timeout)
2022-03-30 21:53:05 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (477621ms till timeout)
2022-03-30 21:53:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:06 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (476618ms till timeout)
2022-03-30 21:53:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:07 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (475615ms till timeout)
2022-03-30 21:53:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:08 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (474611ms till timeout)
2022-03-30 21:53:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:53:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:09 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (473608ms till timeout)
2022-03-30 21:53:10 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (472605ms till timeout)
2022-03-30 21:53:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:11 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (471602ms till timeout)
2022-03-30 21:53:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:12 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (470599ms till timeout)
2022-03-30 21:53:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:13 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (469595ms till timeout)
2022-03-30 21:53:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:53:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:14 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (468593ms till timeout)
2022-03-30 21:53:15 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (467589ms till timeout)
2022-03-30 21:53:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:16 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (466586ms till timeout)
2022-03-30 21:53:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:17 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (465583ms till timeout)
2022-03-30 21:53:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:18 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (464580ms till timeout)
2022-03-30 21:53:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:53:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:19 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (463577ms till timeout)
2022-03-30 21:53:20 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (462574ms till timeout)
2022-03-30 21:53:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:21 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (461571ms till timeout)
2022-03-30 21:53:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:22 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (460568ms till timeout)
2022-03-30 21:53:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:23 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (459564ms till timeout)
2022-03-30 21:53:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:53:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:24 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (458562ms till timeout)
2022-03-30 21:53:25 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (457558ms till timeout)
2022-03-30 21:53:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:26 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (456555ms till timeout)
2022-03-30 21:53:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:27 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (455552ms till timeout)
2022-03-30 21:53:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:28 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (454549ms till timeout)
2022-03-30 21:53:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:53:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:29 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (453546ms till timeout)
2022-03-30 21:53:30 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (452543ms till timeout)
2022-03-30 21:53:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:31 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (451539ms till timeout)
2022-03-30 21:53:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:32 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (450536ms till timeout)
2022-03-30 21:53:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:33 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (449533ms till timeout)
2022-03-30 21:53:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:53:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:34 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (448530ms till timeout)
2022-03-30 21:53:35 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (447527ms till timeout)
2022-03-30 21:53:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:36 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (446523ms till timeout)
2022-03-30 21:53:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:37 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (445520ms till timeout)
2022-03-30 21:53:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:38 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (444517ms till timeout)
2022-03-30 21:53:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:53:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:39 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (443514ms till timeout)
2022-03-30 21:53:40 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (442511ms till timeout)
2022-03-30 21:53:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:41 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (441508ms till timeout)
2022-03-30 21:53:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:42 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (440505ms till timeout)
2022-03-30 21:53:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:43 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (439501ms till timeout)
2022-03-30 21:53:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:53:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:44 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (438498ms till timeout)
2022-03-30 21:53:45 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (437495ms till timeout)
2022-03-30 21:53:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:46 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (436492ms till timeout)
2022-03-30 21:53:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:47 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (435490ms till timeout)
2022-03-30 21:53:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:48 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (434486ms till timeout)
2022-03-30 21:53:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:53:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:49 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (433483ms till timeout)
2022-03-30 21:53:50 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (432479ms till timeout)
2022-03-30 21:53:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:51 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (431476ms till timeout)
2022-03-30 21:53:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:52 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (430473ms till timeout)
2022-03-30 21:53:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:53 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (429470ms till timeout)
2022-03-30 21:53:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:53:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:54 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (428467ms till timeout)
2022-03-30 21:53:55 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (427464ms till timeout)
2022-03-30 21:53:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:56 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (426461ms till timeout)
2022-03-30 21:53:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:57 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (425457ms till timeout)
2022-03-30 21:53:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:58 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (424454ms till timeout)
2022-03-30 21:53:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:53:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:53:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:53:59 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (423451ms till timeout)
2022-03-30 21:54:00 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (422448ms till timeout)
2022-03-30 21:54:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:01 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (421445ms till timeout)
2022-03-30 21:54:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:02 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (420442ms till timeout)
2022-03-30 21:54:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:03 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (419438ms till timeout)
2022-03-30 21:54:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:54:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:04 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (418435ms till timeout)
2022-03-30 21:54:05 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (417432ms till timeout)
2022-03-30 21:54:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:06 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (416429ms till timeout)
2022-03-30 21:54:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:07 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (415426ms till timeout)
2022-03-30 21:54:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:54:08 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (414423ms till timeout)
2022-03-30 21:54:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:09 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (413420ms till timeout)
2022-03-30 21:54:10 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (412417ms till timeout)
2022-03-30 21:54:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:11 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (411413ms till timeout)
2022-03-30 21:54:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:13 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (410410ms till timeout)
2022-03-30 21:54:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:54:14 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (409407ms till timeout)
2022-03-30 21:54:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:15 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (408404ms till timeout)
2022-03-30 21:54:16 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (407401ms till timeout)
2022-03-30 21:54:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:17 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (406398ms till timeout)
2022-03-30 21:54:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:18 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (405396ms till timeout)
2022-03-30 21:54:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:54:19 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (404393ms till timeout)
2022-03-30 21:54:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:20 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (403389ms till timeout)
2022-03-30 21:54:21 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (402386ms till timeout)
2022-03-30 21:54:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:22 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (401383ms till timeout)
2022-03-30 21:54:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:23 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (400380ms till timeout)
2022-03-30 21:54:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:54:24 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (399377ms till timeout)
2022-03-30 21:54:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:25 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (398373ms till timeout)
2022-03-30 21:54:26 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (397370ms till timeout)
2022-03-30 21:54:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:27 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (396367ms till timeout)
2022-03-30 21:54:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:28 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (395364ms till timeout)
2022-03-30 21:54:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:54:29 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (394361ms till timeout)
2022-03-30 21:54:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:30 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (393358ms till timeout)
2022-03-30 21:54:31 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (392355ms till timeout)
2022-03-30 21:54:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:32 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (391351ms till timeout)
2022-03-30 21:54:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:33 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (390348ms till timeout)
2022-03-30 21:54:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:54:34 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (389345ms till timeout)
2022-03-30 21:54:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:35 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (388342ms till timeout)
2022-03-30 21:54:36 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (387338ms till timeout)
2022-03-30 21:54:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:37 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (386335ms till timeout)
2022-03-30 21:54:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:38 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (385332ms till timeout)
2022-03-30 21:54:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:54:39 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (384329ms till timeout)
2022-03-30 21:54:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:40 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (383326ms till timeout)
2022-03-30 21:54:41 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (382323ms till timeout)
2022-03-30 21:54:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:42 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (381320ms till timeout)
2022-03-30 21:54:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:43 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (380317ms till timeout)
2022-03-30 21:54:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:54:44 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (379314ms till timeout)
2022-03-30 21:54:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:45 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (378311ms till timeout)
2022-03-30 21:54:46 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (377308ms till timeout)
2022-03-30 21:54:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:47 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (376305ms till timeout)
2022-03-30 21:54:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:48 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (375302ms till timeout)
2022-03-30 21:54:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:54:49 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (374299ms till timeout)
2022-03-30 21:54:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:50 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (373296ms till timeout)
2022-03-30 21:54:51 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (372293ms till timeout)
2022-03-30 21:54:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:52 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (371289ms till timeout)
2022-03-30 21:54:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:53 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (370286ms till timeout)
2022-03-30 21:54:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:54:54 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (369283ms till timeout)
2022-03-30 21:54:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:55 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (368280ms till timeout)
2022-03-30 21:54:56 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (367277ms till timeout)
2022-03-30 21:54:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:57 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (366274ms till timeout)
2022-03-30 21:54:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:58 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (365271ms till timeout)
2022-03-30 21:54:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:54:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:54:59 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (364267ms till timeout)
2022-03-30 21:54:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:54:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:00 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (363264ms till timeout)
2022-03-30 21:55:01 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (362261ms till timeout)
2022-03-30 21:55:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:02 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (361258ms till timeout)
2022-03-30 21:55:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:03 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (360255ms till timeout)
2022-03-30 21:55:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:55:04 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (359251ms till timeout)
2022-03-30 21:55:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:05 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (358248ms till timeout)
2022-03-30 21:55:06 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (357245ms till timeout)
2022-03-30 21:55:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:07 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (356242ms till timeout)
2022-03-30 21:55:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:08 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (355239ms till timeout)
2022-03-30 21:55:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:55:09 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (354236ms till timeout)
2022-03-30 21:55:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:10 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (353232ms till timeout)
2022-03-30 21:55:11 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (352229ms till timeout)
2022-03-30 21:55:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:12 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (351226ms till timeout)
2022-03-30 21:55:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:13 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (350223ms till timeout)
2022-03-30 21:55:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:55:14 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (349219ms till timeout)
2022-03-30 21:55:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:15 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (348216ms till timeout)
2022-03-30 21:55:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:16 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (347213ms till timeout)
2022-03-30 21:55:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:17 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (346210ms till timeout)
2022-03-30 21:55:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:18 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (345207ms till timeout)
2022-03-30 21:55:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:55:19 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (344204ms till timeout)
2022-03-30 21:55:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:20 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (343201ms till timeout)
2022-03-30 21:55:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:21 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (342198ms till timeout)
2022-03-30 21:55:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:22 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (341194ms till timeout)
2022-03-30 21:55:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:23 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (340191ms till timeout)
2022-03-30 21:55:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:55:24 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (339188ms till timeout)
2022-03-30 21:55:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:25 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (338185ms till timeout)
2022-03-30 21:55:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:26 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (337182ms till timeout)
2022-03-30 21:55:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:27 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (336178ms till timeout)
2022-03-30 21:55:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:28 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (335175ms till timeout)
2022-03-30 21:55:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:55:29 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (334172ms till timeout)
2022-03-30 21:55:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:30 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (333169ms till timeout)
2022-03-30 21:55:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:31 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (332166ms till timeout)
2022-03-30 21:55:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:32 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (331163ms till timeout)
2022-03-30 21:55:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:33 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (330160ms till timeout)
2022-03-30 21:55:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:55:34 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (329157ms till timeout)
2022-03-30 21:55:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:35 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (328153ms till timeout)
2022-03-30 21:55:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:36 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (327150ms till timeout)
2022-03-30 21:55:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:37 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (326147ms till timeout)
2022-03-30 21:55:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:38 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (325144ms till timeout)
2022-03-30 21:55:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:55:39 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (324141ms till timeout)
2022-03-30 21:55:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:40 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (323138ms till timeout)
2022-03-30 21:55:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:41 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (322135ms till timeout)
2022-03-30 21:55:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:42 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (321131ms till timeout)
2022-03-30 21:55:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:43 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (320128ms till timeout)
2022-03-30 21:55:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:55:44 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (319125ms till timeout)
2022-03-30 21:55:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:45 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (318121ms till timeout)
2022-03-30 21:55:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:46 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (317118ms till timeout)
2022-03-30 21:55:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:47 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (316115ms till timeout)
2022-03-30 21:55:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:48 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (315112ms till timeout)
2022-03-30 21:55:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:55:49 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (314109ms till timeout)
2022-03-30 21:55:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:50 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (313106ms till timeout)
2022-03-30 21:55:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:51 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (312103ms till timeout)
2022-03-30 21:55:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:52 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (311100ms till timeout)
2022-03-30 21:55:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:53 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (310097ms till timeout)
2022-03-30 21:55:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:55:54 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (309094ms till timeout)
2022-03-30 21:55:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:55 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (308091ms till timeout)
2022-03-30 21:55:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:56 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (307088ms till timeout)
2022-03-30 21:55:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:57 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (306085ms till timeout)
2022-03-30 21:55:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:55:58 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (305082ms till timeout)
2022-03-30 21:55:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:55:59 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (304079ms till timeout)
2022-03-30 21:55:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:55:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:00 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (303076ms till timeout)
2022-03-30 21:56:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:01 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (302073ms till timeout)
2022-03-30 21:56:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:02 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (301069ms till timeout)
2022-03-30 21:56:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:03 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (300067ms till timeout)
2022-03-30 21:56:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:56:04 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (299064ms till timeout)
2022-03-30 21:56:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:05 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (298061ms till timeout)
2022-03-30 21:56:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:06 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (297057ms till timeout)
2022-03-30 21:56:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:07 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (296054ms till timeout)
2022-03-30 21:56:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:08 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (295051ms till timeout)
2022-03-30 21:56:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:56:09 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (294049ms till timeout)
2022-03-30 21:56:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:10 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (293045ms till timeout)
2022-03-30 21:56:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:11 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (292042ms till timeout)
2022-03-30 21:56:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:12 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (291039ms till timeout)
2022-03-30 21:56:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:13 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (290036ms till timeout)
2022-03-30 21:56:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:56:14 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (289033ms till timeout)
2022-03-30 21:56:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:15 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (288030ms till timeout)
2022-03-30 21:56:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:16 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (287027ms till timeout)
2022-03-30 21:56:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:17 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (286023ms till timeout)
2022-03-30 21:56:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:18 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (285021ms till timeout)
2022-03-30 21:56:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:56:19 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (284018ms till timeout)
2022-03-30 21:56:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:20 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (283015ms till timeout)
2022-03-30 21:56:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:21 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (282012ms till timeout)
2022-03-30 21:56:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:22 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (281008ms till timeout)
2022-03-30 21:56:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:23 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (280005ms till timeout)
2022-03-30 21:56:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:56:24 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (279002ms till timeout)
2022-03-30 21:56:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:25 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (277999ms till timeout)
2022-03-30 21:56:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:26 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (276996ms till timeout)
2022-03-30 21:56:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:27 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (275993ms till timeout)
2022-03-30 21:56:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:28 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (274990ms till timeout)
2022-03-30 21:56:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:56:29 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (273987ms till timeout)
2022-03-30 21:56:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:30 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (272984ms till timeout)
2022-03-30 21:56:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:31 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (271981ms till timeout)
2022-03-30 21:56:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:32 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (270977ms till timeout)
2022-03-30 21:56:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:33 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (269974ms till timeout)
2022-03-30 21:56:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:56:34 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (268971ms till timeout)
2022-03-30 21:56:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:35 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (267968ms till timeout)
2022-03-30 21:56:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:36 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (266965ms till timeout)
2022-03-30 21:56:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:37 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (265962ms till timeout)
2022-03-30 21:56:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:38 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (264959ms till timeout)
2022-03-30 21:56:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:56:39 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (263956ms till timeout)
2022-03-30 21:56:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:40 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (262953ms till timeout)
2022-03-30 21:56:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:41 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (261950ms till timeout)
2022-03-30 21:56:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:42 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (260947ms till timeout)
2022-03-30 21:56:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:43 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (259944ms till timeout)
2022-03-30 21:56:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:56:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:44 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (258941ms till timeout)
2022-03-30 21:56:45 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (257937ms till timeout)
2022-03-30 21:56:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:46 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (256934ms till timeout)
2022-03-30 21:56:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:47 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (255930ms till timeout)
2022-03-30 21:56:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:48 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (254928ms till timeout)
2022-03-30 21:56:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:56:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:49 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (253925ms till timeout)
2022-03-30 21:56:50 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (252922ms till timeout)
2022-03-30 21:56:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:51 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (251919ms till timeout)
2022-03-30 21:56:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:52 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (250915ms till timeout)
2022-03-30 21:56:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:53 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-b2680d3c is in desired state: ProposalReady
2022-03-30 21:56:53 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:90] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-b2680d3c): ============================================================================
2022-03-30 21:56:53 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:91] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-b2680d3c): ProposalReady
2022-03-30 21:56:53 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:92] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-b2680d3c): ============================================================================
2022-03-30 21:56:53 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:94] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-b2680d3c): Triggering the rebalance with annotation strimzi.io/rebalance=approve of KafkaRebalance resource
2022-03-30 21:56:53 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:63] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-b2680d3c): Annotating KafkaRebalance:my-cluster-b2680d3c with annotation approve
2022-03-30 21:56:53 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st annotate kafkarebalance my-cluster-b2680d3c strimzi.io/rebalance=approve
2022-03-30 21:56:53 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st annotate kafkarebalance my-cluster-b2680d3c strimzi.io/rebalance=approve
2022-03-30 21:56:53 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:56:53 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:98] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-b2680d3c): Response from the annotation process kafkarebalance.kafka.strimzi.io/my-cluster-b2680d3c annotated
2022-03-30 21:56:53 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:100] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-b2680d3c): Verifying that annotation triggers the Rebalancing state
2022-03-30 21:56:53 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-b2680d3c will have desired state: Rebalancing
2022-03-30 21:56:53 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for KafkaRebalance: my-cluster-b2680d3c will have desired state: Rebalancing
2022-03-30 21:56:53 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Rebalancing not ready, will try again in 1000 ms (599999ms till timeout)
2022-03-30 21:56:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:56:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:54 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-b2680d3c is in desired state: Rebalancing
2022-03-30 21:56:54 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:104] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-b2680d3c): Verifying that KafkaRebalance is in the Ready state
2022-03-30 21:56:54 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready
2022-03-30 21:56:54 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready
2022-03-30 21:56:54 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 21:56:55 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (598995ms till timeout)
2022-03-30 21:56:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:56 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (597991ms till timeout)
2022-03-30 21:56:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:57 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (596988ms till timeout)
2022-03-30 21:56:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:58 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (595985ms till timeout)
2022-03-30 21:56:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:56:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:56:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:56:59 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (594982ms till timeout)
2022-03-30 21:57:00 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (593978ms till timeout)
2022-03-30 21:57:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:01 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (592975ms till timeout)
2022-03-30 21:57:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:02 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (591972ms till timeout)
2022-03-30 21:57:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:03 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (590967ms till timeout)
2022-03-30 21:57:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:57:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:04 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (589964ms till timeout)
2022-03-30 21:57:05 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (588961ms till timeout)
2022-03-30 21:57:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:06 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (587958ms till timeout)
2022-03-30 21:57:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:07 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (586955ms till timeout)
2022-03-30 21:57:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:08 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (585951ms till timeout)
2022-03-30 21:57:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:57:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:09 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (584948ms till timeout)
2022-03-30 21:57:10 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (583944ms till timeout)
2022-03-30 21:57:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:11 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (582941ms till timeout)
2022-03-30 21:57:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:12 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (581938ms till timeout)
2022-03-30 21:57:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:13 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (580935ms till timeout)
2022-03-30 21:57:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:57:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:14 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (579931ms till timeout)
2022-03-30 21:57:15 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (578928ms till timeout)
2022-03-30 21:57:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:16 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (577925ms till timeout)
2022-03-30 21:57:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:17 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (576922ms till timeout)
2022-03-30 21:57:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:18 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (575919ms till timeout)
2022-03-30 21:57:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:57:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:19 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (574916ms till timeout)
2022-03-30 21:57:20 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (573912ms till timeout)
2022-03-30 21:57:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:21 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (572909ms till timeout)
2022-03-30 21:57:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:22 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (571906ms till timeout)
2022-03-30 21:57:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:23 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (570903ms till timeout)
2022-03-30 21:57:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:57:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:24 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (569900ms till timeout)
2022-03-30 21:57:25 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (568897ms till timeout)
2022-03-30 21:57:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:26 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (567893ms till timeout)
2022-03-30 21:57:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:27 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (566890ms till timeout)
2022-03-30 21:57:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:28 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (565887ms till timeout)
2022-03-30 21:57:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:57:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:29 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (564883ms till timeout)
2022-03-30 21:57:30 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (563880ms till timeout)
2022-03-30 21:57:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:31 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (562876ms till timeout)
2022-03-30 21:57:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:32 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (561873ms till timeout)
2022-03-30 21:57:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:33 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (560870ms till timeout)
2022-03-30 21:57:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:57:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:34 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (559866ms till timeout)
2022-03-30 21:57:35 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (558863ms till timeout)
2022-03-30 21:57:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:36 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (557860ms till timeout)
2022-03-30 21:57:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:37 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (556857ms till timeout)
2022-03-30 21:57:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:38 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (555853ms till timeout)
2022-03-30 21:57:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:57:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:39 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (554850ms till timeout)
2022-03-30 21:57:40 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (553847ms till timeout)
2022-03-30 21:57:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:41 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (552844ms till timeout)
2022-03-30 21:57:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:42 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (551841ms till timeout)
2022-03-30 21:57:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:43 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (550838ms till timeout)
2022-03-30 21:57:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:57:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:44 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (549835ms till timeout)
2022-03-30 21:57:45 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (548831ms till timeout)
2022-03-30 21:57:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:46 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (547828ms till timeout)
2022-03-30 21:57:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:47 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (546825ms till timeout)
2022-03-30 21:57:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:48 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (545822ms till timeout)
2022-03-30 21:57:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:57:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:49 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (544819ms till timeout)
2022-03-30 21:57:50 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (543805ms till timeout)
2022-03-30 21:57:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:51 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (542802ms till timeout)
2022-03-30 21:57:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:52 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (541798ms till timeout)
2022-03-30 21:57:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:53 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (540795ms till timeout)
2022-03-30 21:57:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:57:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:54 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (539792ms till timeout)
2022-03-30 21:57:55 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (538789ms till timeout)
2022-03-30 21:57:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:56 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (537786ms till timeout)
2022-03-30 21:57:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:57 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (536783ms till timeout)
2022-03-30 21:57:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:58 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (535780ms till timeout)
2022-03-30 21:57:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:57:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:57:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:57:59 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (534776ms till timeout)
2022-03-30 21:58:00 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (533773ms till timeout)
2022-03-30 21:58:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:01 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (532770ms till timeout)
2022-03-30 21:58:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:02 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (531766ms till timeout)
2022-03-30 21:58:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:03 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (530763ms till timeout)
2022-03-30 21:58:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:58:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:04 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (529760ms till timeout)
2022-03-30 21:58:05 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (528756ms till timeout)
2022-03-30 21:58:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:06 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (527753ms till timeout)
2022-03-30 21:58:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:07 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (526750ms till timeout)
2022-03-30 21:58:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:08 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (525746ms till timeout)
2022-03-30 21:58:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:58:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:09 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (524743ms till timeout)
2022-03-30 21:58:10 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (523740ms till timeout)
2022-03-30 21:58:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:11 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (522737ms till timeout)
2022-03-30 21:58:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:12 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (521733ms till timeout)
2022-03-30 21:58:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:13 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (520730ms till timeout)
2022-03-30 21:58:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:58:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:14 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (519727ms till timeout)
2022-03-30 21:58:15 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (518724ms till timeout)
2022-03-30 21:58:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:16 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (517720ms till timeout)
2022-03-30 21:58:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:17 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (516718ms till timeout)
2022-03-30 21:58:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:18 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-b2680d3c is in desired state: Ready
2022-03-30 21:58:18 [ForkJoinPool-3-worker-11] [32mINFO [m [CruiseControlST:152] Annotating KafkaRebalance: my-cluster-b2680d3c with 'refresh' anno
2022-03-30 21:58:18 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:63] Reconciliation #2(test) KafkaRebalance(cruise-control-st/my-cluster-b2680d3c): Annotating KafkaRebalance:my-cluster-b2680d3c with annotation refresh
2022-03-30 21:58:18 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st annotate kafkarebalance my-cluster-b2680d3c strimzi.io/rebalance=refresh
2022-03-30 21:58:18 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st annotate kafkarebalance my-cluster-b2680d3c strimzi.io/rebalance=refresh
2022-03-30 21:58:18 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:58:18 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady
2022-03-30 21:58:18 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady
2022-03-30 21:58:18 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: ProposalReady not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 21:58:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:58:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:19 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-b2680d3c is in desired state: ProposalReady
2022-03-30 21:58:19 [ForkJoinPool-3-worker-11] [32mINFO [m [CruiseControlST:156] Trying rebalancing process again
2022-03-30 21:58:19 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:75] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-b2680d3c): ============================================================================
2022-03-30 21:58:19 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:76] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-b2680d3c): ProposalReady
2022-03-30 21:58:19 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:77] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-b2680d3c): ============================================================================
2022-03-30 21:58:19 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:90] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-b2680d3c): ============================================================================
2022-03-30 21:58:19 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:91] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-b2680d3c): ProposalReady
2022-03-30 21:58:19 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:92] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-b2680d3c): ============================================================================
2022-03-30 21:58:19 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:94] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-b2680d3c): Triggering the rebalance with annotation strimzi.io/rebalance=approve of KafkaRebalance resource
2022-03-30 21:58:19 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:63] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-b2680d3c): Annotating KafkaRebalance:my-cluster-b2680d3c with annotation approve
2022-03-30 21:58:19 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st annotate kafkarebalance my-cluster-b2680d3c strimzi.io/rebalance=approve
2022-03-30 21:58:20 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st annotate kafkarebalance my-cluster-b2680d3c strimzi.io/rebalance=approve
2022-03-30 21:58:20 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:58:20 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:98] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-b2680d3c): Response from the annotation process kafkarebalance.kafka.strimzi.io/my-cluster-b2680d3c annotated
2022-03-30 21:58:20 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:100] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-b2680d3c): Verifying that annotation triggers the Rebalancing state
2022-03-30 21:58:20 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-b2680d3c will have desired state: Rebalancing
2022-03-30 21:58:20 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for KafkaRebalance: my-cluster-b2680d3c will have desired state: Rebalancing
2022-03-30 21:58:20 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Rebalancing not ready, will try again in 1000 ms (599999ms till timeout)
2022-03-30 21:58:21 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-b2680d3c is in desired state: Rebalancing
2022-03-30 21:58:21 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRebalanceUtils:104] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-b2680d3c): Verifying that KafkaRebalance is in the Ready state
2022-03-30 21:58:21 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready
2022-03-30 21:58:21 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready
2022-03-30 21:58:21 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-30 21:58:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:22 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-30 21:58:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:23 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (597991ms till timeout)
2022-03-30 21:58:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:58:24 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (596988ms till timeout)
2022-03-30 21:58:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:25 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-b2680d3c will have desired state: Ready not ready, will try again in 1000 ms (595984ms till timeout)
2022-03-30 21:58:26 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-b2680d3c is in desired state: Ready
2022-03-30 21:58:26 [ForkJoinPool-3-worker-11] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 21:58:26 [ForkJoinPool-3-worker-11] [36mDEBUG[m [AbstractST:675] [cruisecontrol.CruiseControlST - After Each] - Clean up after test
2022-03-30 21:58:26 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 21:58:26 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:348] Delete all resources for testCruiseControlWithRebalanceResourceAndRefreshAnnotation
2022-03-30 21:58:26 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaRebalance my-cluster-b2680d3c in namespace cruise-control-st
2022-03-30 21:58:26 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaRebalance:my-cluster-b2680d3c
2022-03-30 21:58:26 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaRebalance:my-cluster-b2680d3c not ready, will try again in 10000 ms (179995ms till timeout)
2022-03-30 21:58:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:58:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:58:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:36 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-b2680d3c in namespace cruise-control-st
2022-03-30 21:58:36 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace cruise-control-st, for cruise control Kafka cluster my-cluster-b2680d3c
2022-03-30 21:58:36 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-b2680d3c
2022-03-30 21:58:36 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-b2680d3c not ready, will try again in 10000 ms (839995ms till timeout)
2022-03-30 21:58:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:58:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:58:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:46 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 21:58:46 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlWithRebalanceResourceAndRefreshAnnotation-FINISHED
2022-03-30 21:58:46 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 21:58:46 [ForkJoinPool-3-worker-11] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 21:58:46 [ForkJoinPool-3-worker-11] [36mDEBUG[m [AbstractST:690] [cruisecontrol.CruiseControlST - After All] - Clean up after test suite
2022-03-30 21:58:46 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 21:58:46 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:346] In context CruiseControlST is everything deleted.
2022-03-30 21:58:46 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 21:58:46 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Namespace cruise-control-st removal
2022-03-30 21:58:46 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:58:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:46 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:58:46 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:58:46 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (479925ms till timeout)
2022-03-30 21:58:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:47 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:58:47 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:58:47 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:58:47 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (478847ms till timeout)
2022-03-30 21:58:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:48 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:58:48 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:58:48 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:58:48 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (477776ms till timeout)
2022-03-30 21:58:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:58:49 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:58:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:49 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:58:49 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:58:49 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (476705ms till timeout)
2022-03-30 21:58:50 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:58:50 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:58:50 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:58:50 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (475634ms till timeout)
2022-03-30 21:58:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:51 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:58:51 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:58:51 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:58:51 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (474560ms till timeout)
2022-03-30 21:58:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:52 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:58:52 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:58:52 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:58:52 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (473486ms till timeout)
2022-03-30 21:58:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:53 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:58:53 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:58:53 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:58:53 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (472412ms till timeout)
2022-03-30 21:58:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:58:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:54 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:58:55 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:58:55 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:58:55 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (471179ms till timeout)
2022-03-30 21:58:56 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:58:56 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:58:56 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:58:56 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (470107ms till timeout)
2022-03-30 21:58:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:57 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:58:57 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:58:57 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:58:57 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (469032ms till timeout)
2022-03-30 21:58:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:58 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:58:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:58:58 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:58:58 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:58:58 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (467957ms till timeout)
2022-03-30 21:58:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:58:59 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:58:59 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:58:59 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:58:59 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (466890ms till timeout)
2022-03-30 21:58:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:58:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:59:00 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:00 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:00 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:59:00 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (465811ms till timeout)
2022-03-30 21:59:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:59:01 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:01 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:01 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:59:01 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (464737ms till timeout)
2022-03-30 21:59:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:59:02 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:59:02 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:02 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:59:02 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (463658ms till timeout)
2022-03-30 21:59:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:59:03 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:03 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:03 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:59:03 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (462585ms till timeout)
2022-03-30 21:59:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:59:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:59:04 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:04 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:04 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:59:04 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (461508ms till timeout)
2022-03-30 21:59:05 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:05 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:05 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:59:05 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (460435ms till timeout)
2022-03-30 21:59:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:59:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:59:06 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:06 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:06 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:59:06 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (459357ms till timeout)
2022-03-30 21:59:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:59:07 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:07 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:07 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:59:07 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (458284ms till timeout)
2022-03-30 21:59:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:59:08 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:08 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:08 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:59:08 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (457216ms till timeout)
2022-03-30 21:59:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:59:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:59:09 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:10 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:10 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:59:10 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (456146ms till timeout)
2022-03-30 21:59:11 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:11 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:11 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:59:11 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (455068ms till timeout)
2022-03-30 21:59:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:59:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:59:12 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:12 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:12 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:59:12 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (453996ms till timeout)
2022-03-30 21:59:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:59:13 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:59:13 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:13 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:59:13 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (452921ms till timeout)
2022-03-30 21:59:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:59:14 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:14 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:14 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:59:14 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (451848ms till timeout)
2022-03-30 21:59:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:59:15 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:15 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:15 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:59:15 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (450772ms till timeout)
2022-03-30 21:59:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:59:16 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:16 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:16 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:59:16 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (449693ms till timeout)
2022-03-30 21:59:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:59:17 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:59:17 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:17 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:59:17 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (448620ms till timeout)
2022-03-30 21:59:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:59:18 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:18 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:18 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:59:18 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (447546ms till timeout)
2022-03-30 21:59:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:59:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:59:19 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:19 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:19 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:59:19 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (446472ms till timeout)
2022-03-30 21:59:20 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:20 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:20 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:59:20 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (445399ms till timeout)
2022-03-30 21:59:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:59:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:59:21 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:21 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:21 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:59:21 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (444323ms till timeout)
2022-03-30 21:59:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:59:22 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:22 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:22 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:59:22 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (443250ms till timeout)
2022-03-30 21:59:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:59:23 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:24 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:24 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:59:24 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (442178ms till timeout)
2022-03-30 21:59:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:59:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 21:59:25 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:25 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:25 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:59:25 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (441111ms till timeout)
2022-03-30 21:59:26 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:26 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:26 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:59:26 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (440043ms till timeout)
2022-03-30 21:59:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 21:59:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 21:59:27 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:27 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:27 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:59:27 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (438972ms till timeout)
2022-03-30 21:59:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 21:59:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 21:59:28 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:28 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:28 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:59:28 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (437901ms till timeout)
2022-03-30 21:59:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 21:59:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 21:59:29 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:29 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 21:59:29 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 21:59:29 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 21:59:29 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "cruise-control-st" not found
2022-03-30 21:59:29 [ForkJoinPool-3-worker-11] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 21:59:29 [ForkJoinPool-3-worker-11] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[], io.strimzi.test.logs.CollectorElement@3d7a59f3=[], io.strimzi.test.logs.CollectorElement@dce7372d=[], io.strimzi.test.logs.CollectorElement@7386938d=[], io.strimzi.test.logs.CollectorElement@afd54539=[], io.strimzi.test.logs.CollectorElement@cf7c9d53=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@bfb2174d=[]}
2022-03-30 21:59:29 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:254] CruiseControlST - Notifies waiting test suites:[CruiseControlST, HttpBridgeTlsST, UserST, CruiseControlApiST, ListenersST, SecurityST, RollingUpdateST] to and randomly select one to start execution
2022-03-30 21:59:29 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:85] [cruisecontrol.CruiseControlST] - Removing parallel suite: CruiseControlST
2022-03-30 21:59:29 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:89] [cruisecontrol.CruiseControlST] - Parallel suites count: 0
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,853.059 s - in io.strimzi.systemtest.cruisecontrol.CruiseControlST
2022-03-30 21:59:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 1
2022-03-30 21:59:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 0
2022-03-30 21:59:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 1
2022-03-30 21:59:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 0
2022-03-30 21:59:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 1
2022-03-30 21:59:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 0
2022-03-30 21:59:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 1
2022-03-30 21:59:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 0
2022-03-30 21:59:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 1
2022-03-30 21:59:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 0
2022-03-30 21:59:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 1
2022-03-30 21:59:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 0
2022-03-30 21:59:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 2
2022-03-30 21:59:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 0
2022-03-30 21:59:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 2
2022-03-30 21:59:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 0
2022-03-30 21:59:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 2
2022-03-30 21:59:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 0
2022-03-30 21:59:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 2
2022-03-30 21:59:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 0
2022-03-30 21:59:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 2
2022-03-30 21:59:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 0
2022-03-30 21:59:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 2
2022-03-30 21:59:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 0
2022-03-30 21:59:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 3
2022-03-30 21:59:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 0
2022-03-30 21:59:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 3
2022-03-30 21:59:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 0
2022-03-30 21:59:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 3
2022-03-30 21:59:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 0
2022-03-30 21:59:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 3
2022-03-30 21:59:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 0
2022-03-30 21:59:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 3
2022-03-30 21:59:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 0
2022-03-30 21:59:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 3
2022-03-30 21:59:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 0
2022-03-30 21:59:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 4
2022-03-30 21:59:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 0
2022-03-30 21:59:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 4
2022-03-30 21:59:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 0
2022-03-30 21:59:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 4
2022-03-30 21:59:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 0
2022-03-30 21:59:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 4
2022-03-30 21:59:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 0
2022-03-30 21:59:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 4
2022-03-30 21:59:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 0
2022-03-30 21:59:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 4
2022-03-30 21:59:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 0
2022-03-30 21:59:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 5
2022-03-30 21:59:49 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:136] Suite mirrormaker.MirrorMakerIsolatedST has locked the @IsolatedSuite and other @IsolatedSuites must wait until lock is released.
2022-03-30 21:59:49 [ForkJoinPool-3-worker-9] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-30 21:59:49 [ForkJoinPool-3-worker-9] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-30 21:59:49 [ForkJoinPool-3-worker-9] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-30 21:59:49 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 21:59:49 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-30 21:59:49 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 21:59:49 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 21:59:49 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 21:59:49 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 21:59:49 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 21:59:49 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 21:59:49 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 21:59:49 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 21:59:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io not ready, will try again in 10000 ms (179992ms till timeout)
2022-03-30 21:59:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 5
2022-03-30 21:59:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:59:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 5
2022-03-30 21:59:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 5
2022-03-30 21:59:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 5
2022-03-30 21:59:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 5
2022-03-30 21:59:56 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:59:59 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 21:59:59 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 21:59:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179914ms till timeout)
2022-03-30 22:00:01 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:00:06 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:00:09 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 22:00:09 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 22:00:09 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 22:00:09 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 22:00:09 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 22:00:09 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 22:00:09 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:00:09 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 22:00:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator not ready, will try again in 10000 ms (479952ms till timeout)
2022-03-30 22:00:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:00:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:00:19 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 22:00:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 22:00:19 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 22:00:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 22:00:19 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:00:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 22:00:19 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 22:00:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 22:00:19 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 22:00:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 22:00:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179928ms till timeout)
2022-03-30 22:00:21 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:00:26 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:00:29 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 22:00:29 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 22:00:29 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 22:00:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 22:00:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179834ms till timeout)
2022-03-30 22:00:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:00:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:00:40 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:00:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 22:00:40 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:00:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 22:00:40 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 22:00:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 22:00:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io not ready, will try again in 10000 ms (179992ms till timeout)
2022-03-30 22:00:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:00:46 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:00:50 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 22:00:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 22:00:50 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 22:00:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 22:00:50 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 22:00:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 22:00:50 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 22:00:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 22:00:50 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 22:00:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 22:00:50 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:00:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 22:00:50 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:00:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-30 22:00:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 22:00:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 22:00:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v104804
2022-03-30 22:00:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v104804
2022-03-30 22:00:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=104804&allowWatchBookmarks=true&watch=true...
2022-03-30 22:00:50 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 22:00:50 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 104805
2022-03-30 22:00:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:00:55 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 104820
2022-03-30 22:00:55 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 104821
2022-03-30 22:00:55 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v104820 in namespace default
2022-03-30 22:00:55 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@3bb0db2a
2022-03-30 22:00:55 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 22:00:55 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@67860184
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@26fcd291
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=120000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@3f2e6467, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@26fcd291, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=120000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-30 22:00:55 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@67860184
2022-03-30 22:00:55 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@67860184
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-30 22:00:55 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 22:00:55 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T22:00:55Z",
        "name": "infra-namespace",
        "resourceVersion": "104822",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "a2682253-177d-48d0-b5b3-a31e7da9aefb"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 22:00:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 22:00:56 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 22:00:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479988ms till timeout)
2022-03-30 22:00:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478985ms till timeout)
2022-03-30 22:00:58 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477982ms till timeout)
2022-03-30 22:00:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476979ms till timeout)
2022-03-30 22:01:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475975ms till timeout)
2022-03-30 22:01:01 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:01:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474972ms till timeout)
2022-03-30 22:01:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473969ms till timeout)
2022-03-30 22:01:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472966ms till timeout)
2022-03-30 22:01:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471962ms till timeout)
2022-03-30 22:01:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470959ms till timeout)
2022-03-30 22:01:06 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:01:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469956ms till timeout)
2022-03-30 22:01:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468952ms till timeout)
2022-03-30 22:01:08 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467949ms till timeout)
2022-03-30 22:01:09 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-30 22:01:09 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-30 22:01:09 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-30 22:01:09 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-dqdp5 not ready: strimzi-cluster-operator)
2022-03-30 22:01:09 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-dqdp5 are ready
2022-03-30 22:01:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 22:01:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-dqdp5 not ready: strimzi-cluster-operator)
2022-03-30 22:01:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-dqdp5 are ready
2022-03-30 22:01:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-30 22:01:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:01:11 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-dqdp5 not ready: strimzi-cluster-operator)
2022-03-30 22:01:11 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-dqdp5 are ready
2022-03-30 22:01:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597990ms till timeout)
2022-03-30 22:01:12 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-dqdp5 not ready: strimzi-cluster-operator)
2022-03-30 22:01:12 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-dqdp5 are ready
2022-03-30 22:01:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596986ms till timeout)
2022-03-30 22:01:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-dqdp5 not ready: strimzi-cluster-operator)
2022-03-30 22:01:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-dqdp5 are ready
2022-03-30 22:01:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595983ms till timeout)
2022-03-30 22:01:14 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-dqdp5 not ready: strimzi-cluster-operator)
2022-03-30 22:01:14 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-dqdp5 are ready
2022-03-30 22:01:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594979ms till timeout)
2022-03-30 22:01:15 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-dqdp5 not ready: strimzi-cluster-operator)
2022-03-30 22:01:15 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-dqdp5 are ready
2022-03-30 22:01:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593975ms till timeout)
2022-03-30 22:01:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:01:16 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-dqdp5 not ready: strimzi-cluster-operator)
2022-03-30 22:01:16 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-dqdp5 are ready
2022-03-30 22:01:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592971ms till timeout)
2022-03-30 22:01:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-dqdp5 not ready: strimzi-cluster-operator)
2022-03-30 22:01:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-dqdp5 are ready
2022-03-30 22:01:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591967ms till timeout)
2022-03-30 22:01:18 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-dqdp5 not ready: strimzi-cluster-operator)
2022-03-30 22:01:18 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-dqdp5 are ready
2022-03-30 22:01:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590964ms till timeout)
2022-03-30 22:01:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-dqdp5 not ready: strimzi-cluster-operator)
2022-03-30 22:01:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-dqdp5 are ready
2022-03-30 22:01:19 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-30 22:01:19 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 22:01:19 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST.testMirrorMakerTlsAuthenticated-STARTED
2022-03-30 22:01:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 22:01:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:659] [mirrormaker.MirrorMakerIsolatedST - Before Each] - Setup test case environment
2022-03-30 22:01:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:77] [mirrormaker.MirrorMakerIsolatedST] - Adding parallel test: testMirrorMakerTlsAuthenticated
2022-03-30 22:01:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:81] [mirrormaker.MirrorMakerIsolatedST] - Parallel test count: 1
2022-03-30 22:01:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:230] testMirrorMakerTlsAuthenticated test now can proceed its execution
2022-03-30 22:01:19 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 22:01:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349, testSendSimpleMessageTls=my-cluster-b70e5342, testSendMessagesTlsScramSha=my-cluster-380e6586, testUpdateUser=my-cluster-9b2bd187, testReceiveSimpleMessageTls=my-cluster-e8a518cd, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9}
2022-03-30 22:01:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1001559761-691587912, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1590558165-46977776, testSendSimpleMessageTls=my-user-602433797-900694548, testSendMessagesTlsScramSha=my-user-2039401547-533835955, testUpdateUser=my-user-595127408-1604948726, testReceiveSimpleMessageTls=my-user-761922164-2019863647, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1606351157-1815623087, testSendMessagesCustomListenerTlsScramSha=my-user-445988529-51741427, testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659, testMirrorMakerTlsAuthenticated=my-user-692603309-1711649825}
2022-03-30 22:01:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-741363275-285355018, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1210910585-1669754462, testSendSimpleMessageTls=my-topic-1299629356-597168683, testSendMessagesTlsScramSha=my-topic-988094002-1400465950, testUpdateUser=my-topic-1875736194-1340884809, testReceiveSimpleMessageTls=my-topic-1007790874-1032272408, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1931531910-629202741, testSendMessagesCustomListenerTlsScramSha=my-topic-1039904883-1044305687, testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393, testMirrorMakerTlsAuthenticated=my-topic-1945138991-1836415682}
2022-03-30 22:01:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349-kafka-clients, testSendSimpleMessageTls=my-cluster-b70e5342-kafka-clients, testSendMessagesTlsScramSha=my-cluster-380e6586-kafka-clients, testUpdateUser=my-cluster-9b2bd187-kafka-clients, testReceiveSimpleMessageTls=my-cluster-e8a518cd-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9-kafka-clients}
2022-03-30 22:01:19 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-6 for test case:testMirrorMakerTlsAuthenticated
2022-03-30 22:01:19 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-6
2022-03-30 22:01:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-6
2022-03-30 22:01:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-6 -o json
2022-03-30 22:01:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-6 -o json
2022-03-30 22:01:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:01:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T22:01:19Z",
        "name": "namespace-6",
        "resourceVersion": "104915",
        "selfLink": "/api/v1/namespaces/namespace-6",
        "uid": "04f8f15b-e186-4495-8507-252c594d559d"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 22:01:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@6a42f58d=[namespace-6]}
2022-03-30 22:01:19 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-6
2022-03-30 22:01:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-6, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 22:01:19 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-6
2022-03-30 22:01:19 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-a0cac5a9-source in namespace namespace-6
2022-03-30 22:01:19 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-03-30 22:01:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-a0cac5a9-source
2022-03-30 22:01:19 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-a0cac5a9-source will have desired state: Ready
2022-03-30 22:01:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-a0cac5a9-source will have desired state: Ready
2022-03-30 22:01:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (839998ms till timeout)
2022-03-30 22:01:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (838995ms till timeout)
2022-03-30 22:01:21 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:01:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (837991ms till timeout)
2022-03-30 22:01:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (836988ms till timeout)
2022-03-30 22:01:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (835985ms till timeout)
2022-03-30 22:01:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (834982ms till timeout)
2022-03-30 22:01:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (833979ms till timeout)
2022-03-30 22:01:26 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:01:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (832975ms till timeout)
2022-03-30 22:01:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (831972ms till timeout)
2022-03-30 22:01:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (830968ms till timeout)
2022-03-30 22:01:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (829965ms till timeout)
2022-03-30 22:01:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (828961ms till timeout)
2022-03-30 22:01:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:01:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (827958ms till timeout)
2022-03-30 22:01:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (826955ms till timeout)
2022-03-30 22:01:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (825952ms till timeout)
2022-03-30 22:01:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (824948ms till timeout)
2022-03-30 22:01:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (823945ms till timeout)
2022-03-30 22:01:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:01:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (822942ms till timeout)
2022-03-30 22:01:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (821939ms till timeout)
2022-03-30 22:01:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (820936ms till timeout)
2022-03-30 22:01:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (819933ms till timeout)
2022-03-30 22:01:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (818929ms till timeout)
2022-03-30 22:01:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:01:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (817926ms till timeout)
2022-03-30 22:01:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (816922ms till timeout)
2022-03-30 22:01:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (815919ms till timeout)
2022-03-30 22:01:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (814915ms till timeout)
2022-03-30 22:01:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (813912ms till timeout)
2022-03-30 22:01:46 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:01:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (812909ms till timeout)
2022-03-30 22:01:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (811905ms till timeout)
2022-03-30 22:01:48 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (810902ms till timeout)
2022-03-30 22:01:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (809899ms till timeout)
2022-03-30 22:01:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (808896ms till timeout)
2022-03-30 22:01:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:01:51 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (807893ms till timeout)
2022-03-30 22:01:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (806889ms till timeout)
2022-03-30 22:01:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (805886ms till timeout)
2022-03-30 22:01:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (804882ms till timeout)
2022-03-30 22:01:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (803879ms till timeout)
2022-03-30 22:01:56 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:01:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (802875ms till timeout)
2022-03-30 22:01:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (801872ms till timeout)
2022-03-30 22:01:58 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (800869ms till timeout)
2022-03-30 22:01:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (799865ms till timeout)
2022-03-30 22:02:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (798861ms till timeout)
2022-03-30 22:02:01 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:02:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (797858ms till timeout)
2022-03-30 22:02:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (796855ms till timeout)
2022-03-30 22:02:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (795851ms till timeout)
2022-03-30 22:02:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (794848ms till timeout)
2022-03-30 22:02:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (793845ms till timeout)
2022-03-30 22:02:06 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:02:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (792841ms till timeout)
2022-03-30 22:02:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (791838ms till timeout)
2022-03-30 22:02:08 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (790835ms till timeout)
2022-03-30 22:02:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (789832ms till timeout)
2022-03-30 22:02:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (788829ms till timeout)
2022-03-30 22:02:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:02:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (787825ms till timeout)
2022-03-30 22:02:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (786822ms till timeout)
2022-03-30 22:02:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (785818ms till timeout)
2022-03-30 22:02:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (784815ms till timeout)
2022-03-30 22:02:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (783810ms till timeout)
2022-03-30 22:02:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:02:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (782807ms till timeout)
2022-03-30 22:02:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (781803ms till timeout)
2022-03-30 22:02:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (780800ms till timeout)
2022-03-30 22:02:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (779796ms till timeout)
2022-03-30 22:02:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (778793ms till timeout)
2022-03-30 22:02:21 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:02:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (777790ms till timeout)
2022-03-30 22:02:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (776786ms till timeout)
2022-03-30 22:02:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (775783ms till timeout)
2022-03-30 22:02:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (774780ms till timeout)
2022-03-30 22:02:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (773777ms till timeout)
2022-03-30 22:02:26 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:02:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (772773ms till timeout)
2022-03-30 22:02:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (771770ms till timeout)
2022-03-30 22:02:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (770767ms till timeout)
2022-03-30 22:02:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-source will have desired state: Ready not ready, will try again in 1000 ms (769763ms till timeout)
2022-03-30 22:02:30 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-a0cac5a9-source is in desired state: Ready
2022-03-30 22:02:30 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-a0cac5a9-target in namespace namespace-6
2022-03-30 22:02:30 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-03-30 22:02:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-a0cac5a9-target
2022-03-30 22:02:30 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-a0cac5a9-target will have desired state: Ready
2022-03-30 22:02:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-a0cac5a9-target will have desired state: Ready
2022-03-30 22:02:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (839997ms till timeout)
2022-03-30 22:02:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:02:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (838994ms till timeout)
2022-03-30 22:02:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (837991ms till timeout)
2022-03-30 22:02:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (836988ms till timeout)
2022-03-30 22:02:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (835984ms till timeout)
2022-03-30 22:02:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (834981ms till timeout)
2022-03-30 22:02:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:02:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (833978ms till timeout)
2022-03-30 22:02:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (832974ms till timeout)
2022-03-30 22:02:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (831971ms till timeout)
2022-03-30 22:02:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (830968ms till timeout)
2022-03-30 22:02:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (829964ms till timeout)
2022-03-30 22:02:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:02:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (828961ms till timeout)
2022-03-30 22:02:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (827958ms till timeout)
2022-03-30 22:02:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (826954ms till timeout)
2022-03-30 22:02:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (825951ms till timeout)
2022-03-30 22:02:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (824948ms till timeout)
2022-03-30 22:02:46 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:02:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (823945ms till timeout)
2022-03-30 22:02:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (822941ms till timeout)
2022-03-30 22:02:48 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (821938ms till timeout)
2022-03-30 22:02:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (820935ms till timeout)
2022-03-30 22:02:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (819931ms till timeout)
2022-03-30 22:02:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:02:51 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (818928ms till timeout)
2022-03-30 22:02:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (817925ms till timeout)
2022-03-30 22:02:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (816922ms till timeout)
2022-03-30 22:02:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (815918ms till timeout)
2022-03-30 22:02:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (814915ms till timeout)
2022-03-30 22:02:56 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:02:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (813911ms till timeout)
2022-03-30 22:02:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (812908ms till timeout)
2022-03-30 22:02:58 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (811905ms till timeout)
2022-03-30 22:02:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (810901ms till timeout)
2022-03-30 22:03:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (809898ms till timeout)
2022-03-30 22:03:01 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:03:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (808895ms till timeout)
2022-03-30 22:03:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (807891ms till timeout)
2022-03-30 22:03:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (806888ms till timeout)
2022-03-30 22:03:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (805885ms till timeout)
2022-03-30 22:03:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (804882ms till timeout)
2022-03-30 22:03:06 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:03:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (803879ms till timeout)
2022-03-30 22:03:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (802875ms till timeout)
2022-03-30 22:03:08 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (801872ms till timeout)
2022-03-30 22:03:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (800869ms till timeout)
2022-03-30 22:03:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (799866ms till timeout)
2022-03-30 22:03:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:03:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (798862ms till timeout)
2022-03-30 22:03:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (797859ms till timeout)
2022-03-30 22:03:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (796856ms till timeout)
2022-03-30 22:03:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (795852ms till timeout)
2022-03-30 22:03:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (794849ms till timeout)
2022-03-30 22:03:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:03:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (793845ms till timeout)
2022-03-30 22:03:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (792842ms till timeout)
2022-03-30 22:03:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (791838ms till timeout)
2022-03-30 22:03:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (790835ms till timeout)
2022-03-30 22:03:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (789831ms till timeout)
2022-03-30 22:03:21 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:03:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (788828ms till timeout)
2022-03-30 22:03:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (787824ms till timeout)
2022-03-30 22:03:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (786821ms till timeout)
2022-03-30 22:03:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (785818ms till timeout)
2022-03-30 22:03:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (784814ms till timeout)
2022-03-30 22:03:26 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:03:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (783811ms till timeout)
2022-03-30 22:03:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (782808ms till timeout)
2022-03-30 22:03:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (781805ms till timeout)
2022-03-30 22:03:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (780802ms till timeout)
2022-03-30 22:03:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (779798ms till timeout)
2022-03-30 22:03:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:03:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (778795ms till timeout)
2022-03-30 22:03:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (777792ms till timeout)
2022-03-30 22:03:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (776789ms till timeout)
2022-03-30 22:03:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (775785ms till timeout)
2022-03-30 22:03:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (774782ms till timeout)
2022-03-30 22:03:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:03:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (773779ms till timeout)
2022-03-30 22:03:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (772776ms till timeout)
2022-03-30 22:03:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (771772ms till timeout)
2022-03-30 22:03:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (770769ms till timeout)
2022-03-30 22:03:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (769765ms till timeout)
2022-03-30 22:03:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:03:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a0cac5a9-target will have desired state: Ready not ready, will try again in 1000 ms (768762ms till timeout)
2022-03-30 22:03:43 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-a0cac5a9-target is in desired state: Ready
2022-03-30 22:03:43 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1057826384-1887953069-source-703984749 in namespace namespace-6
2022-03-30 22:03:43 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-03-30 22:03:43 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1057826384-1887953069-source-703984749
2022-03-30 22:03:43 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1057826384-1887953069-source-703984749 will have desired state: Ready
2022-03-30 22:03:43 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1057826384-1887953069-source-703984749 will have desired state: Ready
2022-03-30 22:03:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1057826384-1887953069-source-703984749 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 22:03:44 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1057826384-1887953069-source-703984749 is in desired state: Ready
2022-03-30 22:03:44 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-a0cac5a9-my-user-source in namespace namespace-6
2022-03-30 22:03:44 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-03-30 22:03:44 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-cluster-a0cac5a9-my-user-source
2022-03-30 22:03:44 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-a0cac5a9-my-user-source will have desired state: Ready
2022-03-30 22:03:44 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-cluster-a0cac5a9-my-user-source will have desired state: Ready
2022-03-30 22:03:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaUser: my-cluster-a0cac5a9-my-user-source will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 22:03:45 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-a0cac5a9-my-user-source is in desired state: Ready
2022-03-30 22:03:45 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-a0cac5a9-my-user-target in namespace namespace-6
2022-03-30 22:03:45 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-03-30 22:03:45 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-cluster-a0cac5a9-my-user-target
2022-03-30 22:03:45 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-a0cac5a9-my-user-target will have desired state: Ready
2022-03-30 22:03:45 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-cluster-a0cac5a9-my-user-target will have desired state: Ready
2022-03-30 22:03:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaUser: my-cluster-a0cac5a9-my-user-target will have desired state: Ready not ready, will try again in 1000 ms (179997ms till timeout)
2022-03-30 22:03:46 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-a0cac5a9-my-user-target is in desired state: Ready
2022-03-30 22:03:46 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-a0cac5a9-kafka-clients in namespace namespace-6
2022-03-30 22:03:46 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-03-30 22:03:46 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-a0cac5a9-kafka-clients
2022-03-30 22:03:46 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-a0cac5a9-kafka-clients will be ready
2022-03-30 22:03:46 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-a0cac5a9-kafka-clients will be ready
2022-03-30 22:03:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-a0cac5a9-kafka-clients will be ready not ready, will try again in 1000 ms (479998ms till timeout)
2022-03-30 22:03:46 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:03:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-a0cac5a9-kafka-clients will be ready not ready, will try again in 1000 ms (478995ms till timeout)
2022-03-30 22:03:48 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-a0cac5a9-kafka-clients is ready
2022-03-30 22:03:48 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for pod with prefixmy-cluster-a0cac5a9-kafka-clients is present.
2022-03-30 22:03:48 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1945138991-1836415682-test-1 in namespace namespace-6
2022-03-30 22:03:48 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-03-30 22:03:48 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1945138991-1836415682-test-1
2022-03-30 22:03:48 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1945138991-1836415682-test-1 will have desired state: Ready
2022-03-30 22:03:48 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1945138991-1836415682-test-1 will have desired state: Ready
2022-03-30 22:03:48 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1945138991-1836415682-test-1 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 22:03:49 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1945138991-1836415682-test-1 is in desired state: Ready
2022-03-30 22:03:49 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1945138991-1836415682-test-2 in namespace namespace-6
2022-03-30 22:03:49 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-03-30 22:03:49 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1945138991-1836415682-test-2
2022-03-30 22:03:49 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1945138991-1836415682-test-2 will have desired state: Ready
2022-03-30 22:03:49 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1945138991-1836415682-test-2 will have desired state: Ready
2022-03-30 22:03:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1945138991-1836415682-test-2 will have desired state: Ready not ready, will try again in 1000 ms (179999ms till timeout)
2022-03-30 22:03:50 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1945138991-1836415682-test-2 is in desired state: Ready
2022-03-30 22:03:50 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-30 22:03:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Producer and consumer will successfully send and receive messages.
2022-03-30 22:03:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4cb597ec, which are set.
2022-03-30 22:03:50 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@4dcc19f1, messages=[], arguments=[--topic, my-topic-1945138991-1836415682-test-1, --bootstrap-server, my-cluster-a0cac5a9-source-kafka-bootstrap.namespace-6.svc:9093, --max-messages, 200, USER=my_cluster_a0cac5a9_my_user_source], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-a0cac5a9-kafka-clients-55d47ccdcd-nr9z6', podNamespace='namespace-6', bootstrapServer='my-cluster-a0cac5a9-source-kafka-bootstrap.namespace-6.svc:9093', topicName='my-topic-1945138991-1836415682-test-1', maxMessages=200, kafkaUsername='my-cluster-a0cac5a9-my-user-source', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4cb597ec}
2022-03-30 22:03:50 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-a0cac5a9-source-kafka-bootstrap.namespace-6.svc:9093:my-topic-1945138991-1836415682-test-1 from pod my-cluster-a0cac5a9-kafka-clients-55d47ccdcd-nr9z6
2022-03-30 22:03:50 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-a0cac5a9-kafka-clients-55d47ccdcd-nr9z6 -n namespace-6 -- /opt/kafka/producer.sh --topic my-topic-1945138991-1836415682-test-1 --bootstrap-server my-cluster-a0cac5a9-source-kafka-bootstrap.namespace-6.svc:9093 --max-messages 200 USER=my_cluster_a0cac5a9_my_user_source
2022-03-30 22:03:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-a0cac5a9-kafka-clients-55d47ccdcd-nr9z6 -n namespace-6 -- /opt/kafka/producer.sh --topic my-topic-1945138991-1836415682-test-1 --bootstrap-server my-cluster-a0cac5a9-source-kafka-bootstrap.namespace-6.svc:9093 --max-messages 200 USER=my_cluster_a0cac5a9_my_user_source
2022-03-30 22:03:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:03:53 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-30 22:03:53 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-03-30 22:03:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@b2cbfcd, which are set.
2022-03-30 22:03:53 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@6b4653b3, messages=[], arguments=[--topic, my-topic-1945138991-1836415682-test-1, --bootstrap-server, my-cluster-a0cac5a9-source-kafka-bootstrap.namespace-6.svc:9093, --max-messages, 200, USER=my_cluster_a0cac5a9_my_user_source, --group-id, my-consumer-group-1962217097, --group-instance-id, instance781534907], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-a0cac5a9-kafka-clients-55d47ccdcd-nr9z6', podNamespace='namespace-6', bootstrapServer='my-cluster-a0cac5a9-source-kafka-bootstrap.namespace-6.svc:9093', topicName='my-topic-1945138991-1836415682-test-1', maxMessages=200, kafkaUsername='my-cluster-a0cac5a9-my-user-source', consumerGroupName='my-consumer-group-1962217097', consumerInstanceId='instance781534907', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@b2cbfcd}
2022-03-30 22:03:53 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-a0cac5a9-source-kafka-bootstrap.namespace-6.svc:9093:my-topic-1945138991-1836415682-test-1 from pod my-cluster-a0cac5a9-kafka-clients-55d47ccdcd-nr9z6
2022-03-30 22:03:53 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-a0cac5a9-kafka-clients-55d47ccdcd-nr9z6 -n namespace-6 -- /opt/kafka/consumer.sh --topic my-topic-1945138991-1836415682-test-1 --bootstrap-server my-cluster-a0cac5a9-source-kafka-bootstrap.namespace-6.svc:9093 --max-messages 200 USER=my_cluster_a0cac5a9_my_user_source --group-id my-consumer-group-1962217097 --group-instance-id instance781534907
2022-03-30 22:03:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-a0cac5a9-kafka-clients-55d47ccdcd-nr9z6 -n namespace-6 -- /opt/kafka/consumer.sh --topic my-topic-1945138991-1836415682-test-1 --bootstrap-server my-cluster-a0cac5a9-source-kafka-bootstrap.namespace-6.svc:9093 --max-messages 200 USER=my_cluster_a0cac5a9_my_user_source --group-id my-consumer-group-1962217097 --group-instance-id instance781534907
2022-03-30 22:03:56 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:04:00 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 22:04:00 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-03-30 22:04:00 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Producer and consumer will successfully send and receive messages.
2022-03-30 22:04:00 [ForkJoinPool-3-worker-9] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4668dcbb, which are set.
2022-03-30 22:04:00 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@5dc870a5, messages=[], arguments=[--topic, my-topic-1945138991-1836415682-test-2, --bootstrap-server, my-cluster-a0cac5a9-target-kafka-bootstrap.namespace-6.svc:9093, --max-messages, 200, USER=my_cluster_a0cac5a9_my_user_target], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-a0cac5a9-kafka-clients-55d47ccdcd-nr9z6', podNamespace='namespace-6', bootstrapServer='my-cluster-a0cac5a9-target-kafka-bootstrap.namespace-6.svc:9093', topicName='my-topic-1945138991-1836415682-test-2', maxMessages=200, kafkaUsername='my-cluster-a0cac5a9-my-user-target', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4668dcbb}
2022-03-30 22:04:00 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-a0cac5a9-target-kafka-bootstrap.namespace-6.svc:9093:my-topic-1945138991-1836415682-test-2 from pod my-cluster-a0cac5a9-kafka-clients-55d47ccdcd-nr9z6
2022-03-30 22:04:00 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-a0cac5a9-kafka-clients-55d47ccdcd-nr9z6 -n namespace-6 -- /opt/kafka/producer.sh --topic my-topic-1945138991-1836415682-test-2 --bootstrap-server my-cluster-a0cac5a9-target-kafka-bootstrap.namespace-6.svc:9093 --max-messages 200 USER=my_cluster_a0cac5a9_my_user_target
2022-03-30 22:04:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-a0cac5a9-kafka-clients-55d47ccdcd-nr9z6 -n namespace-6 -- /opt/kafka/producer.sh --topic my-topic-1945138991-1836415682-test-2 --bootstrap-server my-cluster-a0cac5a9-target-kafka-bootstrap.namespace-6.svc:9093 --max-messages 200 USER=my_cluster_a0cac5a9_my_user_target
2022-03-30 22:04:01 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:04:04 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-30 22:04:04 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-03-30 22:04:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@33f46e06, which are set.
2022-03-30 22:04:04 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@6af67800, messages=[], arguments=[--topic, my-topic-1945138991-1836415682-test-2, --bootstrap-server, my-cluster-a0cac5a9-target-kafka-bootstrap.namespace-6.svc:9093, --max-messages, 200, USER=my_cluster_a0cac5a9_my_user_target, --group-id, my-consumer-group-296767799, --group-instance-id, instance747986335], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-a0cac5a9-kafka-clients-55d47ccdcd-nr9z6', podNamespace='namespace-6', bootstrapServer='my-cluster-a0cac5a9-target-kafka-bootstrap.namespace-6.svc:9093', topicName='my-topic-1945138991-1836415682-test-2', maxMessages=200, kafkaUsername='my-cluster-a0cac5a9-my-user-target', consumerGroupName='my-consumer-group-296767799', consumerInstanceId='instance747986335', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@33f46e06}
2022-03-30 22:04:04 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-a0cac5a9-target-kafka-bootstrap.namespace-6.svc:9093:my-topic-1945138991-1836415682-test-2 from pod my-cluster-a0cac5a9-kafka-clients-55d47ccdcd-nr9z6
2022-03-30 22:04:04 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-a0cac5a9-kafka-clients-55d47ccdcd-nr9z6 -n namespace-6 -- /opt/kafka/consumer.sh --topic my-topic-1945138991-1836415682-test-2 --bootstrap-server my-cluster-a0cac5a9-target-kafka-bootstrap.namespace-6.svc:9093 --max-messages 200 USER=my_cluster_a0cac5a9_my_user_target --group-id my-consumer-group-296767799 --group-instance-id instance747986335
2022-03-30 22:04:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-a0cac5a9-kafka-clients-55d47ccdcd-nr9z6 -n namespace-6 -- /opt/kafka/consumer.sh --topic my-topic-1945138991-1836415682-test-2 --bootstrap-server my-cluster-a0cac5a9-target-kafka-bootstrap.namespace-6.svc:9093 --max-messages 200 USER=my_cluster_a0cac5a9_my_user_target --group-id my-consumer-group-296767799 --group-instance-id instance747986335
2022-03-30 22:04:06 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:04:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:04:11 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 22:04:11 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-03-30 22:04:11 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker my-cluster-a0cac5a9 in namespace namespace-6
2022-03-30 22:04:11 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-03-30 22:04:11 [ForkJoinPool-3-worker-9] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkamirrormakers' with unstable version 'v1beta2'
2022-03-30 22:04:11 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaMirrorMaker:my-cluster-a0cac5a9
2022-03-30 22:04:11 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready
2022-03-30 22:04:11 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready
2022-03-30 22:04:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (479998ms till timeout)
2022-03-30 22:04:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (478995ms till timeout)
2022-03-30 22:04:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (477991ms till timeout)
2022-03-30 22:04:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (476988ms till timeout)
2022-03-30 22:04:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (475984ms till timeout)
2022-03-30 22:04:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:04:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (474980ms till timeout)
2022-03-30 22:04:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (473977ms till timeout)
2022-03-30 22:04:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (472974ms till timeout)
2022-03-30 22:04:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (471970ms till timeout)
2022-03-30 22:04:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (470963ms till timeout)
2022-03-30 22:04:21 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:04:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (469960ms till timeout)
2022-03-30 22:04:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (468954ms till timeout)
2022-03-30 22:04:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (467951ms till timeout)
2022-03-30 22:04:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (466948ms till timeout)
2022-03-30 22:04:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (465945ms till timeout)
2022-03-30 22:04:26 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:04:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (464941ms till timeout)
2022-03-30 22:04:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (463938ms till timeout)
2022-03-30 22:04:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (462935ms till timeout)
2022-03-30 22:04:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (461931ms till timeout)
2022-03-30 22:04:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (460928ms till timeout)
2022-03-30 22:04:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:04:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (459925ms till timeout)
2022-03-30 22:04:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (458922ms till timeout)
2022-03-30 22:04:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (457919ms till timeout)
2022-03-30 22:04:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (456916ms till timeout)
2022-03-30 22:04:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (455913ms till timeout)
2022-03-30 22:04:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:04:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (454909ms till timeout)
2022-03-30 22:04:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (453906ms till timeout)
2022-03-30 22:04:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (452903ms till timeout)
2022-03-30 22:04:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (451900ms till timeout)
2022-03-30 22:04:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (450896ms till timeout)
2022-03-30 22:04:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:04:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (449893ms till timeout)
2022-03-30 22:04:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (448890ms till timeout)
2022-03-30 22:04:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (447887ms till timeout)
2022-03-30 22:04:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (446884ms till timeout)
2022-03-30 22:04:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (445880ms till timeout)
2022-03-30 22:04:46 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:04:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (444877ms till timeout)
2022-03-30 22:04:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (443874ms till timeout)
2022-03-30 22:04:48 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (442871ms till timeout)
2022-03-30 22:04:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (441867ms till timeout)
2022-03-30 22:04:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (440864ms till timeout)
2022-03-30 22:04:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:04:51 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (439861ms till timeout)
2022-03-30 22:04:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (438857ms till timeout)
2022-03-30 22:04:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (437854ms till timeout)
2022-03-30 22:04:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (436851ms till timeout)
2022-03-30 22:04:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (435848ms till timeout)
2022-03-30 22:04:56 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:04:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (434844ms till timeout)
2022-03-30 22:04:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (433841ms till timeout)
2022-03-30 22:04:58 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (432838ms till timeout)
2022-03-30 22:04:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (431835ms till timeout)
2022-03-30 22:05:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (430832ms till timeout)
2022-03-30 22:05:01 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:05:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (429828ms till timeout)
2022-03-30 22:05:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (428825ms till timeout)
2022-03-30 22:05:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (427822ms till timeout)
2022-03-30 22:05:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (426819ms till timeout)
2022-03-30 22:05:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (425815ms till timeout)
2022-03-30 22:05:06 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:05:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (424812ms till timeout)
2022-03-30 22:05:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (423809ms till timeout)
2022-03-30 22:05:08 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (422805ms till timeout)
2022-03-30 22:05:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (421802ms till timeout)
2022-03-30 22:05:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (420799ms till timeout)
2022-03-30 22:05:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:05:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (419796ms till timeout)
2022-03-30 22:05:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-a0cac5a9 will have desired state: Ready not ready, will try again in 1000 ms (418792ms till timeout)
2022-03-30 22:05:13 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-a0cac5a9 is in desired state: Ready
2022-03-30 22:05:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Producer and consumer will successfully send and receive messages.
2022-03-30 22:05:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@17b7a145, which are set.
2022-03-30 22:05:13 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@4cc3e3ba, messages=[], arguments=[--topic, my-topic-1057826384-1887953069-source-703984749, --bootstrap-server, my-cluster-a0cac5a9-source-kafka-bootstrap.namespace-6.svc:9093, --max-messages, 200, USER=my_cluster_a0cac5a9_my_user_source], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-a0cac5a9-kafka-clients-55d47ccdcd-nr9z6', podNamespace='namespace-6', bootstrapServer='my-cluster-a0cac5a9-source-kafka-bootstrap.namespace-6.svc:9093', topicName='my-topic-1057826384-1887953069-source-703984749', maxMessages=200, kafkaUsername='my-cluster-a0cac5a9-my-user-source', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@17b7a145}
2022-03-30 22:05:13 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-a0cac5a9-source-kafka-bootstrap.namespace-6.svc:9093:my-topic-1057826384-1887953069-source-703984749 from pod my-cluster-a0cac5a9-kafka-clients-55d47ccdcd-nr9z6
2022-03-30 22:05:13 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-a0cac5a9-kafka-clients-55d47ccdcd-nr9z6 -n namespace-6 -- /opt/kafka/producer.sh --topic my-topic-1057826384-1887953069-source-703984749 --bootstrap-server my-cluster-a0cac5a9-source-kafka-bootstrap.namespace-6.svc:9093 --max-messages 200 USER=my_cluster_a0cac5a9_my_user_source
2022-03-30 22:05:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-a0cac5a9-kafka-clients-55d47ccdcd-nr9z6 -n namespace-6 -- /opt/kafka/producer.sh --topic my-topic-1057826384-1887953069-source-703984749 --bootstrap-server my-cluster-a0cac5a9-source-kafka-bootstrap.namespace-6.svc:9093 --max-messages 200 USER=my_cluster_a0cac5a9_my_user_source
2022-03-30 22:05:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:05:17 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-30 22:05:17 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-03-30 22:05:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5b8e99dd, which are set.
2022-03-30 22:05:17 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@7119b660, messages=[], arguments=[--topic, my-topic-1057826384-1887953069-source-703984749, --bootstrap-server, my-cluster-a0cac5a9-source-kafka-bootstrap.namespace-6.svc:9093, --max-messages, 200, USER=my_cluster_a0cac5a9_my_user_source, --group-id, my-consumer-group-267614319, --group-instance-id, instance1433114794], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-a0cac5a9-kafka-clients-55d47ccdcd-nr9z6', podNamespace='namespace-6', bootstrapServer='my-cluster-a0cac5a9-source-kafka-bootstrap.namespace-6.svc:9093', topicName='my-topic-1057826384-1887953069-source-703984749', maxMessages=200, kafkaUsername='my-cluster-a0cac5a9-my-user-source', consumerGroupName='my-consumer-group-267614319', consumerInstanceId='instance1433114794', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5b8e99dd}
2022-03-30 22:05:17 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-a0cac5a9-source-kafka-bootstrap.namespace-6.svc:9093:my-topic-1057826384-1887953069-source-703984749 from pod my-cluster-a0cac5a9-kafka-clients-55d47ccdcd-nr9z6
2022-03-30 22:05:17 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-a0cac5a9-kafka-clients-55d47ccdcd-nr9z6 -n namespace-6 -- /opt/kafka/consumer.sh --topic my-topic-1057826384-1887953069-source-703984749 --bootstrap-server my-cluster-a0cac5a9-source-kafka-bootstrap.namespace-6.svc:9093 --max-messages 200 USER=my_cluster_a0cac5a9_my_user_source --group-id my-consumer-group-267614319 --group-instance-id instance1433114794
2022-03-30 22:05:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-a0cac5a9-kafka-clients-55d47ccdcd-nr9z6 -n namespace-6 -- /opt/kafka/consumer.sh --topic my-topic-1057826384-1887953069-source-703984749 --bootstrap-server my-cluster-a0cac5a9-source-kafka-bootstrap.namespace-6.svc:9093 --max-messages 200 USER=my_cluster_a0cac5a9_my_user_source --group-id my-consumer-group-267614319 --group-instance-id instance1433114794
2022-03-30 22:05:21 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:05:24 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 22:05:24 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-03-30 22:05:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Consumer will successfully receive messages.
2022-03-30 22:05:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@221445ca, which are set.
2022-03-30 22:05:24 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@677271f3, messages=[], arguments=[--topic, my-topic-1057826384-1887953069-source-703984749, --bootstrap-server, my-cluster-a0cac5a9-target-kafka-bootstrap.namespace-6.svc:9093, --max-messages, 200, USER=my_cluster_a0cac5a9_my_user_target, --group-id, my-consumer-group-770988313, --group-instance-id, instance331194072], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-a0cac5a9-kafka-clients-55d47ccdcd-nr9z6', podNamespace='namespace-6', bootstrapServer='my-cluster-a0cac5a9-target-kafka-bootstrap.namespace-6.svc:9093', topicName='my-topic-1057826384-1887953069-source-703984749', maxMessages=200, kafkaUsername='my-cluster-a0cac5a9-my-user-target', consumerGroupName='my-consumer-group-770988313', consumerInstanceId='instance331194072', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@221445ca}
2022-03-30 22:05:24 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-a0cac5a9-target-kafka-bootstrap.namespace-6.svc:9093:my-topic-1057826384-1887953069-source-703984749 from pod my-cluster-a0cac5a9-kafka-clients-55d47ccdcd-nr9z6
2022-03-30 22:05:24 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-a0cac5a9-kafka-clients-55d47ccdcd-nr9z6 -n namespace-6 -- /opt/kafka/consumer.sh --topic my-topic-1057826384-1887953069-source-703984749 --bootstrap-server my-cluster-a0cac5a9-target-kafka-bootstrap.namespace-6.svc:9093 --max-messages 200 USER=my_cluster_a0cac5a9_my_user_target --group-id my-consumer-group-770988313 --group-instance-id instance331194072
2022-03-30 22:05:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-a0cac5a9-kafka-clients-55d47ccdcd-nr9z6 -n namespace-6 -- /opt/kafka/consumer.sh --topic my-topic-1057826384-1887953069-source-703984749 --bootstrap-server my-cluster-a0cac5a9-target-kafka-bootstrap.namespace-6.svc:9093 --max-messages 200 USER=my_cluster_a0cac5a9_my_user_target --group-id my-consumer-group-770988313 --group-instance-id instance331194072
2022-03-30 22:05:26 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:05:30 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 22:05:30 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-03-30 22:05:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 22:05:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:675] [mirrormaker.MirrorMakerIsolatedST - After Each] - Clean up after test
2022-03-30 22:05:30 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:05:30 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for testMirrorMakerTlsAuthenticated
2022-03-30 22:05:30 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-a0cac5a9-kafka-clients in namespace namespace-6
2022-03-30 22:05:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-a0cac5a9-kafka-clients
2022-03-30 22:05:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-a0cac5a9-kafka-clients not ready, will try again in 10000 ms (479991ms till timeout)
2022-03-30 22:05:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:05:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:05:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-a0cac5a9-kafka-clients not ready, will try again in 10000 ms (469981ms till timeout)
2022-03-30 22:05:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:05:46 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:05:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-a0cac5a9-kafka-clients not ready, will try again in 10000 ms (459971ms till timeout)
2022-03-30 22:05:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:05:56 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:06:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-a0cac5a9-kafka-clients not ready, will try again in 10000 ms (449961ms till timeout)
2022-03-30 22:06:01 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:06:06 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:06:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-a0cac5a9-kafka-clients not ready, will try again in 10000 ms (439951ms till timeout)
2022-03-30 22:06:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:06:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:06:20 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-a0cac5a9-my-user-target in namespace namespace-6
2022-03-30 22:06:20 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-cluster-a0cac5a9-my-user-target
2022-03-30 22:06:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-cluster-a0cac5a9-my-user-target not ready, will try again in 10000 ms (179993ms till timeout)
2022-03-30 22:06:21 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:06:26 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:06:30 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1945138991-1836415682-test-2 in namespace namespace-6
2022-03-30 22:06:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1945138991-1836415682-test-2
2022-03-30 22:06:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1945138991-1836415682-test-2 not ready, will try again in 10000 ms (179994ms till timeout)
2022-03-30 22:06:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:06:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:06:40 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker my-cluster-a0cac5a9 in namespace namespace-6
2022-03-30 22:06:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaMirrorMaker:my-cluster-a0cac5a9
2022-03-30 22:06:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaMirrorMaker:my-cluster-a0cac5a9 not ready, will try again in 10000 ms (479992ms till timeout)
2022-03-30 22:06:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:06:46 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:06:50 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1945138991-1836415682-test-1 in namespace namespace-6
2022-03-30 22:06:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1945138991-1836415682-test-1
2022-03-30 22:06:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1945138991-1836415682-test-1 not ready, will try again in 10000 ms (179994ms till timeout)
2022-03-30 22:06:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:06:56 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:07:01 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1057826384-1887953069-source-703984749 in namespace namespace-6
2022-03-30 22:07:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1057826384-1887953069-source-703984749
2022-03-30 22:07:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1057826384-1887953069-source-703984749 not ready, will try again in 10000 ms (179993ms till timeout)
2022-03-30 22:07:01 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:07:06 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:07:11 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-a0cac5a9-my-user-source in namespace namespace-6
2022-03-30 22:07:11 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-cluster-a0cac5a9-my-user-source
2022-03-30 22:07:11 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-a0cac5a9-target in namespace namespace-6
2022-03-30 22:07:11 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-a0cac5a9-target
2022-03-30 22:07:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-a0cac5a9-target not ready, will try again in 10000 ms (839989ms till timeout)
2022-03-30 22:07:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:07:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:07:21 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-a0cac5a9-source in namespace namespace-6
2022-03-30 22:07:21 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-a0cac5a9-source
2022-03-30 22:07:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-a0cac5a9-source not ready, will try again in 10000 ms (839957ms till timeout)
2022-03-30 22:07:21 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:07:26 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:07:31 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:07:31 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-6 for test case:testMirrorMakerTlsAuthenticated
2022-03-30 22:07:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-6 removal
2022-03-30 22:07:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 22:07:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 22:07:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:07:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (479924ms till timeout)
2022-03-30 22:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:07:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 22:07:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 22:07:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:07:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (478852ms till timeout)
2022-03-30 22:07:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 22:07:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 22:07:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:07:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (477776ms till timeout)
2022-03-30 22:07:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 22:07:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 22:07:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:07:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (476702ms till timeout)
2022-03-30 22:07:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 22:07:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 22:07:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:07:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (475626ms till timeout)
2022-03-30 22:07:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:07:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 22:07:36 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 22:07:36 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:07:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (474554ms till timeout)
2022-03-30 22:07:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 22:07:37 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 22:07:37 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:07:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (473479ms till timeout)
2022-03-30 22:07:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 22:07:38 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 22:07:38 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:07:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (472407ms till timeout)
2022-03-30 22:07:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 22:07:39 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 22:07:39 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:07:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (471335ms till timeout)
2022-03-30 22:07:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 22:07:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 22:07:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:07:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (470260ms till timeout)
2022-03-30 22:07:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 22:07:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 22:07:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 22:07:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 22:07:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 22:07:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-6" not found
2022-03-30 22:07:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 22:07:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@6a42f58d=[]}
2022-03-30 22:07:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:267] testMirrorMakerTlsAuthenticated - Notifies waiting test cases:[testCruiseControlBasicAPIRequests, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testUpdateUser, testSendSimpleMessageTls, testAutoRenewAllCaCertsTriggeredByAnno, testReceiveSimpleMessageTls, testSendMessagesCustomListenerTlsScramSha, testKafkaAndZookeeperScaleUpScaleDown, testMirrorMakerTlsAuthenticated] to and randomly select one to start execution
2022-03-30 22:07:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:93] [mirrormaker.MirrorMakerIsolatedST] - Removing parallel test: testMirrorMakerTlsAuthenticated
2022-03-30 22:07:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:97] [mirrormaker.MirrorMakerIsolatedST] - Parallel test count: 0
2022-03-30 22:07:41 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST.testMirrorMakerTlsAuthenticated-FINISHED
2022-03-30 22:07:41 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 22:07:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 22:07:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:690] [mirrormaker.MirrorMakerIsolatedST - After All] - Clean up after test suite
2022-03-30 22:07:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:07:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:346] In context MirrorMakerIsolatedST is everything deleted.
2022-03-30 22:07:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,842.585 s - in io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST
2022-03-30 22:07:46 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:136] Suite watcher.AllNamespaceIsolatedST has locked the @IsolatedSuite and other @IsolatedSuites must wait until lock is released.
2022-03-30 22:07:46 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:07:46 [ForkJoinPool-3-worker-1] [32mINFO [m [AllNamespaceIsolatedST:190] Creating resources before the test class
2022-03-30 22:07:46 [ForkJoinPool-3-worker-1] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-30 22:07:46 [ForkJoinPool-3-worker-1] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-30 22:07:46 [ForkJoinPool-3-worker-1] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-30 22:07:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:07:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-30 22:07:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 22:07:46 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 22:07:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 22:07:46 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 22:07:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:07:46 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 22:07:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 22:07:46 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 22:07:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 22:07:46 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 22:07:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179948ms till timeout)
2022-03-30 22:07:51 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:07:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:07:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 22:07:56 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 22:07:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 22:07:56 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 22:07:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 22:07:56 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 22:07:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:07:56 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 22:07:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 22:07:56 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 22:07:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 22:07:56 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 22:07:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:07:56 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 22:07:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 22:07:56 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 22:07:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 22:07:56 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 22:07:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179925ms till timeout)
2022-03-30 22:08:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:08:06 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:08:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 22:08:06 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 22:08:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 22:08:06 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 22:08:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179817ms till timeout)
2022-03-30 22:08:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:08:16 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:08:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:08:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 22:08:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:08:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 22:08:17 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 22:08:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 22:08:17 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 22:08:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 22:08:17 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 22:08:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 22:08:17 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 22:08:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 22:08:17 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 22:08:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 22:08:17 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 22:08:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 22:08:17 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:08:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 22:08:17 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:08:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-30 22:08:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 22:08:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 22:08:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v105921
2022-03-30 22:08:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v105921
2022-03-30 22:08:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=105921&allowWatchBookmarks=true&watch=true...
2022-03-30 22:08:17 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 22:08:17 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 105922
2022-03-30 22:08:21 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:08:22 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 105943
2022-03-30 22:08:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:08:27 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 105951
2022-03-30 22:08:27 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v105943 in namespace default
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@26fcd291
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace, second-namespace-test, third-namespace-test]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-30 22:08:27 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@6b0ab47
2022-03-30 22:08:27 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@3f2e6467, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@26fcd291, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace, second-namespace-test, third-namespace-test], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-30 22:08:27 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@7006b6ae
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-30 22:08:27 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@7006b6ae
2022-03-30 22:08:27 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@7006b6ae
2022-03-30 22:08:27 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 22:08:27 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace infra-namespace -o json
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace infra-namespace -o json
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T22:08:27Z",
        "name": "infra-namespace",
        "resourceVersion": "105952",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "ed64e4e3-12d5-4a48-8b5b-23fe39e09630"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: second-namespace-test
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Namespace second-namespace-test
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace second-namespace-test -o json
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace second-namespace-test -o json
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T22:08:27Z",
        "name": "second-namespace-test",
        "resourceVersion": "105956",
        "selfLink": "/api/v1/namespaces/second-namespace-test",
        "uid": "2af5c478-0f87-4a53-ba24-5d3331935687"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace, second-namespace-test]}
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: third-namespace-test
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Namespace third-namespace-test
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace third-namespace-test -o json
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace third-namespace-test -o json
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T22:08:27Z",
        "name": "third-namespace-test",
        "resourceVersion": "105960",
        "selfLink": "/api/v1/namespaces/third-namespace-test",
        "uid": "cce07875-0770-45e2-b1f9-2afb927e4d20"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace, second-namespace-test, third-namespace-test]}
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:08:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=second-namespace-test, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: second-namespace-test
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=third-namespace-test, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: third-namespace-test
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace second-namespace-test
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace second-namespace-test
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace third-namespace-test
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace third-namespace-test
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 22:08:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479995ms till timeout)
2022-03-30 22:08:29 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478993ms till timeout)
2022-03-30 22:08:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477989ms till timeout)
2022-03-30 22:08:31 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:08:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476986ms till timeout)
2022-03-30 22:08:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475983ms till timeout)
2022-03-30 22:08:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474979ms till timeout)
2022-03-30 22:08:34 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473977ms till timeout)
2022-03-30 22:08:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472973ms till timeout)
2022-03-30 22:08:36 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:08:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471970ms till timeout)
2022-03-30 22:08:37 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470967ms till timeout)
2022-03-30 22:08:38 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469964ms till timeout)
2022-03-30 22:08:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468961ms till timeout)
2022-03-30 22:08:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467958ms till timeout)
2022-03-30 22:08:41 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:08:41 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-30 22:08:41 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-30 22:08:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-30 22:08:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-l2q7c not ready: strimzi-cluster-operator)
2022-03-30 22:08:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-l2q7c are ready
2022-03-30 22:08:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-30 22:08:42 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-l2q7c not ready: strimzi-cluster-operator)
2022-03-30 22:08:42 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-l2q7c are ready
2022-03-30 22:08:42 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598993ms till timeout)
2022-03-30 22:08:43 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-l2q7c not ready: strimzi-cluster-operator)
2022-03-30 22:08:43 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-l2q7c are ready
2022-03-30 22:08:43 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597989ms till timeout)
2022-03-30 22:08:44 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-l2q7c not ready: strimzi-cluster-operator)
2022-03-30 22:08:44 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-l2q7c are ready
2022-03-30 22:08:44 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596985ms till timeout)
2022-03-30 22:08:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-l2q7c not ready: strimzi-cluster-operator)
2022-03-30 22:08:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-l2q7c are ready
2022-03-30 22:08:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595982ms till timeout)
2022-03-30 22:08:46 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:08:46 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-l2q7c not ready: strimzi-cluster-operator)
2022-03-30 22:08:46 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-l2q7c are ready
2022-03-30 22:08:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594978ms till timeout)
2022-03-30 22:08:47 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-l2q7c not ready: strimzi-cluster-operator)
2022-03-30 22:08:47 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-l2q7c are ready
2022-03-30 22:08:47 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593974ms till timeout)
2022-03-30 22:08:48 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-l2q7c not ready: strimzi-cluster-operator)
2022-03-30 22:08:48 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-l2q7c are ready
2022-03-30 22:08:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592971ms till timeout)
2022-03-30 22:08:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-l2q7c not ready: strimzi-cluster-operator)
2022-03-30 22:08:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-l2q7c are ready
2022-03-30 22:08:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591967ms till timeout)
2022-03-30 22:08:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-l2q7c not ready: strimzi-cluster-operator)
2022-03-30 22:08:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-l2q7c are ready
2022-03-30 22:08:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590964ms till timeout)
2022-03-30 22:08:51 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:08:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-l2q7c not ready: strimzi-cluster-operator)
2022-03-30 22:08:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-l2q7c are ready
2022-03-30 22:08:51 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-30 22:08:51 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: third-namespace-test
2022-03-30 22:08:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster in namespace third-namespace-test
2022-03-30 22:08:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster
2022-03-30 22:08:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster will have desired state: Ready
2022-03-30 22:08:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster will have desired state: Ready
2022-03-30 22:08:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (839998ms till timeout)
2022-03-30 22:08:52 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (838995ms till timeout)
2022-03-30 22:08:53 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (837991ms till timeout)
2022-03-30 22:08:54 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (836988ms till timeout)
2022-03-30 22:08:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (835985ms till timeout)
2022-03-30 22:08:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:08:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (834981ms till timeout)
2022-03-30 22:08:57 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (833978ms till timeout)
2022-03-30 22:08:58 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (832975ms till timeout)
2022-03-30 22:08:59 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (831972ms till timeout)
2022-03-30 22:09:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (830968ms till timeout)
2022-03-30 22:09:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:09:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (829965ms till timeout)
2022-03-30 22:09:02 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (828961ms till timeout)
2022-03-30 22:09:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (827957ms till timeout)
2022-03-30 22:09:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (826954ms till timeout)
2022-03-30 22:09:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (825951ms till timeout)
2022-03-30 22:09:06 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:09:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (824948ms till timeout)
2022-03-30 22:09:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (823944ms till timeout)
2022-03-30 22:09:08 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (822941ms till timeout)
2022-03-30 22:09:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (821938ms till timeout)
2022-03-30 22:09:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (820935ms till timeout)
2022-03-30 22:09:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:09:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (819931ms till timeout)
2022-03-30 22:09:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (818928ms till timeout)
2022-03-30 22:09:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (817925ms till timeout)
2022-03-30 22:09:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (816922ms till timeout)
2022-03-30 22:09:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (815918ms till timeout)
2022-03-30 22:09:16 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:09:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (814915ms till timeout)
2022-03-30 22:09:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (813912ms till timeout)
2022-03-30 22:09:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (812909ms till timeout)
2022-03-30 22:09:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (811906ms till timeout)
2022-03-30 22:09:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (810903ms till timeout)
2022-03-30 22:09:21 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:09:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (809900ms till timeout)
2022-03-30 22:09:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (808897ms till timeout)
2022-03-30 22:09:23 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (807894ms till timeout)
2022-03-30 22:09:24 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (806890ms till timeout)
2022-03-30 22:09:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (805887ms till timeout)
2022-03-30 22:09:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:09:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (804884ms till timeout)
2022-03-30 22:09:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (803881ms till timeout)
2022-03-30 22:09:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (802877ms till timeout)
2022-03-30 22:09:29 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (801874ms till timeout)
2022-03-30 22:09:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (800870ms till timeout)
2022-03-30 22:09:31 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:09:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (799867ms till timeout)
2022-03-30 22:09:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (798864ms till timeout)
2022-03-30 22:09:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (797861ms till timeout)
2022-03-30 22:09:34 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (796858ms till timeout)
2022-03-30 22:09:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (795855ms till timeout)
2022-03-30 22:09:36 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:09:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (794852ms till timeout)
2022-03-30 22:09:37 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (793849ms till timeout)
2022-03-30 22:09:38 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (792846ms till timeout)
2022-03-30 22:09:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (791842ms till timeout)
2022-03-30 22:09:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (790839ms till timeout)
2022-03-30 22:09:41 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:09:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (789836ms till timeout)
2022-03-30 22:09:42 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (788832ms till timeout)
2022-03-30 22:09:43 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (787829ms till timeout)
2022-03-30 22:09:44 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (786826ms till timeout)
2022-03-30 22:09:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (785823ms till timeout)
2022-03-30 22:09:46 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:09:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (784820ms till timeout)
2022-03-30 22:09:47 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (783817ms till timeout)
2022-03-30 22:09:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (782813ms till timeout)
2022-03-30 22:09:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (781810ms till timeout)
2022-03-30 22:09:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (780807ms till timeout)
2022-03-30 22:09:51 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:09:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (779804ms till timeout)
2022-03-30 22:09:52 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (778800ms till timeout)
2022-03-30 22:09:53 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (777797ms till timeout)
2022-03-30 22:09:54 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (776794ms till timeout)
2022-03-30 22:09:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (775791ms till timeout)
2022-03-30 22:09:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:09:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (774788ms till timeout)
2022-03-30 22:09:57 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (773785ms till timeout)
2022-03-30 22:09:58 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (772782ms till timeout)
2022-03-30 22:09:59 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (771779ms till timeout)
2022-03-30 22:10:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (770775ms till timeout)
2022-03-30 22:10:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:10:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (769772ms till timeout)
2022-03-30 22:10:02 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (768769ms till timeout)
2022-03-30 22:10:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (767765ms till timeout)
2022-03-30 22:10:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (766763ms till timeout)
2022-03-30 22:10:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster is in desired state: Ready
2022-03-30 22:10:05 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-namespace-test
2022-03-30 22:10:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-second in namespace second-namespace-test
2022-03-30 22:10:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-second
2022-03-30 22:10:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-second will have desired state: Ready
2022-03-30 22:10:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-second will have desired state: Ready
2022-03-30 22:10:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (839998ms till timeout)
2022-03-30 22:10:06 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:10:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (838995ms till timeout)
2022-03-30 22:10:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (837992ms till timeout)
2022-03-30 22:10:08 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (836983ms till timeout)
2022-03-30 22:10:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (835980ms till timeout)
2022-03-30 22:10:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (834976ms till timeout)
2022-03-30 22:10:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:10:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (833973ms till timeout)
2022-03-30 22:10:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (832970ms till timeout)
2022-03-30 22:10:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (831964ms till timeout)
2022-03-30 22:10:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (830961ms till timeout)
2022-03-30 22:10:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (829958ms till timeout)
2022-03-30 22:10:16 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:10:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (828953ms till timeout)
2022-03-30 22:10:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (827950ms till timeout)
2022-03-30 22:10:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (826947ms till timeout)
2022-03-30 22:10:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (825944ms till timeout)
2022-03-30 22:10:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (824940ms till timeout)
2022-03-30 22:10:21 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:10:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (823937ms till timeout)
2022-03-30 22:10:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (822934ms till timeout)
2022-03-30 22:10:23 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (821931ms till timeout)
2022-03-30 22:10:24 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (820928ms till timeout)
2022-03-30 22:10:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (819925ms till timeout)
2022-03-30 22:10:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:10:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (818922ms till timeout)
2022-03-30 22:10:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (817918ms till timeout)
2022-03-30 22:10:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (816915ms till timeout)
2022-03-30 22:10:29 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (815911ms till timeout)
2022-03-30 22:10:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (814908ms till timeout)
2022-03-30 22:10:31 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:10:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (813905ms till timeout)
2022-03-30 22:10:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (812902ms till timeout)
2022-03-30 22:10:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (811898ms till timeout)
2022-03-30 22:10:34 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (810888ms till timeout)
2022-03-30 22:10:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (809885ms till timeout)
2022-03-30 22:10:36 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:10:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (808880ms till timeout)
2022-03-30 22:10:37 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (807874ms till timeout)
2022-03-30 22:10:38 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (806870ms till timeout)
2022-03-30 22:10:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (805864ms till timeout)
2022-03-30 22:10:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (804853ms till timeout)
2022-03-30 22:10:41 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:10:42 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (803850ms till timeout)
2022-03-30 22:10:43 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (802846ms till timeout)
2022-03-30 22:10:44 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (801843ms till timeout)
2022-03-30 22:10:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (800840ms till timeout)
2022-03-30 22:10:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (799837ms till timeout)
2022-03-30 22:10:46 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:10:47 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (798834ms till timeout)
2022-03-30 22:10:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (797830ms till timeout)
2022-03-30 22:10:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (796827ms till timeout)
2022-03-30 22:10:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (795824ms till timeout)
2022-03-30 22:10:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (794821ms till timeout)
2022-03-30 22:10:51 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:10:52 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (793817ms till timeout)
2022-03-30 22:10:53 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (792814ms till timeout)
2022-03-30 22:10:54 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (791810ms till timeout)
2022-03-30 22:10:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (790807ms till timeout)
2022-03-30 22:10:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (789804ms till timeout)
2022-03-30 22:10:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:10:57 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (788801ms till timeout)
2022-03-30 22:10:58 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (787798ms till timeout)
2022-03-30 22:10:59 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (786794ms till timeout)
2022-03-30 22:11:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (785791ms till timeout)
2022-03-30 22:11:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (784788ms till timeout)
2022-03-30 22:11:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:11:02 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (783784ms till timeout)
2022-03-30 22:11:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (782781ms till timeout)
2022-03-30 22:11:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (781778ms till timeout)
2022-03-30 22:11:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (780774ms till timeout)
2022-03-30 22:11:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (779771ms till timeout)
2022-03-30 22:11:06 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:11:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (778768ms till timeout)
2022-03-30 22:11:08 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (777764ms till timeout)
2022-03-30 22:11:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (776761ms till timeout)
2022-03-30 22:11:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (775758ms till timeout)
2022-03-30 22:11:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (774754ms till timeout)
2022-03-30 22:11:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:11:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (773751ms till timeout)
2022-03-30 22:11:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (772748ms till timeout)
2022-03-30 22:11:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (771745ms till timeout)
2022-03-30 22:11:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (770741ms till timeout)
2022-03-30 22:11:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (769738ms till timeout)
2022-03-30 22:11:16 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:11:17 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-second is in desired state: Ready
2022-03-30 22:11:17 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 22:11:17 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 22:11:17 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.watcher.AllNamespaceIsolatedST.testKafkaInDifferentNsThanClusterOperator-STARTED
2022-03-30 22:11:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 22:11:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:659] [watcher.AllNamespaceIsolatedST - Before Each] - Setup test case environment
2022-03-30 22:11:17 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 22:11:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349, testSendSimpleMessageTls=my-cluster-b70e5342, testSendMessagesTlsScramSha=my-cluster-380e6586, testUpdateUser=my-cluster-9b2bd187, testReceiveSimpleMessageTls=my-cluster-e8a518cd, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9}
2022-03-30 22:11:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1001559761-691587912, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1590558165-46977776, testSendSimpleMessageTls=my-user-602433797-900694548, testSendMessagesTlsScramSha=my-user-2039401547-533835955, testUpdateUser=my-user-595127408-1604948726, testReceiveSimpleMessageTls=my-user-761922164-2019863647, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1606351157-1815623087, testKafkaInDifferentNsThanClusterOperator=my-user-1518021213-326071957, testSendMessagesCustomListenerTlsScramSha=my-user-445988529-51741427, testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659, testMirrorMakerTlsAuthenticated=my-user-692603309-1711649825}
2022-03-30 22:11:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-741363275-285355018, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1210910585-1669754462, testSendSimpleMessageTls=my-topic-1299629356-597168683, testSendMessagesTlsScramSha=my-topic-988094002-1400465950, testUpdateUser=my-topic-1875736194-1340884809, testReceiveSimpleMessageTls=my-topic-1007790874-1032272408, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1931531910-629202741, testKafkaInDifferentNsThanClusterOperator=my-topic-2034182350-493335345, testSendMessagesCustomListenerTlsScramSha=my-topic-1039904883-1044305687, testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393, testMirrorMakerTlsAuthenticated=my-topic-1945138991-1836415682}
2022-03-30 22:11:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349-kafka-clients, testSendSimpleMessageTls=my-cluster-b70e5342-kafka-clients, testSendMessagesTlsScramSha=my-cluster-380e6586-kafka-clients, testUpdateUser=my-cluster-9b2bd187-kafka-clients, testReceiveSimpleMessageTls=my-cluster-e8a518cd-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9-kafka-clients}
2022-03-30 22:11:17 [ForkJoinPool-3-worker-1] [32mINFO [m [AllNamespaceIsolatedST:82] Deploying Kafka cluster in different namespace than CO when CO watches all namespaces
2022-03-30 22:11:17 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-namespace-test
2022-03-30 22:11:17 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractNamespaceST:46] Check if Kafka Cluster my-cluster-second in namespace second-namespace-test
2022-03-30 22:11:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Kafka Cluster status is not in desired state: Ready
2022-03-30 22:11:17 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractNamespaceST:51] Kafka condition status: True
2022-03-30 22:11:17 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractNamespaceST:52] Kafka condition type: Ready
2022-03-30 22:11:17 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 22:11:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 22:11:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:675] [watcher.AllNamespaceIsolatedST - After Each] - Clean up after test
2022-03-30 22:11:17 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:11:17 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:346] In context testKafkaInDifferentNsThanClusterOperator is everything deleted.
2022-03-30 22:11:17 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:11:17 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.watcher.AllNamespaceIsolatedST.testKafkaInDifferentNsThanClusterOperator-FINISHED
2022-03-30 22:11:17 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 22:11:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 22:11:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:690] [watcher.AllNamespaceIsolatedST - After All] - Clean up after test suite
2022-03-30 22:11:17 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:11:17 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for AllNamespaceIsolatedST
2022-03-30 22:11:17 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-second in namespace second-namespace-test
2022-03-30 22:11:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-second
2022-03-30 22:11:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-second not ready, will try again in 10000 ms (839998ms till timeout)
2022-03-30 22:11:21 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:11:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:11:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster in namespace third-namespace-test
2022-03-30 22:11:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster
2022-03-30 22:11:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster not ready, will try again in 10000 ms (839954ms till timeout)
2022-03-30 22:11:31 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:11:36 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 22:11:37 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2,121.106 s - in io.strimzi.systemtest.watcher.AllNamespaceIsolatedST
2022-03-30 22:11:41 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:136] Suite mirrormaker.MirrorMaker2IsolatedST has locked the @IsolatedSuite and other @IsolatedSuites must wait until lock is released.
2022-03-30 22:11:41 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:11:41 [ForkJoinPool-3-worker-13] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-30 22:11:41 [ForkJoinPool-3-worker-13] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-30 22:11:41 [ForkJoinPool-3-worker-13] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-30 22:11:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:11:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-30 22:11:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 22:11:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:11:41 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 22:11:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 22:11:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 22:11:41 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 22:11:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io not ready, will try again in 10000 ms (179983ms till timeout)
2022-03-30 22:11:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 22:11:41 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 22:11:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace second-namespace-test
2022-03-30 22:11:41 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 22:11:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 22:11:41 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 22:11:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 22:11:41 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 22:11:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:11:41 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 22:11:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 22:11:41 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 22:11:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 22:11:41 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 22:11:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace third-namespace-test
2022-03-30 22:11:41 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 22:11:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace third-namespace-test
2022-03-30 22:11:41 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 22:11:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 22:11:41 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 22:11:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:11:41 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 22:11:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator not ready, will try again in 10000 ms (479952ms till timeout)
2022-03-30 22:11:46 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:11:51 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:11:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:11:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 22:11:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 22:11:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 22:11:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 22:11:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 22:11:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io not ready, will try again in 10000 ms (179992ms till timeout)
2022-03-30 22:11:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 22:11:51 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 22:11:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 22:11:51 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 22:11:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 22:11:51 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 22:11:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 22:11:51 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 22:11:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace second-namespace-test
2022-03-30 22:11:51 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 22:11:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 22:11:51 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 22:11:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 22:11:51 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 22:11:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179949ms till timeout)
2022-03-30 22:11:56 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:12:01 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:12:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 22:12:01 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 22:12:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 22:12:01 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 22:12:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 22:12:01 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 22:12:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:12:01 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 22:12:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 22:12:01 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 22:12:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179949ms till timeout)
2022-03-30 22:12:01 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 22:12:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 22:12:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179815ms till timeout)
2022-03-30 22:12:06 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:12:11 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:12:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 22:12:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 22:12:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 22:12:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 22:12:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 22:12:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 22:12:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:12:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 22:12:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 22:12:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 22:12:11 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:12:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: second-namespace-test
2022-03-30 22:12:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-30 22:12:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 22:12:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 22:12:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 22:12:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 22:12:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v106820
2022-03-30 22:12:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v106820
2022-03-30 22:12:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dsecond-namespace-test&resourceVersion=106820&allowWatchBookmarks=true&watch=true...
2022-03-30 22:12:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v106820
2022-03-30 22:12:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v106820
2022-03-30 22:12:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=106820&allowWatchBookmarks=true&watch=true...
2022-03-30 22:12:11 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 22:12:11 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 22:12:11 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 106821
2022-03-30 22:12:11 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 106822
2022-03-30 22:12:16 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:12:16 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 106876
2022-03-30 22:12:16 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 106880
2022-03-30 22:12:16 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v106876 in namespace default
2022-03-30 22:12:16 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@4d038d9a
2022-03-30 22:12:16 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 22:12:16 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@493c00e2
2022-03-30 22:12:16 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@493c00e2
2022-03-30 22:12:16 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@493c00e2
2022-03-30 22:12:16 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 22:12:16 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 22:12:17 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 106903
2022-03-30 22:12:17 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 106904
2022-03-30 22:12:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: third-namespace-test
2022-03-30 22:12:17 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v106903 in namespace default
2022-03-30 22:12:17 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@2eb2093d
2022-03-30 22:12:17 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 22:12:17 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@2a8d8833
2022-03-30 22:12:17 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@2a8d8833
2022-03-30 22:12:17 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@2a8d8833
2022-03-30 22:12:17 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 22:12:17 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 22:12:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 22:12:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 22:12:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v106905
2022-03-30 22:12:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v106905
2022-03-30 22:12:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dthird-namespace-test&resourceVersion=106905&allowWatchBookmarks=true&watch=true...
2022-03-30 22:12:17 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 22:12:17 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 106906
2022-03-30 22:12:21 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:12:22 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 106942
2022-03-30 22:12:22 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 106943
2022-03-30 22:12:22 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v106942 in namespace default
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@26fcd291
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=30000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-30 22:12:22 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@60ef9a4f
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@3f2e6467, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@26fcd291, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=30000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-30 22:12:22 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 22:12:22 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@5eaa8c8e
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-30 22:12:22 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@5eaa8c8e
2022-03-30 22:12:22 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@5eaa8c8e
2022-03-30 22:12:22 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 22:12:22 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T22:12:22Z",
        "name": "infra-namespace",
        "resourceVersion": "106944",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "eb797f26-0c1d-438d-9b32-908487829d87"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 22:12:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 22:12:23 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:12:23 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 22:12:23 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 22:12:23 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 22:12:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479997ms till timeout)
2022-03-30 22:12:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478990ms till timeout)
2022-03-30 22:12:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477987ms till timeout)
2022-03-30 22:12:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476984ms till timeout)
2022-03-30 22:12:26 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:12:27 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475981ms till timeout)
2022-03-30 22:12:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474977ms till timeout)
2022-03-30 22:12:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473975ms till timeout)
2022-03-30 22:12:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472971ms till timeout)
2022-03-30 22:12:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471968ms till timeout)
2022-03-30 22:12:31 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:12:32 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470965ms till timeout)
2022-03-30 22:12:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469962ms till timeout)
2022-03-30 22:12:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468959ms till timeout)
2022-03-30 22:12:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467956ms till timeout)
2022-03-30 22:12:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466953ms till timeout)
2022-03-30 22:12:36 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:12:37 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465950ms till timeout)
2022-03-30 22:12:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464947ms till timeout)
2022-03-30 22:12:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463944ms till timeout)
2022-03-30 22:12:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462941ms till timeout)
2022-03-30 22:12:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461937ms till timeout)
2022-03-30 22:12:41 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:12:42 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (460935ms till timeout)
2022-03-30 22:12:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (459931ms till timeout)
2022-03-30 22:12:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (458928ms till timeout)
2022-03-30 22:12:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (457925ms till timeout)
2022-03-30 22:12:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (456922ms till timeout)
2022-03-30 22:12:46 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:12:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (455919ms till timeout)
2022-03-30 22:12:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (454916ms till timeout)
2022-03-30 22:12:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (453913ms till timeout)
2022-03-30 22:12:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (452910ms till timeout)
2022-03-30 22:12:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (451907ms till timeout)
2022-03-30 22:12:51 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:12:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (450904ms till timeout)
2022-03-30 22:12:53 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-30 22:12:53 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-30 22:12:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-30 22:12:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-7v7np not ready: strimzi-cluster-operator)
2022-03-30 22:12:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-7v7np are ready
2022-03-30 22:12:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-30 22:12:54 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-7v7np not ready: strimzi-cluster-operator)
2022-03-30 22:12:54 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-7v7np are ready
2022-03-30 22:12:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-30 22:12:55 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-7v7np not ready: strimzi-cluster-operator)
2022-03-30 22:12:55 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-7v7np are ready
2022-03-30 22:12:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597990ms till timeout)
2022-03-30 22:12:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-7v7np not ready: strimzi-cluster-operator)
2022-03-30 22:12:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-7v7np are ready
2022-03-30 22:12:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596987ms till timeout)
2022-03-30 22:12:56 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:12:57 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-7v7np not ready: strimzi-cluster-operator)
2022-03-30 22:12:57 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-7v7np are ready
2022-03-30 22:12:57 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595983ms till timeout)
2022-03-30 22:12:58 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-7v7np not ready: strimzi-cluster-operator)
2022-03-30 22:12:58 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-7v7np are ready
2022-03-30 22:12:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594979ms till timeout)
2022-03-30 22:12:59 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-7v7np not ready: strimzi-cluster-operator)
2022-03-30 22:12:59 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-7v7np are ready
2022-03-30 22:12:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593976ms till timeout)
2022-03-30 22:13:00 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-7v7np not ready: strimzi-cluster-operator)
2022-03-30 22:13:00 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-7v7np are ready
2022-03-30 22:13:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592972ms till timeout)
2022-03-30 22:13:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-7v7np not ready: strimzi-cluster-operator)
2022-03-30 22:13:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-7v7np are ready
2022-03-30 22:13:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591968ms till timeout)
2022-03-30 22:13:01 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:13:02 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-7v7np not ready: strimzi-cluster-operator)
2022-03-30 22:13:02 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-7v7np are ready
2022-03-30 22:13:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590965ms till timeout)
2022-03-30 22:13:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-7v7np not ready: strimzi-cluster-operator)
2022-03-30 22:13:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-7v7np are ready
2022-03-30 22:13:03 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-30 22:13:03 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 22:13:03 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testMirrorMaker2TlsAndTlsClientAuth-STARTED
2022-03-30 22:13:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 22:13:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:659] [mirrormaker.MirrorMaker2IsolatedST - Before Each] - Setup test case environment
2022-03-30 22:13:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:77] [mirrormaker.MirrorMaker2IsolatedST] - Adding parallel test: testMirrorMaker2TlsAndTlsClientAuth
2022-03-30 22:13:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:81] [mirrormaker.MirrorMaker2IsolatedST] - Parallel test count: 1
2022-03-30 22:13:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:230] testMirrorMaker2TlsAndTlsClientAuth test now can proceed its execution
2022-03-30 22:13:03 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 22:13:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070, testSendSimpleMessageTls=my-cluster-b70e5342, testUpdateUser=my-cluster-9b2bd187, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349, testSendMessagesTlsScramSha=my-cluster-380e6586, testReceiveSimpleMessageTls=my-cluster-e8a518cd, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9}
2022-03-30 22:13:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1001559761-691587912, testSendSimpleMessageTls=my-user-602433797-900694548, testUpdateUser=my-user-595127408-1604948726, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1606351157-1815623087, testKafkaInDifferentNsThanClusterOperator=my-user-1518021213-326071957, testSendMessagesCustomListenerTlsScramSha=my-user-445988529-51741427, testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1590558165-46977776, testSendMessagesTlsScramSha=my-user-2039401547-533835955, testReceiveSimpleMessageTls=my-user-761922164-2019863647, testMirrorMaker2TlsAndTlsClientAuth=my-user-707354298-1518057957, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659, testMirrorMakerTlsAuthenticated=my-user-692603309-1711649825}
2022-03-30 22:13:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-741363275-285355018, testSendSimpleMessageTls=my-topic-1299629356-597168683, testUpdateUser=my-topic-1875736194-1340884809, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1931531910-629202741, testKafkaInDifferentNsThanClusterOperator=my-topic-2034182350-493335345, testSendMessagesCustomListenerTlsScramSha=my-topic-1039904883-1044305687, testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1210910585-1669754462, testSendMessagesTlsScramSha=my-topic-988094002-1400465950, testReceiveSimpleMessageTls=my-topic-1007790874-1032272408, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1279769671-1440257906, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393, testMirrorMakerTlsAuthenticated=my-topic-1945138991-1836415682}
2022-03-30 22:13:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070-kafka-clients, testSendSimpleMessageTls=my-cluster-b70e5342-kafka-clients, testUpdateUser=my-cluster-9b2bd187-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349-kafka-clients, testSendMessagesTlsScramSha=my-cluster-380e6586-kafka-clients, testReceiveSimpleMessageTls=my-cluster-e8a518cd-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9-kafka-clients}
2022-03-30 22:13:03 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-7 for test case:testMirrorMaker2TlsAndTlsClientAuth
2022-03-30 22:13:03 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-7
2022-03-30 22:13:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-7
2022-03-30 22:13:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-7 -o json
2022-03-30 22:13:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-7 -o json
2022-03-30 22:13:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:13:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T22:13:03Z",
        "name": "namespace-7",
        "resourceVersion": "107048",
        "selfLink": "/api/v1/namespaces/namespace-7",
        "uid": "536ed44e-675e-4eea-873d-df25990b76d6"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 22:13:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@529573c9=[namespace-7]}
2022-03-30 22:13:03 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-7
2022-03-30 22:13:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-7, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 22:13:03 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-7
2022-03-30 22:13:03 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-6e67dcda-source in namespace namespace-7
2022-03-30 22:13:03 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-30 22:13:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-6e67dcda-source
2022-03-30 22:13:03 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-6e67dcda-source will have desired state: Ready
2022-03-30 22:13:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-6e67dcda-source will have desired state: Ready
2022-03-30 22:13:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (839998ms till timeout)
2022-03-30 22:13:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (838995ms till timeout)
2022-03-30 22:13:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (837992ms till timeout)
2022-03-30 22:13:06 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:13:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (836988ms till timeout)
2022-03-30 22:13:07 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (835985ms till timeout)
2022-03-30 22:13:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (834982ms till timeout)
2022-03-30 22:13:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (833979ms till timeout)
2022-03-30 22:13:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (832976ms till timeout)
2022-03-30 22:13:11 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:13:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (831973ms till timeout)
2022-03-30 22:13:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (830969ms till timeout)
2022-03-30 22:13:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (829966ms till timeout)
2022-03-30 22:13:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (828963ms till timeout)
2022-03-30 22:13:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (827959ms till timeout)
2022-03-30 22:13:16 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:13:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (826956ms till timeout)
2022-03-30 22:13:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (825953ms till timeout)
2022-03-30 22:13:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (824950ms till timeout)
2022-03-30 22:13:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (823947ms till timeout)
2022-03-30 22:13:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (822944ms till timeout)
2022-03-30 22:13:21 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:13:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (821941ms till timeout)
2022-03-30 22:13:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (820938ms till timeout)
2022-03-30 22:13:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (819934ms till timeout)
2022-03-30 22:13:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (818931ms till timeout)
2022-03-30 22:13:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (817928ms till timeout)
2022-03-30 22:13:26 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:13:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (816925ms till timeout)
2022-03-30 22:13:27 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (815922ms till timeout)
2022-03-30 22:13:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (814918ms till timeout)
2022-03-30 22:13:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (813915ms till timeout)
2022-03-30 22:13:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (812912ms till timeout)
2022-03-30 22:13:31 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:13:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (811909ms till timeout)
2022-03-30 22:13:32 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (810906ms till timeout)
2022-03-30 22:13:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (809903ms till timeout)
2022-03-30 22:13:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (808900ms till timeout)
2022-03-30 22:13:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (807897ms till timeout)
2022-03-30 22:13:36 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:13:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (806893ms till timeout)
2022-03-30 22:13:37 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (805890ms till timeout)
2022-03-30 22:13:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (804887ms till timeout)
2022-03-30 22:13:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (803884ms till timeout)
2022-03-30 22:13:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (802880ms till timeout)
2022-03-30 22:13:41 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:13:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (801877ms till timeout)
2022-03-30 22:13:42 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (800874ms till timeout)
2022-03-30 22:13:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (799871ms till timeout)
2022-03-30 22:13:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (798868ms till timeout)
2022-03-30 22:13:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (797865ms till timeout)
2022-03-30 22:13:46 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:13:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (796862ms till timeout)
2022-03-30 22:13:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (795859ms till timeout)
2022-03-30 22:13:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (794856ms till timeout)
2022-03-30 22:13:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (793853ms till timeout)
2022-03-30 22:13:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (792850ms till timeout)
2022-03-30 22:13:51 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:13:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (791847ms till timeout)
2022-03-30 22:13:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (790844ms till timeout)
2022-03-30 22:13:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (789841ms till timeout)
2022-03-30 22:13:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (788838ms till timeout)
2022-03-30 22:13:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (787835ms till timeout)
2022-03-30 22:13:56 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:13:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (786832ms till timeout)
2022-03-30 22:13:57 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (785829ms till timeout)
2022-03-30 22:13:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (784826ms till timeout)
2022-03-30 22:13:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (783823ms till timeout)
2022-03-30 22:14:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (782819ms till timeout)
2022-03-30 22:14:01 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:14:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (781815ms till timeout)
2022-03-30 22:14:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (780812ms till timeout)
2022-03-30 22:14:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (779809ms till timeout)
2022-03-30 22:14:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (778805ms till timeout)
2022-03-30 22:14:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (777802ms till timeout)
2022-03-30 22:14:06 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:14:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (776799ms till timeout)
2022-03-30 22:14:07 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (775795ms till timeout)
2022-03-30 22:14:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (774792ms till timeout)
2022-03-30 22:14:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (773788ms till timeout)
2022-03-30 22:14:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (772785ms till timeout)
2022-03-30 22:14:11 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:14:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (771782ms till timeout)
2022-03-30 22:14:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (770778ms till timeout)
2022-03-30 22:14:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (769775ms till timeout)
2022-03-30 22:14:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (768772ms till timeout)
2022-03-30 22:14:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (767769ms till timeout)
2022-03-30 22:14:16 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:14:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-source will have desired state: Ready not ready, will try again in 1000 ms (766766ms till timeout)
2022-03-30 22:14:17 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-6e67dcda-source is in desired state: Ready
2022-03-30 22:14:17 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-6e67dcda-target in namespace namespace-7
2022-03-30 22:14:17 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-30 22:14:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-6e67dcda-target
2022-03-30 22:14:17 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-6e67dcda-target will have desired state: Ready
2022-03-30 22:14:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-6e67dcda-target will have desired state: Ready
2022-03-30 22:14:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (839998ms till timeout)
2022-03-30 22:14:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (838995ms till timeout)
2022-03-30 22:14:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (837992ms till timeout)
2022-03-30 22:14:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (836989ms till timeout)
2022-03-30 22:14:21 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:14:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (835986ms till timeout)
2022-03-30 22:14:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (834983ms till timeout)
2022-03-30 22:14:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (833981ms till timeout)
2022-03-30 22:14:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (832977ms till timeout)
2022-03-30 22:14:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (831974ms till timeout)
2022-03-30 22:14:26 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:14:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (830970ms till timeout)
2022-03-30 22:14:27 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (829967ms till timeout)
2022-03-30 22:14:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (828964ms till timeout)
2022-03-30 22:14:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (827961ms till timeout)
2022-03-30 22:14:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (826958ms till timeout)
2022-03-30 22:14:31 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:14:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (825954ms till timeout)
2022-03-30 22:14:32 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (824951ms till timeout)
2022-03-30 22:14:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (823948ms till timeout)
2022-03-30 22:14:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (822945ms till timeout)
2022-03-30 22:14:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (821942ms till timeout)
2022-03-30 22:14:36 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:14:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (820939ms till timeout)
2022-03-30 22:14:37 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (819935ms till timeout)
2022-03-30 22:14:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (818932ms till timeout)
2022-03-30 22:14:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (817929ms till timeout)
2022-03-30 22:14:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (816925ms till timeout)
2022-03-30 22:14:41 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:14:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (815922ms till timeout)
2022-03-30 22:14:42 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (814919ms till timeout)
2022-03-30 22:14:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (813915ms till timeout)
2022-03-30 22:14:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (812912ms till timeout)
2022-03-30 22:14:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (811909ms till timeout)
2022-03-30 22:14:46 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:14:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (810905ms till timeout)
2022-03-30 22:14:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (809902ms till timeout)
2022-03-30 22:14:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (808899ms till timeout)
2022-03-30 22:14:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (807896ms till timeout)
2022-03-30 22:14:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (806893ms till timeout)
2022-03-30 22:14:51 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:14:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (805890ms till timeout)
2022-03-30 22:14:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (804887ms till timeout)
2022-03-30 22:14:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (803884ms till timeout)
2022-03-30 22:14:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (802880ms till timeout)
2022-03-30 22:14:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (801877ms till timeout)
2022-03-30 22:14:56 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:14:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (800874ms till timeout)
2022-03-30 22:14:57 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (799871ms till timeout)
2022-03-30 22:14:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (798868ms till timeout)
2022-03-30 22:14:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (797865ms till timeout)
2022-03-30 22:15:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (796862ms till timeout)
2022-03-30 22:15:01 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:15:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (795859ms till timeout)
2022-03-30 22:15:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (794856ms till timeout)
2022-03-30 22:15:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (793852ms till timeout)
2022-03-30 22:15:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (792849ms till timeout)
2022-03-30 22:15:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (791846ms till timeout)
2022-03-30 22:15:06 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:15:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (790842ms till timeout)
2022-03-30 22:15:07 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (789839ms till timeout)
2022-03-30 22:15:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (788836ms till timeout)
2022-03-30 22:15:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (787833ms till timeout)
2022-03-30 22:15:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (786829ms till timeout)
2022-03-30 22:15:11 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:15:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (785826ms till timeout)
2022-03-30 22:15:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (784822ms till timeout)
2022-03-30 22:15:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (783819ms till timeout)
2022-03-30 22:15:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (782816ms till timeout)
2022-03-30 22:15:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (781813ms till timeout)
2022-03-30 22:15:16 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:15:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (780810ms till timeout)
2022-03-30 22:15:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (779807ms till timeout)
2022-03-30 22:15:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (778804ms till timeout)
2022-03-30 22:15:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (777801ms till timeout)
2022-03-30 22:15:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (776798ms till timeout)
2022-03-30 22:15:21 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:15:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (775794ms till timeout)
2022-03-30 22:15:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (774791ms till timeout)
2022-03-30 22:15:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (773788ms till timeout)
2022-03-30 22:15:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (772785ms till timeout)
2022-03-30 22:15:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (771782ms till timeout)
2022-03-30 22:15:26 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:15:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (770779ms till timeout)
2022-03-30 22:15:27 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6e67dcda-target will have desired state: Ready not ready, will try again in 1000 ms (769776ms till timeout)
2022-03-30 22:15:28 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-6e67dcda-target is in desired state: Ready
2022-03-30 22:15:28 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic mirrormaker2-topic-example-1294186046 in namespace namespace-7
2022-03-30 22:15:28 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-30 22:15:28 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:mirrormaker2-topic-example-1294186046
2022-03-30 22:15:28 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: mirrormaker2-topic-example-1294186046 will have desired state: Ready
2022-03-30 22:15:28 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: mirrormaker2-topic-example-1294186046 will have desired state: Ready
2022-03-30 22:15:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaTopic: mirrormaker2-topic-example-1294186046 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 22:15:29 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaTopic: mirrormaker2-topic-example-1294186046 is in desired state: Ready
2022-03-30 22:15:29 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-6e67dcda-my-user-source in namespace namespace-7
2022-03-30 22:15:29 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-30 22:15:29 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-cluster-6e67dcda-my-user-source
2022-03-30 22:15:29 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-6e67dcda-my-user-source will have desired state: Ready
2022-03-30 22:15:29 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-cluster-6e67dcda-my-user-source will have desired state: Ready
2022-03-30 22:15:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaUser: my-cluster-6e67dcda-my-user-source will have desired state: Ready not ready, will try again in 1000 ms (179958ms till timeout)
2022-03-30 22:15:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-6e67dcda-my-user-source is in desired state: Ready
2022-03-30 22:15:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-6e67dcda-my-user-target in namespace namespace-7
2022-03-30 22:15:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-30 22:15:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-cluster-6e67dcda-my-user-target
2022-03-30 22:15:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-6e67dcda-my-user-target will have desired state: Ready
2022-03-30 22:15:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-cluster-6e67dcda-my-user-target will have desired state: Ready
2022-03-30 22:15:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaUser: my-cluster-6e67dcda-my-user-target will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 22:15:31 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:15:31 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-6e67dcda-my-user-target is in desired state: Ready
2022-03-30 22:15:31 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-6e67dcda-kafka-clients in namespace namespace-7
2022-03-30 22:15:31 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-30 22:15:31 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for pod with prefixmy-cluster-6e67dcda-kafka-clients is present.
2022-03-30 22:15:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] pod with prefixmy-cluster-6e67dcda-kafka-clients is present. not ready, will try again in 10000 ms (299996ms till timeout)
2022-03-30 22:15:36 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:15:41 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:15:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1279769671-1440257906-test-1 in namespace namespace-7
2022-03-30 22:15:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-30 22:15:41 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1279769671-1440257906-test-1
2022-03-30 22:15:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1279769671-1440257906-test-1 will have desired state: Ready
2022-03-30 22:15:41 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1279769671-1440257906-test-1 will have desired state: Ready
2022-03-30 22:15:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1279769671-1440257906-test-1 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 22:15:42 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1279769671-1440257906-test-1 is in desired state: Ready
2022-03-30 22:15:42 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1279769671-1440257906-test-2 in namespace namespace-7
2022-03-30 22:15:42 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-30 22:15:42 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1279769671-1440257906-test-2
2022-03-30 22:15:42 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1279769671-1440257906-test-2 will have desired state: Ready
2022-03-30 22:15:42 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1279769671-1440257906-test-2 will have desired state: Ready
2022-03-30 22:15:42 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1279769671-1440257906-test-2 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 22:15:43 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1279769671-1440257906-test-2 is in desired state: Ready
2022-03-30 22:15:43 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-30 22:15:43 [ForkJoinPool-3-worker-13] [32mINFO [m [MirrorMaker2IsolatedST:328] Setting topic to my-topic-1279769671-1440257906-test-2, cluster to my-cluster-6e67dcda-target and changing user to my-cluster-6e67dcda-my-user-target
2022-03-30 22:15:43 [ForkJoinPool-3-worker-13] [32mINFO [m [MirrorMaker2IsolatedST:337] Sending messages to - topic my-topic-1279769671-1440257906-test-2, cluster my-cluster-6e67dcda-target and message count of 200
2022-03-30 22:15:43 [ForkJoinPool-3-worker-13] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@60524fc7, which are set.
2022-03-30 22:15:43 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@53c2651d, messages=[], arguments=[--topic, my-topic-1279769671-1440257906-test-2, --bootstrap-server, my-cluster-6e67dcda-target-kafka-bootstrap.namespace-7.svc:9093, --max-messages, 200, USER=my_cluster_6e67dcda_my_user_target], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-6e67dcda-kafka-clients-69477f4498-6p4lw', podNamespace='namespace-7', bootstrapServer='my-cluster-6e67dcda-target-kafka-bootstrap.namespace-7.svc:9093', topicName='my-topic-1279769671-1440257906-test-2', maxMessages=200, kafkaUsername='my-cluster-6e67dcda-my-user-target', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@60524fc7}
2022-03-30 22:15:43 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-6e67dcda-target-kafka-bootstrap.namespace-7.svc:9093:my-topic-1279769671-1440257906-test-2 from pod my-cluster-6e67dcda-kafka-clients-69477f4498-6p4lw
2022-03-30 22:15:43 [ForkJoinPool-3-worker-13] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-6e67dcda-kafka-clients-69477f4498-6p4lw -n namespace-7 -- /opt/kafka/producer.sh --topic my-topic-1279769671-1440257906-test-2 --bootstrap-server my-cluster-6e67dcda-target-kafka-bootstrap.namespace-7.svc:9093 --max-messages 200 USER=my_cluster_6e67dcda_my_user_target
2022-03-30 22:15:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-6e67dcda-kafka-clients-69477f4498-6p4lw -n namespace-7 -- /opt/kafka/producer.sh --topic my-topic-1279769671-1440257906-test-2 --bootstrap-server my-cluster-6e67dcda-target-kafka-bootstrap.namespace-7.svc:9093 --max-messages 200 USER=my_cluster_6e67dcda_my_user_target
2022-03-30 22:15:46 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:15:47 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-30 22:15:47 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-03-30 22:15:47 [ForkJoinPool-3-worker-13] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@24125530, which are set.
2022-03-30 22:15:47 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@564c0a02, messages=[], arguments=[--topic, my-topic-1279769671-1440257906-test-2, --bootstrap-server, my-cluster-6e67dcda-target-kafka-bootstrap.namespace-7.svc:9093, --max-messages, 200, USER=my_cluster_6e67dcda_my_user_target, --group-id, my-consumer-group-419766957, --group-instance-id, instance709124949], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-6e67dcda-kafka-clients-69477f4498-6p4lw', podNamespace='namespace-7', bootstrapServer='my-cluster-6e67dcda-target-kafka-bootstrap.namespace-7.svc:9093', topicName='my-topic-1279769671-1440257906-test-2', maxMessages=200, kafkaUsername='my-cluster-6e67dcda-my-user-target', consumerGroupName='my-consumer-group-419766957', consumerInstanceId='instance709124949', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@24125530}
2022-03-30 22:15:47 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-6e67dcda-target-kafka-bootstrap.namespace-7.svc:9093:my-topic-1279769671-1440257906-test-2 from pod my-cluster-6e67dcda-kafka-clients-69477f4498-6p4lw
2022-03-30 22:15:47 [ForkJoinPool-3-worker-13] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-6e67dcda-kafka-clients-69477f4498-6p4lw -n namespace-7 -- /opt/kafka/consumer.sh --topic my-topic-1279769671-1440257906-test-2 --bootstrap-server my-cluster-6e67dcda-target-kafka-bootstrap.namespace-7.svc:9093 --max-messages 200 USER=my_cluster_6e67dcda_my_user_target --group-id my-consumer-group-419766957 --group-instance-id instance709124949
2022-03-30 22:15:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-6e67dcda-kafka-clients-69477f4498-6p4lw -n namespace-7 -- /opt/kafka/consumer.sh --topic my-topic-1279769671-1440257906-test-2 --bootstrap-server my-cluster-6e67dcda-target-kafka-bootstrap.namespace-7.svc:9093 --max-messages 200 USER=my_cluster_6e67dcda_my_user_target --group-id my-consumer-group-419766957 --group-instance-id instance709124949
2022-03-30 22:15:51 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:15:54 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 22:15:54 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-03-30 22:15:54 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-6e67dcda in namespace namespace-7
2022-03-30 22:15:54 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-30 22:15:54 [ForkJoinPool-3-worker-13] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkamirrormaker2s' with unstable version 'v1beta2'
2022-03-30 22:15:54 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaMirrorMaker2:my-cluster-6e67dcda
2022-03-30 22:15:54 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready
2022-03-30 22:15:54 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready
2022-03-30 22:15:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 22:15:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (598993ms till timeout)
2022-03-30 22:15:56 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:15:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (597989ms till timeout)
2022-03-30 22:15:57 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (596986ms till timeout)
2022-03-30 22:15:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (595982ms till timeout)
2022-03-30 22:15:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (594979ms till timeout)
2022-03-30 22:16:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (593976ms till timeout)
2022-03-30 22:16:01 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:16:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (592972ms till timeout)
2022-03-30 22:16:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (591968ms till timeout)
2022-03-30 22:16:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (590964ms till timeout)
2022-03-30 22:16:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (589958ms till timeout)
2022-03-30 22:16:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (588954ms till timeout)
2022-03-30 22:16:06 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:16:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (587950ms till timeout)
2022-03-30 22:16:07 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (586947ms till timeout)
2022-03-30 22:16:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (585944ms till timeout)
2022-03-30 22:16:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (584941ms till timeout)
2022-03-30 22:16:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (583937ms till timeout)
2022-03-30 22:16:11 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:16:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (582934ms till timeout)
2022-03-30 22:16:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (581931ms till timeout)
2022-03-30 22:16:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (580928ms till timeout)
2022-03-30 22:16:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (579924ms till timeout)
2022-03-30 22:16:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (578921ms till timeout)
2022-03-30 22:16:16 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:16:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (577917ms till timeout)
2022-03-30 22:16:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (576914ms till timeout)
2022-03-30 22:16:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (575911ms till timeout)
2022-03-30 22:16:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (574907ms till timeout)
2022-03-30 22:16:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (573904ms till timeout)
2022-03-30 22:16:21 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:16:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (572900ms till timeout)
2022-03-30 22:16:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (571897ms till timeout)
2022-03-30 22:16:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (570894ms till timeout)
2022-03-30 22:16:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (569890ms till timeout)
2022-03-30 22:16:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (568887ms till timeout)
2022-03-30 22:16:26 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:16:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (567884ms till timeout)
2022-03-30 22:16:27 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (566880ms till timeout)
2022-03-30 22:16:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (565877ms till timeout)
2022-03-30 22:16:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (564874ms till timeout)
2022-03-30 22:16:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (563871ms till timeout)
2022-03-30 22:16:31 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:16:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (562868ms till timeout)
2022-03-30 22:16:32 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (561864ms till timeout)
2022-03-30 22:16:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (560861ms till timeout)
2022-03-30 22:16:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (559858ms till timeout)
2022-03-30 22:16:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (558855ms till timeout)
2022-03-30 22:16:36 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:16:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (557851ms till timeout)
2022-03-30 22:16:37 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (556848ms till timeout)
2022-03-30 22:16:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (555845ms till timeout)
2022-03-30 22:16:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (554842ms till timeout)
2022-03-30 22:16:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (553838ms till timeout)
2022-03-30 22:16:41 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:16:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (552834ms till timeout)
2022-03-30 22:16:42 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (551830ms till timeout)
2022-03-30 22:16:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (550827ms till timeout)
2022-03-30 22:16:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (549823ms till timeout)
2022-03-30 22:16:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (548820ms till timeout)
2022-03-30 22:16:46 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:16:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (547817ms till timeout)
2022-03-30 22:16:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (546814ms till timeout)
2022-03-30 22:16:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (545811ms till timeout)
2022-03-30 22:16:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (544807ms till timeout)
2022-03-30 22:16:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (543804ms till timeout)
2022-03-30 22:16:51 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:16:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (542801ms till timeout)
2022-03-30 22:16:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (541797ms till timeout)
2022-03-30 22:16:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (540794ms till timeout)
2022-03-30 22:16:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (539791ms till timeout)
2022-03-30 22:16:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (538788ms till timeout)
2022-03-30 22:16:56 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:16:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-6e67dcda will have desired state: Ready not ready, will try again in 1000 ms (537778ms till timeout)
2022-03-30 22:16:57 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-6e67dcda is in desired state: Ready
2022-03-30 22:16:57 [ForkJoinPool-3-worker-13] [32mINFO [m [MirrorMaker2IsolatedST:397] Setting topic to mirrormaker2-topic-example-1294186046, cluster to my-cluster-6e67dcda-source and changing user to my-cluster-6e67dcda-my-user-source
2022-03-30 22:16:57 [ForkJoinPool-3-worker-13] [32mINFO [m [MirrorMaker2IsolatedST:407] Sending messages to - topic mirrormaker2-topic-example-1294186046, cluster my-cluster-6e67dcda-source and message count of 200
2022-03-30 22:16:57 [ForkJoinPool-3-worker-13] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@728aa326, which are set.
2022-03-30 22:16:57 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@6cad65d3, messages=[], arguments=[--topic, mirrormaker2-topic-example-1294186046, --bootstrap-server, my-cluster-6e67dcda-source-kafka-bootstrap.namespace-7.svc:9093, --max-messages, 200, USER=my_cluster_6e67dcda_my_user_source], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-6e67dcda-kafka-clients-69477f4498-6p4lw', podNamespace='namespace-7', bootstrapServer='my-cluster-6e67dcda-source-kafka-bootstrap.namespace-7.svc:9093', topicName='mirrormaker2-topic-example-1294186046', maxMessages=200, kafkaUsername='my-cluster-6e67dcda-my-user-source', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@728aa326}
2022-03-30 22:16:57 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-6e67dcda-source-kafka-bootstrap.namespace-7.svc:9093:mirrormaker2-topic-example-1294186046 from pod my-cluster-6e67dcda-kafka-clients-69477f4498-6p4lw
2022-03-30 22:16:57 [ForkJoinPool-3-worker-13] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-6e67dcda-kafka-clients-69477f4498-6p4lw -n namespace-7 -- /opt/kafka/producer.sh --topic mirrormaker2-topic-example-1294186046 --bootstrap-server my-cluster-6e67dcda-source-kafka-bootstrap.namespace-7.svc:9093 --max-messages 200 USER=my_cluster_6e67dcda_my_user_source
2022-03-30 22:16:57 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-6e67dcda-kafka-clients-69477f4498-6p4lw -n namespace-7 -- /opt/kafka/producer.sh --topic mirrormaker2-topic-example-1294186046 --bootstrap-server my-cluster-6e67dcda-source-kafka-bootstrap.namespace-7.svc:9093 --max-messages 200 USER=my_cluster_6e67dcda_my_user_source
2022-03-30 22:17:01 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:17:01 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-30 22:17:01 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-03-30 22:17:01 [ForkJoinPool-3-worker-13] [32mINFO [m [MirrorMaker2IsolatedST:411] Receiving messages from - topic mirrormaker2-topic-example-1294186046, cluster my-cluster-6e67dcda-source and message count of 200
2022-03-30 22:17:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@570b6c67, which are set.
2022-03-30 22:17:01 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@70bf6633, messages=[], arguments=[--topic, mirrormaker2-topic-example-1294186046, --bootstrap-server, my-cluster-6e67dcda-source-kafka-bootstrap.namespace-7.svc:9093, --max-messages, 200, USER=my_cluster_6e67dcda_my_user_source, --group-id, my-consumer-group-419766957, --group-instance-id, instance1362838324], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-6e67dcda-kafka-clients-69477f4498-6p4lw', podNamespace='namespace-7', bootstrapServer='my-cluster-6e67dcda-source-kafka-bootstrap.namespace-7.svc:9093', topicName='mirrormaker2-topic-example-1294186046', maxMessages=200, kafkaUsername='my-cluster-6e67dcda-my-user-source', consumerGroupName='my-consumer-group-419766957', consumerInstanceId='instance1362838324', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@570b6c67}
2022-03-30 22:17:01 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-6e67dcda-source-kafka-bootstrap.namespace-7.svc:9093:mirrormaker2-topic-example-1294186046 from pod my-cluster-6e67dcda-kafka-clients-69477f4498-6p4lw
2022-03-30 22:17:01 [ForkJoinPool-3-worker-13] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-6e67dcda-kafka-clients-69477f4498-6p4lw -n namespace-7 -- /opt/kafka/consumer.sh --topic mirrormaker2-topic-example-1294186046 --bootstrap-server my-cluster-6e67dcda-source-kafka-bootstrap.namespace-7.svc:9093 --max-messages 200 USER=my_cluster_6e67dcda_my_user_source --group-id my-consumer-group-419766957 --group-instance-id instance1362838324
2022-03-30 22:17:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-6e67dcda-kafka-clients-69477f4498-6p4lw -n namespace-7 -- /opt/kafka/consumer.sh --topic mirrormaker2-topic-example-1294186046 --bootstrap-server my-cluster-6e67dcda-source-kafka-bootstrap.namespace-7.svc:9093 --max-messages 200 USER=my_cluster_6e67dcda_my_user_source --group-id my-consumer-group-419766957 --group-instance-id instance1362838324
2022-03-30 22:17:06 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:17:08 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 22:17:08 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-03-30 22:17:08 [ForkJoinPool-3-worker-13] [32mINFO [m [MirrorMaker2IsolatedST:418] Now setting topic to my-cluster-6e67dcda-source.mirrormaker2-topic-example-1294186046, cluster to my-cluster-6e67dcda-target and user to my-cluster-6e67dcda-my-user-target - the messages should be mirrored
2022-03-30 22:17:08 [ForkJoinPool-3-worker-13] [32mINFO [m [MirrorMaker2IsolatedST:427] Consumer in target cluster and topic should receive 200 messages
2022-03-30 22:17:08 [ForkJoinPool-3-worker-13] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@773b160a, which are set.
2022-03-30 22:17:08 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@496a9ecc, messages=[], arguments=[--topic, my-cluster-6e67dcda-source.mirrormaker2-topic-example-1294186046, --bootstrap-server, my-cluster-6e67dcda-target-kafka-bootstrap.namespace-7.svc:9093, --max-messages, 200, USER=my_cluster_6e67dcda_my_user_target, --group-id, my-consumer-group-419766957, --group-instance-id, instance1250620016], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-6e67dcda-kafka-clients-69477f4498-6p4lw', podNamespace='namespace-7', bootstrapServer='my-cluster-6e67dcda-target-kafka-bootstrap.namespace-7.svc:9093', topicName='my-cluster-6e67dcda-source.mirrormaker2-topic-example-1294186046', maxMessages=200, kafkaUsername='my-cluster-6e67dcda-my-user-target', consumerGroupName='my-consumer-group-419766957', consumerInstanceId='instance1250620016', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@773b160a}
2022-03-30 22:17:08 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-6e67dcda-target-kafka-bootstrap.namespace-7.svc:9093:my-cluster-6e67dcda-source.mirrormaker2-topic-example-1294186046 from pod my-cluster-6e67dcda-kafka-clients-69477f4498-6p4lw
2022-03-30 22:17:08 [ForkJoinPool-3-worker-13] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-6e67dcda-kafka-clients-69477f4498-6p4lw -n namespace-7 -- /opt/kafka/consumer.sh --topic my-cluster-6e67dcda-source.mirrormaker2-topic-example-1294186046 --bootstrap-server my-cluster-6e67dcda-target-kafka-bootstrap.namespace-7.svc:9093 --max-messages 200 USER=my_cluster_6e67dcda_my_user_target --group-id my-consumer-group-419766957 --group-instance-id instance1250620016
2022-03-30 22:17:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-6e67dcda-kafka-clients-69477f4498-6p4lw -n namespace-7 -- /opt/kafka/consumer.sh --topic my-cluster-6e67dcda-source.mirrormaker2-topic-example-1294186046 --bootstrap-server my-cluster-6e67dcda-target-kafka-bootstrap.namespace-7.svc:9093 --max-messages 200 USER=my_cluster_6e67dcda_my_user_target --group-id my-consumer-group-419766957 --group-instance-id instance1250620016
2022-03-30 22:17:11 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:17:15 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 22:17:15 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-03-30 22:17:15 [ForkJoinPool-3-worker-13] [32mINFO [m [MirrorMaker2IsolatedST:432] Messages successfully mirrored
2022-03-30 22:17:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 22:17:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:675] [mirrormaker.MirrorMaker2IsolatedST - After Each] - Clean up after test
2022-03-30 22:17:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:17:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testMirrorMaker2TlsAndTlsClientAuth
2022-03-30 22:17:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-6e67dcda-kafka-clients in namespace namespace-7
2022-03-30 22:17:15 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic mirrormaker2-topic-example-1294186046 in namespace namespace-7
2022-03-30 22:17:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-6e67dcda-kafka-clients
2022-03-30 22:17:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:mirrormaker2-topic-example-1294186046
2022-03-30 22:17:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:mirrormaker2-topic-example-1294186046 not ready, will try again in 10000 ms (179989ms till timeout)
2022-03-30 22:17:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-6e67dcda-kafka-clients not ready, will try again in 10000 ms (479987ms till timeout)
2022-03-30 22:17:16 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:17:21 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:17:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:mirrormaker2-topic-example-1294186046 not ready, will try again in 10000 ms (169983ms till timeout)
2022-03-30 22:17:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-6e67dcda-kafka-clients not ready, will try again in 10000 ms (469978ms till timeout)
2022-03-30 22:17:26 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:17:31 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:17:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:mirrormaker2-topic-example-1294186046 not ready, will try again in 10000 ms (159974ms till timeout)
2022-03-30 22:17:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-6e67dcda-kafka-clients not ready, will try again in 10000 ms (459969ms till timeout)
2022-03-30 22:17:36 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:17:41 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:17:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:mirrormaker2-topic-example-1294186046 not ready, will try again in 10000 ms (149964ms till timeout)
2022-03-30 22:17:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-6e67dcda-kafka-clients not ready, will try again in 10000 ms (449960ms till timeout)
2022-03-30 22:17:46 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:17:51 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:17:55 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-6e67dcda-my-user-target in namespace namespace-7
2022-03-30 22:17:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:mirrormaker2-topic-example-1294186046 not ready, will try again in 10000 ms (139955ms till timeout)
2022-03-30 22:17:55 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-cluster-6e67dcda-my-user-target
2022-03-30 22:17:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-cluster-6e67dcda-my-user-target not ready, will try again in 10000 ms (179952ms till timeout)
2022-03-30 22:17:56 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:18:01 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:18:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:mirrormaker2-topic-example-1294186046 not ready, will try again in 10000 ms (129949ms till timeout)
2022-03-30 22:18:05 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1279769671-1440257906-test-2 in namespace namespace-7
2022-03-30 22:18:05 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1279769671-1440257906-test-2
2022-03-30 22:18:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1279769671-1440257906-test-2 not ready, will try again in 10000 ms (179993ms till timeout)
2022-03-30 22:18:06 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:18:11 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:18:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:mirrormaker2-topic-example-1294186046 not ready, will try again in 10000 ms (119942ms till timeout)
2022-03-30 22:18:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-6e67dcda in namespace namespace-7
2022-03-30 22:18:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaMirrorMaker2:my-cluster-6e67dcda
2022-03-30 22:18:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1279769671-1440257906-test-1 in namespace namespace-7
2022-03-30 22:18:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1279769671-1440257906-test-1
2022-03-30 22:18:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1279769671-1440257906-test-1 not ready, will try again in 10000 ms (179994ms till timeout)
2022-03-30 22:18:16 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:18:21 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:18:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:mirrormaker2-topic-example-1294186046 not ready, will try again in 10000 ms (109935ms till timeout)
2022-03-30 22:18:25 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-6e67dcda-target in namespace namespace-7
2022-03-30 22:18:25 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-6e67dcda-target
2022-03-30 22:18:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-6e67dcda-target not ready, will try again in 10000 ms (839957ms till timeout)
2022-03-30 22:18:26 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:18:31 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:18:35 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-6e67dcda-my-user-source in namespace namespace-7
2022-03-30 22:18:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-cluster-6e67dcda-my-user-source
2022-03-30 22:18:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-cluster-6e67dcda-my-user-source not ready, will try again in 10000 ms (179990ms till timeout)
2022-03-30 22:18:35 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-6e67dcda-source in namespace namespace-7
2022-03-30 22:18:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-6e67dcda-source
2022-03-30 22:18:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-6e67dcda-source not ready, will try again in 10000 ms (839957ms till timeout)
2022-03-30 22:18:36 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:18:41 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:18:45 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:18:45 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-7 for test case:testMirrorMaker2TlsAndTlsClientAuth
2022-03-30 22:18:45 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-7 removal
2022-03-30 22:18:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 22:18:45 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 22:18:45 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:18:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (479928ms till timeout)
2022-03-30 22:18:46 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:18:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 22:18:46 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 22:18:46 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:18:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (478860ms till timeout)
2022-03-30 22:18:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 22:18:47 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 22:18:47 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:18:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (477790ms till timeout)
2022-03-30 22:18:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 22:18:48 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 22:18:48 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:18:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (476718ms till timeout)
2022-03-30 22:18:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 22:18:49 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 22:18:49 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:18:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (475647ms till timeout)
2022-03-30 22:18:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 22:18:50 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 22:18:50 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:18:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (474568ms till timeout)
2022-03-30 22:18:51 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:18:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 22:18:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 22:18:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:18:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (473495ms till timeout)
2022-03-30 22:18:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 22:18:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 22:18:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:18:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (472422ms till timeout)
2022-03-30 22:18:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 22:18:54 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 22:18:54 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:18:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (471344ms till timeout)
2022-03-30 22:18:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 22:18:55 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 22:18:55 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:18:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (470267ms till timeout)
2022-03-30 22:18:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 22:18:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 22:18:56 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 22:18:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 22:18:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 22:18:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-7" not found
2022-03-30 22:18:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 22:18:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@529573c9=[]}
2022-03-30 22:18:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:267] testMirrorMaker2TlsAndTlsClientAuth - Notifies waiting test cases:[testCruiseControlBasicAPIRequests, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testUpdateUser, testSendSimpleMessageTls, testAutoRenewAllCaCertsTriggeredByAnno, testReceiveSimpleMessageTls, testSendMessagesCustomListenerTlsScramSha, testKafkaAndZookeeperScaleUpScaleDown, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth] to and randomly select one to start execution
2022-03-30 22:18:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:93] [mirrormaker.MirrorMaker2IsolatedST] - Removing parallel test: testMirrorMaker2TlsAndTlsClientAuth
2022-03-30 22:18:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:97] [mirrormaker.MirrorMaker2IsolatedST] - Parallel test count: 0
2022-03-30 22:18:56 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testMirrorMaker2TlsAndTlsClientAuth-FINISHED
2022-03-30 22:18:56 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 22:18:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 22:18:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:690] [mirrormaker.MirrorMaker2IsolatedST - After All] - Clean up after test suite
2022-03-30 22:18:56 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:18:56 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:346] In context MirrorMaker2IsolatedST is everything deleted.
2022-03-30 22:18:56 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2,522.406 s - in io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST
2022-03-30 22:19:01 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:136] Suite specific.SpecificIsolatedST has locked the @IsolatedSuite and other @IsolatedSuites must wait until lock is released.
2022-03-30 22:19:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:19:01 [ForkJoinPool-3-worker-17] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-30 22:19:01 [ForkJoinPool-3-worker-17] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-30 22:19:01 [ForkJoinPool-3-worker-17] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-30 22:19:01 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:19:01 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-30 22:19:01 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 22:19:01 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 22:19:01 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 22:19:01 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 22:19:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 22:19:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io not ready, will try again in 10000 ms (179984ms till timeout)
2022-03-30 22:19:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 22:19:01 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 22:19:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 22:19:01 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:19:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 22:19:01 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 22:19:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 22:19:01 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 22:19:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179957ms till timeout)
2022-03-30 22:19:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 22:19:01 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:19:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 22:19:01 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 22:19:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 22:19:01 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 22:19:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 22:19:01 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 22:19:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 22:19:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179947ms till timeout)
2022-03-30 22:19:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:19:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:19:11 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 22:19:11 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 22:19:11 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:19:11 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 22:19:11 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 22:19:11 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 22:19:11 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:19:11 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 22:19:11 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 22:19:11 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 22:19:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 22:19:11 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 22:19:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 22:19:11 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 22:19:11 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 22:19:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 22:19:11 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:19:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 22:19:11 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 22:19:11 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 22:19:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 22:19:11 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 22:19:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 22:19:11 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 22:19:11 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 22:19:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 22:19:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io not ready, will try again in 10000 ms (179961ms till timeout)
2022-03-30 22:19:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io not ready, will try again in 10000 ms (179960ms till timeout)
2022-03-30 22:19:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179830ms till timeout)
2022-03-30 22:19:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:19:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:19:21 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:19:21 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 22:19:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io not ready, will try again in 10000 ms (179930ms till timeout)
2022-03-30 22:19:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:19:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:19:31 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:19:31 [ForkJoinPool-3-worker-17] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-30 22:19:31 [ForkJoinPool-3-worker-17] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 22:19:31 [ForkJoinPool-3-worker-17] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 22:19:31 [ForkJoinPool-3-worker-17] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v108148
2022-03-30 22:19:31 [ForkJoinPool-3-worker-17] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v108148
2022-03-30 22:19:31 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=108148&allowWatchBookmarks=true&watch=true...
2022-03-30 22:19:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 22:19:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 108149
2022-03-30 22:19:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:19:36 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 108169
2022-03-30 22:19:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:19:41 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 108175
2022-03-30 22:19:41 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v108169 in namespace default
2022-03-30 22:19:41 [ForkJoinPool-3-worker-17] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@26fcd291
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-30 22:19:41 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@5812d2fa
2022-03-30 22:19:41 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 22:19:41 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@3f2e6467, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@26fcd291, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-30 22:19:41 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@212c7377
2022-03-30 22:19:41 [ForkJoinPool-3-worker-17] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-30 22:19:41 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@212c7377
2022-03-30 22:19:41 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@212c7377
2022-03-30 22:19:41 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 22:19:41 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 22:19:41 [ForkJoinPool-3-worker-17] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-30 22:19:41 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-30 22:19:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace infra-namespace -o json
2022-03-30 22:19:41 [ForkJoinPool-3-worker-17] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace infra-namespace -o json
2022-03-30 22:19:41 [ForkJoinPool-3-worker-17] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:19:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T22:19:41Z",
        "name": "infra-namespace",
        "resourceVersion": "108176",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "93ffc243-e3d1-446d-af8d-24bb798270e0"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 22:19:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-30 22:19:41 [ForkJoinPool-3-worker-17] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 22:19:41 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-30 22:19:41 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 22:19:42 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479996ms till timeout)
2022-03-30 22:19:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478993ms till timeout)
2022-03-30 22:19:44 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477990ms till timeout)
2022-03-30 22:19:45 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476987ms till timeout)
2022-03-30 22:19:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:19:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475983ms till timeout)
2022-03-30 22:19:47 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474980ms till timeout)
2022-03-30 22:19:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473977ms till timeout)
2022-03-30 22:19:49 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472974ms till timeout)
2022-03-30 22:19:50 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471971ms till timeout)
2022-03-30 22:19:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:19:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470968ms till timeout)
2022-03-30 22:19:52 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469965ms till timeout)
2022-03-30 22:19:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468962ms till timeout)
2022-03-30 22:19:54 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467959ms till timeout)
2022-03-30 22:19:55 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466956ms till timeout)
2022-03-30 22:19:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:19:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465953ms till timeout)
2022-03-30 22:19:57 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464949ms till timeout)
2022-03-30 22:19:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463946ms till timeout)
2022-03-30 22:19:59 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462943ms till timeout)
2022-03-30 22:20:00 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461940ms till timeout)
2022-03-30 22:20:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:20:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (460937ms till timeout)
2022-03-30 22:20:02 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (459934ms till timeout)
2022-03-30 22:20:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (458931ms till timeout)
2022-03-30 22:20:04 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (457927ms till timeout)
2022-03-30 22:20:05 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (456924ms till timeout)
2022-03-30 22:20:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:20:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (455921ms till timeout)
2022-03-30 22:20:07 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-30 22:20:07 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-30 22:20:07 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-30 22:20:07 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-wjszx not ready: strimzi-cluster-operator)
2022-03-30 22:20:07 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-wjszx are ready
2022-03-30 22:20:07 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-30 22:20:08 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-wjszx not ready: strimzi-cluster-operator)
2022-03-30 22:20:08 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-wjszx are ready
2022-03-30 22:20:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-30 22:20:09 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-wjszx not ready: strimzi-cluster-operator)
2022-03-30 22:20:09 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-wjszx are ready
2022-03-30 22:20:09 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597990ms till timeout)
2022-03-30 22:20:10 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-wjszx not ready: strimzi-cluster-operator)
2022-03-30 22:20:10 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-wjszx are ready
2022-03-30 22:20:10 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596986ms till timeout)
2022-03-30 22:20:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:20:11 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-wjszx not ready: strimzi-cluster-operator)
2022-03-30 22:20:11 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-wjszx are ready
2022-03-30 22:20:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595982ms till timeout)
2022-03-30 22:20:12 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-wjszx not ready: strimzi-cluster-operator)
2022-03-30 22:20:12 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-wjszx are ready
2022-03-30 22:20:12 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594979ms till timeout)
2022-03-30 22:20:13 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-wjszx not ready: strimzi-cluster-operator)
2022-03-30 22:20:13 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-wjszx are ready
2022-03-30 22:20:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593975ms till timeout)
2022-03-30 22:20:14 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-wjszx not ready: strimzi-cluster-operator)
2022-03-30 22:20:14 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-wjszx are ready
2022-03-30 22:20:14 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592971ms till timeout)
2022-03-30 22:20:15 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-wjszx not ready: strimzi-cluster-operator)
2022-03-30 22:20:15 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-wjszx are ready
2022-03-30 22:20:15 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591968ms till timeout)
2022-03-30 22:20:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:20:16 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-wjszx not ready: strimzi-cluster-operator)
2022-03-30 22:20:16 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-wjszx are ready
2022-03-30 22:20:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590964ms till timeout)
2022-03-30 22:20:17 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-wjszx not ready: strimzi-cluster-operator)
2022-03-30 22:20:17 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-wjszx are ready
2022-03-30 22:20:17 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-30 22:20:17 [ForkJoinPool-3-worker-17] [32mINFO [m [SpecificIsolatedST:503] 0.21.4
2022-03-30 22:20:17 [ForkJoinPool-3-worker-17] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 22:20:17 [ForkJoinPool-3-worker-17] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.specific.SpecificIsolatedST.testRackAwareConnectCorrectDeployment-STARTED
2022-03-30 22:20:17 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 22:20:17 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractST:659] [specific.SpecificIsolatedST - Before Each] - Setup test case environment
2022-03-30 22:20:17 [ForkJoinPool-3-worker-17] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 22:20:17 [ForkJoinPool-3-worker-17] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070, testSendSimpleMessageTls=my-cluster-b70e5342, testUpdateUser=my-cluster-9b2bd187, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349, testSendMessagesTlsScramSha=my-cluster-380e6586, testReceiveSimpleMessageTls=my-cluster-e8a518cd, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9}
2022-03-30 22:20:17 [ForkJoinPool-3-worker-17] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1001559761-691587912, testSendSimpleMessageTls=my-user-602433797-900694548, testUpdateUser=my-user-595127408-1604948726, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1606351157-1815623087, testKafkaInDifferentNsThanClusterOperator=my-user-1518021213-326071957, testSendMessagesCustomListenerTlsScramSha=my-user-445988529-51741427, testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1590558165-46977776, testSendMessagesTlsScramSha=my-user-2039401547-533835955, testReceiveSimpleMessageTls=my-user-761922164-2019863647, testMirrorMaker2TlsAndTlsClientAuth=my-user-707354298-1518057957, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659, testRackAwareConnectCorrectDeployment=my-user-389825219-1608762022, testMirrorMakerTlsAuthenticated=my-user-692603309-1711649825}
2022-03-30 22:20:17 [ForkJoinPool-3-worker-17] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-741363275-285355018, testSendSimpleMessageTls=my-topic-1299629356-597168683, testUpdateUser=my-topic-1875736194-1340884809, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1931531910-629202741, testKafkaInDifferentNsThanClusterOperator=my-topic-2034182350-493335345, testSendMessagesCustomListenerTlsScramSha=my-topic-1039904883-1044305687, testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1210910585-1669754462, testSendMessagesTlsScramSha=my-topic-988094002-1400465950, testReceiveSimpleMessageTls=my-topic-1007790874-1032272408, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1279769671-1440257906, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393, testRackAwareConnectCorrectDeployment=my-topic-1801184984-1468632754, testMirrorMakerTlsAuthenticated=my-topic-1945138991-1836415682}
2022-03-30 22:20:17 [ForkJoinPool-3-worker-17] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070-kafka-clients, testSendSimpleMessageTls=my-cluster-b70e5342-kafka-clients, testUpdateUser=my-cluster-9b2bd187-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349-kafka-clients, testSendMessagesTlsScramSha=my-cluster-380e6586-kafka-clients, testReceiveSimpleMessageTls=my-cluster-e8a518cd-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9-kafka-clients}
2022-03-30 22:20:17 [ForkJoinPool-3-worker-17] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-30 22:20:17 [ForkJoinPool-3-worker-17] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-30 22:20:17 [ForkJoinPool-3-worker-17] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-30 22:20:17 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:20:17 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-30 22:20:17 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 22:20:17 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 22:20:17 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 22:20:17 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 22:20:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 22:20:17 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 22:20:17 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io not ready, will try again in 10000 ms (179988ms till timeout)
2022-03-30 22:20:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 22:20:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 22:20:17 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:20:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 22:20:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179944ms till timeout)
2022-03-30 22:20:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator not ready, will try again in 10000 ms (479951ms till timeout)
2022-03-30 22:20:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:20:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:20:27 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 22:20:27 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 22:20:27 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:20:27 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 22:20:27 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 22:20:27 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 22:20:27 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 22:20:27 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 22:20:27 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 22:20:27 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 22:20:27 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 22:20:27 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 22:20:27 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 22:20:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io not ready, will try again in 10000 ms (179977ms till timeout)
2022-03-30 22:20:27 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 22:20:27 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:20:27 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 22:20:27 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 22:20:27 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 22:20:27 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 22:20:27 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179911ms till timeout)
2022-03-30 22:20:27 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 22:20:27 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 22:20:27 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 22:20:28 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 22:20:28 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 22:20:28 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 22:20:28 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 22:20:28 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 22:20:28 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 22:20:28 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 22:20:28 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 22:20:28 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:20:28 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 22:20:28 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:20:28 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 22:20:28 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:20:28 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 22:20:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io not ready, will try again in 10000 ms (179978ms till timeout)
2022-03-30 22:20:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:20:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:20:37 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 22:20:37 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 22:20:37 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 22:20:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 22:20:37 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io not ready, will try again in 10000 ms (179972ms till timeout)
2022-03-30 22:20:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179877ms till timeout)
2022-03-30 22:20:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:20:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:20:48 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:20:48 [ForkJoinPool-3-worker-17] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-30 22:20:48 [ForkJoinPool-3-worker-17] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 22:20:48 [ForkJoinPool-3-worker-17] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 22:20:48 [ForkJoinPool-3-worker-17] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v108366
2022-03-30 22:20:48 [ForkJoinPool-3-worker-17] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v108366
2022-03-30 22:20:48 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=108366&allowWatchBookmarks=true&watch=true...
2022-03-30 22:20:48 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 22:20:48 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 108369
2022-03-30 22:20:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:20:53 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 108383
2022-03-30 22:20:53 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 108384
2022-03-30 22:20:53 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v108383 in namespace default
2022-03-30 22:20:53 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@328fbd32
2022-03-30 22:20:53 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 22:20:53 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@50e38c1c
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [SetupClusterOperator:223] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@3f2e6467, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@26fcd291, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=30000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-30 22:20:53 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@50e38c1c
2022-03-30 22:20:53 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@50e38c1c
2022-03-30 22:20:53 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 22:20:53 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T22:20:53Z",
        "name": "infra-namespace",
        "resourceVersion": "108385",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "abd44e69-f356-4840-898e-8068359f677c"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 22:20:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479957ms till timeout)
2022-03-30 22:20:54 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478954ms till timeout)
2022-03-30 22:20:55 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477952ms till timeout)
2022-03-30 22:20:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:20:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476948ms till timeout)
2022-03-30 22:20:57 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475946ms till timeout)
2022-03-30 22:20:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474942ms till timeout)
2022-03-30 22:20:59 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473939ms till timeout)
2022-03-30 22:21:00 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472936ms till timeout)
2022-03-30 22:21:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:21:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471933ms till timeout)
2022-03-30 22:21:02 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470930ms till timeout)
2022-03-30 22:21:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469927ms till timeout)
2022-03-30 22:21:04 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468924ms till timeout)
2022-03-30 22:21:05 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467921ms till timeout)
2022-03-30 22:21:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:21:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466918ms till timeout)
2022-03-30 22:21:07 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465915ms till timeout)
2022-03-30 22:21:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464912ms till timeout)
2022-03-30 22:21:09 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463909ms till timeout)
2022-03-30 22:21:10 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462906ms till timeout)
2022-03-30 22:21:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:21:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461903ms till timeout)
2022-03-30 22:21:12 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (460899ms till timeout)
2022-03-30 22:21:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (459896ms till timeout)
2022-03-30 22:21:14 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (458894ms till timeout)
2022-03-30 22:21:15 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (457890ms till timeout)
2022-03-30 22:21:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:21:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (456887ms till timeout)
2022-03-30 22:21:17 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (455884ms till timeout)
2022-03-30 22:21:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (454881ms till timeout)
2022-03-30 22:21:19 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (453878ms till timeout)
2022-03-30 22:21:20 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (452875ms till timeout)
2022-03-30 22:21:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:21:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (451872ms till timeout)
2022-03-30 22:21:22 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (450869ms till timeout)
2022-03-30 22:21:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (449866ms till timeout)
2022-03-30 22:21:24 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (448863ms till timeout)
2022-03-30 22:21:25 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (447860ms till timeout)
2022-03-30 22:21:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:21:26 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-30 22:21:26 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-30 22:21:26 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-30 22:21:26 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-z6f2w not ready: strimzi-cluster-operator)
2022-03-30 22:21:26 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-z6f2w are ready
2022-03-30 22:21:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-30 22:21:27 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-z6f2w not ready: strimzi-cluster-operator)
2022-03-30 22:21:27 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-z6f2w are ready
2022-03-30 22:21:27 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598993ms till timeout)
2022-03-30 22:21:28 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-z6f2w not ready: strimzi-cluster-operator)
2022-03-30 22:21:28 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-z6f2w are ready
2022-03-30 22:21:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597990ms till timeout)
2022-03-30 22:21:29 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-z6f2w not ready: strimzi-cluster-operator)
2022-03-30 22:21:29 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-z6f2w are ready
2022-03-30 22:21:29 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596987ms till timeout)
2022-03-30 22:21:30 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-z6f2w not ready: strimzi-cluster-operator)
2022-03-30 22:21:30 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-z6f2w are ready
2022-03-30 22:21:30 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595983ms till timeout)
2022-03-30 22:21:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:21:31 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-z6f2w not ready: strimzi-cluster-operator)
2022-03-30 22:21:31 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-z6f2w are ready
2022-03-30 22:21:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594980ms till timeout)
2022-03-30 22:21:32 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-z6f2w not ready: strimzi-cluster-operator)
2022-03-30 22:21:32 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-z6f2w are ready
2022-03-30 22:21:32 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593976ms till timeout)
2022-03-30 22:21:33 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-z6f2w not ready: strimzi-cluster-operator)
2022-03-30 22:21:33 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-z6f2w are ready
2022-03-30 22:21:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592972ms till timeout)
2022-03-30 22:21:34 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-z6f2w not ready: strimzi-cluster-operator)
2022-03-30 22:21:34 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-z6f2w are ready
2022-03-30 22:21:34 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591968ms till timeout)
2022-03-30 22:21:35 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-z6f2w not ready: strimzi-cluster-operator)
2022-03-30 22:21:35 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-z6f2w are ready
2022-03-30 22:21:35 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590965ms till timeout)
2022-03-30 22:21:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:21:36 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-z6f2w not ready: strimzi-cluster-operator)
2022-03-30 22:21:36 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-z6f2w are ready
2022-03-30 22:21:36 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-30 22:21:36 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment strimzi-cluster-operator rolling update
2022-03-30 22:21:36 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Deployment strimzi-cluster-operator rolling update in namespace:infra-namespace
2022-03-30 22:21:36 [ForkJoinPool-3-worker-17] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {strimzi-cluster-operator-78689684d4-wjszx=8fae50b4-01da-45b0-967d-025acca9dfed}
2022-03-30 22:21:36 [ForkJoinPool-3-worker-17] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {strimzi-cluster-operator-77554ffdfb-z6f2w=cee11a00-412c-4d26-aa76-5674d77a87b0}
2022-03-30 22:21:36 [ForkJoinPool-3-worker-17] [36mDEBUG[m [DeploymentUtils:119] All pods seem to have rolled
2022-03-30 22:21:36 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 22:21:36 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 22:21:36 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-30 22:21:36 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-30 22:21:36 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-z6f2w not ready: strimzi-cluster-operator)
2022-03-30 22:21:36 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-z6f2w are ready
2022-03-30 22:21:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 22:21:37 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-z6f2w not ready: strimzi-cluster-operator)
2022-03-30 22:21:37 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-z6f2w are ready
2022-03-30 22:21:37 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598995ms till timeout)
2022-03-30 22:21:38 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-z6f2w not ready: strimzi-cluster-operator)
2022-03-30 22:21:38 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-z6f2w are ready
2022-03-30 22:21:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597991ms till timeout)
2022-03-30 22:21:39 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-z6f2w not ready: strimzi-cluster-operator)
2022-03-30 22:21:39 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-z6f2w are ready
2022-03-30 22:21:39 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596987ms till timeout)
2022-03-30 22:21:41 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-z6f2w not ready: strimzi-cluster-operator)
2022-03-30 22:21:41 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-z6f2w are ready
2022-03-30 22:21:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595983ms till timeout)
2022-03-30 22:21:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:21:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-z6f2w not ready: strimzi-cluster-operator)
2022-03-30 22:21:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-z6f2w are ready
2022-03-30 22:21:42 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594980ms till timeout)
2022-03-30 22:21:43 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-z6f2w not ready: strimzi-cluster-operator)
2022-03-30 22:21:43 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-z6f2w are ready
2022-03-30 22:21:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593976ms till timeout)
2022-03-30 22:21:44 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-z6f2w not ready: strimzi-cluster-operator)
2022-03-30 22:21:44 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-z6f2w are ready
2022-03-30 22:21:44 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592973ms till timeout)
2022-03-30 22:21:45 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-z6f2w not ready: strimzi-cluster-operator)
2022-03-30 22:21:45 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-z6f2w are ready
2022-03-30 22:21:45 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591970ms till timeout)
2022-03-30 22:21:46 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-z6f2w not ready: strimzi-cluster-operator)
2022-03-30 22:21:46 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-z6f2w are ready
2022-03-30 22:21:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590966ms till timeout)
2022-03-30 22:21:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:21:47 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-z6f2w not ready: strimzi-cluster-operator)
2022-03-30 22:21:47 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-z6f2w are ready
2022-03-30 22:21:47 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:141] Deployment strimzi-cluster-operator rolling update finished
2022-03-30 22:21:47 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-bc24959a in namespace infra-namespace
2022-03-30 22:21:47 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-bc24959a
2022-03-30 22:21:47 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-bc24959a will have desired state: Ready
2022-03-30 22:21:47 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-bc24959a will have desired state: Ready
2022-03-30 22:21:47 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (839998ms till timeout)
2022-03-30 22:21:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (838994ms till timeout)
2022-03-30 22:21:49 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (837991ms till timeout)
2022-03-30 22:21:50 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (836989ms till timeout)
2022-03-30 22:21:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (835986ms till timeout)
2022-03-30 22:21:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:21:52 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (834983ms till timeout)
2022-03-30 22:21:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (833979ms till timeout)
2022-03-30 22:21:54 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (832976ms till timeout)
2022-03-30 22:21:55 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (831966ms till timeout)
2022-03-30 22:21:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (830962ms till timeout)
2022-03-30 22:21:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:21:57 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (829959ms till timeout)
2022-03-30 22:21:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (828956ms till timeout)
2022-03-30 22:21:59 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (827952ms till timeout)
2022-03-30 22:22:00 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (826949ms till timeout)
2022-03-30 22:22:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (825946ms till timeout)
2022-03-30 22:22:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:22:02 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (824942ms till timeout)
2022-03-30 22:22:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (823939ms till timeout)
2022-03-30 22:22:04 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (822935ms till timeout)
2022-03-30 22:22:05 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (821932ms till timeout)
2022-03-30 22:22:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (820929ms till timeout)
2022-03-30 22:22:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:22:07 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (819926ms till timeout)
2022-03-30 22:22:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (818923ms till timeout)
2022-03-30 22:22:09 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (817919ms till timeout)
2022-03-30 22:22:10 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (816916ms till timeout)
2022-03-30 22:22:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (815913ms till timeout)
2022-03-30 22:22:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:22:12 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (814909ms till timeout)
2022-03-30 22:22:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (813906ms till timeout)
2022-03-30 22:22:14 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (812903ms till timeout)
2022-03-30 22:22:15 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (811900ms till timeout)
2022-03-30 22:22:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (810897ms till timeout)
2022-03-30 22:22:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:22:17 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (809894ms till timeout)
2022-03-30 22:22:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (808890ms till timeout)
2022-03-30 22:22:19 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (807887ms till timeout)
2022-03-30 22:22:20 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (806883ms till timeout)
2022-03-30 22:22:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (805880ms till timeout)
2022-03-30 22:22:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:22:22 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (804875ms till timeout)
2022-03-30 22:22:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (803869ms till timeout)
2022-03-30 22:22:24 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (802866ms till timeout)
2022-03-30 22:22:25 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (801863ms till timeout)
2022-03-30 22:22:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (800857ms till timeout)
2022-03-30 22:22:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:22:27 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (799854ms till timeout)
2022-03-30 22:22:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (798851ms till timeout)
2022-03-30 22:22:29 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (797847ms till timeout)
2022-03-30 22:22:30 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (796844ms till timeout)
2022-03-30 22:22:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (795841ms till timeout)
2022-03-30 22:22:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:22:32 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (794838ms till timeout)
2022-03-30 22:22:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (793835ms till timeout)
2022-03-30 22:22:34 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (792832ms till timeout)
2022-03-30 22:22:35 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (791828ms till timeout)
2022-03-30 22:22:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (790825ms till timeout)
2022-03-30 22:22:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:22:37 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (789822ms till timeout)
2022-03-30 22:22:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (788819ms till timeout)
2022-03-30 22:22:39 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (787815ms till timeout)
2022-03-30 22:22:40 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (786812ms till timeout)
2022-03-30 22:22:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (785809ms till timeout)
2022-03-30 22:22:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:22:42 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (784806ms till timeout)
2022-03-30 22:22:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (783803ms till timeout)
2022-03-30 22:22:44 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (782800ms till timeout)
2022-03-30 22:22:45 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (781796ms till timeout)
2022-03-30 22:22:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (780792ms till timeout)
2022-03-30 22:22:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:22:47 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (779789ms till timeout)
2022-03-30 22:22:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (778786ms till timeout)
2022-03-30 22:22:49 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (777783ms till timeout)
2022-03-30 22:22:50 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (776780ms till timeout)
2022-03-30 22:22:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (775776ms till timeout)
2022-03-30 22:22:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:22:52 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (774773ms till timeout)
2022-03-30 22:22:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (773769ms till timeout)
2022-03-30 22:22:54 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (772761ms till timeout)
2022-03-30 22:22:55 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (771758ms till timeout)
2022-03-30 22:22:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (770754ms till timeout)
2022-03-30 22:22:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:22:57 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (769751ms till timeout)
2022-03-30 22:22:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (768746ms till timeout)
2022-03-30 22:22:59 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (767742ms till timeout)
2022-03-30 22:23:00 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (766739ms till timeout)
2022-03-30 22:23:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (765736ms till timeout)
2022-03-30 22:23:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:23:02 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (764732ms till timeout)
2022-03-30 22:23:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (763729ms till timeout)
2022-03-30 22:23:04 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (762726ms till timeout)
2022-03-30 22:23:05 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (761722ms till timeout)
2022-03-30 22:23:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:23:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (760719ms till timeout)
2022-03-30 22:23:07 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (759716ms till timeout)
2022-03-30 22:23:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (758713ms till timeout)
2022-03-30 22:23:09 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (757710ms till timeout)
2022-03-30 22:23:10 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (756706ms till timeout)
2022-03-30 22:23:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:23:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (755703ms till timeout)
2022-03-30 22:23:12 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (754700ms till timeout)
2022-03-30 22:23:13 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-bc24959a is in desired state: Ready
2022-03-30 22:23:13 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic rw-my-topic-134968587-37367908 in namespace infra-namespace
2022-03-30 22:23:13 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:rw-my-topic-134968587-37367908
2022-03-30 22:23:13 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: rw-my-topic-134968587-37367908 will have desired state: Ready
2022-03-30 22:23:13 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: rw-my-topic-134968587-37367908 will have desired state: Ready
2022-03-30 22:23:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaTopic: rw-my-topic-134968587-37367908 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 22:23:14 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:444] KafkaTopic: rw-my-topic-134968587-37367908 is in desired state: Ready
2022-03-30 22:23:14 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-bc24959a-kafka-clients in namespace infra-namespace
2022-03-30 22:23:14 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-bc24959a-kafka-clients
2022-03-30 22:23:14 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-bc24959a-kafka-clients will be ready
2022-03-30 22:23:14 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-bc24959a-kafka-clients will be ready
2022-03-30 22:23:14 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-bc24959a-kafka-clients will be ready not ready, will try again in 1000 ms (479996ms till timeout)
2022-03-30 22:23:15 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-bc24959a-kafka-clients will be ready not ready, will try again in 1000 ms (478994ms till timeout)
2022-03-30 22:23:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:23:16 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-bc24959a-kafka-clients is ready
2022-03-30 22:23:16 [ForkJoinPool-3-worker-17] [32mINFO [m [SpecificIsolatedST:308] Deploy KafkaConnect with correct rack-aware topology key: rack-key
2022-03-30 22:23:16 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-bc24959a-scraper in namespace infra-namespace
2022-03-30 22:23:16 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-bc24959a-scraper
2022-03-30 22:23:16 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-bc24959a-scraper will be ready
2022-03-30 22:23:16 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-bc24959a-scraper will be ready
2022-03-30 22:23:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-bc24959a-scraper will be ready not ready, will try again in 1000 ms (479998ms till timeout)
2022-03-30 22:23:17 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-bc24959a-scraper will be ready not ready, will try again in 1000 ms (478996ms till timeout)
2022-03-30 22:23:18 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-bc24959a-scraper is ready
2022-03-30 22:23:18 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-bc24959a-scraper to be ready
2022-03-30 22:23:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-bc24959a-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready
2022-03-30 22:23:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-bc24959a-scraper-7664c9bdc7-5fcb6 not ready: my-cluster-bc24959a-scraper)
2022-03-30 22:23:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods my-cluster-bc24959a-scraper-7664c9bdc7-5fcb6 are ready
2022-03-30 22:23:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-bc24959a-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-30 22:23:19 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-bc24959a-scraper-7664c9bdc7-5fcb6 not ready: my-cluster-bc24959a-scraper)
2022-03-30 22:23:19 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods my-cluster-bc24959a-scraper-7664c9bdc7-5fcb6 are ready
2022-03-30 22:23:19 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-bc24959a-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598993ms till timeout)
2022-03-30 22:23:20 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-bc24959a-scraper-7664c9bdc7-5fcb6 not ready: my-cluster-bc24959a-scraper)
2022-03-30 22:23:20 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods my-cluster-bc24959a-scraper-7664c9bdc7-5fcb6 are ready
2022-03-30 22:23:20 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-bc24959a-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597988ms till timeout)
2022-03-30 22:23:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:23:21 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-bc24959a-scraper-7664c9bdc7-5fcb6 not ready: my-cluster-bc24959a-scraper)
2022-03-30 22:23:21 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods my-cluster-bc24959a-scraper-7664c9bdc7-5fcb6 are ready
2022-03-30 22:23:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-bc24959a-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596984ms till timeout)
2022-03-30 22:23:22 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-bc24959a-scraper-7664c9bdc7-5fcb6 not ready: my-cluster-bc24959a-scraper)
2022-03-30 22:23:22 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods my-cluster-bc24959a-scraper-7664c9bdc7-5fcb6 are ready
2022-03-30 22:23:22 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-bc24959a-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595980ms till timeout)
2022-03-30 22:23:23 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-bc24959a-scraper-7664c9bdc7-5fcb6 not ready: my-cluster-bc24959a-scraper)
2022-03-30 22:23:23 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods my-cluster-bc24959a-scraper-7664c9bdc7-5fcb6 are ready
2022-03-30 22:23:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-bc24959a-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594976ms till timeout)
2022-03-30 22:23:24 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-bc24959a-scraper-7664c9bdc7-5fcb6 not ready: my-cluster-bc24959a-scraper)
2022-03-30 22:23:24 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods my-cluster-bc24959a-scraper-7664c9bdc7-5fcb6 are ready
2022-03-30 22:23:24 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-bc24959a-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593972ms till timeout)
2022-03-30 22:23:25 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-bc24959a-scraper-7664c9bdc7-5fcb6 not ready: my-cluster-bc24959a-scraper)
2022-03-30 22:23:25 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods my-cluster-bc24959a-scraper-7664c9bdc7-5fcb6 are ready
2022-03-30 22:23:25 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-bc24959a-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592968ms till timeout)
2022-03-30 22:23:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:23:26 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-bc24959a-scraper-7664c9bdc7-5fcb6 not ready: my-cluster-bc24959a-scraper)
2022-03-30 22:23:26 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods my-cluster-bc24959a-scraper-7664c9bdc7-5fcb6 are ready
2022-03-30 22:23:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-bc24959a-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591964ms till timeout)
2022-03-30 22:23:27 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-bc24959a-scraper-7664c9bdc7-5fcb6 not ready: my-cluster-bc24959a-scraper)
2022-03-30 22:23:27 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods my-cluster-bc24959a-scraper-7664c9bdc7-5fcb6 are ready
2022-03-30 22:23:27 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-bc24959a-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590959ms till timeout)
2022-03-30 22:23:28 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-bc24959a-scraper-7664c9bdc7-5fcb6 not ready: my-cluster-bc24959a-scraper)
2022-03-30 22:23:28 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods my-cluster-bc24959a-scraper-7664c9bdc7-5fcb6 are ready
2022-03-30 22:23:28 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-bc24959a-scraper is ready
2022-03-30 22:23:28 [ForkJoinPool-3-worker-17] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-bc24959a-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-03-30 22:23:28 [ForkJoinPool-3-worker-17] [36mDEBUG[m [NetworkPolicyResource:227] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=my-cluster-bc24959a-allow, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8083, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9404, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8080, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9999, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-bc24959a, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-bc24959a-connect}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 22:23:28 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-bc24959a-allow in namespace infra-namespace
2022-03-30 22:23:28 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:my-cluster-bc24959a-allow
2022-03-30 22:23:28 [ForkJoinPool-3-worker-17] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-03-30 22:23:28 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-bc24959a in namespace infra-namespace
2022-03-30 22:23:28 [ForkJoinPool-3-worker-17] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkaconnects' with unstable version 'v1beta2'
2022-03-30 22:23:28 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaConnect:my-cluster-bc24959a
2022-03-30 22:23:28 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-bc24959a will have desired state: Ready
2022-03-30 22:23:28 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for KafkaConnect: my-cluster-bc24959a will have desired state: Ready
2022-03-30 22:23:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 22:23:29 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (598995ms till timeout)
2022-03-30 22:23:30 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (597991ms till timeout)
2022-03-30 22:23:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:23:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (596988ms till timeout)
2022-03-30 22:23:32 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (595985ms till timeout)
2022-03-30 22:23:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (594982ms till timeout)
2022-03-30 22:23:34 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (593979ms till timeout)
2022-03-30 22:23:35 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (592975ms till timeout)
2022-03-30 22:23:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:23:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (591972ms till timeout)
2022-03-30 22:23:37 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (590968ms till timeout)
2022-03-30 22:23:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (589964ms till timeout)
2022-03-30 22:23:39 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (588960ms till timeout)
2022-03-30 22:23:40 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (587957ms till timeout)
2022-03-30 22:23:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:23:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (586953ms till timeout)
2022-03-30 22:23:42 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (585951ms till timeout)
2022-03-30 22:23:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (584947ms till timeout)
2022-03-30 22:23:44 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (583944ms till timeout)
2022-03-30 22:23:45 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (582941ms till timeout)
2022-03-30 22:23:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:23:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (581938ms till timeout)
2022-03-30 22:23:47 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (580934ms till timeout)
2022-03-30 22:23:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (579931ms till timeout)
2022-03-30 22:23:49 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (578928ms till timeout)
2022-03-30 22:23:50 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (577925ms till timeout)
2022-03-30 22:23:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:23:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (576922ms till timeout)
2022-03-30 22:23:52 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (575918ms till timeout)
2022-03-30 22:23:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (574915ms till timeout)
2022-03-30 22:23:54 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (573912ms till timeout)
2022-03-30 22:23:55 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (572909ms till timeout)
2022-03-30 22:23:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:23:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (571906ms till timeout)
2022-03-30 22:23:57 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (570903ms till timeout)
2022-03-30 22:23:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (569900ms till timeout)
2022-03-30 22:23:59 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (568897ms till timeout)
2022-03-30 22:24:00 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (567893ms till timeout)
2022-03-30 22:24:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:24:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (566888ms till timeout)
2022-03-30 22:24:02 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (565885ms till timeout)
2022-03-30 22:24:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (564882ms till timeout)
2022-03-30 22:24:04 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (563878ms till timeout)
2022-03-30 22:24:05 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (562875ms till timeout)
2022-03-30 22:24:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:24:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (561872ms till timeout)
2022-03-30 22:24:07 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (560869ms till timeout)
2022-03-30 22:24:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (559865ms till timeout)
2022-03-30 22:24:09 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (558862ms till timeout)
2022-03-30 22:24:10 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (557859ms till timeout)
2022-03-30 22:24:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:24:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (556856ms till timeout)
2022-03-30 22:24:12 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (555852ms till timeout)
2022-03-30 22:24:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (554849ms till timeout)
2022-03-30 22:24:14 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (553845ms till timeout)
2022-03-30 22:24:15 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (552842ms till timeout)
2022-03-30 22:24:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:24:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (551838ms till timeout)
2022-03-30 22:24:17 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (550835ms till timeout)
2022-03-30 22:24:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (549832ms till timeout)
2022-03-30 22:24:19 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (548829ms till timeout)
2022-03-30 22:24:20 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (547825ms till timeout)
2022-03-30 22:24:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:24:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (546822ms till timeout)
2022-03-30 22:24:22 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (545819ms till timeout)
2022-03-30 22:24:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (544816ms till timeout)
2022-03-30 22:24:24 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (543812ms till timeout)
2022-03-30 22:24:25 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (542809ms till timeout)
2022-03-30 22:24:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:24:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (541806ms till timeout)
2022-03-30 22:24:27 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (540803ms till timeout)
2022-03-30 22:24:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (539800ms till timeout)
2022-03-30 22:24:29 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (538796ms till timeout)
2022-03-30 22:24:30 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (537793ms till timeout)
2022-03-30 22:24:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:24:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (536790ms till timeout)
2022-03-30 22:24:32 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (535787ms till timeout)
2022-03-30 22:24:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (534784ms till timeout)
2022-03-30 22:24:34 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (533781ms till timeout)
2022-03-30 22:24:35 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-bc24959a will have desired state: Ready not ready, will try again in 1000 ms (532778ms till timeout)
2022-03-30 22:24:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:24:36 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-bc24959a is in desired state: Ready
2022-03-30 22:24:36 [ForkJoinPool-3-worker-17] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-bc24959a-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-03-30 22:24:36 [ForkJoinPool-3-worker-17] [36mDEBUG[m [NetworkPolicyResource:227] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=my-cluster-bc24959a-allow, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8083, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9404, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8080, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9999, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-bc24959a, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-bc24959a-connect}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 22:24:36 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-bc24959a-allow in namespace infra-namespace
2022-03-30 22:24:36 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:my-cluster-bc24959a-allow
2022-03-30 22:24:36 [ForkJoinPool-3-worker-17] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-03-30 22:24:36 [ForkJoinPool-3-worker-17] [32mINFO [m [SpecificIsolatedST:333] PodName: my-cluster-bc24959a-connect-57767484fb-847kz
NodeAffinity: NodeAffinity(preferredDuringSchedulingIgnoredDuringExecution=[], requiredDuringSchedulingIgnoredDuringExecution=NodeSelector(nodeSelectorTerms=[NodeSelectorTerm(matchExpressions=[NodeSelectorRequirement(key=rack-key, operator=Exists, values=[], additionalProperties={})], matchFields=[], additionalProperties={})], additionalProperties={}), additionalProperties={})
2022-03-30 22:24:36 [ForkJoinPool-3-worker-17] [32mINFO [m [KafkaConnectUtils:154] Send and receive messages through KafkaConnect
2022-03-30 22:24:36 [ForkJoinPool-3-worker-17] [32mINFO [m [KafkaConnectUtils:63] Waiting until KafkaConnect API is available
2022-03-30 22:24:36 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Waiting until KafkaConnect API is available
2022-03-30 22:24:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace exec my-cluster-bc24959a-connect-57767484fb-847kz -- /bin/bash -c curl -I http://localhost:8083/connectors
2022-03-30 22:24:37 [ForkJoinPool-3-worker-17] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-bc24959a-connect-57767484fb-847kz -- /bin/bash -c curl -I http://localhost:8083/connectors
2022-03-30 22:24:37 [ForkJoinPool-3-worker-17] [32mINFO [m [Exec:417] Return code: 0
2022-03-30 22:24:37 [ForkJoinPool-3-worker-17] [32mINFO [m [KafkaConnectUtils:66] KafkaConnect API is available
2022-03-30 22:24:37 [ForkJoinPool-3-worker-17] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace exec my-cluster-bc24959a-kafka-clients-65cd48676-78gbm -- /bin/bash -c curl -X POST -H "Content-Type: application/json" --data '{ "name": "sink-test", "config": { "connector.class": "FileStreamSink", "tasks.max": "1", "topics": "rw-my-topic-134968587-37367908", "file": "/tmp/test-file-sink.txt" } }' http://my-cluster-bc24959a-connect-api.infra-namespace.svc:8083/connectors
2022-03-30 22:24:37 [ForkJoinPool-3-worker-17] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-bc24959a-kafka-clients-65cd48676-78gbm -- /bin/bash -c curl -X POST -H "Content-Type: application/json" --data '{ "name": "sink-test", "config": { "connector.class": "FileStreamSink", "tasks.max": "1", "topics": "rw-my-topic-134968587-37367908", "file": "/tmp/test-file-sink.txt" } }' http://my-cluster-bc24959a-connect-api.infra-namespace.svc:8083/connectors
2022-03-30 22:24:37 [ForkJoinPool-3-worker-17] [32mINFO [m [Exec:417] Return code: 0
2022-03-30 22:24:37 [ForkJoinPool-3-worker-17] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-30 22:24:37 [ForkJoinPool-3-worker-17] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@a16825e, which are set.
2022-03-30 22:24:37 [ForkJoinPool-3-worker-17] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@1bb9f6a9, messages=[], arguments=[--topic, rw-my-topic-134968587-37367908, --bootstrap-server, my-cluster-bc24959a-kafka-bootstrap.infra-namespace.svc:9092, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-bc24959a-kafka-clients-65cd48676-78gbm', podNamespace='infra-namespace', bootstrapServer='my-cluster-bc24959a-kafka-bootstrap.infra-namespace.svc:9092', topicName='rw-my-topic-134968587-37367908', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@a16825e}
2022-03-30 22:24:37 [ForkJoinPool-3-worker-17] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-bc24959a-kafka-bootstrap.infra-namespace.svc:9092:rw-my-topic-134968587-37367908 from pod my-cluster-bc24959a-kafka-clients-65cd48676-78gbm
2022-03-30 22:24:37 [ForkJoinPool-3-worker-17] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-bc24959a-kafka-clients-65cd48676-78gbm -n infra-namespace -- /opt/kafka/producer.sh --topic rw-my-topic-134968587-37367908 --bootstrap-server my-cluster-bc24959a-kafka-bootstrap.infra-namespace.svc:9092 --max-messages 100
2022-03-30 22:24:37 [ForkJoinPool-3-worker-17] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-bc24959a-kafka-clients-65cd48676-78gbm -n infra-namespace -- /opt/kafka/producer.sh --topic rw-my-topic-134968587-37367908 --bootstrap-server my-cluster-bc24959a-kafka-bootstrap.infra-namespace.svc:9092 --max-messages 100
2022-03-30 22:24:39 [ForkJoinPool-3-worker-17] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-03-30 22:24:39 [ForkJoinPool-3-worker-17] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-03-30 22:24:39 [ForkJoinPool-3-worker-17] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@41acf3c7, which are set.
2022-03-30 22:24:39 [ForkJoinPool-3-worker-17] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@43921f0a, messages=[], arguments=[--topic, rw-my-topic-134968587-37367908, --bootstrap-server, my-cluster-bc24959a-kafka-bootstrap.infra-namespace.svc:9092, --max-messages, 100, --group-id, my-consumer-group-1047594062, --group-instance-id, instance958453836], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-bc24959a-kafka-clients-65cd48676-78gbm', podNamespace='infra-namespace', bootstrapServer='my-cluster-bc24959a-kafka-bootstrap.infra-namespace.svc:9092', topicName='rw-my-topic-134968587-37367908', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1047594062', consumerInstanceId='instance958453836', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@41acf3c7}
2022-03-30 22:24:39 [ForkJoinPool-3-worker-17] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-bc24959a-kafka-bootstrap.infra-namespace.svc:9092#rw-my-topic-134968587-37367908 from pod my-cluster-bc24959a-kafka-clients-65cd48676-78gbm
2022-03-30 22:24:39 [ForkJoinPool-3-worker-17] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-bc24959a-kafka-clients-65cd48676-78gbm -n infra-namespace -- /opt/kafka/consumer.sh --topic rw-my-topic-134968587-37367908 --bootstrap-server my-cluster-bc24959a-kafka-bootstrap.infra-namespace.svc:9092 --max-messages 100 --group-id my-consumer-group-1047594062 --group-instance-id instance958453836
2022-03-30 22:24:39 [ForkJoinPool-3-worker-17] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-bc24959a-kafka-clients-65cd48676-78gbm -n infra-namespace -- /opt/kafka/consumer.sh --topic rw-my-topic-134968587-37367908 --bootstrap-server my-cluster-bc24959a-kafka-bootstrap.infra-namespace.svc:9092 --max-messages 100 --group-id my-consumer-group-1047594062 --group-instance-id instance958453836
2022-03-30 22:24:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:24:45 [ForkJoinPool-3-worker-17] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-30 22:24:45 [ForkJoinPool-3-worker-17] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-03-30 22:24:45 [ForkJoinPool-3-worker-17] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-bc24959a-connect-57767484fb-847kz
2022-03-30 22:24:45 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for messages in file sink
2022-03-30 22:24:45 [ForkJoinPool-3-worker-17] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace exec my-cluster-bc24959a-connect-57767484fb-847kz -- /bin/bash -c cat /tmp/test-file-sink.txt
2022-03-30 22:24:45 [ForkJoinPool-3-worker-17] [30mTRACE[m [Exec:421] Command: kubectl --namespace infra-namespace exec my-cluster-bc24959a-connect-57767484fb-847kz -- /bin/bash -c cat /tmp/test-file-sink.txt
2022-03-30 22:24:45 [ForkJoinPool-3-worker-17] [30mTRACE[m [Exec:421] Return code: 0
2022-03-30 22:24:45 [ForkJoinPool-3-worker-17] [32mINFO [m [KafkaConnectUtils:77] Expected messages are in file sink on my-cluster-bc24959a-connect-57767484fb-847kz
2022-03-30 22:24:45 [ForkJoinPool-3-worker-17] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-30 22:24:45 [ForkJoinPool-3-worker-17] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-30 22:24:45 [ForkJoinPool-3-worker-17] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-30 22:24:45 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:24:45 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-30 22:24:45 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 22:24:45 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 22:24:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 22:24:45 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 22:24:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 22:24:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 22:24:45 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io not ready, will try again in 10000 ms (179993ms till timeout)
2022-03-30 22:24:45 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 22:24:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 22:24:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:24:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 22:24:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179952ms till timeout)
2022-03-30 22:24:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator not ready, will try again in 10000 ms (479960ms till timeout)
2022-03-30 22:24:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:24:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:24:55 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 22:24:55 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 22:24:55 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:24:55 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 22:24:55 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 22:24:55 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 22:24:55 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 22:24:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 22:24:55 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 22:24:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 22:24:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io not ready, will try again in 10000 ms (179981ms till timeout)
2022-03-30 22:24:55 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io not ready, will try again in 10000 ms (179983ms till timeout)
2022-03-30 22:24:55 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 22:24:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 22:24:55 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:24:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 22:24:55 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 22:24:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 22:24:55 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 22:24:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 22:24:55 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 22:24:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 22:24:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179946ms till timeout)
2022-03-30 22:24:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:25:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:25:05 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 22:25:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 22:25:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 22:25:05 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:25:05 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 22:25:05 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 22:25:05 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 22:25:05 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 22:25:05 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 22:25:05 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 22:25:05 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:25:05 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 22:25:05 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 22:25:05 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 22:25:05 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 22:25:05 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 22:25:05 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 22:25:05 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 22:25:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:25:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io not ready, will try again in 10000 ms (179892ms till timeout)
2022-03-30 22:25:06 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 22:25:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179825ms till timeout)
2022-03-30 22:25:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io not ready, will try again in 10000 ms (179955ms till timeout)
2022-03-30 22:25:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:25:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:25:16 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:25:16 [ForkJoinPool-3-worker-17] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-30 22:25:16 [ForkJoinPool-3-worker-17] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 22:25:16 [ForkJoinPool-3-worker-17] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 22:25:16 [ForkJoinPool-3-worker-17] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v109105
2022-03-30 22:25:16 [ForkJoinPool-3-worker-17] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v109105
2022-03-30 22:25:16 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=109105&allowWatchBookmarks=true&watch=true...
2022-03-30 22:25:16 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 22:25:16 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 109106
2022-03-30 22:25:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:25:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:25:21 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 109218
2022-03-30 22:25:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:25:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:25:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:25:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:25:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:25:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:25:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:25:58 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 109261
2022-03-30 22:25:58 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v109218 in namespace default
2022-03-30 22:25:58 [ForkJoinPool-3-worker-17] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@26fcd291
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-30 22:25:58 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@2fd0a235
2022-03-30 22:25:58 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@3f2e6467, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@26fcd291, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-30 22:25:58 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 22:25:58 [ForkJoinPool-3-worker-17] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-30 22:25:58 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@2bef2317
2022-03-30 22:25:58 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@2bef2317
2022-03-30 22:25:58 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@2bef2317
2022-03-30 22:25:58 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 22:25:58 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 22:25:58 [ForkJoinPool-3-worker-17] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-30 22:25:58 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-30 22:25:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 22:25:58 [ForkJoinPool-3-worker-17] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 22:25:58 [ForkJoinPool-3-worker-17] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:25:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T22:25:58Z",
        "name": "infra-namespace",
        "resourceVersion": "109262",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "6c2d0f97-1402-4c65-8769-a766a8196927"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 22:25:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-30 22:25:58 [ForkJoinPool-3-worker-17] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 22:25:58 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-30 22:25:58 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:25:58 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 22:25:58 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:25:58 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 22:25:58 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 22:25:58 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:25:58 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 22:25:58 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 22:25:58 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:25:58 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 22:25:58 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 22:25:58 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:25:58 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 22:25:58 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 22:25:58 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:25:58 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 22:25:58 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 22:25:58 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:25:58 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 22:25:59 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479998ms till timeout)
2022-03-30 22:26:00 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478995ms till timeout)
2022-03-30 22:26:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:26:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477992ms till timeout)
2022-03-30 22:26:02 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476989ms till timeout)
2022-03-30 22:26:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475986ms till timeout)
2022-03-30 22:26:04 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474983ms till timeout)
2022-03-30 22:26:05 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473980ms till timeout)
2022-03-30 22:26:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:26:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472977ms till timeout)
2022-03-30 22:26:07 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471974ms till timeout)
2022-03-30 22:26:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470971ms till timeout)
2022-03-30 22:26:09 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469968ms till timeout)
2022-03-30 22:26:10 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468965ms till timeout)
2022-03-30 22:26:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:26:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467962ms till timeout)
2022-03-30 22:26:12 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466959ms till timeout)
2022-03-30 22:26:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465956ms till timeout)
2022-03-30 22:26:14 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464952ms till timeout)
2022-03-30 22:26:15 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463950ms till timeout)
2022-03-30 22:26:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:26:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462947ms till timeout)
2022-03-30 22:26:17 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461944ms till timeout)
2022-03-30 22:26:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (460941ms till timeout)
2022-03-30 22:26:19 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (459938ms till timeout)
2022-03-30 22:26:20 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (458935ms till timeout)
2022-03-30 22:26:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:26:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (457932ms till timeout)
2022-03-30 22:26:22 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (456929ms till timeout)
2022-03-30 22:26:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (455926ms till timeout)
2022-03-30 22:26:24 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (454923ms till timeout)
2022-03-30 22:26:25 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (453920ms till timeout)
2022-03-30 22:26:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:26:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (452917ms till timeout)
2022-03-30 22:26:27 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (451914ms till timeout)
2022-03-30 22:26:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (450911ms till timeout)
2022-03-30 22:26:29 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (449908ms till timeout)
2022-03-30 22:26:30 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (448905ms till timeout)
2022-03-30 22:26:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:26:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (447902ms till timeout)
2022-03-30 22:26:32 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (446899ms till timeout)
2022-03-30 22:26:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (445896ms till timeout)
2022-03-30 22:26:34 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (444892ms till timeout)
2022-03-30 22:26:35 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (443889ms till timeout)
2022-03-30 22:26:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:26:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (442886ms till timeout)
2022-03-30 22:26:37 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-30 22:26:37 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-30 22:26:37 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-30 22:26:37 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-cc59r not ready: strimzi-cluster-operator)
2022-03-30 22:26:37 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-cc59r are ready
2022-03-30 22:26:37 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 22:26:38 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-cc59r not ready: strimzi-cluster-operator)
2022-03-30 22:26:38 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-cc59r are ready
2022-03-30 22:26:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-30 22:26:39 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-cc59r not ready: strimzi-cluster-operator)
2022-03-30 22:26:39 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-cc59r are ready
2022-03-30 22:26:39 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597991ms till timeout)
2022-03-30 22:26:40 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-cc59r not ready: strimzi-cluster-operator)
2022-03-30 22:26:40 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-cc59r are ready
2022-03-30 22:26:40 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596988ms till timeout)
2022-03-30 22:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:26:41 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-cc59r not ready: strimzi-cluster-operator)
2022-03-30 22:26:41 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-cc59r are ready
2022-03-30 22:26:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595984ms till timeout)
2022-03-30 22:26:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-cc59r not ready: strimzi-cluster-operator)
2022-03-30 22:26:42 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-cc59r are ready
2022-03-30 22:26:42 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594980ms till timeout)
2022-03-30 22:26:43 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-cc59r not ready: strimzi-cluster-operator)
2022-03-30 22:26:43 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-cc59r are ready
2022-03-30 22:26:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593977ms till timeout)
2022-03-30 22:26:44 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-cc59r not ready: strimzi-cluster-operator)
2022-03-30 22:26:44 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-cc59r are ready
2022-03-30 22:26:44 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592973ms till timeout)
2022-03-30 22:26:45 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-cc59r not ready: strimzi-cluster-operator)
2022-03-30 22:26:45 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-cc59r are ready
2022-03-30 22:26:45 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591969ms till timeout)
2022-03-30 22:26:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:26:46 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-cc59r not ready: strimzi-cluster-operator)
2022-03-30 22:26:46 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-cc59r are ready
2022-03-30 22:26:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590966ms till timeout)
2022-03-30 22:26:47 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-cc59r not ready: strimzi-cluster-operator)
2022-03-30 22:26:47 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-cc59r are ready
2022-03-30 22:26:47 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-30 22:26:47 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment strimzi-cluster-operator rolling update
2022-03-30 22:26:47 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Deployment strimzi-cluster-operator rolling update in namespace:infra-namespace
2022-03-30 22:26:47 [ForkJoinPool-3-worker-17] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {strimzi-cluster-operator-77554ffdfb-z6f2w=cee11a00-412c-4d26-aa76-5674d77a87b0}
2022-03-30 22:26:47 [ForkJoinPool-3-worker-17] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {strimzi-cluster-operator-78689684d4-cc59r=605dbe18-0964-4a5b-87a5-051f4185b0f9}
2022-03-30 22:26:47 [ForkJoinPool-3-worker-17] [36mDEBUG[m [DeploymentUtils:119] All pods seem to have rolled
2022-03-30 22:26:47 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 22:26:47 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 22:26:47 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-30 22:26:47 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-30 22:26:47 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-cc59r not ready: strimzi-cluster-operator)
2022-03-30 22:26:47 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-cc59r are ready
2022-03-30 22:26:47 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 22:26:48 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-cc59r not ready: strimzi-cluster-operator)
2022-03-30 22:26:48 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-cc59r are ready
2022-03-30 22:26:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-30 22:26:49 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-cc59r not ready: strimzi-cluster-operator)
2022-03-30 22:26:49 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-cc59r are ready
2022-03-30 22:26:49 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597991ms till timeout)
2022-03-30 22:26:50 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-cc59r not ready: strimzi-cluster-operator)
2022-03-30 22:26:50 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-cc59r are ready
2022-03-30 22:26:50 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596987ms till timeout)
2022-03-30 22:26:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:26:51 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-cc59r not ready: strimzi-cluster-operator)
2022-03-30 22:26:51 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-cc59r are ready
2022-03-30 22:26:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595983ms till timeout)
2022-03-30 22:26:52 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-cc59r not ready: strimzi-cluster-operator)
2022-03-30 22:26:52 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-cc59r are ready
2022-03-30 22:26:52 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594980ms till timeout)
2022-03-30 22:26:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-cc59r not ready: strimzi-cluster-operator)
2022-03-30 22:26:53 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-cc59r are ready
2022-03-30 22:26:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593976ms till timeout)
2022-03-30 22:26:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-cc59r not ready: strimzi-cluster-operator)
2022-03-30 22:26:54 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-cc59r are ready
2022-03-30 22:26:54 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592973ms till timeout)
2022-03-30 22:26:55 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-cc59r not ready: strimzi-cluster-operator)
2022-03-30 22:26:55 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-cc59r are ready
2022-03-30 22:26:55 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591969ms till timeout)
2022-03-30 22:26:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 22:26:56 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-cc59r not ready: strimzi-cluster-operator)
2022-03-30 22:26:56 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-cc59r are ready
2022-03-30 22:26:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590966ms till timeout)
2022-03-30 22:26:57 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-cc59r not ready: strimzi-cluster-operator)
2022-03-30 22:26:57 [ForkJoinPool-3-worker-17] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-cc59r are ready
2022-03-30 22:26:57 [ForkJoinPool-3-worker-17] [32mINFO [m [DeploymentUtils:141] Deployment strimzi-cluster-operator rolling update finished
2022-03-30 22:26:57 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 22:26:57 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractST:675] [specific.SpecificIsolatedST - After Each] - Clean up after test
2022-03-30 22:26:57 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:26:57 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:348] Delete all resources for testRackAwareConnectCorrectDeployment
2022-03-30 22:26:57 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-bc24959a-allow in namespace infra-namespace
2022-03-30 22:26:57 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic rw-my-topic-134968587-37367908 in namespace infra-namespace
2022-03-30 22:26:57 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-bc24959a-allow in namespace infra-namespace
2022-03-30 22:26:57 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:rw-my-topic-134968587-37367908
2022-03-30 22:26:57 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:my-cluster-bc24959a-allow
2022-03-30 22:26:57 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:my-cluster-bc24959a-allow
2022-03-30 22:26:57 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-bc24959a-kafka-clients in namespace infra-namespace
2022-03-30 22:26:57 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-bc24959a-scraper in namespace infra-namespace
2022-03-30 22:26:57 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-bc24959a in namespace infra-namespace
2022-03-30 22:26:57 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-bc24959a-kafka-clients
2022-03-30 22:26:57 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaConnect:my-cluster-bc24959a
2022-03-30 22:26:57 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-bc24959a in namespace infra-namespace
2022-03-30 22:26:57 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-bc24959a-scraper
2022-03-30 22:26:57 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-bc24959a
2022-03-30 22:26:57 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:26:57 [ForkJoinPool-3-worker-17] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.specific.SpecificIsolatedST.testRackAwareConnectCorrectDeployment-FINISHED
2022-03-30 22:26:57 [ForkJoinPool-3-worker-17] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 22:26:57 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 22:26:57 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractST:690] [specific.SpecificIsolatedST - After All] - Clean up after test suite
2022-03-30 22:26:57 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:26:57 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:346] In context SpecificIsolatedST is everything deleted.
2022-03-30 22:26:57 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3,054.715 s - in io.strimzi.systemtest.specific.SpecificIsolatedST
2022-03-30 22:27:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:136] Suite metrics.MetricsIsolatedST has locked the @IsolatedSuite and other @IsolatedSuites must wait until lock is released.
2022-03-30 22:27:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:27:01 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-30 22:27:01 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-30 22:27:01 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-30 22:27:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:27:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-30 22:27:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 22:27:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 22:27:01 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 22:27:01 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 22:27:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 22:27:01 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 22:27:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 22:27:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 22:27:01 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 22:27:01 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 22:27:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io not ready, will try again in 10000 ms (179987ms till timeout)
2022-03-30 22:27:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 22:27:01 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 22:27:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:27:01 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 22:27:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 22:27:01 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 22:27:01 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 22:27:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator not ready, will try again in 10000 ms (479985ms till timeout)
2022-03-30 22:27:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 22:27:01 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 22:27:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 22:27:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179959ms till timeout)
2022-03-30 22:27:01 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:27:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 22:27:01 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:27:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 22:27:01 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:27:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 22:27:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io not ready, will try again in 10000 ms (179926ms till timeout)
2022-03-30 22:27:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:27:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:27:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 22:27:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 22:27:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:27:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 22:27:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 22:27:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 22:27:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 22:27:11 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 22:27:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 22:27:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 22:27:11 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 22:27:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 22:27:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 22:27:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io not ready, will try again in 10000 ms (179988ms till timeout)
2022-03-30 22:27:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 22:27:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:27:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 22:27:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 22:27:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 22:27:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 22:27:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 22:27:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179958ms till timeout)
2022-03-30 22:27:11 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 22:27:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io not ready, will try again in 10000 ms (179920ms till timeout)
2022-03-30 22:27:11 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 22:27:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io not ready, will try again in 10000 ms (179989ms till timeout)
2022-03-30 22:27:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:27:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:27:21 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 22:27:21 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 22:27:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179835ms till timeout)
2022-03-30 22:27:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:27:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:27:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:27:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-30 22:27:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 22:27:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 22:27:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v109474
2022-03-30 22:27:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v109474
2022-03-30 22:27:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=109474&allowWatchBookmarks=true&watch=true...
2022-03-30 22:27:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 22:27:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 109475
2022-03-30 22:27:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:27:36 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 109490
2022-03-30 22:27:36 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 109491
2022-03-30 22:27:36 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v109490 in namespace default
2022-03-30 22:27:36 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@18ffce59
2022-03-30 22:27:36 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 22:27:36 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@1cc0499e
2022-03-30 22:27:36 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@1cc0499e
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@26fcd291
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace,second-metrics-cluster-test
bindingsNamespaces=[infra-namespace, second-metrics-cluster-test]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-30 22:27:36 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@1cc0499e
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@3f2e6467, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@26fcd291, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='infra-namespace,second-metrics-cluster-test', bindingsNamespaces=[infra-namespace, second-metrics-cluster-test], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-30 22:27:36 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 22:27:36 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T22:27:36Z",
        "name": "infra-namespace",
        "resourceVersion": "109492",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "cb14ce44-c26b-4981-9239-4d73b0158f20"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: second-metrics-cluster-test
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace second-metrics-cluster-test
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace second-metrics-cluster-test -o json
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace second-metrics-cluster-test -o json
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T22:27:36Z",
        "name": "second-metrics-cluster-test",
        "resourceVersion": "109496",
        "selfLink": "/api/v1/namespaces/second-metrics-cluster-test",
        "uid": "622dd557-9143-4e9d-abf9-d13790957944"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace, second-metrics-cluster-test]}
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 22:27:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=second-metrics-cluster-test, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: second-metrics-cluster-test
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace second-metrics-cluster-test
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace second-metrics-cluster-test
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 22:27:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479996ms till timeout)
2022-03-30 22:27:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478993ms till timeout)
2022-03-30 22:27:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477991ms till timeout)
2022-03-30 22:27:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476987ms till timeout)
2022-03-30 22:27:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:27:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475984ms till timeout)
2022-03-30 22:27:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474981ms till timeout)
2022-03-30 22:27:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473978ms till timeout)
2022-03-30 22:27:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472975ms till timeout)
2022-03-30 22:27:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471972ms till timeout)
2022-03-30 22:27:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:27:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470969ms till timeout)
2022-03-30 22:27:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469966ms till timeout)
2022-03-30 22:27:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468963ms till timeout)
2022-03-30 22:27:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467960ms till timeout)
2022-03-30 22:27:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466957ms till timeout)
2022-03-30 22:27:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:27:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465954ms till timeout)
2022-03-30 22:27:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464951ms till timeout)
2022-03-30 22:27:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463948ms till timeout)
2022-03-30 22:27:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462945ms till timeout)
2022-03-30 22:27:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461942ms till timeout)
2022-03-30 22:27:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:27:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (460939ms till timeout)
2022-03-30 22:27:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (459936ms till timeout)
2022-03-30 22:27:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (458933ms till timeout)
2022-03-30 22:27:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (457930ms till timeout)
2022-03-30 22:28:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (456927ms till timeout)
2022-03-30 22:28:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:28:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (455924ms till timeout)
2022-03-30 22:28:02 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-30 22:28:02 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-30 22:28:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-30 22:28:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-rcp96 not ready: strimzi-cluster-operator)
2022-03-30 22:28:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-rcp96 are ready
2022-03-30 22:28:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-30 22:28:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-rcp96 not ready: strimzi-cluster-operator)
2022-03-30 22:28:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-rcp96 are ready
2022-03-30 22:28:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-30 22:28:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-rcp96 not ready: strimzi-cluster-operator)
2022-03-30 22:28:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-rcp96 are ready
2022-03-30 22:28:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597990ms till timeout)
2022-03-30 22:28:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-rcp96 not ready: strimzi-cluster-operator)
2022-03-30 22:28:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-rcp96 are ready
2022-03-30 22:28:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596986ms till timeout)
2022-03-30 22:28:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:28:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-rcp96 not ready: strimzi-cluster-operator)
2022-03-30 22:28:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-rcp96 are ready
2022-03-30 22:28:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595983ms till timeout)
2022-03-30 22:28:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-rcp96 not ready: strimzi-cluster-operator)
2022-03-30 22:28:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-rcp96 are ready
2022-03-30 22:28:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594979ms till timeout)
2022-03-30 22:28:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-rcp96 not ready: strimzi-cluster-operator)
2022-03-30 22:28:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-rcp96 are ready
2022-03-30 22:28:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593976ms till timeout)
2022-03-30 22:28:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-rcp96 not ready: strimzi-cluster-operator)
2022-03-30 22:28:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-rcp96 are ready
2022-03-30 22:28:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592972ms till timeout)
2022-03-30 22:28:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-rcp96 not ready: strimzi-cluster-operator)
2022-03-30 22:28:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-rcp96 are ready
2022-03-30 22:28:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591968ms till timeout)
2022-03-30 22:28:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:28:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-rcp96 not ready: strimzi-cluster-operator)
2022-03-30 22:28:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-rcp96 are ready
2022-03-30 22:28:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590965ms till timeout)
2022-03-30 22:28:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-rcp96 not ready: strimzi-cluster-operator)
2022-03-30 22:28:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-rcp96 are ready
2022-03-30 22:28:12 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-30 22:28:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 22:28:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka metrics-cluster-name in namespace infra-namespace
2022-03-30 22:28:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka second-kafka-cluster in namespace second-metrics-cluster-test
2022-03-30 22:28:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment infra-namespace-kafka-clients in namespace infra-namespace
2022-03-30 22:28:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment second-metrics-cluster-test-kafka-clients in namespace second-metrics-cluster-test
2022-03-30 22:28:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:metrics-cluster-name
2022-03-30 22:28:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: metrics-cluster-name will have desired state: Ready
2022-03-30 22:28:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: metrics-cluster-name will have desired state: Ready
2022-03-30 22:28:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1799993ms till timeout)
2022-03-30 22:28:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1798990ms till timeout)
2022-03-30 22:28:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1797986ms till timeout)
2022-03-30 22:28:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1796983ms till timeout)
2022-03-30 22:28:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:28:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1795980ms till timeout)
2022-03-30 22:28:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1794976ms till timeout)
2022-03-30 22:28:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1793972ms till timeout)
2022-03-30 22:28:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1792968ms till timeout)
2022-03-30 22:28:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1791964ms till timeout)
2022-03-30 22:28:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:28:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1790961ms till timeout)
2022-03-30 22:28:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1789957ms till timeout)
2022-03-30 22:28:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1788949ms till timeout)
2022-03-30 22:28:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1787944ms till timeout)
2022-03-30 22:28:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1786941ms till timeout)
2022-03-30 22:28:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:28:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1785937ms till timeout)
2022-03-30 22:28:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1784934ms till timeout)
2022-03-30 22:28:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1783931ms till timeout)
2022-03-30 22:28:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1782927ms till timeout)
2022-03-30 22:28:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1781924ms till timeout)
2022-03-30 22:28:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:28:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1780920ms till timeout)
2022-03-30 22:28:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1779917ms till timeout)
2022-03-30 22:28:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1778913ms till timeout)
2022-03-30 22:28:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1777910ms till timeout)
2022-03-30 22:28:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1776906ms till timeout)
2022-03-30 22:28:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:28:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1775903ms till timeout)
2022-03-30 22:28:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1774900ms till timeout)
2022-03-30 22:28:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1773897ms till timeout)
2022-03-30 22:28:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1772893ms till timeout)
2022-03-30 22:28:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1771890ms till timeout)
2022-03-30 22:28:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:28:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1770887ms till timeout)
2022-03-30 22:28:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1769884ms till timeout)
2022-03-30 22:28:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1768881ms till timeout)
2022-03-30 22:28:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1767877ms till timeout)
2022-03-30 22:28:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1766874ms till timeout)
2022-03-30 22:28:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:28:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1765870ms till timeout)
2022-03-30 22:28:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1764867ms till timeout)
2022-03-30 22:28:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1763864ms till timeout)
2022-03-30 22:28:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1762861ms till timeout)
2022-03-30 22:28:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1761858ms till timeout)
2022-03-30 22:28:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:28:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1760855ms till timeout)
2022-03-30 22:28:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1759852ms till timeout)
2022-03-30 22:28:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1758848ms till timeout)
2022-03-30 22:28:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1757844ms till timeout)
2022-03-30 22:28:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1756841ms till timeout)
2022-03-30 22:28:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:28:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1755838ms till timeout)
2022-03-30 22:28:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1754834ms till timeout)
2022-03-30 22:28:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1753831ms till timeout)
2022-03-30 22:28:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1752827ms till timeout)
2022-03-30 22:29:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1751824ms till timeout)
2022-03-30 22:29:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:29:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1750820ms till timeout)
2022-03-30 22:29:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1749817ms till timeout)
2022-03-30 22:29:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1748813ms till timeout)
2022-03-30 22:29:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1747810ms till timeout)
2022-03-30 22:29:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1746806ms till timeout)
2022-03-30 22:29:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:29:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1745803ms till timeout)
2022-03-30 22:29:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1744799ms till timeout)
2022-03-30 22:29:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1743796ms till timeout)
2022-03-30 22:29:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1742792ms till timeout)
2022-03-30 22:29:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1741787ms till timeout)
2022-03-30 22:29:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:29:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1740783ms till timeout)
2022-03-30 22:29:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1739779ms till timeout)
2022-03-30 22:29:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1738775ms till timeout)
2022-03-30 22:29:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1737771ms till timeout)
2022-03-30 22:29:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1736768ms till timeout)
2022-03-30 22:29:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:29:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1735764ms till timeout)
2022-03-30 22:29:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1734761ms till timeout)
2022-03-30 22:29:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1733756ms till timeout)
2022-03-30 22:29:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1732750ms till timeout)
2022-03-30 22:29:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1731745ms till timeout)
2022-03-30 22:29:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:29:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1730740ms till timeout)
2022-03-30 22:29:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1729737ms till timeout)
2022-03-30 22:29:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1728733ms till timeout)
2022-03-30 22:29:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1727725ms till timeout)
2022-03-30 22:29:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1726721ms till timeout)
2022-03-30 22:29:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:29:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1725714ms till timeout)
2022-03-30 22:29:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1724708ms till timeout)
2022-03-30 22:29:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1723705ms till timeout)
2022-03-30 22:29:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1722702ms till timeout)
2022-03-30 22:29:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1721698ms till timeout)
2022-03-30 22:29:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:29:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1720695ms till timeout)
2022-03-30 22:29:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1719691ms till timeout)
2022-03-30 22:29:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1718688ms till timeout)
2022-03-30 22:29:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1717684ms till timeout)
2022-03-30 22:29:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1716681ms till timeout)
2022-03-30 22:29:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:29:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1715677ms till timeout)
2022-03-30 22:29:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1714674ms till timeout)
2022-03-30 22:29:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1713670ms till timeout)
2022-03-30 22:29:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1712666ms till timeout)
2022-03-30 22:29:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1711663ms till timeout)
2022-03-30 22:29:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:29:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1710659ms till timeout)
2022-03-30 22:29:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1709656ms till timeout)
2022-03-30 22:29:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1708653ms till timeout)
2022-03-30 22:29:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1707649ms till timeout)
2022-03-30 22:29:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1706646ms till timeout)
2022-03-30 22:29:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:29:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1705643ms till timeout)
2022-03-30 22:29:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1704639ms till timeout)
2022-03-30 22:29:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1703636ms till timeout)
2022-03-30 22:29:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1702631ms till timeout)
2022-03-30 22:29:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1701628ms till timeout)
2022-03-30 22:29:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:29:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1700621ms till timeout)
2022-03-30 22:29:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1699618ms till timeout)
2022-03-30 22:29:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1698614ms till timeout)
2022-03-30 22:29:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1697610ms till timeout)
2022-03-30 22:29:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1696607ms till timeout)
2022-03-30 22:29:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:29:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1695604ms till timeout)
2022-03-30 22:29:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1694600ms till timeout)
2022-03-30 22:29:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1693597ms till timeout)
2022-03-30 22:30:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1692593ms till timeout)
2022-03-30 22:30:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1691590ms till timeout)
2022-03-30 22:30:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:30:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1690586ms till timeout)
2022-03-30 22:30:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1689582ms till timeout)
2022-03-30 22:30:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1688579ms till timeout)
2022-03-30 22:30:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1687575ms till timeout)
2022-03-30 22:30:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1686572ms till timeout)
2022-03-30 22:30:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:30:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1685569ms till timeout)
2022-03-30 22:30:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1684566ms till timeout)
2022-03-30 22:30:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1683562ms till timeout)
2022-03-30 22:30:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1682558ms till timeout)
2022-03-30 22:30:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1681555ms till timeout)
2022-03-30 22:30:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:30:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1680551ms till timeout)
2022-03-30 22:30:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1679548ms till timeout)
2022-03-30 22:30:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1678545ms till timeout)
2022-03-30 22:30:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1677541ms till timeout)
2022-03-30 22:30:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1676537ms till timeout)
2022-03-30 22:30:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:30:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1675534ms till timeout)
2022-03-30 22:30:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1674530ms till timeout)
2022-03-30 22:30:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1673526ms till timeout)
2022-03-30 22:30:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1672522ms till timeout)
2022-03-30 22:30:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1671518ms till timeout)
2022-03-30 22:30:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:30:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1670515ms till timeout)
2022-03-30 22:30:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1669511ms till timeout)
2022-03-30 22:30:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1668508ms till timeout)
2022-03-30 22:30:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1667504ms till timeout)
2022-03-30 22:30:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1666501ms till timeout)
2022-03-30 22:30:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:30:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1665498ms till timeout)
2022-03-30 22:30:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1664493ms till timeout)
2022-03-30 22:30:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1663490ms till timeout)
2022-03-30 22:30:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1662486ms till timeout)
2022-03-30 22:30:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1661482ms till timeout)
2022-03-30 22:30:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:30:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1660479ms till timeout)
2022-03-30 22:30:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1659475ms till timeout)
2022-03-30 22:30:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1658472ms till timeout)
2022-03-30 22:30:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1657468ms till timeout)
2022-03-30 22:30:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1656465ms till timeout)
2022-03-30 22:30:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:30:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1655461ms till timeout)
2022-03-30 22:30:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1654458ms till timeout)
2022-03-30 22:30:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1653454ms till timeout)
2022-03-30 22:30:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1652451ms till timeout)
2022-03-30 22:30:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1651448ms till timeout)
2022-03-30 22:30:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:30:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1650444ms till timeout)
2022-03-30 22:30:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1649440ms till timeout)
2022-03-30 22:30:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1648437ms till timeout)
2022-03-30 22:30:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1647434ms till timeout)
2022-03-30 22:30:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1646430ms till timeout)
2022-03-30 22:30:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:30:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1645427ms till timeout)
2022-03-30 22:30:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1644423ms till timeout)
2022-03-30 22:30:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1643420ms till timeout)
2022-03-30 22:30:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1642416ms till timeout)
2022-03-30 22:30:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:30:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1641412ms till timeout)
2022-03-30 22:30:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1640409ms till timeout)
2022-03-30 22:30:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1639405ms till timeout)
2022-03-30 22:30:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1638402ms till timeout)
2022-03-30 22:30:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1637398ms till timeout)
2022-03-30 22:30:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:30:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1636395ms till timeout)
2022-03-30 22:30:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1635392ms till timeout)
2022-03-30 22:30:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1634388ms till timeout)
2022-03-30 22:30:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1633385ms till timeout)
2022-03-30 22:31:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1632381ms till timeout)
2022-03-30 22:31:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:31:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1631377ms till timeout)
2022-03-30 22:31:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1630374ms till timeout)
2022-03-30 22:31:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1629371ms till timeout)
2022-03-30 22:31:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1628367ms till timeout)
2022-03-30 22:31:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1627364ms till timeout)
2022-03-30 22:31:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:31:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1626361ms till timeout)
2022-03-30 22:31:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1625357ms till timeout)
2022-03-30 22:31:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1624354ms till timeout)
2022-03-30 22:31:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: metrics-cluster-name is in desired state: Ready
2022-03-30 22:31:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:second-kafka-cluster
2022-03-30 22:31:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: second-kafka-cluster will have desired state: Ready
2022-03-30 22:31:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: second-kafka-cluster will have desired state: Ready
2022-03-30 22:31:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: second-kafka-cluster is in desired state: Ready
2022-03-30 22:31:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:infra-namespace-kafka-clients
2022-03-30 22:31:09 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: infra-namespace-kafka-clients will be ready
2022-03-30 22:31:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: infra-namespace-kafka-clients will be ready
2022-03-30 22:31:09 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: infra-namespace-kafka-clients is ready
2022-03-30 22:31:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:second-metrics-cluster-test-kafka-clients
2022-03-30 22:31:09 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: second-metrics-cluster-test-kafka-clients will be ready
2022-03-30 22:31:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: second-metrics-cluster-test-kafka-clients will be ready
2022-03-30 22:31:09 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: second-metrics-cluster-test-kafka-clients is ready
2022-03-30 22:31:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge my-bridge in namespace infra-namespace
2022-03-30 22:31:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaBridge:my-bridge
2022-03-30 22:31:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: my-bridge will have desired state: Ready
2022-03-30 22:31:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaBridge: my-bridge will have desired state: Ready
2022-03-30 22:31:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (479998ms till timeout)
2022-03-30 22:31:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (478995ms till timeout)
2022-03-30 22:31:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:31:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (477993ms till timeout)
2022-03-30 22:31:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (476990ms till timeout)
2022-03-30 22:31:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (475987ms till timeout)
2022-03-30 22:31:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (474983ms till timeout)
2022-03-30 22:31:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (473980ms till timeout)
2022-03-30 22:31:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:31:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (472978ms till timeout)
2022-03-30 22:31:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (471975ms till timeout)
2022-03-30 22:31:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (470972ms till timeout)
2022-03-30 22:31:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (469968ms till timeout)
2022-03-30 22:31:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (468965ms till timeout)
2022-03-30 22:31:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:31:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (467962ms till timeout)
2022-03-30 22:31:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (466959ms till timeout)
2022-03-30 22:31:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (465956ms till timeout)
2022-03-30 22:31:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (464953ms till timeout)
2022-03-30 22:31:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (463950ms till timeout)
2022-03-30 22:31:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:31:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (462947ms till timeout)
2022-03-30 22:31:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (461944ms till timeout)
2022-03-30 22:31:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (460941ms till timeout)
2022-03-30 22:31:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (459938ms till timeout)
2022-03-30 22:31:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (458935ms till timeout)
2022-03-30 22:31:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:31:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (457932ms till timeout)
2022-03-30 22:31:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (456929ms till timeout)
2022-03-30 22:31:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (455926ms till timeout)
2022-03-30 22:31:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (454923ms till timeout)
2022-03-30 22:31:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (453919ms till timeout)
2022-03-30 22:31:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:31:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: my-bridge is in desired state: Ready
2022-03-30 22:31:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 mm2-cluster in namespace infra-namespace
2022-03-30 22:31:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaMirrorMaker2:mm2-cluster
2022-03-30 22:31:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: mm2-cluster will have desired state: Ready
2022-03-30 22:31:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaMirrorMaker2: mm2-cluster will have desired state: Ready
2022-03-30 22:31:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 22:31:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (598995ms till timeout)
2022-03-30 22:31:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (597991ms till timeout)
2022-03-30 22:31:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (596988ms till timeout)
2022-03-30 22:31:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (595984ms till timeout)
2022-03-30 22:31:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:31:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (594981ms till timeout)
2022-03-30 22:31:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (593978ms till timeout)
2022-03-30 22:31:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (592975ms till timeout)
2022-03-30 22:31:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (591971ms till timeout)
2022-03-30 22:31:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (590969ms till timeout)
2022-03-30 22:31:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:31:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (589965ms till timeout)
2022-03-30 22:31:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (588962ms till timeout)
2022-03-30 22:31:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (587959ms till timeout)
2022-03-30 22:31:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (586955ms till timeout)
2022-03-30 22:31:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (585952ms till timeout)
2022-03-30 22:31:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:31:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (584949ms till timeout)
2022-03-30 22:31:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (583946ms till timeout)
2022-03-30 22:31:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (582942ms till timeout)
2022-03-30 22:31:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (581939ms till timeout)
2022-03-30 22:31:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (580936ms till timeout)
2022-03-30 22:31:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:31:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (579932ms till timeout)
2022-03-30 22:31:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (578929ms till timeout)
2022-03-30 22:31:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (577926ms till timeout)
2022-03-30 22:31:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (576922ms till timeout)
2022-03-30 22:32:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (575919ms till timeout)
2022-03-30 22:32:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:32:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (574916ms till timeout)
2022-03-30 22:32:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (573912ms till timeout)
2022-03-30 22:32:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (572909ms till timeout)
2022-03-30 22:32:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (571905ms till timeout)
2022-03-30 22:32:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (570902ms till timeout)
2022-03-30 22:32:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:32:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (569899ms till timeout)
2022-03-30 22:32:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (568896ms till timeout)
2022-03-30 22:32:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (567893ms till timeout)
2022-03-30 22:32:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (566889ms till timeout)
2022-03-30 22:32:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (565886ms till timeout)
2022-03-30 22:32:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:32:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (564882ms till timeout)
2022-03-30 22:32:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (563880ms till timeout)
2022-03-30 22:32:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (562876ms till timeout)
2022-03-30 22:32:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (561873ms till timeout)
2022-03-30 22:32:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (560866ms till timeout)
2022-03-30 22:32:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:32:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (559863ms till timeout)
2022-03-30 22:32:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (558860ms till timeout)
2022-03-30 22:32:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (557857ms till timeout)
2022-03-30 22:32:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (556854ms till timeout)
2022-03-30 22:32:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (555850ms till timeout)
2022-03-30 22:32:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:32:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (554846ms till timeout)
2022-03-30 22:32:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (553844ms till timeout)
2022-03-30 22:32:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (552840ms till timeout)
2022-03-30 22:32:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (551837ms till timeout)
2022-03-30 22:32:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (550832ms till timeout)
2022-03-30 22:32:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:32:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (549829ms till timeout)
2022-03-30 22:32:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (548826ms till timeout)
2022-03-30 22:32:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (547823ms till timeout)
2022-03-30 22:32:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (546819ms till timeout)
2022-03-30 22:32:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (545816ms till timeout)
2022-03-30 22:32:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:32:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (544813ms till timeout)
2022-03-30 22:32:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (543809ms till timeout)
2022-03-30 22:32:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (542806ms till timeout)
2022-03-30 22:32:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (541803ms till timeout)
2022-03-30 22:32:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (540798ms till timeout)
2022-03-30 22:32:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:32:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (539795ms till timeout)
2022-03-30 22:32:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (538792ms till timeout)
2022-03-30 22:32:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (537789ms till timeout)
2022-03-30 22:32:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (536786ms till timeout)
2022-03-30 22:32:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (535782ms till timeout)
2022-03-30 22:32:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:32:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: mm2-cluster is in desired state: Ready
2022-03-30 22:32:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1427443759-776507328 in namespace infra-namespace
2022-03-30 22:32:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1427443759-776507328
2022-03-30 22:32:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1427443759-776507328 will have desired state: Ready
2022-03-30 22:32:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1427443759-776507328 will have desired state: Ready
2022-03-30 22:32:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1427443759-776507328 will have desired state: Ready not ready, will try again in 1000 ms (179999ms till timeout)
2022-03-30 22:32:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1427443759-776507328 is in desired state: Ready
2022-03-30 22:32:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1172351742-1939014984 in namespace infra-namespace
2022-03-30 22:32:42 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1172351742-1939014984
2022-03-30 22:32:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1172351742-1939014984 will have desired state: Ready
2022-03-30 22:32:42 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1172351742-1939014984 will have desired state: Ready
2022-03-30 22:32:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1172351742-1939014984 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 22:32:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1172351742-1939014984 is in desired state: Ready
2022-03-30 22:32:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-260886687-1962678096 in namespace infra-namespace
2022-03-30 22:32:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-260886687-1962678096
2022-03-30 22:32:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-260886687-1962678096 will have desired state: Ready
2022-03-30 22:32:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-260886687-1962678096 will have desired state: Ready
2022-03-30 22:32:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-260886687-1962678096 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 22:32:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-260886687-1962678096 is in desired state: Ready
2022-03-30 22:32:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-120969012-2034167634 in namespace infra-namespace
2022-03-30 22:32:44 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-120969012-2034167634
2022-03-30 22:32:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-120969012-2034167634 will have desired state: Ready
2022-03-30 22:32:44 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-120969012-2034167634 will have desired state: Ready
2022-03-30 22:32:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-120969012-2034167634 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 22:32:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-120969012-2034167634 is in desired state: Ready
2022-03-30 22:32:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-284089119-285277287 in namespace infra-namespace
2022-03-30 22:32:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-284089119-285277287
2022-03-30 22:32:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-284089119-285277287 will have desired state: Ready
2022-03-30 22:32:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-284089119-285277287 will have desired state: Ready
2022-03-30 22:32:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-284089119-285277287 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 22:32:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:32:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-284089119-285277287 is in desired state: Ready
2022-03-30 22:32:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect metrics-cluster-name in namespace infra-namespace
2022-03-30 22:32:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaConnect:metrics-cluster-name
2022-03-30 22:32:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: metrics-cluster-name will have desired state: Ready
2022-03-30 22:32:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaConnect: metrics-cluster-name will have desired state: Ready
2022-03-30 22:32:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 22:32:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (598995ms till timeout)
2022-03-30 22:32:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (597992ms till timeout)
2022-03-30 22:32:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (596989ms till timeout)
2022-03-30 22:32:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (595986ms till timeout)
2022-03-30 22:32:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:32:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (594983ms till timeout)
2022-03-30 22:32:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (593979ms till timeout)
2022-03-30 22:32:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (592976ms till timeout)
2022-03-30 22:32:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (591973ms till timeout)
2022-03-30 22:32:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (590970ms till timeout)
2022-03-30 22:32:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:32:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (589966ms till timeout)
2022-03-30 22:32:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (588963ms till timeout)
2022-03-30 22:32:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (587960ms till timeout)
2022-03-30 22:32:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (586957ms till timeout)
2022-03-30 22:33:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (585954ms till timeout)
2022-03-30 22:33:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:33:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (584951ms till timeout)
2022-03-30 22:33:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (583948ms till timeout)
2022-03-30 22:33:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (582945ms till timeout)
2022-03-30 22:33:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (581942ms till timeout)
2022-03-30 22:33:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (580938ms till timeout)
2022-03-30 22:33:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:33:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (579935ms till timeout)
2022-03-30 22:33:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (578932ms till timeout)
2022-03-30 22:33:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (577929ms till timeout)
2022-03-30 22:33:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (576925ms till timeout)
2022-03-30 22:33:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (575922ms till timeout)
2022-03-30 22:33:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:33:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (574919ms till timeout)
2022-03-30 22:33:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (573916ms till timeout)
2022-03-30 22:33:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (572913ms till timeout)
2022-03-30 22:33:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (571910ms till timeout)
2022-03-30 22:33:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (570907ms till timeout)
2022-03-30 22:33:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:33:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (569904ms till timeout)
2022-03-30 22:33:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (568901ms till timeout)
2022-03-30 22:33:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (567898ms till timeout)
2022-03-30 22:33:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (566895ms till timeout)
2022-03-30 22:33:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (565892ms till timeout)
2022-03-30 22:33:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:33:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (564888ms till timeout)
2022-03-30 22:33:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (563885ms till timeout)
2022-03-30 22:33:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (562882ms till timeout)
2022-03-30 22:33:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (561879ms till timeout)
2022-03-30 22:33:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (560876ms till timeout)
2022-03-30 22:33:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:33:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (559872ms till timeout)
2022-03-30 22:33:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (558869ms till timeout)
2022-03-30 22:33:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (557866ms till timeout)
2022-03-30 22:33:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (556863ms till timeout)
2022-03-30 22:33:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (555859ms till timeout)
2022-03-30 22:33:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:33:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (554856ms till timeout)
2022-03-30 22:33:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (553853ms till timeout)
2022-03-30 22:33:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (552850ms till timeout)
2022-03-30 22:33:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (551847ms till timeout)
2022-03-30 22:33:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (550844ms till timeout)
2022-03-30 22:33:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:33:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (549840ms till timeout)
2022-03-30 22:33:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (548837ms till timeout)
2022-03-30 22:33:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (547834ms till timeout)
2022-03-30 22:33:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (546831ms till timeout)
2022-03-30 22:33:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (545828ms till timeout)
2022-03-30 22:33:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:33:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (544825ms till timeout)
2022-03-30 22:33:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (543822ms till timeout)
2022-03-30 22:33:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (542818ms till timeout)
2022-03-30 22:33:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (541815ms till timeout)
2022-03-30 22:33:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (540813ms till timeout)
2022-03-30 22:33:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:33:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (539810ms till timeout)
2022-03-30 22:33:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (538806ms till timeout)
2022-03-30 22:33:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: metrics-cluster-name is in desired state: Ready
2022-03-30 22:33:49 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:72] Apply NetworkPolicy access to cluster-operator from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={})
2022-03-30 22:33:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyResource:88] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=cluster-operator-allow, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8080, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/kind=cluster-operator}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 22:33:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy cluster-operator-allow in namespace infra-namespace
2022-03-30 22:33:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:cluster-operator-allow
2022-03-30 22:33:49 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:90] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}) successfully created
2022-03-30 22:33:49 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:104] Apply NetworkPolicy access to metrics-cluster-name-entity-operator from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={})
2022-03-30 22:33:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyResource:126] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=metrics-cluster-name-entity-operator-allow, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8080, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8081, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-entity-operator}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 22:33:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy metrics-cluster-name-entity-operator-allow in namespace infra-namespace
2022-03-30 22:33:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:metrics-cluster-name-entity-operator-allow
2022-03-30 22:33:49 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:128] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}) successfully created
2022-03-30 22:33:49 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:104] Apply NetworkPolicy access to second-kafka-cluster-entity-operator from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={})
2022-03-30 22:33:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyResource:126] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=second-kafka-cluster-entity-operator-allow, namespace=second-metrics-cluster-test, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8080, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8081, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=second-kafka-cluster, strimzi.io/kind=Kafka, strimzi.io/name=second-kafka-cluster-entity-operator}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 22:33:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy second-kafka-cluster-entity-operator-allow in namespace second-metrics-cluster-test
2022-03-30 22:33:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:second-kafka-cluster-entity-operator-allow
2022-03-30 22:33:49 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:128] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}) successfully created
2022-03-30 22:33:49 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:141] Apply NetworkPolicy access to metrics-cluster-name-kafka-exporter from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={})
2022-03-30 22:33:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyResource:159] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=metrics-cluster-name-kafka-exporter-allow, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9404, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 22:33:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy metrics-cluster-name-kafka-exporter-allow in namespace infra-namespace
2022-03-30 22:33:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:metrics-cluster-name-kafka-exporter-allow
2022-03-30 22:33:49 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:161] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}) successfully created
2022-03-30 22:33:49 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:141] Apply NetworkPolicy access to second-kafka-cluster-kafka-exporter from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={})
2022-03-30 22:33:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyResource:159] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=second-kafka-cluster-kafka-exporter-allow, namespace=second-metrics-cluster-test, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9404, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=second-kafka-cluster, strimzi.io/kind=Kafka, strimzi.io/name=second-kafka-cluster-kafka-exporter}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 22:33:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy second-kafka-cluster-kafka-exporter-allow in namespace second-metrics-cluster-test
2022-03-30 22:33:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:second-kafka-cluster-kafka-exporter-allow
2022-03-30 22:33:49 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:161] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}) successfully created
2022-03-30 22:33:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:33:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:34:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:34:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:34:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:34:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:34:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:34:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:34:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:34:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:34:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:34:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:34:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:34:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:35:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:35:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:35:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 22:35:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-8r49n -n infra-namespace -- curl 172.17.0.15:9404
2022-03-30 22:35:11 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.15 from Pod infra-namespace-kafka-clients-748578f786-8r49n finished with return code: 0
2022-03-30 22:35:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-8r49n -n infra-namespace -- curl 172.17.0.17:9404
2022-03-30 22:35:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:35:13 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.17 from Pod infra-namespace-kafka-clients-748578f786-8r49n finished with return code: 0
2022-03-30 22:35:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-8r49n -n infra-namespace -- curl 172.17.0.16:9404
2022-03-30 22:35:15 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.16 from Pod infra-namespace-kafka-clients-748578f786-8r49n finished with return code: 0
2022-03-30 22:35:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 22:35:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-8r49n -n infra-namespace -- curl 172.17.0.11:9404
2022-03-30 22:35:15 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.11 from Pod infra-namespace-kafka-clients-748578f786-8r49n finished with return code: 0
2022-03-30 22:35:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-8r49n -n infra-namespace -- curl 172.17.0.9:9404
2022-03-30 22:35:16 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.9 from Pod infra-namespace-kafka-clients-748578f786-8r49n finished with return code: 0
2022-03-30 22:35:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-8r49n -n infra-namespace -- curl 172.17.0.10:9404
2022-03-30 22:35:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:35:16 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.10 from Pod infra-namespace-kafka-clients-748578f786-8r49n finished with return code: 0
2022-03-30 22:35:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 22:35:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-8r49n -n infra-namespace -- curl 172.17.0.24:9404
2022-03-30 22:35:17 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.24 from Pod infra-namespace-kafka-clients-748578f786-8r49n finished with return code: 0
2022-03-30 22:35:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 22:35:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-8r49n -n infra-namespace -- curl 172.17.0.21:9404/metrics
2022-03-30 22:35:17 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.21 from Pod infra-namespace-kafka-clients-748578f786-8r49n finished with return code: 0
2022-03-30 22:35:17 [ForkJoinPool-3-worker-17] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 22:35:17 [ForkJoinPool-3-worker-17] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testUserOperatorMetrics-STARTED
2022-03-30 22:35:17 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 22:35:17 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 22:35:17 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testUserOperatorMetrics
2022-03-30 22:35:17 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 1
2022-03-30 22:35:17 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:230] testUserOperatorMetrics test now can proceed its execution
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 22:35:17 [ForkJoinPool-3-worker-17] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testZookeeperWatchersCount-STARTED
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaBrokersCount-STARTED
2022-03-30 22:35:17 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 22:35:17 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaConnectIoNetwork-STARTED
2022-03-30 22:35:17 [ForkJoinPool-3-worker-17] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070, testSendSimpleMessageTls=my-cluster-b70e5342, testUpdateUser=my-cluster-9b2bd187, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349, testSendMessagesTlsScramSha=my-cluster-380e6586, testReceiveSimpleMessageTls=my-cluster-e8a518cd, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda, testUserOperatorMetrics=my-cluster-0ae7d432, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-17] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1001559761-691587912, testSendSimpleMessageTls=my-user-602433797-900694548, testUpdateUser=my-user-595127408-1604948726, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1606351157-1815623087, testKafkaInDifferentNsThanClusterOperator=my-user-1518021213-326071957, testSendMessagesCustomListenerTlsScramSha=my-user-445988529-51741427, testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1590558165-46977776, testSendMessagesTlsScramSha=my-user-2039401547-533835955, testReceiveSimpleMessageTls=my-user-761922164-2019863647, testMirrorMaker2TlsAndTlsClientAuth=my-user-707354298-1518057957, testUserOperatorMetrics=my-user-366296042-1312500013, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659, testRackAwareConnectCorrectDeployment=my-user-389825219-1608762022, testMirrorMakerTlsAuthenticated=my-user-692603309-1711649825}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-17] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-741363275-285355018, testSendSimpleMessageTls=my-topic-1299629356-597168683, testUpdateUser=my-topic-1875736194-1340884809, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1931531910-629202741, testKafkaInDifferentNsThanClusterOperator=my-topic-2034182350-493335345, testSendMessagesCustomListenerTlsScramSha=my-topic-1039904883-1044305687, testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1210910585-1669754462, testSendMessagesTlsScramSha=my-topic-988094002-1400465950, testReceiveSimpleMessageTls=my-topic-1007790874-1032272408, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1279769671-1440257906, testUserOperatorMetrics=my-topic-1504927319-1084001771, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393, testRackAwareConnectCorrectDeployment=my-topic-1801184984-1468632754, testMirrorMakerTlsAuthenticated=my-topic-1945138991-1836415682}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-17] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070-kafka-clients, testSendSimpleMessageTls=my-cluster-b70e5342-kafka-clients, testUpdateUser=my-cluster-9b2bd187-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349-kafka-clients, testSendMessagesTlsScramSha=my-cluster-380e6586-kafka-clients, testReceiveSimpleMessageTls=my-cluster-e8a518cd-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda-kafka-clients, testUserOperatorMetrics=my-cluster-0ae7d432-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9-kafka-clients}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 22:35:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaBrokersCount
2022-03-30 22:35:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 2
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:230] testKafkaBrokersCount test now can proceed its execution
2022-03-30 22:35:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaConnectIoNetwork
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 22:35:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 3
2022-03-30 22:35:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:230] testKafkaConnectIoNetwork test now can proceed its execution
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070, testSendSimpleMessageTls=my-cluster-b70e5342, testUpdateUser=my-cluster-9b2bd187, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349, testSendMessagesTlsScramSha=my-cluster-380e6586, testReceiveSimpleMessageTls=my-cluster-e8a518cd, testKafkaBrokersCount=my-cluster-b29913ee, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda, testUserOperatorMetrics=my-cluster-0ae7d432, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1001559761-691587912, testSendSimpleMessageTls=my-user-602433797-900694548, testUpdateUser=my-user-595127408-1604948726, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1606351157-1815623087, testKafkaInDifferentNsThanClusterOperator=my-user-1518021213-326071957, testSendMessagesCustomListenerTlsScramSha=my-user-445988529-51741427, testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1590558165-46977776, testSendMessagesTlsScramSha=my-user-2039401547-533835955, testReceiveSimpleMessageTls=my-user-761922164-2019863647, testKafkaBrokersCount=my-user-161347164-197258481, testMirrorMaker2TlsAndTlsClientAuth=my-user-707354298-1518057957, testUserOperatorMetrics=my-user-366296042-1312500013, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659, testRackAwareConnectCorrectDeployment=my-user-389825219-1608762022, testMirrorMakerTlsAuthenticated=my-user-692603309-1711649825}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-741363275-285355018, testSendSimpleMessageTls=my-topic-1299629356-597168683, testUpdateUser=my-topic-1875736194-1340884809, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1931531910-629202741, testKafkaInDifferentNsThanClusterOperator=my-topic-2034182350-493335345, testSendMessagesCustomListenerTlsScramSha=my-topic-1039904883-1044305687, testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1210910585-1669754462, testSendMessagesTlsScramSha=my-topic-988094002-1400465950, testReceiveSimpleMessageTls=my-topic-1007790874-1032272408, testKafkaBrokersCount=my-topic-480196974-662544545, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1279769671-1440257906, testUserOperatorMetrics=my-topic-1504927319-1084001771, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393, testRackAwareConnectCorrectDeployment=my-topic-1801184984-1468632754, testMirrorMakerTlsAuthenticated=my-topic-1945138991-1836415682}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testZookeeperWatchersCount
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070-kafka-clients, testSendSimpleMessageTls=my-cluster-b70e5342-kafka-clients, testUpdateUser=my-cluster-9b2bd187-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349-kafka-clients, testSendMessagesTlsScramSha=my-cluster-380e6586-kafka-clients, testReceiveSimpleMessageTls=my-cluster-e8a518cd-kafka-clients, testKafkaBrokersCount=my-cluster-b29913ee-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda-kafka-clients, testUserOperatorMetrics=my-cluster-0ae7d432-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9-kafka-clients}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 4
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:230] testZookeeperWatchersCount test now can proceed its execution
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070, testSendSimpleMessageTls=my-cluster-b70e5342, testUpdateUser=my-cluster-9b2bd187, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testZookeeperWatchersCount=my-cluster-402c5b37, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349, testSendMessagesTlsScramSha=my-cluster-380e6586, testReceiveSimpleMessageTls=my-cluster-e8a518cd, testKafkaBrokersCount=my-cluster-b29913ee, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda, testUserOperatorMetrics=my-cluster-0ae7d432, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1001559761-691587912, testSendSimpleMessageTls=my-user-602433797-900694548, testUpdateUser=my-user-595127408-1604948726, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1606351157-1815623087, testKafkaInDifferentNsThanClusterOperator=my-user-1518021213-326071957, testSendMessagesCustomListenerTlsScramSha=my-user-445988529-51741427, testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testZookeeperWatchersCount=my-user-262847981-879141545, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1590558165-46977776, testSendMessagesTlsScramSha=my-user-2039401547-533835955, testReceiveSimpleMessageTls=my-user-761922164-2019863647, testKafkaBrokersCount=my-user-161347164-197258481, testMirrorMaker2TlsAndTlsClientAuth=my-user-707354298-1518057957, testUserOperatorMetrics=my-user-366296042-1312500013, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659, testRackAwareConnectCorrectDeployment=my-user-389825219-1608762022, testMirrorMakerTlsAuthenticated=my-user-692603309-1711649825}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-741363275-285355018, testSendSimpleMessageTls=my-topic-1299629356-597168683, testUpdateUser=my-topic-1875736194-1340884809, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1931531910-629202741, testKafkaInDifferentNsThanClusterOperator=my-topic-2034182350-493335345, testSendMessagesCustomListenerTlsScramSha=my-topic-1039904883-1044305687, testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testZookeeperWatchersCount=my-topic-464196692-1269638121, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1210910585-1669754462, testSendMessagesTlsScramSha=my-topic-988094002-1400465950, testReceiveSimpleMessageTls=my-topic-1007790874-1032272408, testKafkaBrokersCount=my-topic-480196974-662544545, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1279769671-1440257906, testUserOperatorMetrics=my-topic-1504927319-1084001771, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393, testRackAwareConnectCorrectDeployment=my-topic-1801184984-1468632754, testMirrorMakerTlsAuthenticated=my-topic-1945138991-1836415682}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070-kafka-clients, testSendSimpleMessageTls=my-cluster-b70e5342-kafka-clients, testUpdateUser=my-cluster-9b2bd187-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testZookeeperWatchersCount=my-cluster-402c5b37-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349-kafka-clients, testSendMessagesTlsScramSha=my-cluster-380e6586-kafka-clients, testReceiveSimpleMessageTls=my-cluster-e8a518cd-kafka-clients, testKafkaBrokersCount=my-cluster-b29913ee-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda-kafka-clients, testUserOperatorMetrics=my-cluster-0ae7d432-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9-kafka-clients}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:346] In context testZookeeperWatchersCount is everything deleted.
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:267] testZookeeperWatchersCount - Notifies waiting test cases:[testCruiseControlBasicAPIRequests, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testUpdateUser, testSendSimpleMessageTls, testAutoRenewAllCaCertsTriggeredByAnno, testReceiveSimpleMessageTls, testSendMessagesCustomListenerTlsScramSha, testKafkaAndZookeeperScaleUpScaleDown, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testUserOperatorMetrics, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperWatchersCount] to and randomly select one to start execution
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testZookeeperWatchersCount
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 3
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testZookeeperWatchersCount-FINISHED
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 22:35:17 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 22:35:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070, testSendSimpleMessageTls=my-cluster-b70e5342, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7, testUpdateUser=my-cluster-9b2bd187, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testZookeeperWatchersCount=my-cluster-402c5b37, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349, testSendMessagesTlsScramSha=my-cluster-380e6586, testReceiveSimpleMessageTls=my-cluster-e8a518cd, testKafkaBrokersCount=my-cluster-b29913ee, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda, testUserOperatorMetrics=my-cluster-0ae7d432, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1001559761-691587912, testSendSimpleMessageTls=my-user-602433797-900694548, testKafkaConnectIoNetwork=my-user-1415252823-246514724, testUpdateUser=my-user-595127408-1604948726, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1606351157-1815623087, testKafkaInDifferentNsThanClusterOperator=my-user-1518021213-326071957, testSendMessagesCustomListenerTlsScramSha=my-user-445988529-51741427, testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testZookeeperWatchersCount=my-user-262847981-879141545, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1590558165-46977776, testSendMessagesTlsScramSha=my-user-2039401547-533835955, testReceiveSimpleMessageTls=my-user-761922164-2019863647, testKafkaBrokersCount=my-user-161347164-197258481, testMirrorMaker2TlsAndTlsClientAuth=my-user-707354298-1518057957, testUserOperatorMetrics=my-user-366296042-1312500013, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659, testRackAwareConnectCorrectDeployment=my-user-389825219-1608762022, testMirrorMakerTlsAuthenticated=my-user-692603309-1711649825}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-741363275-285355018, testSendSimpleMessageTls=my-topic-1299629356-597168683, testKafkaConnectIoNetwork=my-topic-1319353181-1427134540, testUpdateUser=my-topic-1875736194-1340884809, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1931531910-629202741, testKafkaInDifferentNsThanClusterOperator=my-topic-2034182350-493335345, testSendMessagesCustomListenerTlsScramSha=my-topic-1039904883-1044305687, testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testZookeeperWatchersCount=my-topic-464196692-1269638121, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1210910585-1669754462, testSendMessagesTlsScramSha=my-topic-988094002-1400465950, testReceiveSimpleMessageTls=my-topic-1007790874-1032272408, testKafkaBrokersCount=my-topic-480196974-662544545, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1279769671-1440257906, testUserOperatorMetrics=my-topic-1504927319-1084001771, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393, testRackAwareConnectCorrectDeployment=my-topic-1801184984-1468632754, testMirrorMakerTlsAuthenticated=my-topic-1945138991-1836415682}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070-kafka-clients, testSendSimpleMessageTls=my-cluster-b70e5342-kafka-clients, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7-kafka-clients, testUpdateUser=my-cluster-9b2bd187-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testZookeeperWatchersCount=my-cluster-402c5b37-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349-kafka-clients, testSendMessagesTlsScramSha=my-cluster-380e6586-kafka-clients, testReceiveSimpleMessageTls=my-cluster-e8a518cd-kafka-clients, testKafkaBrokersCount=my-cluster-b29913ee-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda-kafka-clients, testUserOperatorMetrics=my-cluster-0ae7d432-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9-kafka-clients}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaActiveControllers-STARTED
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaActiveControllers
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 4
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:230] testKafkaActiveControllers test now can proceed its execution
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070, testSendSimpleMessageTls=my-cluster-b70e5342, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7, testUpdateUser=my-cluster-9b2bd187, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testKafkaActiveControllers=my-cluster-8ce86250, testZookeeperWatchersCount=my-cluster-402c5b37, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349, testSendMessagesTlsScramSha=my-cluster-380e6586, testReceiveSimpleMessageTls=my-cluster-e8a518cd, testKafkaBrokersCount=my-cluster-b29913ee, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda, testUserOperatorMetrics=my-cluster-0ae7d432, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1001559761-691587912, testSendSimpleMessageTls=my-user-602433797-900694548, testKafkaConnectIoNetwork=my-user-1415252823-246514724, testUpdateUser=my-user-595127408-1604948726, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1606351157-1815623087, testKafkaInDifferentNsThanClusterOperator=my-user-1518021213-326071957, testSendMessagesCustomListenerTlsScramSha=my-user-445988529-51741427, testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testKafkaActiveControllers=my-user-1625454548-1904690658, testZookeeperWatchersCount=my-user-262847981-879141545, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1590558165-46977776, testSendMessagesTlsScramSha=my-user-2039401547-533835955, testReceiveSimpleMessageTls=my-user-761922164-2019863647, testKafkaBrokersCount=my-user-161347164-197258481, testMirrorMaker2TlsAndTlsClientAuth=my-user-707354298-1518057957, testUserOperatorMetrics=my-user-366296042-1312500013, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659, testRackAwareConnectCorrectDeployment=my-user-389825219-1608762022, testMirrorMakerTlsAuthenticated=my-user-692603309-1711649825}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-741363275-285355018, testSendSimpleMessageTls=my-topic-1299629356-597168683, testKafkaConnectIoNetwork=my-topic-1319353181-1427134540, testUpdateUser=my-topic-1875736194-1340884809, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1931531910-629202741, testKafkaInDifferentNsThanClusterOperator=my-topic-2034182350-493335345, testSendMessagesCustomListenerTlsScramSha=my-topic-1039904883-1044305687, testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testKafkaActiveControllers=my-topic-1892159362-1633250172, testZookeeperWatchersCount=my-topic-464196692-1269638121, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1210910585-1669754462, testSendMessagesTlsScramSha=my-topic-988094002-1400465950, testReceiveSimpleMessageTls=my-topic-1007790874-1032272408, testKafkaBrokersCount=my-topic-480196974-662544545, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1279769671-1440257906, testUserOperatorMetrics=my-topic-1504927319-1084001771, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393, testRackAwareConnectCorrectDeployment=my-topic-1801184984-1468632754, testMirrorMakerTlsAuthenticated=my-topic-1945138991-1836415682}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070-kafka-clients, testSendSimpleMessageTls=my-cluster-b70e5342-kafka-clients, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7-kafka-clients, testUpdateUser=my-cluster-9b2bd187-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testKafkaActiveControllers=my-cluster-8ce86250-kafka-clients, testZookeeperWatchersCount=my-cluster-402c5b37-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349-kafka-clients, testSendMessagesTlsScramSha=my-cluster-380e6586-kafka-clients, testReceiveSimpleMessageTls=my-cluster-e8a518cd-kafka-clients, testKafkaBrokersCount=my-cluster-b29913ee-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda-kafka-clients, testUserOperatorMetrics=my-cluster-0ae7d432-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9-kafka-clients}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:346] In context testKafkaBrokersCount is everything deleted.
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:267] testKafkaBrokersCount - Notifies waiting test cases:[testCruiseControlBasicAPIRequests, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testUpdateUser, testSendSimpleMessageTls, testAutoRenewAllCaCertsTriggeredByAnno, testReceiveSimpleMessageTls, testSendMessagesCustomListenerTlsScramSha, testKafkaAndZookeeperScaleUpScaleDown, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testUserOperatorMetrics, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperWatchersCount, testKafkaActiveControllers] to and randomly select one to start execution
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaBrokersCount
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 3
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaBrokersCount-FINISHED
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaTopicUnderReplicatedPartitions-STARTED
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaTopicUnderReplicatedPartitions
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 4
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:230] testKafkaTopicUnderReplicatedPartitions test now can proceed its execution
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070, testSendSimpleMessageTls=my-cluster-b70e5342, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7, testUpdateUser=my-cluster-9b2bd187, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testKafkaActiveControllers=my-cluster-8ce86250, testZookeeperWatchersCount=my-cluster-402c5b37, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349, testKafkaTopicUnderReplicatedPartitions=my-cluster-be9a0596, testSendMessagesTlsScramSha=my-cluster-380e6586, testReceiveSimpleMessageTls=my-cluster-e8a518cd, testKafkaBrokersCount=my-cluster-b29913ee, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda, testUserOperatorMetrics=my-cluster-0ae7d432, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1001559761-691587912, testSendSimpleMessageTls=my-user-602433797-900694548, testKafkaConnectIoNetwork=my-user-1415252823-246514724, testUpdateUser=my-user-595127408-1604948726, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1606351157-1815623087, testKafkaInDifferentNsThanClusterOperator=my-user-1518021213-326071957, testSendMessagesCustomListenerTlsScramSha=my-user-445988529-51741427, testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testKafkaActiveControllers=my-user-1625454548-1904690658, testZookeeperWatchersCount=my-user-262847981-879141545, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1590558165-46977776, testKafkaTopicUnderReplicatedPartitions=my-user-2067443766-1667934716, testSendMessagesTlsScramSha=my-user-2039401547-533835955, testReceiveSimpleMessageTls=my-user-761922164-2019863647, testKafkaBrokersCount=my-user-161347164-197258481, testMirrorMaker2TlsAndTlsClientAuth=my-user-707354298-1518057957, testUserOperatorMetrics=my-user-366296042-1312500013, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659, testRackAwareConnectCorrectDeployment=my-user-389825219-1608762022, testMirrorMakerTlsAuthenticated=my-user-692603309-1711649825}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-741363275-285355018, testSendSimpleMessageTls=my-topic-1299629356-597168683, testKafkaConnectIoNetwork=my-topic-1319353181-1427134540, testUpdateUser=my-topic-1875736194-1340884809, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1931531910-629202741, testKafkaInDifferentNsThanClusterOperator=my-topic-2034182350-493335345, testSendMessagesCustomListenerTlsScramSha=my-topic-1039904883-1044305687, testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testKafkaActiveControllers=my-topic-1892159362-1633250172, testZookeeperWatchersCount=my-topic-464196692-1269638121, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1210910585-1669754462, testKafkaTopicUnderReplicatedPartitions=my-topic-1554151013-1160196250, testSendMessagesTlsScramSha=my-topic-988094002-1400465950, testReceiveSimpleMessageTls=my-topic-1007790874-1032272408, testKafkaBrokersCount=my-topic-480196974-662544545, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1279769671-1440257906, testUserOperatorMetrics=my-topic-1504927319-1084001771, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393, testRackAwareConnectCorrectDeployment=my-topic-1801184984-1468632754, testMirrorMakerTlsAuthenticated=my-topic-1945138991-1836415682}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070-kafka-clients, testSendSimpleMessageTls=my-cluster-b70e5342-kafka-clients, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7-kafka-clients, testUpdateUser=my-cluster-9b2bd187-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testKafkaActiveControllers=my-cluster-8ce86250-kafka-clients, testZookeeperWatchersCount=my-cluster-402c5b37-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-be9a0596-kafka-clients, testSendMessagesTlsScramSha=my-cluster-380e6586-kafka-clients, testReceiveSimpleMessageTls=my-cluster-e8a518cd-kafka-clients, testKafkaBrokersCount=my-cluster-b29913ee-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda-kafka-clients, testUserOperatorMetrics=my-cluster-0ae7d432-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9-kafka-clients}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:346] In context testKafkaActiveControllers is everything deleted.
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:267] testKafkaActiveControllers - Notifies waiting test cases:[testCruiseControlBasicAPIRequests, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testUpdateUser, testSendSimpleMessageTls, testAutoRenewAllCaCertsTriggeredByAnno, testReceiveSimpleMessageTls, testSendMessagesCustomListenerTlsScramSha, testKafkaAndZookeeperScaleUpScaleDown, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testUserOperatorMetrics, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperWatchersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions] to and randomly select one to start execution
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaActiveControllers
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 3
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaActiveControllers-FINISHED
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testZookeeperQuorumSize-STARTED
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testZookeeperQuorumSize
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 4
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:230] testZookeeperQuorumSize test now can proceed its execution
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testZookeeperQuorumSize=my-cluster-6e3782cf, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070, testSendSimpleMessageTls=my-cluster-b70e5342, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7, testUpdateUser=my-cluster-9b2bd187, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testKafkaActiveControllers=my-cluster-8ce86250, testZookeeperWatchersCount=my-cluster-402c5b37, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349, testKafkaTopicUnderReplicatedPartitions=my-cluster-be9a0596, testSendMessagesTlsScramSha=my-cluster-380e6586, testReceiveSimpleMessageTls=my-cluster-e8a518cd, testKafkaBrokersCount=my-cluster-b29913ee, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda, testUserOperatorMetrics=my-cluster-0ae7d432, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testZookeeperQuorumSize=my-user-625293736-680311428, testKafkaAndZookeeperScaleUpScaleDown=my-user-1001559761-691587912, testSendSimpleMessageTls=my-user-602433797-900694548, testKafkaConnectIoNetwork=my-user-1415252823-246514724, testUpdateUser=my-user-595127408-1604948726, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1606351157-1815623087, testKafkaInDifferentNsThanClusterOperator=my-user-1518021213-326071957, testSendMessagesCustomListenerTlsScramSha=my-user-445988529-51741427, testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testKafkaActiveControllers=my-user-1625454548-1904690658, testZookeeperWatchersCount=my-user-262847981-879141545, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1590558165-46977776, testKafkaTopicUnderReplicatedPartitions=my-user-2067443766-1667934716, testSendMessagesTlsScramSha=my-user-2039401547-533835955, testReceiveSimpleMessageTls=my-user-761922164-2019863647, testKafkaBrokersCount=my-user-161347164-197258481, testMirrorMaker2TlsAndTlsClientAuth=my-user-707354298-1518057957, testUserOperatorMetrics=my-user-366296042-1312500013, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659, testRackAwareConnectCorrectDeployment=my-user-389825219-1608762022, testMirrorMakerTlsAuthenticated=my-user-692603309-1711649825}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testZookeeperQuorumSize=my-topic-2058761647-1946180799, testKafkaAndZookeeperScaleUpScaleDown=my-topic-741363275-285355018, testSendSimpleMessageTls=my-topic-1299629356-597168683, testKafkaConnectIoNetwork=my-topic-1319353181-1427134540, testUpdateUser=my-topic-1875736194-1340884809, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1931531910-629202741, testKafkaInDifferentNsThanClusterOperator=my-topic-2034182350-493335345, testSendMessagesCustomListenerTlsScramSha=my-topic-1039904883-1044305687, testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testKafkaActiveControllers=my-topic-1892159362-1633250172, testZookeeperWatchersCount=my-topic-464196692-1269638121, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1210910585-1669754462, testKafkaTopicUnderReplicatedPartitions=my-topic-1554151013-1160196250, testSendMessagesTlsScramSha=my-topic-988094002-1400465950, testReceiveSimpleMessageTls=my-topic-1007790874-1032272408, testKafkaBrokersCount=my-topic-480196974-662544545, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1279769671-1440257906, testUserOperatorMetrics=my-topic-1504927319-1084001771, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393, testRackAwareConnectCorrectDeployment=my-topic-1801184984-1468632754, testMirrorMakerTlsAuthenticated=my-topic-1945138991-1836415682}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testZookeeperQuorumSize=my-cluster-6e3782cf-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070-kafka-clients, testSendSimpleMessageTls=my-cluster-b70e5342-kafka-clients, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7-kafka-clients, testUpdateUser=my-cluster-9b2bd187-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testKafkaActiveControllers=my-cluster-8ce86250-kafka-clients, testZookeeperWatchersCount=my-cluster-402c5b37-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-be9a0596-kafka-clients, testSendMessagesTlsScramSha=my-cluster-380e6586-kafka-clients, testReceiveSimpleMessageTls=my-cluster-e8a518cd-kafka-clients, testKafkaBrokersCount=my-cluster-b29913ee-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda-kafka-clients, testUserOperatorMetrics=my-cluster-0ae7d432-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9-kafka-clients}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:346] In context testKafkaTopicUnderReplicatedPartitions is everything deleted.
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:267] testKafkaTopicUnderReplicatedPartitions - Notifies waiting test cases:[testCruiseControlBasicAPIRequests, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testUpdateUser, testSendSimpleMessageTls, testAutoRenewAllCaCertsTriggeredByAnno, testReceiveSimpleMessageTls, testSendMessagesCustomListenerTlsScramSha, testKafkaAndZookeeperScaleUpScaleDown, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testUserOperatorMetrics, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperWatchersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize] to and randomly select one to start execution
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaTopicUnderReplicatedPartitions
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 3
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaTopicUnderReplicatedPartitions-FINISHED
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:346] In context testZookeeperQuorumSize is everything deleted.
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:267] testZookeeperQuorumSize - Notifies waiting test cases:[testCruiseControlBasicAPIRequests, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testUpdateUser, testSendSimpleMessageTls, testAutoRenewAllCaCertsTriggeredByAnno, testReceiveSimpleMessageTls, testSendMessagesCustomListenerTlsScramSha, testKafkaAndZookeeperScaleUpScaleDown, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testUserOperatorMetrics, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperWatchersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize] to and randomly select one to start execution
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testZookeeperQuorumSize
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testZookeeperAliveConnections-STARTED
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 2
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testZookeeperQuorumSize-FINISHED
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testZookeeperAliveConnections
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 3
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:230] testZookeeperAliveConnections test now can proceed its execution
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testZookeeperQuorumSize=my-cluster-6e3782cf, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070, testSendSimpleMessageTls=my-cluster-b70e5342, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7, testUpdateUser=my-cluster-9b2bd187, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testKafkaActiveControllers=my-cluster-8ce86250, testZookeeperWatchersCount=my-cluster-402c5b37, testZookeeperAliveConnections=my-cluster-914c3bfa, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349, testKafkaTopicUnderReplicatedPartitions=my-cluster-be9a0596, testSendMessagesTlsScramSha=my-cluster-380e6586, testReceiveSimpleMessageTls=my-cluster-e8a518cd, testKafkaBrokersCount=my-cluster-b29913ee, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda, testUserOperatorMetrics=my-cluster-0ae7d432, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testZookeeperQuorumSize=my-user-625293736-680311428, testKafkaAndZookeeperScaleUpScaleDown=my-user-1001559761-691587912, testSendSimpleMessageTls=my-user-602433797-900694548, testKafkaConnectIoNetwork=my-user-1415252823-246514724, testUpdateUser=my-user-595127408-1604948726, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1606351157-1815623087, testKafkaInDifferentNsThanClusterOperator=my-user-1518021213-326071957, testSendMessagesCustomListenerTlsScramSha=my-user-445988529-51741427, testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testKafkaActiveControllers=my-user-1625454548-1904690658, testZookeeperWatchersCount=my-user-262847981-879141545, testZookeeperAliveConnections=my-user-1378509155-786076411, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1590558165-46977776, testKafkaTopicUnderReplicatedPartitions=my-user-2067443766-1667934716, testSendMessagesTlsScramSha=my-user-2039401547-533835955, testReceiveSimpleMessageTls=my-user-761922164-2019863647, testKafkaBrokersCount=my-user-161347164-197258481, testMirrorMaker2TlsAndTlsClientAuth=my-user-707354298-1518057957, testUserOperatorMetrics=my-user-366296042-1312500013, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659, testRackAwareConnectCorrectDeployment=my-user-389825219-1608762022, testMirrorMakerTlsAuthenticated=my-user-692603309-1711649825}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testZookeeperQuorumSize=my-topic-2058761647-1946180799, testKafkaAndZookeeperScaleUpScaleDown=my-topic-741363275-285355018, testSendSimpleMessageTls=my-topic-1299629356-597168683, testKafkaConnectIoNetwork=my-topic-1319353181-1427134540, testUpdateUser=my-topic-1875736194-1340884809, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1931531910-629202741, testKafkaInDifferentNsThanClusterOperator=my-topic-2034182350-493335345, testSendMessagesCustomListenerTlsScramSha=my-topic-1039904883-1044305687, testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testKafkaActiveControllers=my-topic-1892159362-1633250172, testZookeeperWatchersCount=my-topic-464196692-1269638121, testZookeeperAliveConnections=my-topic-142178161-401786225, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1210910585-1669754462, testKafkaTopicUnderReplicatedPartitions=my-topic-1554151013-1160196250, testSendMessagesTlsScramSha=my-topic-988094002-1400465950, testReceiveSimpleMessageTls=my-topic-1007790874-1032272408, testKafkaBrokersCount=my-topic-480196974-662544545, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1279769671-1440257906, testUserOperatorMetrics=my-topic-1504927319-1084001771, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393, testRackAwareConnectCorrectDeployment=my-topic-1801184984-1468632754, testMirrorMakerTlsAuthenticated=my-topic-1945138991-1836415682}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testZookeeperQuorumSize=my-cluster-6e3782cf-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070-kafka-clients, testSendSimpleMessageTls=my-cluster-b70e5342-kafka-clients, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7-kafka-clients, testUpdateUser=my-cluster-9b2bd187-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testKafkaActiveControllers=my-cluster-8ce86250-kafka-clients, testZookeeperWatchersCount=my-cluster-402c5b37-kafka-clients, testZookeeperAliveConnections=my-cluster-914c3bfa-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-be9a0596-kafka-clients, testSendMessagesTlsScramSha=my-cluster-380e6586-kafka-clients, testReceiveSimpleMessageTls=my-cluster-e8a518cd-kafka-clients, testKafkaBrokersCount=my-cluster-b29913ee-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda-kafka-clients, testUserOperatorMetrics=my-cluster-0ae7d432-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9-kafka-clients}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaTopicPartitions-STARTED
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaTopicPartitions
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 4
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:230] testKafkaTopicPartitions test now can proceed its execution
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testZookeeperQuorumSize=my-cluster-6e3782cf, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070, testSendSimpleMessageTls=my-cluster-b70e5342, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7, testUpdateUser=my-cluster-9b2bd187, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testKafkaActiveControllers=my-cluster-8ce86250, testZookeeperWatchersCount=my-cluster-402c5b37, testKafkaTopicPartitions=my-cluster-a4922820, testZookeeperAliveConnections=my-cluster-914c3bfa, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349, testKafkaTopicUnderReplicatedPartitions=my-cluster-be9a0596, testSendMessagesTlsScramSha=my-cluster-380e6586, testReceiveSimpleMessageTls=my-cluster-e8a518cd, testKafkaBrokersCount=my-cluster-b29913ee, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda, testUserOperatorMetrics=my-cluster-0ae7d432, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testZookeeperQuorumSize=my-user-625293736-680311428, testKafkaAndZookeeperScaleUpScaleDown=my-user-1001559761-691587912, testSendSimpleMessageTls=my-user-602433797-900694548, testKafkaConnectIoNetwork=my-user-1415252823-246514724, testUpdateUser=my-user-595127408-1604948726, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1606351157-1815623087, testKafkaInDifferentNsThanClusterOperator=my-user-1518021213-326071957, testSendMessagesCustomListenerTlsScramSha=my-user-445988529-51741427, testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testKafkaActiveControllers=my-user-1625454548-1904690658, testZookeeperWatchersCount=my-user-262847981-879141545, testKafkaTopicPartitions=my-user-560785779-785480841, testZookeeperAliveConnections=my-user-1378509155-786076411, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1590558165-46977776, testKafkaTopicUnderReplicatedPartitions=my-user-2067443766-1667934716, testSendMessagesTlsScramSha=my-user-2039401547-533835955, testReceiveSimpleMessageTls=my-user-761922164-2019863647, testKafkaBrokersCount=my-user-161347164-197258481, testMirrorMaker2TlsAndTlsClientAuth=my-user-707354298-1518057957, testUserOperatorMetrics=my-user-366296042-1312500013, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659, testRackAwareConnectCorrectDeployment=my-user-389825219-1608762022, testMirrorMakerTlsAuthenticated=my-user-692603309-1711649825}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testZookeeperQuorumSize=my-topic-2058761647-1946180799, testKafkaAndZookeeperScaleUpScaleDown=my-topic-741363275-285355018, testSendSimpleMessageTls=my-topic-1299629356-597168683, testKafkaConnectIoNetwork=my-topic-1319353181-1427134540, testUpdateUser=my-topic-1875736194-1340884809, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1931531910-629202741, testKafkaInDifferentNsThanClusterOperator=my-topic-2034182350-493335345, testSendMessagesCustomListenerTlsScramSha=my-topic-1039904883-1044305687, testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testKafkaActiveControllers=my-topic-1892159362-1633250172, testZookeeperWatchersCount=my-topic-464196692-1269638121, testKafkaTopicPartitions=my-topic-554356311-897225030, testZookeeperAliveConnections=my-topic-142178161-401786225, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1210910585-1669754462, testKafkaTopicUnderReplicatedPartitions=my-topic-1554151013-1160196250, testSendMessagesTlsScramSha=my-topic-988094002-1400465950, testReceiveSimpleMessageTls=my-topic-1007790874-1032272408, testKafkaBrokersCount=my-topic-480196974-662544545, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1279769671-1440257906, testUserOperatorMetrics=my-topic-1504927319-1084001771, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393, testRackAwareConnectCorrectDeployment=my-topic-1801184984-1468632754, testMirrorMakerTlsAuthenticated=my-topic-1945138991-1836415682}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testZookeeperQuorumSize=my-cluster-6e3782cf-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070-kafka-clients, testSendSimpleMessageTls=my-cluster-b70e5342-kafka-clients, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7-kafka-clients, testUpdateUser=my-cluster-9b2bd187-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testKafkaActiveControllers=my-cluster-8ce86250-kafka-clients, testZookeeperWatchersCount=my-cluster-402c5b37-kafka-clients, testKafkaTopicPartitions=my-cluster-a4922820-kafka-clients, testZookeeperAliveConnections=my-cluster-914c3bfa-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-be9a0596-kafka-clients, testSendMessagesTlsScramSha=my-cluster-380e6586-kafka-clients, testReceiveSimpleMessageTls=my-cluster-e8a518cd-kafka-clients, testKafkaBrokersCount=my-cluster-b29913ee-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda-kafka-clients, testUserOperatorMetrics=my-cluster-0ae7d432-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9-kafka-clients}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:346] In context testZookeeperAliveConnections is everything deleted.
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:267] testZookeeperAliveConnections - Notifies waiting test cases:[testCruiseControlBasicAPIRequests, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testUpdateUser, testSendSimpleMessageTls, testAutoRenewAllCaCertsTriggeredByAnno, testReceiveSimpleMessageTls, testSendMessagesCustomListenerTlsScramSha, testKafkaAndZookeeperScaleUpScaleDown, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testUserOperatorMetrics, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperWatchersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions] to and randomly select one to start execution
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testZookeeperAliveConnections
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 3
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testZookeeperAliveConnections-FINISHED
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testMirrorMaker2Metrics-STARTED
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testMirrorMaker2Metrics
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 4
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:230] testMirrorMaker2Metrics test now can proceed its execution
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testZookeeperQuorumSize=my-cluster-6e3782cf, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070, testSendSimpleMessageTls=my-cluster-b70e5342, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7, testUpdateUser=my-cluster-9b2bd187, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testKafkaActiveControllers=my-cluster-8ce86250, testZookeeperWatchersCount=my-cluster-402c5b37, testKafkaTopicPartitions=my-cluster-a4922820, testMirrorMaker2Metrics=my-cluster-3313dd99, testZookeeperAliveConnections=my-cluster-914c3bfa, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349, testKafkaTopicUnderReplicatedPartitions=my-cluster-be9a0596, testSendMessagesTlsScramSha=my-cluster-380e6586, testReceiveSimpleMessageTls=my-cluster-e8a518cd, testKafkaBrokersCount=my-cluster-b29913ee, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda, testUserOperatorMetrics=my-cluster-0ae7d432, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testZookeeperQuorumSize=my-user-625293736-680311428, testKafkaAndZookeeperScaleUpScaleDown=my-user-1001559761-691587912, testSendSimpleMessageTls=my-user-602433797-900694548, testKafkaConnectIoNetwork=my-user-1415252823-246514724, testUpdateUser=my-user-595127408-1604948726, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1606351157-1815623087, testKafkaInDifferentNsThanClusterOperator=my-user-1518021213-326071957, testSendMessagesCustomListenerTlsScramSha=my-user-445988529-51741427, testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testKafkaActiveControllers=my-user-1625454548-1904690658, testZookeeperWatchersCount=my-user-262847981-879141545, testKafkaTopicPartitions=my-user-560785779-785480841, testMirrorMaker2Metrics=my-user-1961983524-1687595462, testZookeeperAliveConnections=my-user-1378509155-786076411, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1590558165-46977776, testKafkaTopicUnderReplicatedPartitions=my-user-2067443766-1667934716, testSendMessagesTlsScramSha=my-user-2039401547-533835955, testReceiveSimpleMessageTls=my-user-761922164-2019863647, testKafkaBrokersCount=my-user-161347164-197258481, testMirrorMaker2TlsAndTlsClientAuth=my-user-707354298-1518057957, testUserOperatorMetrics=my-user-366296042-1312500013, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659, testRackAwareConnectCorrectDeployment=my-user-389825219-1608762022, testMirrorMakerTlsAuthenticated=my-user-692603309-1711649825}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testZookeeperQuorumSize=my-topic-2058761647-1946180799, testKafkaAndZookeeperScaleUpScaleDown=my-topic-741363275-285355018, testSendSimpleMessageTls=my-topic-1299629356-597168683, testKafkaConnectIoNetwork=my-topic-1319353181-1427134540, testUpdateUser=my-topic-1875736194-1340884809, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1931531910-629202741, testKafkaInDifferentNsThanClusterOperator=my-topic-2034182350-493335345, testSendMessagesCustomListenerTlsScramSha=my-topic-1039904883-1044305687, testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testKafkaActiveControllers=my-topic-1892159362-1633250172, testZookeeperWatchersCount=my-topic-464196692-1269638121, testKafkaTopicPartitions=my-topic-554356311-897225030, testMirrorMaker2Metrics=my-topic-972857008-735153820, testZookeeperAliveConnections=my-topic-142178161-401786225, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1210910585-1669754462, testKafkaTopicUnderReplicatedPartitions=my-topic-1554151013-1160196250, testSendMessagesTlsScramSha=my-topic-988094002-1400465950, testReceiveSimpleMessageTls=my-topic-1007790874-1032272408, testKafkaBrokersCount=my-topic-480196974-662544545, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1279769671-1440257906, testUserOperatorMetrics=my-topic-1504927319-1084001771, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393, testRackAwareConnectCorrectDeployment=my-topic-1801184984-1468632754, testMirrorMakerTlsAuthenticated=my-topic-1945138991-1836415682}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testZookeeperQuorumSize=my-cluster-6e3782cf-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070-kafka-clients, testSendSimpleMessageTls=my-cluster-b70e5342-kafka-clients, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7-kafka-clients, testUpdateUser=my-cluster-9b2bd187-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testKafkaActiveControllers=my-cluster-8ce86250-kafka-clients, testZookeeperWatchersCount=my-cluster-402c5b37-kafka-clients, testKafkaTopicPartitions=my-cluster-a4922820-kafka-clients, testMirrorMaker2Metrics=my-cluster-3313dd99-kafka-clients, testZookeeperAliveConnections=my-cluster-914c3bfa-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-be9a0596-kafka-clients, testSendMessagesTlsScramSha=my-cluster-380e6586-kafka-clients, testReceiveSimpleMessageTls=my-cluster-e8a518cd-kafka-clients, testKafkaBrokersCount=my-cluster-b29913ee-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda-kafka-clients, testUserOperatorMetrics=my-cluster-0ae7d432-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9-kafka-clients}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:346] In context testKafkaTopicPartitions is everything deleted.
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:267] testKafkaTopicPartitions - Notifies waiting test cases:[testCruiseControlBasicAPIRequests, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testUpdateUser, testSendSimpleMessageTls, testAutoRenewAllCaCertsTriggeredByAnno, testReceiveSimpleMessageTls, testSendMessagesCustomListenerTlsScramSha, testKafkaAndZookeeperScaleUpScaleDown, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testUserOperatorMetrics, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperWatchersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics] to and randomly select one to start execution
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaTopicPartitions
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 3
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaTopicPartitions-FINISHED
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaBridgeMetrics-STARTED
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaBridgeMetrics
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 4
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:230] testKafkaBridgeMetrics test now can proceed its execution
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-b70e5342, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testZookeeperWatchersCount=my-cluster-402c5b37, testKafkaTopicPartitions=my-cluster-a4922820, testMirrorMaker2Metrics=my-cluster-3313dd99, testZookeeperAliveConnections=my-cluster-914c3bfa, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349, testKafkaTopicUnderReplicatedPartitions=my-cluster-be9a0596, testSendMessagesTlsScramSha=my-cluster-380e6586, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a, testZookeeperQuorumSize=my-cluster-6e3782cf, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070, testKafkaBridgeMetrics=my-cluster-047c2ac1, testUpdateUser=my-cluster-9b2bd187, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574, testKafkaActiveControllers=my-cluster-8ce86250, testReceiveSimpleMessageTls=my-cluster-e8a518cd, testKafkaBrokersCount=my-cluster-b29913ee, testUserOperatorMetrics=my-cluster-0ae7d432, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-602433797-900694548, testKafkaConnectIoNetwork=my-user-1415252823-246514724, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1606351157-1815623087, testSendMessagesCustomListenerTlsScramSha=my-user-445988529-51741427, testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testZookeeperWatchersCount=my-user-262847981-879141545, testKafkaTopicPartitions=my-user-560785779-785480841, testMirrorMaker2Metrics=my-user-1961983524-1687595462, testZookeeperAliveConnections=my-user-1378509155-786076411, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1590558165-46977776, testKafkaTopicUnderReplicatedPartitions=my-user-2067443766-1667934716, testSendMessagesTlsScramSha=my-user-2039401547-533835955, testMirrorMaker2TlsAndTlsClientAuth=my-user-707354298-1518057957, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659, testRackAwareConnectCorrectDeployment=my-user-389825219-1608762022, testZookeeperQuorumSize=my-user-625293736-680311428, testKafkaAndZookeeperScaleUpScaleDown=my-user-1001559761-691587912, testKafkaBridgeMetrics=my-user-2093621856-1949785375, testUpdateUser=my-user-595127408-1604948726, testKafkaInDifferentNsThanClusterOperator=my-user-1518021213-326071957, testKafkaActiveControllers=my-user-1625454548-1904690658, testReceiveSimpleMessageTls=my-user-761922164-2019863647, testKafkaBrokersCount=my-user-161347164-197258481, testUserOperatorMetrics=my-user-366296042-1312500013, testMirrorMakerTlsAuthenticated=my-user-692603309-1711649825}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-1299629356-597168683, testKafkaConnectIoNetwork=my-topic-1319353181-1427134540, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1931531910-629202741, testSendMessagesCustomListenerTlsScramSha=my-topic-1039904883-1044305687, testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testZookeeperWatchersCount=my-topic-464196692-1269638121, testKafkaTopicPartitions=my-topic-554356311-897225030, testMirrorMaker2Metrics=my-topic-972857008-735153820, testZookeeperAliveConnections=my-topic-142178161-401786225, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1210910585-1669754462, testKafkaTopicUnderReplicatedPartitions=my-topic-1554151013-1160196250, testSendMessagesTlsScramSha=my-topic-988094002-1400465950, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1279769671-1440257906, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393, testRackAwareConnectCorrectDeployment=my-topic-1801184984-1468632754, testZookeeperQuorumSize=my-topic-2058761647-1946180799, testKafkaAndZookeeperScaleUpScaleDown=my-topic-741363275-285355018, testKafkaBridgeMetrics=my-topic-784030803-1986191588, testUpdateUser=my-topic-1875736194-1340884809, testKafkaInDifferentNsThanClusterOperator=my-topic-2034182350-493335345, testKafkaActiveControllers=my-topic-1892159362-1633250172, testReceiveSimpleMessageTls=my-topic-1007790874-1032272408, testKafkaBrokersCount=my-topic-480196974-662544545, testUserOperatorMetrics=my-topic-1504927319-1084001771, testMirrorMakerTlsAuthenticated=my-topic-1945138991-1836415682}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-b70e5342-kafka-clients, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testZookeeperWatchersCount=my-cluster-402c5b37-kafka-clients, testKafkaTopicPartitions=my-cluster-a4922820-kafka-clients, testMirrorMaker2Metrics=my-cluster-3313dd99-kafka-clients, testZookeeperAliveConnections=my-cluster-914c3bfa-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-be9a0596-kafka-clients, testSendMessagesTlsScramSha=my-cluster-380e6586-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a-kafka-clients, testZookeeperQuorumSize=my-cluster-6e3782cf-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070-kafka-clients, testKafkaBridgeMetrics=my-cluster-047c2ac1-kafka-clients, testUpdateUser=my-cluster-9b2bd187-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574-kafka-clients, testKafkaActiveControllers=my-cluster-8ce86250-kafka-clients, testReceiveSimpleMessageTls=my-cluster-e8a518cd-kafka-clients, testKafkaBrokersCount=my-cluster-b29913ee-kafka-clients, testUserOperatorMetrics=my-cluster-0ae7d432-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9-kafka-clients}
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Job bridge-producer in namespace infra-namespace
2022-03-30 22:35:17 [ForkJoinPool-3-worker-17] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-8r49n -n infra-namespace -- curl 172.17.0.19:8081/metrics
2022-03-30 22:35:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 22:35:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-8r49n -n infra-namespace -- curl 172.17.0.23:9404
2022-03-30 22:35:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-8r49n -n infra-namespace -- curl 172.17.0.24:9404
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:bridge-producer
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [32mINFO [m [JobUtils:81] Waiting for job: bridge-producer will be in active state
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for job active
2022-03-30 22:35:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] job active not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 22:35:18 [ForkJoinPool-3-worker-17] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.19 from Pod infra-namespace-kafka-clients-748578f786-8r49n finished with return code: 0
2022-03-30 22:35:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 22:35:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 22:35:18 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:35:18 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:346] In context testUserOperatorMetrics is everything deleted.
2022-03-30 22:35:18 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:35:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:267] testUserOperatorMetrics - Notifies waiting test cases:[testCruiseControlBasicAPIRequests, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testUpdateUser, testSendSimpleMessageTls, testAutoRenewAllCaCertsTriggeredByAnno, testReceiveSimpleMessageTls, testSendMessagesCustomListenerTlsScramSha, testKafkaAndZookeeperScaleUpScaleDown, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testUserOperatorMetrics, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperWatchersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics] to and randomly select one to start execution
2022-03-30 22:35:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testUserOperatorMetrics
2022-03-30 22:35:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 3
2022-03-30 22:35:18 [ForkJoinPool-3-worker-17] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testUserOperatorMetrics-FINISHED
2022-03-30 22:35:18 [ForkJoinPool-3-worker-17] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 22:35:18 [ForkJoinPool-3-worker-17] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 22:35:18 [ForkJoinPool-3-worker-17] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaMetricsSettings-STARTED
2022-03-30 22:35:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 22:35:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 22:35:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaMetricsSettings
2022-03-30 22:35:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 4
2022-03-30 22:35:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:230] testKafkaMetricsSettings test now can proceed its execution
2022-03-30 22:35:18 [ForkJoinPool-3-worker-17] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 22:35:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-b70e5342, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testZookeeperWatchersCount=my-cluster-402c5b37, testKafkaTopicPartitions=my-cluster-a4922820, testMirrorMaker2Metrics=my-cluster-3313dd99, testZookeeperAliveConnections=my-cluster-914c3bfa, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349, testKafkaTopicUnderReplicatedPartitions=my-cluster-be9a0596, testSendMessagesTlsScramSha=my-cluster-380e6586, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a, testZookeeperQuorumSize=my-cluster-6e3782cf, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070, testKafkaBridgeMetrics=my-cluster-047c2ac1, testUpdateUser=my-cluster-9b2bd187, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574, testKafkaActiveControllers=my-cluster-8ce86250, testReceiveSimpleMessageTls=my-cluster-e8a518cd, testKafkaBrokersCount=my-cluster-b29913ee, testUserOperatorMetrics=my-cluster-0ae7d432, testKafkaMetricsSettings=my-cluster-cdcb75c7, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9}
2022-03-30 22:35:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-602433797-900694548, testKafkaConnectIoNetwork=my-user-1415252823-246514724, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1606351157-1815623087, testSendMessagesCustomListenerTlsScramSha=my-user-445988529-51741427, testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testZookeeperWatchersCount=my-user-262847981-879141545, testKafkaTopicPartitions=my-user-560785779-785480841, testMirrorMaker2Metrics=my-user-1961983524-1687595462, testZookeeperAliveConnections=my-user-1378509155-786076411, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1590558165-46977776, testKafkaTopicUnderReplicatedPartitions=my-user-2067443766-1667934716, testSendMessagesTlsScramSha=my-user-2039401547-533835955, testMirrorMaker2TlsAndTlsClientAuth=my-user-707354298-1518057957, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659, testRackAwareConnectCorrectDeployment=my-user-389825219-1608762022, testZookeeperQuorumSize=my-user-625293736-680311428, testKafkaAndZookeeperScaleUpScaleDown=my-user-1001559761-691587912, testKafkaBridgeMetrics=my-user-2093621856-1949785375, testUpdateUser=my-user-595127408-1604948726, testKafkaInDifferentNsThanClusterOperator=my-user-1518021213-326071957, testKafkaActiveControllers=my-user-1625454548-1904690658, testReceiveSimpleMessageTls=my-user-761922164-2019863647, testKafkaBrokersCount=my-user-161347164-197258481, testUserOperatorMetrics=my-user-366296042-1312500013, testKafkaMetricsSettings=my-user-58143961-1414938572, testMirrorMakerTlsAuthenticated=my-user-692603309-1711649825}
2022-03-30 22:35:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-1299629356-597168683, testKafkaConnectIoNetwork=my-topic-1319353181-1427134540, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1931531910-629202741, testSendMessagesCustomListenerTlsScramSha=my-topic-1039904883-1044305687, testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testZookeeperWatchersCount=my-topic-464196692-1269638121, testKafkaTopicPartitions=my-topic-554356311-897225030, testMirrorMaker2Metrics=my-topic-972857008-735153820, testZookeeperAliveConnections=my-topic-142178161-401786225, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1210910585-1669754462, testKafkaTopicUnderReplicatedPartitions=my-topic-1554151013-1160196250, testSendMessagesTlsScramSha=my-topic-988094002-1400465950, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1279769671-1440257906, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393, testRackAwareConnectCorrectDeployment=my-topic-1801184984-1468632754, testZookeeperQuorumSize=my-topic-2058761647-1946180799, testKafkaAndZookeeperScaleUpScaleDown=my-topic-741363275-285355018, testKafkaBridgeMetrics=my-topic-784030803-1986191588, testUpdateUser=my-topic-1875736194-1340884809, testKafkaInDifferentNsThanClusterOperator=my-topic-2034182350-493335345, testKafkaActiveControllers=my-topic-1892159362-1633250172, testReceiveSimpleMessageTls=my-topic-1007790874-1032272408, testKafkaBrokersCount=my-topic-480196974-662544545, testUserOperatorMetrics=my-topic-1504927319-1084001771, testKafkaMetricsSettings=my-topic-66444693-1542984361, testMirrorMakerTlsAuthenticated=my-topic-1945138991-1836415682}
2022-03-30 22:35:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-b70e5342-kafka-clients, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testZookeeperWatchersCount=my-cluster-402c5b37-kafka-clients, testKafkaTopicPartitions=my-cluster-a4922820-kafka-clients, testMirrorMaker2Metrics=my-cluster-3313dd99-kafka-clients, testZookeeperAliveConnections=my-cluster-914c3bfa-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-be9a0596-kafka-clients, testSendMessagesTlsScramSha=my-cluster-380e6586-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a-kafka-clients, testZookeeperQuorumSize=my-cluster-6e3782cf-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070-kafka-clients, testKafkaBridgeMetrics=my-cluster-047c2ac1-kafka-clients, testUpdateUser=my-cluster-9b2bd187-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574-kafka-clients, testKafkaActiveControllers=my-cluster-8ce86250-kafka-clients, testReceiveSimpleMessageTls=my-cluster-e8a518cd-kafka-clients, testKafkaBrokersCount=my-cluster-b29913ee-kafka-clients, testUserOperatorMetrics=my-cluster-0ae7d432-kafka-clients, testKafkaMetricsSettings=my-cluster-cdcb75c7-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9-kafka-clients}
2022-03-30 22:35:18 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: second-kafka-cluster are stable
2022-03-30 22:35:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for pod with prefixsecond-kafka-cluster is present.
2022-03-30 22:35:18 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Pods stability in phase Running
2022-03-30 22:35:18 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 50
2022-03-30 22:35:18 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 50
2022-03-30 22:35:18 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 50
2022-03-30 22:35:18 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 50
2022-03-30 22:35:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (299989ms till timeout)
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.24 from Pod infra-namespace-kafka-clients-748578f786-8r49n finished with return code: 0
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:346] In context testKafkaConnectIoNetwork is everything deleted.
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:267] testKafkaConnectIoNetwork - Notifies waiting test cases:[testCruiseControlBasicAPIRequests, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testUpdateUser, testSendSimpleMessageTls, testAutoRenewAllCaCertsTriggeredByAnno, testReceiveSimpleMessageTls, testSendMessagesCustomListenerTlsScramSha, testKafkaAndZookeeperScaleUpScaleDown, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testUserOperatorMetrics, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperWatchersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics, testKafkaMetricsSettings] to and randomly select one to start execution
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaConnectIoNetwork
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 3
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaConnectIoNetwork-FINISHED
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testClusterOperatorMetrics-STARTED
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testClusterOperatorMetrics
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 4
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:230] testClusterOperatorMetrics test now can proceed its execution
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-b70e5342, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testZookeeperWatchersCount=my-cluster-402c5b37, testKafkaTopicPartitions=my-cluster-a4922820, testMirrorMaker2Metrics=my-cluster-3313dd99, testZookeeperAliveConnections=my-cluster-914c3bfa, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349, testKafkaTopicUnderReplicatedPartitions=my-cluster-be9a0596, testSendMessagesTlsScramSha=my-cluster-380e6586, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a, testZookeeperQuorumSize=my-cluster-6e3782cf, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070, testKafkaBridgeMetrics=my-cluster-047c2ac1, testUpdateUser=my-cluster-9b2bd187, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574, testKafkaActiveControllers=my-cluster-8ce86250, testReceiveSimpleMessageTls=my-cluster-e8a518cd, testKafkaBrokersCount=my-cluster-b29913ee, testClusterOperatorMetrics=my-cluster-f7aec375, testUserOperatorMetrics=my-cluster-0ae7d432, testKafkaMetricsSettings=my-cluster-cdcb75c7, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9}
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-602433797-900694548, testKafkaConnectIoNetwork=my-user-1415252823-246514724, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1606351157-1815623087, testSendMessagesCustomListenerTlsScramSha=my-user-445988529-51741427, testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testZookeeperWatchersCount=my-user-262847981-879141545, testKafkaTopicPartitions=my-user-560785779-785480841, testMirrorMaker2Metrics=my-user-1961983524-1687595462, testZookeeperAliveConnections=my-user-1378509155-786076411, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1590558165-46977776, testKafkaTopicUnderReplicatedPartitions=my-user-2067443766-1667934716, testSendMessagesTlsScramSha=my-user-2039401547-533835955, testMirrorMaker2TlsAndTlsClientAuth=my-user-707354298-1518057957, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659, testRackAwareConnectCorrectDeployment=my-user-389825219-1608762022, testZookeeperQuorumSize=my-user-625293736-680311428, testKafkaAndZookeeperScaleUpScaleDown=my-user-1001559761-691587912, testKafkaBridgeMetrics=my-user-2093621856-1949785375, testUpdateUser=my-user-595127408-1604948726, testKafkaInDifferentNsThanClusterOperator=my-user-1518021213-326071957, testKafkaActiveControllers=my-user-1625454548-1904690658, testReceiveSimpleMessageTls=my-user-761922164-2019863647, testKafkaBrokersCount=my-user-161347164-197258481, testClusterOperatorMetrics=my-user-323771535-901246887, testUserOperatorMetrics=my-user-366296042-1312500013, testKafkaMetricsSettings=my-user-58143961-1414938572, testMirrorMakerTlsAuthenticated=my-user-692603309-1711649825}
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-1299629356-597168683, testKafkaConnectIoNetwork=my-topic-1319353181-1427134540, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1931531910-629202741, testSendMessagesCustomListenerTlsScramSha=my-topic-1039904883-1044305687, testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testZookeeperWatchersCount=my-topic-464196692-1269638121, testKafkaTopicPartitions=my-topic-554356311-897225030, testMirrorMaker2Metrics=my-topic-972857008-735153820, testZookeeperAliveConnections=my-topic-142178161-401786225, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1210910585-1669754462, testKafkaTopicUnderReplicatedPartitions=my-topic-1554151013-1160196250, testSendMessagesTlsScramSha=my-topic-988094002-1400465950, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1279769671-1440257906, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393, testRackAwareConnectCorrectDeployment=my-topic-1801184984-1468632754, testZookeeperQuorumSize=my-topic-2058761647-1946180799, testKafkaAndZookeeperScaleUpScaleDown=my-topic-741363275-285355018, testKafkaBridgeMetrics=my-topic-784030803-1986191588, testUpdateUser=my-topic-1875736194-1340884809, testKafkaInDifferentNsThanClusterOperator=my-topic-2034182350-493335345, testKafkaActiveControllers=my-topic-1892159362-1633250172, testReceiveSimpleMessageTls=my-topic-1007790874-1032272408, testKafkaBrokersCount=my-topic-480196974-662544545, testClusterOperatorMetrics=my-topic-174268591-1154466070, testUserOperatorMetrics=my-topic-1504927319-1084001771, testKafkaMetricsSettings=my-topic-66444693-1542984361, testMirrorMakerTlsAuthenticated=my-topic-1945138991-1836415682}
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-b70e5342-kafka-clients, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testZookeeperWatchersCount=my-cluster-402c5b37-kafka-clients, testKafkaTopicPartitions=my-cluster-a4922820-kafka-clients, testMirrorMaker2Metrics=my-cluster-3313dd99-kafka-clients, testZookeeperAliveConnections=my-cluster-914c3bfa-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-be9a0596-kafka-clients, testSendMessagesTlsScramSha=my-cluster-380e6586-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a-kafka-clients, testZookeeperQuorumSize=my-cluster-6e3782cf-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070-kafka-clients, testKafkaBridgeMetrics=my-cluster-047c2ac1-kafka-clients, testUpdateUser=my-cluster-9b2bd187-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574-kafka-clients, testKafkaActiveControllers=my-cluster-8ce86250-kafka-clients, testReceiveSimpleMessageTls=my-cluster-e8a518cd-kafka-clients, testKafkaBrokersCount=my-cluster-b29913ee-kafka-clients, testClusterOperatorMetrics=my-cluster-f7aec375-kafka-clients, testUserOperatorMetrics=my-cluster-0ae7d432-kafka-clients, testKafkaMetricsSettings=my-cluster-cdcb75c7-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9-kafka-clients}
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-8r49n -n infra-namespace -- curl 172.17.0.5:8080/metrics
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.5 from Pod infra-namespace-kafka-clients-748578f786-8r49n finished with return code: 0
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:346] In context testClusterOperatorMetrics is everything deleted.
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:267] testClusterOperatorMetrics - Notifies waiting test cases:[testCruiseControlBasicAPIRequests, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testUpdateUser, testSendSimpleMessageTls, testAutoRenewAllCaCertsTriggeredByAnno, testReceiveSimpleMessageTls, testSendMessagesCustomListenerTlsScramSha, testKafkaAndZookeeperScaleUpScaleDown, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testUserOperatorMetrics, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperWatchersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics, testKafkaMetricsSettings, testClusterOperatorMetrics] to and randomly select one to start execution
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testClusterOperatorMetrics
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 3
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testClusterOperatorMetrics-FINISHED
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testCruiseControlMetrics-STARTED
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testCruiseControlMetrics
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 4
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:230] testCruiseControlMetrics test now can proceed its execution
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-b70e5342, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testZookeeperWatchersCount=my-cluster-402c5b37, testKafkaTopicPartitions=my-cluster-a4922820, testMirrorMaker2Metrics=my-cluster-3313dd99, testZookeeperAliveConnections=my-cluster-914c3bfa, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349, testKafkaTopicUnderReplicatedPartitions=my-cluster-be9a0596, testSendMessagesTlsScramSha=my-cluster-380e6586, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a, testZookeeperQuorumSize=my-cluster-6e3782cf, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070, testKafkaBridgeMetrics=my-cluster-047c2ac1, testUpdateUser=my-cluster-9b2bd187, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574, testKafkaActiveControllers=my-cluster-8ce86250, testCruiseControlMetrics=my-cluster-967b129c, testReceiveSimpleMessageTls=my-cluster-e8a518cd, testKafkaBrokersCount=my-cluster-b29913ee, testClusterOperatorMetrics=my-cluster-f7aec375, testUserOperatorMetrics=my-cluster-0ae7d432, testKafkaMetricsSettings=my-cluster-cdcb75c7, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9}
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-602433797-900694548, testKafkaConnectIoNetwork=my-user-1415252823-246514724, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1606351157-1815623087, testSendMessagesCustomListenerTlsScramSha=my-user-445988529-51741427, testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testZookeeperWatchersCount=my-user-262847981-879141545, testKafkaTopicPartitions=my-user-560785779-785480841, testMirrorMaker2Metrics=my-user-1961983524-1687595462, testZookeeperAliveConnections=my-user-1378509155-786076411, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1590558165-46977776, testKafkaTopicUnderReplicatedPartitions=my-user-2067443766-1667934716, testSendMessagesTlsScramSha=my-user-2039401547-533835955, testMirrorMaker2TlsAndTlsClientAuth=my-user-707354298-1518057957, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659, testRackAwareConnectCorrectDeployment=my-user-389825219-1608762022, testZookeeperQuorumSize=my-user-625293736-680311428, testKafkaAndZookeeperScaleUpScaleDown=my-user-1001559761-691587912, testKafkaBridgeMetrics=my-user-2093621856-1949785375, testUpdateUser=my-user-595127408-1604948726, testKafkaInDifferentNsThanClusterOperator=my-user-1518021213-326071957, testKafkaActiveControllers=my-user-1625454548-1904690658, testCruiseControlMetrics=my-user-621328593-1412519713, testReceiveSimpleMessageTls=my-user-761922164-2019863647, testKafkaBrokersCount=my-user-161347164-197258481, testClusterOperatorMetrics=my-user-323771535-901246887, testUserOperatorMetrics=my-user-366296042-1312500013, testKafkaMetricsSettings=my-user-58143961-1414938572, testMirrorMakerTlsAuthenticated=my-user-692603309-1711649825}
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-1299629356-597168683, testKafkaConnectIoNetwork=my-topic-1319353181-1427134540, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1931531910-629202741, testSendMessagesCustomListenerTlsScramSha=my-topic-1039904883-1044305687, testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testZookeeperWatchersCount=my-topic-464196692-1269638121, testKafkaTopicPartitions=my-topic-554356311-897225030, testMirrorMaker2Metrics=my-topic-972857008-735153820, testZookeeperAliveConnections=my-topic-142178161-401786225, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1210910585-1669754462, testKafkaTopicUnderReplicatedPartitions=my-topic-1554151013-1160196250, testSendMessagesTlsScramSha=my-topic-988094002-1400465950, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1279769671-1440257906, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393, testRackAwareConnectCorrectDeployment=my-topic-1801184984-1468632754, testZookeeperQuorumSize=my-topic-2058761647-1946180799, testKafkaAndZookeeperScaleUpScaleDown=my-topic-741363275-285355018, testKafkaBridgeMetrics=my-topic-784030803-1986191588, testUpdateUser=my-topic-1875736194-1340884809, testKafkaInDifferentNsThanClusterOperator=my-topic-2034182350-493335345, testKafkaActiveControllers=my-topic-1892159362-1633250172, testCruiseControlMetrics=my-topic-646189816-670248288, testReceiveSimpleMessageTls=my-topic-1007790874-1032272408, testKafkaBrokersCount=my-topic-480196974-662544545, testClusterOperatorMetrics=my-topic-174268591-1154466070, testUserOperatorMetrics=my-topic-1504927319-1084001771, testKafkaMetricsSettings=my-topic-66444693-1542984361, testMirrorMakerTlsAuthenticated=my-topic-1945138991-1836415682}
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-b70e5342-kafka-clients, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testZookeeperWatchersCount=my-cluster-402c5b37-kafka-clients, testKafkaTopicPartitions=my-cluster-a4922820-kafka-clients, testMirrorMaker2Metrics=my-cluster-3313dd99-kafka-clients, testZookeeperAliveConnections=my-cluster-914c3bfa-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-be9a0596-kafka-clients, testSendMessagesTlsScramSha=my-cluster-380e6586-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a-kafka-clients, testZookeeperQuorumSize=my-cluster-6e3782cf-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070-kafka-clients, testKafkaBridgeMetrics=my-cluster-047c2ac1-kafka-clients, testUpdateUser=my-cluster-9b2bd187-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574-kafka-clients, testKafkaActiveControllers=my-cluster-8ce86250-kafka-clients, testCruiseControlMetrics=my-cluster-967b129c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-e8a518cd-kafka-clients, testKafkaBrokersCount=my-cluster-b29913ee-kafka-clients, testClusterOperatorMetrics=my-cluster-f7aec375-kafka-clients, testUserOperatorMetrics=my-cluster-0ae7d432-kafka-clients, testKafkaMetricsSettings=my-cluster-cdcb75c7-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9-kafka-clients}
2022-03-30 22:35:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace exec metrics-cluster-name-cruise-control-8468c977d5-dwph2 -c cruise-control -- /bin/bash -c curl -XGET localhost:9404/metrics
2022-03-30 22:35:18 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Job bridge-consumer in namespace infra-namespace
2022-03-30 22:35:18 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:bridge-consumer
2022-03-30 22:35:18 [ForkJoinPool-3-worker-9] [32mINFO [m [JobUtils:81] Waiting for job: bridge-consumer will be in active state
2022-03-30 22:35:18 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for job active
2022-03-30 22:35:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] job active not ready, will try again in 1000 ms (179997ms till timeout)
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.23 from Pod infra-namespace-kafka-clients-748578f786-8r49n finished with return code: 0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:346] In context testMirrorMaker2Metrics is everything deleted.
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:267] testMirrorMaker2Metrics - Notifies waiting test cases:[testCruiseControlBasicAPIRequests, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testUpdateUser, testSendSimpleMessageTls, testAutoRenewAllCaCertsTriggeredByAnno, testReceiveSimpleMessageTls, testSendMessagesCustomListenerTlsScramSha, testKafkaAndZookeeperScaleUpScaleDown, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testUserOperatorMetrics, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperWatchersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics, testKafkaMetricsSettings, testClusterOperatorMetrics, testCruiseControlMetrics] to and randomly select one to start execution
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testMirrorMaker2Metrics
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 3
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testMirrorMaker2Metrics-FINISHED
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testTopicOperatorMetrics-STARTED
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testTopicOperatorMetrics
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 4
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:230] testTopicOperatorMetrics test now can proceed its execution
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-b70e5342, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testZookeeperWatchersCount=my-cluster-402c5b37, testKafkaTopicPartitions=my-cluster-a4922820, testMirrorMaker2Metrics=my-cluster-3313dd99, testZookeeperAliveConnections=my-cluster-914c3bfa, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349, testKafkaTopicUnderReplicatedPartitions=my-cluster-be9a0596, testSendMessagesTlsScramSha=my-cluster-380e6586, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a, testZookeeperQuorumSize=my-cluster-6e3782cf, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070, testKafkaBridgeMetrics=my-cluster-047c2ac1, testUpdateUser=my-cluster-9b2bd187, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574, testTopicOperatorMetrics=my-cluster-a794fbf8, testKafkaActiveControllers=my-cluster-8ce86250, testCruiseControlMetrics=my-cluster-967b129c, testReceiveSimpleMessageTls=my-cluster-e8a518cd, testKafkaBrokersCount=my-cluster-b29913ee, testClusterOperatorMetrics=my-cluster-f7aec375, testUserOperatorMetrics=my-cluster-0ae7d432, testKafkaMetricsSettings=my-cluster-cdcb75c7, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9}
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-602433797-900694548, testKafkaConnectIoNetwork=my-user-1415252823-246514724, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1606351157-1815623087, testSendMessagesCustomListenerTlsScramSha=my-user-445988529-51741427, testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testZookeeperWatchersCount=my-user-262847981-879141545, testKafkaTopicPartitions=my-user-560785779-785480841, testMirrorMaker2Metrics=my-user-1961983524-1687595462, testZookeeperAliveConnections=my-user-1378509155-786076411, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1590558165-46977776, testKafkaTopicUnderReplicatedPartitions=my-user-2067443766-1667934716, testSendMessagesTlsScramSha=my-user-2039401547-533835955, testMirrorMaker2TlsAndTlsClientAuth=my-user-707354298-1518057957, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659, testRackAwareConnectCorrectDeployment=my-user-389825219-1608762022, testZookeeperQuorumSize=my-user-625293736-680311428, testKafkaAndZookeeperScaleUpScaleDown=my-user-1001559761-691587912, testKafkaBridgeMetrics=my-user-2093621856-1949785375, testUpdateUser=my-user-595127408-1604948726, testKafkaInDifferentNsThanClusterOperator=my-user-1518021213-326071957, testTopicOperatorMetrics=my-user-84445048-466060506, testKafkaActiveControllers=my-user-1625454548-1904690658, testCruiseControlMetrics=my-user-621328593-1412519713, testReceiveSimpleMessageTls=my-user-761922164-2019863647, testKafkaBrokersCount=my-user-161347164-197258481, testClusterOperatorMetrics=my-user-323771535-901246887, testUserOperatorMetrics=my-user-366296042-1312500013, testKafkaMetricsSettings=my-user-58143961-1414938572, testMirrorMakerTlsAuthenticated=my-user-692603309-1711649825}
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-1299629356-597168683, testKafkaConnectIoNetwork=my-topic-1319353181-1427134540, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1931531910-629202741, testSendMessagesCustomListenerTlsScramSha=my-topic-1039904883-1044305687, testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testZookeeperWatchersCount=my-topic-464196692-1269638121, testKafkaTopicPartitions=my-topic-554356311-897225030, testMirrorMaker2Metrics=my-topic-972857008-735153820, testZookeeperAliveConnections=my-topic-142178161-401786225, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1210910585-1669754462, testKafkaTopicUnderReplicatedPartitions=my-topic-1554151013-1160196250, testSendMessagesTlsScramSha=my-topic-988094002-1400465950, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1279769671-1440257906, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393, testRackAwareConnectCorrectDeployment=my-topic-1801184984-1468632754, testZookeeperQuorumSize=my-topic-2058761647-1946180799, testKafkaAndZookeeperScaleUpScaleDown=my-topic-741363275-285355018, testKafkaBridgeMetrics=my-topic-784030803-1986191588, testUpdateUser=my-topic-1875736194-1340884809, testKafkaInDifferentNsThanClusterOperator=my-topic-2034182350-493335345, testTopicOperatorMetrics=my-topic-1485093017-1833103632, testKafkaActiveControllers=my-topic-1892159362-1633250172, testCruiseControlMetrics=my-topic-646189816-670248288, testReceiveSimpleMessageTls=my-topic-1007790874-1032272408, testKafkaBrokersCount=my-topic-480196974-662544545, testClusterOperatorMetrics=my-topic-174268591-1154466070, testUserOperatorMetrics=my-topic-1504927319-1084001771, testKafkaMetricsSettings=my-topic-66444693-1542984361, testMirrorMakerTlsAuthenticated=my-topic-1945138991-1836415682}
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-b70e5342-kafka-clients, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testZookeeperWatchersCount=my-cluster-402c5b37-kafka-clients, testKafkaTopicPartitions=my-cluster-a4922820-kafka-clients, testMirrorMaker2Metrics=my-cluster-3313dd99-kafka-clients, testZookeeperAliveConnections=my-cluster-914c3bfa-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-be9a0596-kafka-clients, testSendMessagesTlsScramSha=my-cluster-380e6586-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a-kafka-clients, testZookeeperQuorumSize=my-cluster-6e3782cf-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070-kafka-clients, testKafkaBridgeMetrics=my-cluster-047c2ac1-kafka-clients, testUpdateUser=my-cluster-9b2bd187-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574-kafka-clients, testTopicOperatorMetrics=my-cluster-a794fbf8-kafka-clients, testKafkaActiveControllers=my-cluster-8ce86250-kafka-clients, testCruiseControlMetrics=my-cluster-967b129c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-e8a518cd-kafka-clients, testKafkaBrokersCount=my-cluster-b29913ee-kafka-clients, testClusterOperatorMetrics=my-cluster-f7aec375-kafka-clients, testUserOperatorMetrics=my-cluster-0ae7d432-kafka-clients, testKafkaMetricsSettings=my-cluster-cdcb75c7-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9-kafka-clients}
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-8r49n -n infra-namespace -- curl 172.17.0.19:8080/metrics
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.19 from Pod infra-namespace-kafka-clients-748578f786-8r49n finished with return code: 0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get KafkaTopic -o jsonpath={range .items[*]}{.metadata.name} 
2022-03-30 22:35:19 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 49
2022-03-30 22:35:19 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 49
2022-03-30 22:35:19 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 49
2022-03-30 22:35:19 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 49
2022-03-30 22:35:19 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (298936ms till timeout)
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get KafkaTopic -o jsonpath={range .items[*]}{.metadata.name} 
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: consumer-offsets---84e7a678d08f4bd226872e5cdd4eb527fadc1c6a
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: heartbeats
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: metrics-cluster-name-connect-config
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: metrics-cluster-name-connect-offsets
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: metrics-cluster-name-connect-status
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: mirrormaker2-cluster-configs
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: mirrormaker2-cluster-offsets
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: mirrormaker2-cluster-status
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: my-topic-1172351742-1939014984
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: my-topic-1427443759-776507328
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: my-topic-260886687-1962678096
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: second-kafka-cluster.checkpoints.internal
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: strimzi-store-topic---effb8e3e057afce1ecf67c3f5d8e4e3ff177fc55
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: strimzi-topic-operator-kstreams-topic-store-changelog---b75e702040b99be8a9263134de3507fc0cc4017b
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: strimzi.cruisecontrol.metrics
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: strimzi.cruisecontrol.modeltrainingsamples
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: strimzi.cruisecontrol.partitionmetricsamples
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:346] In context testTopicOperatorMetrics is everything deleted.
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:267] testTopicOperatorMetrics - Notifies waiting test cases:[testCruiseControlBasicAPIRequests, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testUpdateUser, testSendSimpleMessageTls, testAutoRenewAllCaCertsTriggeredByAnno, testReceiveSimpleMessageTls, testSendMessagesCustomListenerTlsScramSha, testKafkaAndZookeeperScaleUpScaleDown, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testUserOperatorMetrics, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperWatchersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics, testKafkaMetricsSettings, testClusterOperatorMetrics, testCruiseControlMetrics, testTopicOperatorMetrics] to and randomly select one to start execution
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testTopicOperatorMetrics
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 3
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testTopicOperatorMetrics-FINISHED
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaConnectRequests-STARTED
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaConnectRequests
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 4
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:230] testKafkaConnectRequests test now can proceed its execution
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-b70e5342, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testZookeeperWatchersCount=my-cluster-402c5b37, testKafkaTopicPartitions=my-cluster-a4922820, testMirrorMaker2Metrics=my-cluster-3313dd99, testZookeeperAliveConnections=my-cluster-914c3bfa, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349, testKafkaTopicUnderReplicatedPartitions=my-cluster-be9a0596, testSendMessagesTlsScramSha=my-cluster-380e6586, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a, testZookeeperQuorumSize=my-cluster-6e3782cf, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070, testKafkaBridgeMetrics=my-cluster-047c2ac1, testUpdateUser=my-cluster-9b2bd187, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574, testTopicOperatorMetrics=my-cluster-a794fbf8, testKafkaActiveControllers=my-cluster-8ce86250, testKafkaConnectRequests=my-cluster-d11b8099, testCruiseControlMetrics=my-cluster-967b129c, testReceiveSimpleMessageTls=my-cluster-e8a518cd, testKafkaBrokersCount=my-cluster-b29913ee, testClusterOperatorMetrics=my-cluster-f7aec375, testUserOperatorMetrics=my-cluster-0ae7d432, testKafkaMetricsSettings=my-cluster-cdcb75c7, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9}
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-602433797-900694548, testKafkaConnectIoNetwork=my-user-1415252823-246514724, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1606351157-1815623087, testSendMessagesCustomListenerTlsScramSha=my-user-445988529-51741427, testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testZookeeperWatchersCount=my-user-262847981-879141545, testKafkaTopicPartitions=my-user-560785779-785480841, testMirrorMaker2Metrics=my-user-1961983524-1687595462, testZookeeperAliveConnections=my-user-1378509155-786076411, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1590558165-46977776, testKafkaTopicUnderReplicatedPartitions=my-user-2067443766-1667934716, testSendMessagesTlsScramSha=my-user-2039401547-533835955, testMirrorMaker2TlsAndTlsClientAuth=my-user-707354298-1518057957, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659, testRackAwareConnectCorrectDeployment=my-user-389825219-1608762022, testZookeeperQuorumSize=my-user-625293736-680311428, testKafkaAndZookeeperScaleUpScaleDown=my-user-1001559761-691587912, testKafkaBridgeMetrics=my-user-2093621856-1949785375, testUpdateUser=my-user-595127408-1604948726, testKafkaInDifferentNsThanClusterOperator=my-user-1518021213-326071957, testTopicOperatorMetrics=my-user-84445048-466060506, testKafkaActiveControllers=my-user-1625454548-1904690658, testKafkaConnectRequests=my-user-1702280136-701036393, testCruiseControlMetrics=my-user-621328593-1412519713, testReceiveSimpleMessageTls=my-user-761922164-2019863647, testKafkaBrokersCount=my-user-161347164-197258481, testClusterOperatorMetrics=my-user-323771535-901246887, testUserOperatorMetrics=my-user-366296042-1312500013, testKafkaMetricsSettings=my-user-58143961-1414938572, testMirrorMakerTlsAuthenticated=my-user-692603309-1711649825}
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-1299629356-597168683, testKafkaConnectIoNetwork=my-topic-1319353181-1427134540, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1931531910-629202741, testSendMessagesCustomListenerTlsScramSha=my-topic-1039904883-1044305687, testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testZookeeperWatchersCount=my-topic-464196692-1269638121, testKafkaTopicPartitions=my-topic-554356311-897225030, testMirrorMaker2Metrics=my-topic-972857008-735153820, testZookeeperAliveConnections=my-topic-142178161-401786225, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1210910585-1669754462, testKafkaTopicUnderReplicatedPartitions=my-topic-1554151013-1160196250, testSendMessagesTlsScramSha=my-topic-988094002-1400465950, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1279769671-1440257906, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393, testRackAwareConnectCorrectDeployment=my-topic-1801184984-1468632754, testZookeeperQuorumSize=my-topic-2058761647-1946180799, testKafkaAndZookeeperScaleUpScaleDown=my-topic-741363275-285355018, testKafkaBridgeMetrics=my-topic-784030803-1986191588, testUpdateUser=my-topic-1875736194-1340884809, testKafkaInDifferentNsThanClusterOperator=my-topic-2034182350-493335345, testTopicOperatorMetrics=my-topic-1485093017-1833103632, testKafkaActiveControllers=my-topic-1892159362-1633250172, testKafkaConnectRequests=my-topic-250627930-2121139168, testCruiseControlMetrics=my-topic-646189816-670248288, testReceiveSimpleMessageTls=my-topic-1007790874-1032272408, testKafkaBrokersCount=my-topic-480196974-662544545, testClusterOperatorMetrics=my-topic-174268591-1154466070, testUserOperatorMetrics=my-topic-1504927319-1084001771, testKafkaMetricsSettings=my-topic-66444693-1542984361, testMirrorMakerTlsAuthenticated=my-topic-1945138991-1836415682}
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-b70e5342-kafka-clients, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testZookeeperWatchersCount=my-cluster-402c5b37-kafka-clients, testKafkaTopicPartitions=my-cluster-a4922820-kafka-clients, testMirrorMaker2Metrics=my-cluster-3313dd99-kafka-clients, testZookeeperAliveConnections=my-cluster-914c3bfa-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-be9a0596-kafka-clients, testSendMessagesTlsScramSha=my-cluster-380e6586-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a-kafka-clients, testZookeeperQuorumSize=my-cluster-6e3782cf-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070-kafka-clients, testKafkaBridgeMetrics=my-cluster-047c2ac1-kafka-clients, testUpdateUser=my-cluster-9b2bd187-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574-kafka-clients, testTopicOperatorMetrics=my-cluster-a794fbf8-kafka-clients, testKafkaActiveControllers=my-cluster-8ce86250-kafka-clients, testKafkaConnectRequests=my-cluster-d11b8099-kafka-clients, testCruiseControlMetrics=my-cluster-967b129c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-e8a518cd-kafka-clients, testKafkaBrokersCount=my-cluster-b29913ee-kafka-clients, testClusterOperatorMetrics=my-cluster-f7aec375-kafka-clients, testUserOperatorMetrics=my-cluster-0ae7d432-kafka-clients, testKafkaMetricsSettings=my-cluster-cdcb75c7-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9-kafka-clients}
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 22:35:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-8r49n -n infra-namespace -- curl 172.17.0.24:9404
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace exec metrics-cluster-name-cruise-control-8468c977d5-dwph2 -c cruise-control -- /bin/bash -c curl -XGET localhost:9404/metrics
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsIsolatedST:450] Verifying that we have more than 0 groups
2022-03-30 22:35:19 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 22:35:19 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaConnectResponse-STARTED
2022-03-30 22:35:19 [ForkJoinPool-3-worker-11] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 22:35:19 [ForkJoinPool-3-worker-11] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 22:35:19 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaConnectResponse
2022-03-30 22:35:19 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 5
2022-03-30 22:35:19 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:230] testKafkaConnectResponse test now can proceed its execution
2022-03-30 22:35:19 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 22:35:19 [ForkJoinPool-3-worker-11] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaConnectResponse=my-cluster-1adc403d, testSendSimpleMessageTls=my-cluster-b70e5342, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testZookeeperWatchersCount=my-cluster-402c5b37, testKafkaTopicPartitions=my-cluster-a4922820, testMirrorMaker2Metrics=my-cluster-3313dd99, testZookeeperAliveConnections=my-cluster-914c3bfa, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349, testKafkaTopicUnderReplicatedPartitions=my-cluster-be9a0596, testSendMessagesTlsScramSha=my-cluster-380e6586, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a, testZookeeperQuorumSize=my-cluster-6e3782cf, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070, testKafkaBridgeMetrics=my-cluster-047c2ac1, testUpdateUser=my-cluster-9b2bd187, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574, testTopicOperatorMetrics=my-cluster-a794fbf8, testKafkaActiveControllers=my-cluster-8ce86250, testKafkaConnectRequests=my-cluster-d11b8099, testCruiseControlMetrics=my-cluster-967b129c, testReceiveSimpleMessageTls=my-cluster-e8a518cd, testKafkaBrokersCount=my-cluster-b29913ee, testClusterOperatorMetrics=my-cluster-f7aec375, testUserOperatorMetrics=my-cluster-0ae7d432, testKafkaMetricsSettings=my-cluster-cdcb75c7, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9}
2022-03-30 22:35:19 [ForkJoinPool-3-worker-11] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaConnectResponse=my-user-1674486009-1911649798, testSendSimpleMessageTls=my-user-602433797-900694548, testKafkaConnectIoNetwork=my-user-1415252823-246514724, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1606351157-1815623087, testSendMessagesCustomListenerTlsScramSha=my-user-445988529-51741427, testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testZookeeperWatchersCount=my-user-262847981-879141545, testKafkaTopicPartitions=my-user-560785779-785480841, testMirrorMaker2Metrics=my-user-1961983524-1687595462, testZookeeperAliveConnections=my-user-1378509155-786076411, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1590558165-46977776, testKafkaTopicUnderReplicatedPartitions=my-user-2067443766-1667934716, testSendMessagesTlsScramSha=my-user-2039401547-533835955, testMirrorMaker2TlsAndTlsClientAuth=my-user-707354298-1518057957, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659, testRackAwareConnectCorrectDeployment=my-user-389825219-1608762022, testZookeeperQuorumSize=my-user-625293736-680311428, testKafkaAndZookeeperScaleUpScaleDown=my-user-1001559761-691587912, testKafkaBridgeMetrics=my-user-2093621856-1949785375, testUpdateUser=my-user-595127408-1604948726, testKafkaInDifferentNsThanClusterOperator=my-user-1518021213-326071957, testTopicOperatorMetrics=my-user-84445048-466060506, testKafkaActiveControllers=my-user-1625454548-1904690658, testKafkaConnectRequests=my-user-1702280136-701036393, testCruiseControlMetrics=my-user-621328593-1412519713, testReceiveSimpleMessageTls=my-user-761922164-2019863647, testKafkaBrokersCount=my-user-161347164-197258481, testClusterOperatorMetrics=my-user-323771535-901246887, testUserOperatorMetrics=my-user-366296042-1312500013, testKafkaMetricsSettings=my-user-58143961-1414938572, testMirrorMakerTlsAuthenticated=my-user-692603309-1711649825}
2022-03-30 22:35:19 [ForkJoinPool-3-worker-11] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaConnectResponse=my-topic-1058695284-857971809, testSendSimpleMessageTls=my-topic-1299629356-597168683, testKafkaConnectIoNetwork=my-topic-1319353181-1427134540, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1931531910-629202741, testSendMessagesCustomListenerTlsScramSha=my-topic-1039904883-1044305687, testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testZookeeperWatchersCount=my-topic-464196692-1269638121, testKafkaTopicPartitions=my-topic-554356311-897225030, testMirrorMaker2Metrics=my-topic-972857008-735153820, testZookeeperAliveConnections=my-topic-142178161-401786225, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1210910585-1669754462, testKafkaTopicUnderReplicatedPartitions=my-topic-1554151013-1160196250, testSendMessagesTlsScramSha=my-topic-988094002-1400465950, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1279769671-1440257906, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393, testRackAwareConnectCorrectDeployment=my-topic-1801184984-1468632754, testZookeeperQuorumSize=my-topic-2058761647-1946180799, testKafkaAndZookeeperScaleUpScaleDown=my-topic-741363275-285355018, testKafkaBridgeMetrics=my-topic-784030803-1986191588, testUpdateUser=my-topic-1875736194-1340884809, testKafkaInDifferentNsThanClusterOperator=my-topic-2034182350-493335345, testTopicOperatorMetrics=my-topic-1485093017-1833103632, testKafkaActiveControllers=my-topic-1892159362-1633250172, testKafkaConnectRequests=my-topic-250627930-2121139168, testCruiseControlMetrics=my-topic-646189816-670248288, testReceiveSimpleMessageTls=my-topic-1007790874-1032272408, testKafkaBrokersCount=my-topic-480196974-662544545, testClusterOperatorMetrics=my-topic-174268591-1154466070, testUserOperatorMetrics=my-topic-1504927319-1084001771, testKafkaMetricsSettings=my-topic-66444693-1542984361, testMirrorMakerTlsAuthenticated=my-topic-1945138991-1836415682}
2022-03-30 22:35:19 [ForkJoinPool-3-worker-11] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaConnectResponse=my-cluster-1adc403d-kafka-clients, testSendSimpleMessageTls=my-cluster-b70e5342-kafka-clients, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testZookeeperWatchersCount=my-cluster-402c5b37-kafka-clients, testKafkaTopicPartitions=my-cluster-a4922820-kafka-clients, testMirrorMaker2Metrics=my-cluster-3313dd99-kafka-clients, testZookeeperAliveConnections=my-cluster-914c3bfa-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-be9a0596-kafka-clients, testSendMessagesTlsScramSha=my-cluster-380e6586-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a-kafka-clients, testZookeeperQuorumSize=my-cluster-6e3782cf-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070-kafka-clients, testKafkaBridgeMetrics=my-cluster-047c2ac1-kafka-clients, testUpdateUser=my-cluster-9b2bd187-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574-kafka-clients, testTopicOperatorMetrics=my-cluster-a794fbf8-kafka-clients, testKafkaActiveControllers=my-cluster-8ce86250-kafka-clients, testKafkaConnectRequests=my-cluster-d11b8099-kafka-clients, testCruiseControlMetrics=my-cluster-967b129c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-e8a518cd-kafka-clients, testKafkaBrokersCount=my-cluster-b29913ee-kafka-clients, testClusterOperatorMetrics=my-cluster-f7aec375-kafka-clients, testUserOperatorMetrics=my-cluster-0ae7d432-kafka-clients, testKafkaMetricsSettings=my-cluster-cdcb75c7-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9-kafka-clients}
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_info{runtime="OpenJDK Runtime Environment",vendor="Red Hat, Inc.",version="11.0.14.1+1-LTS",} -> 1.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_total{pool="CodeHeap 'profiled nmethods'",} -> 1.361344E7
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_total{pool="G1 Old Gen",} -> 2.2083336E7
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_total{pool="G1 Eden Space",} -> 5.35822336E8
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_total{pool="CodeHeap 'non-profiled nmethods'",} -> 3066112.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_total{pool="G1 Survivor Space",} -> 1.1534336E7
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_total{pool="Compressed Class Space",} -> 5395592.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_total{pool="Metaspace",} -> 4.7401648E7
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_total{pool="CodeHeap 'non-nmethods'",} -> 1478912.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_current -> 54.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_daemon -> 36.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_peak -> 54.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_started_total -> 69.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_deadlocked -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_deadlocked_monitor -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_state{state="RUNNABLE",} -> 14.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_state{state="TIMED_WAITING",} -> 24.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_state{state="WAITING",} -> 16.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_state{state="BLOCKED",} -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_state{state="TERMINATED",} -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_state{state="NEW",} -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_objects_pending_finalization -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_bytes_used{area="heap",} -> 3.1326512E7
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_bytes_used{area="nonheap",} -> 7.147008E7
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_bytes_committed{area="heap",} -> 8.5458944E8
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_bytes_committed{area="nonheap",} -> 7.4907648E7
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_bytes_max{area="heap",} -> 8.37812224E9
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_bytes_max{area="nonheap",} -> -1.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_bytes_init{area="heap",} -> 1.34217728E8
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_bytes_init{area="nonheap",} -> 7667712.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_used{pool="CodeHeap 'non-nmethods'",} -> 1466624.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_used{pool="Metaspace",} -> 4.7608376E7
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_used{pool="CodeHeap 'profiled nmethods'",} -> 1.3860224E7
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_used{pool="Compressed Class Space",} -> 5415624.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_used{pool="G1 Eden Space",} -> 7340032.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_used{pool="G1 Old Gen",} -> 1.9792176E7
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_used{pool="G1 Survivor Space",} -> 4194304.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_used{pool="CodeHeap 'non-profiled nmethods'",} -> 3120000.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_committed{pool="CodeHeap 'non-nmethods'",} -> 2555904.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_committed{pool="Metaspace",} -> 4.9283072E7
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_committed{pool="CodeHeap 'profiled nmethods'",} -> 1.3893632E7
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_committed{pool="Compressed Class Space",} -> 6029312.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_committed{pool="G1 Eden Space",} -> 4.6137344E7
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_committed{pool="G1 Old Gen",} -> 8.04257792E8
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_committed{pool="G1 Survivor Space",} -> 4194304.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_committed{pool="CodeHeap 'non-profiled nmethods'",} -> 3145728.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_max{pool="CodeHeap 'non-nmethods'",} -> 5828608.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_max{pool="Metaspace",} -> -1.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_max{pool="CodeHeap 'profiled nmethods'",} -> 1.22912768E8
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_max{pool="Compressed Class Space",} -> 1.073741824E9
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_max{pool="G1 Eden Space",} -> -1.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_max{pool="G1 Old Gen",} -> 8.37812224E9
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_max{pool="G1 Survivor Space",} -> -1.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_max{pool="CodeHeap 'non-profiled nmethods'",} -> 1.22916864E8
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_init{pool="CodeHeap 'non-nmethods'",} -> 2555904.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_init{pool="Metaspace",} -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_init{pool="CodeHeap 'profiled nmethods'",} -> 2555904.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_init{pool="Compressed Class Space",} -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_init{pool="G1 Eden Space",} -> 7340032.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_init{pool="G1 Old Gen",} -> 1.26877696E8
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_init{pool="G1 Survivor Space",} -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_init{pool="CodeHeap 'non-profiled nmethods'",} -> 2555904.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_used_bytes{pool="G1 Eden Space",} -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_used_bytes{pool="G1 Old Gen",} -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_used_bytes{pool="G1 Survivor Space",} -> 4194304.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_committed_bytes{pool="G1 Eden Space",} -> 4.6137344E7
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_committed_bytes{pool="G1 Old Gen",} -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_committed_bytes{pool="G1 Survivor Space",} -> 4194304.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_max_bytes{pool="G1 Eden Space",} -> -1.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_max_bytes{pool="G1 Old Gen",} -> 8.37812224E9
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_max_bytes{pool="G1 Survivor Space",} -> -1.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_init_bytes{pool="G1 Eden Space",} -> 7340032.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_init_bytes{pool="G1 Old Gen",} -> 1.26877696E8
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_init_bytes{pool="G1 Survivor Space",} -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jmx_exporter_build_info{version="0.16.1",name="jmx_prometheus_javaagent",} -> 1.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_buffer_pool_used_bytes{pool="mapped",} -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_buffer_pool_used_bytes{pool="direct",} -> 314735.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_buffer_pool_capacity_bytes{pool="mapped",} -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_buffer_pool_capacity_bytes{pool="direct",} -> 314735.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_buffer_pool_used_buffers{pool="mapped",} -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_buffer_pool_used_buffers{pool="direct",} -> 23.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] process_cpu_seconds_total -> 18.63
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] process_start_time_seconds -> 1.648679406985E9
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] process_open_fds -> 169.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] process_max_fds -> 1048576.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] process_virtual_memory_bytes -> 1.4651133952E10
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] process_resident_memory_bytes -> 3.39095552E8
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_gc_collection_seconds_count{gc="G1 Young Generation",} -> 25.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_gc_collection_seconds_sum{gc="G1 Young Generation",} -> 0.264
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_gc_collection_seconds_count{gc="G1 Old Generation",} -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_gc_collection_seconds_sum{gc="G1 Old Generation",} -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jmx_config_reload_success_total -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jmx_config_reload_failure_total -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_mean -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_max -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_75thpercentile -> 58.693551
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_request_rate_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_aborting_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_execution_started_kafka_assigner_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_min -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_has_partitions_with_isr_greater_than_replicas_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_count -> 1.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_min -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_failure_rate_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_mean -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_request_rate_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_request_rate_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_stddev -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_rate_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_request_rate_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_stddev -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_enabled_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_request_rate_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_request_rate_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_ongoing_anomaly_duration_ms_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_request_rate_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_request_rate_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_min -> 34.863585
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_stddev -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_request_rate_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_max -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_stddev -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_min -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_min -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_request_rate_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_aborted_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_request_rate_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-11] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-8r49n -n infra-namespace -- curl 172.17.0.24:9404
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_request_rate_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_mean -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_usertaskmanager_num_active_sessions_number -> 12.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_request_rate_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_mean -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_inter_broker_partition_movements_per_broker_cap_value -> 5.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_request_rate_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_min -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_min -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_automated_rightsizing_rate_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_request_rate_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_total_monitored_windows_value -> 2.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_request_rate_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_mean -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_95thpercentile -> 58.693551
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_request_rate_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_number_of_self_healing_failed_to_start_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_fiveminuterate -> 0.008360112078184101
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_movements_global_cap_value -> 1000.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_min -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_99thpercentile -> 175.63645699999998
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_request_rate_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_mean -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_rate_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_request_rate_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_metadata_factor_number -> 1512.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_failure_rate_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_min -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_automated_rightsizing_rate_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_999thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_min -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_max -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_max -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_request_rate_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_max -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_request_rate_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_75thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_98thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_999thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_max -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_request_rate_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_stddev -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_rate_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_mean -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_request_rate_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_999thpercentile -> 1.301064
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_dead_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_number_of_self_healing_failed_to_start_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_75thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_999thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_min -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_mean -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_oneminuterate -> 0.0015506900910289933
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_98thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_request_rate_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_total_monitored_windows_number -> 2.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_in_progress_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_request_rate_meanrate -> 0.19884359147528943
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_95thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_50thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_rate_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_request_rate_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_request_rate_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_stddev -> 19.939474837594624
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_stddev -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_request_rate_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_right_sized_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_request_rate_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_50thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_enabled_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_rate_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_request_rate_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_95thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_99thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_mean -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_rate_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_request_rate_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_execution_started_kafka_assigner_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_max -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_request_rate_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_mean -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_valid_windows_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_999thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_min -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_99thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_completed_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_request_rate_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_rate_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_stddev -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_rate_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_request_rate_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_request_rate_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_mean -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_dead_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_request_rate_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_completed_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_failure_rate_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_95thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_enabled_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_valid_windows_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_mean -> 1.301064
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_request_rate_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_ongoing_execution_non_kafka_assigner_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_request_rate_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_98thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_stddev -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_request_rate_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_50thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_stddev -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_has_unfixable_goals_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_stddev -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_pending_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_count -> 5.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_request_rate_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_stddev -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_stddev -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_monitored_partitions_percentage_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_rate_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_rate_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_num_suspect_metric_anomalies_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_mean -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_max -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_request_rate_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_max -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_number_of_self_healing_started_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_request_rate_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_mean -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_75thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_enabled_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_mean -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_request_rate_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_rate_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_98thpercentile -> 58.693551
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_num_topics_number -> 17.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_dead_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_has_partitions_with_isr_greater_than_replicas_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_aborted_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_mean_time_to_start_fix_ms_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_monitored_partitions_percentage_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_75thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_stddev -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_mean -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_stddev -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_request_rate_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_dead_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_98thpercentile -> 1.301064
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_max -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_execution_stopped_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_dead_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_failure_rate_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_pending_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_request_rate_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_max -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_request_rate_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_request_rate_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_fiveminuterate -> 0.0020729741807790705
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_request_rate_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_request_rate_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_min -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_mean -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_98thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_aborted_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_rate_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_in_progress_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_stddev -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_stddev -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_aborting_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_stddev -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_98thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_rate_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_rate_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_50thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_min -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_under_provisioned_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_enabled_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_pending_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_stddev -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_max -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_mean -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_aborted_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_50thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_request_rate_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_request_rate_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_request_rate_fifteenminuterate -> 0.05685122008543253
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_num_topics_value -> 17.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_aborting_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_partition_movements_per_broker_cap_number -> 2.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_max -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_request_rate_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_enabled_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_stddev -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_mean -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_request_rate_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_failure_rate_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_request_rate_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_ongoing_execution_kafka_assigner_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_movements_global_cap_number -> 1000.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_request_rate_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_stddev -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_rate_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_max -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_has_partitions_with_replication_factor_greater_than_num_racks_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_rate_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_dead_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_98thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_aborting_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_max -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_99thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_request_rate_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_completed_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_min -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_75thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_50thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_enabled_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_request_rate_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_min -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_request_rate_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_request_rate_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_request_rate_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_request_rate_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_75thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_min -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_in_progress_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_ongoing_execution_non_kafka_assigner_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_75thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_enabled_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_95thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_min -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_request_rate_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_num_recent_metric_anomalies_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_stddev -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_mean -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_in_progress_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_enabled_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_request_rate_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_over_provisioned_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_999thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_rate_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_request_rate_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_completed_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_usertaskmanager_num_active_sessions_value -> 12.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_under_provisioned_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_999thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_in_progress_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_has_partitions_with_replication_factor_greater_than_num_racks_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_max -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_min -> 1.301064
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_metadata_factor_value -> 1512.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_min -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_stddev -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_999thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_request_rate_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_in_progress_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_request_rate_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_request_rate_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_right_sized_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_meanrate -> 0.00321756104350408
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_stddev -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_completed_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_fifteenminuterate -> 0.003783098204289629
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_request_rate_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_50thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_request_rate_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_95thpercentile -> 1.301064
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_request_rate_oneminuterate -> 0.20692480766236232
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_rate_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_usertaskmanager_num_active_user_tasks_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_num_persistent_metric_anomalies_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_95thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_balancedness_score_number -> 100.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_max -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_failure_rate_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_rate_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_automated_rightsizing_rate_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_98thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_99thpercentile -> 1.301064
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_rate_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_request_rate_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_stddev -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_request_rate_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_aborted_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_request_rate_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_request_rate_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_mean -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_mean -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_mean -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_fifteenminuterate -> 9.484070116640398E-4
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_request_rate_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_99thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_request_rate_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_usertaskmanager_num_active_user_tasks_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_request_rate_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_enabled_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_request_rate_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_request_rate_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_95thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_mean -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_stddev -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_stddev -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_min -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_request_rate_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_99thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_request_rate_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_mean -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_max -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_num_persistent_metric_anomalies_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_max -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_completed_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_oneminuterate -> 0.009929959674745265
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_request_rate_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_stddev -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_inter_broker_partition_movements_per_broker_cap_number -> 5.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_rate_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_mean -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_98thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_ongoing_anomaly_duration_ms_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_aborting_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_request_rate_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_failure_rate_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_request_rate_count -> 60.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_request_rate_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_98thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_request_rate_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_min -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_75thpercentile -> 1.301064
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_min -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_min -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_mean_time_to_start_fix_ms_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_execution_started_non_kafka_assigner_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_over_provisioned_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_max -> 175.63645699999998
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_min -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_rate_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_rate_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_999thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_execution_stopped_by_user_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_max -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_rate_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_min -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_max -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_stddev -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_aborted_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_ongoing_execution_kafka_assigner_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_mean -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_request_rate_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_request_rate_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_number_of_self_healing_started_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_num_suspect_metric_anomalies_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_execution_stopped_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_failure_rate_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_99thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_failure_rate_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_max -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_max -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_max -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_max -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_request_rate_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_max -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_mean -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_min -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_request_rate_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_rate_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_50thpercentile -> 34.863585
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_request_rate_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_mean -> 43.80257456208461
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_request_rate_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_partition_movements_per_broker_cap_value -> 2.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_rate_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_rate_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_max -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_min -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_request_rate_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_999thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_mean -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_balancedness_score_value -> 100.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_failure_rate_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_50thpercentile -> 1.301064
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_95thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_mean -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_has_unfixable_goals_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_50thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_pending_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_min -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_99thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_enabled_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_automated_rightsizing_rate_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_rate_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_request_rate_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_pending_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_request_rate_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_min -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_stddev -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_num_partitions_with_extrapolations_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_execution_started_non_kafka_assigner_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_request_rate_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_mean -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_max -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_min -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_max -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_75thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_99thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_rate_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_stddev -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_request_rate_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_95thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_min -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_request_rate_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_50thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_request_rate_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_num_recent_metric_anomalies_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_99thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_request_rate_fiveminuterate -> 0.12747762164373594
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_75thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_execution_stopped_by_user_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_count -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_rate_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_request_rate_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_automated_rightsizing_rate_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_aborting_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_num_partitions_with_extrapolations_value -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_999thpercentile -> 175.63645699999998
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_stddev -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_request_rate_oneminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_max -> 1.301064
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_pending_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_meanrate -> 0.01641925711038766
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_enabled_number -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_95thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_request_rate_fifteenminuterate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_request_rate_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_max -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jmx_scrape_duration_seconds -> 0.377594636
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jmx_scrape_error -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jmx_scrape_cached_beans -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_classes_loaded -> 8590.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_classes_loaded_total -> 8590.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_classes_unloaded_total -> 0.0
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jmx_config_reload_failure_created -> 1.648679407162E9
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jmx_config_reload_success_created -> 1.648679407161E9
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_created{pool="CodeHeap 'profiled nmethods'",} -> 1.648679407459E9
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_created{pool="G1 Old Gen",} -> 1.648679407463E9
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_created{pool="G1 Eden Space",} -> 1.648679407463E9
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_created{pool="CodeHeap 'non-profiled nmethods'",} -> 1.648679407463E9
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_created{pool="G1 Survivor Space",} -> 1.648679407463E9
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_created{pool="Compressed Class Space",} -> 1.648679407463E9
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_created{pool="Metaspace",} -> 1.648679407463E9
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_created{pool="CodeHeap 'non-nmethods'",} 1.648679407463E9 -> 
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:346] In context testCruiseControlMetrics is everything deleted.
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:267] testCruiseControlMetrics - Notifies waiting test cases:[testCruiseControlBasicAPIRequests, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testUpdateUser, testSendSimpleMessageTls, testAutoRenewAllCaCertsTriggeredByAnno, testReceiveSimpleMessageTls, testSendMessagesCustomListenerTlsScramSha, testKafkaAndZookeeperScaleUpScaleDown, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testUserOperatorMetrics, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperWatchersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics, testKafkaMetricsSettings, testClusterOperatorMetrics, testCruiseControlMetrics, testTopicOperatorMetrics, testKafkaConnectRequests, testKafkaConnectResponse] to and randomly select one to start execution
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testCruiseControlMetrics
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 4
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testCruiseControlMetrics-FINISHED
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaExporterDifferentSetting-STARTED
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaExporterDifferentSetting
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 5
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:230] testKafkaExporterDifferentSetting test now can proceed its execution
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaConnectResponse=my-cluster-1adc403d, testSendSimpleMessageTls=my-cluster-b70e5342, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testZookeeperWatchersCount=my-cluster-402c5b37, testKafkaTopicPartitions=my-cluster-a4922820, testMirrorMaker2Metrics=my-cluster-3313dd99, testZookeeperAliveConnections=my-cluster-914c3bfa, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349, testKafkaTopicUnderReplicatedPartitions=my-cluster-be9a0596, testSendMessagesTlsScramSha=my-cluster-380e6586, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a, testZookeeperQuorumSize=my-cluster-6e3782cf, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070, testKafkaBridgeMetrics=my-cluster-047c2ac1, testUpdateUser=my-cluster-9b2bd187, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574, testTopicOperatorMetrics=my-cluster-a794fbf8, testKafkaActiveControllers=my-cluster-8ce86250, testKafkaExporterDifferentSetting=my-cluster-14731430, testKafkaConnectRequests=my-cluster-d11b8099, testCruiseControlMetrics=my-cluster-967b129c, testReceiveSimpleMessageTls=my-cluster-e8a518cd, testKafkaBrokersCount=my-cluster-b29913ee, testClusterOperatorMetrics=my-cluster-f7aec375, testUserOperatorMetrics=my-cluster-0ae7d432, testKafkaMetricsSettings=my-cluster-cdcb75c7, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9}
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaConnectResponse=my-user-1674486009-1911649798, testSendSimpleMessageTls=my-user-602433797-900694548, testKafkaConnectIoNetwork=my-user-1415252823-246514724, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1606351157-1815623087, testSendMessagesCustomListenerTlsScramSha=my-user-445988529-51741427, testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testZookeeperWatchersCount=my-user-262847981-879141545, testKafkaTopicPartitions=my-user-560785779-785480841, testMirrorMaker2Metrics=my-user-1961983524-1687595462, testZookeeperAliveConnections=my-user-1378509155-786076411, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1590558165-46977776, testKafkaTopicUnderReplicatedPartitions=my-user-2067443766-1667934716, testSendMessagesTlsScramSha=my-user-2039401547-533835955, testMirrorMaker2TlsAndTlsClientAuth=my-user-707354298-1518057957, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659, testRackAwareConnectCorrectDeployment=my-user-389825219-1608762022, testZookeeperQuorumSize=my-user-625293736-680311428, testKafkaAndZookeeperScaleUpScaleDown=my-user-1001559761-691587912, testKafkaBridgeMetrics=my-user-2093621856-1949785375, testUpdateUser=my-user-595127408-1604948726, testKafkaInDifferentNsThanClusterOperator=my-user-1518021213-326071957, testTopicOperatorMetrics=my-user-84445048-466060506, testKafkaActiveControllers=my-user-1625454548-1904690658, testKafkaExporterDifferentSetting=my-user-1796654337-353542127, testKafkaConnectRequests=my-user-1702280136-701036393, testCruiseControlMetrics=my-user-621328593-1412519713, testReceiveSimpleMessageTls=my-user-761922164-2019863647, testKafkaBrokersCount=my-user-161347164-197258481, testClusterOperatorMetrics=my-user-323771535-901246887, testUserOperatorMetrics=my-user-366296042-1312500013, testKafkaMetricsSettings=my-user-58143961-1414938572, testMirrorMakerTlsAuthenticated=my-user-692603309-1711649825}
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaConnectResponse=my-topic-1058695284-857971809, testSendSimpleMessageTls=my-topic-1299629356-597168683, testKafkaConnectIoNetwork=my-topic-1319353181-1427134540, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1931531910-629202741, testSendMessagesCustomListenerTlsScramSha=my-topic-1039904883-1044305687, testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testZookeeperWatchersCount=my-topic-464196692-1269638121, testKafkaTopicPartitions=my-topic-554356311-897225030, testMirrorMaker2Metrics=my-topic-972857008-735153820, testZookeeperAliveConnections=my-topic-142178161-401786225, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1210910585-1669754462, testKafkaTopicUnderReplicatedPartitions=my-topic-1554151013-1160196250, testSendMessagesTlsScramSha=my-topic-988094002-1400465950, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1279769671-1440257906, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393, testRackAwareConnectCorrectDeployment=my-topic-1801184984-1468632754, testZookeeperQuorumSize=my-topic-2058761647-1946180799, testKafkaAndZookeeperScaleUpScaleDown=my-topic-741363275-285355018, testKafkaBridgeMetrics=my-topic-784030803-1986191588, testUpdateUser=my-topic-1875736194-1340884809, testKafkaInDifferentNsThanClusterOperator=my-topic-2034182350-493335345, testTopicOperatorMetrics=my-topic-1485093017-1833103632, testKafkaActiveControllers=my-topic-1892159362-1633250172, testKafkaExporterDifferentSetting=my-topic-2139827014-879026263, testKafkaConnectRequests=my-topic-250627930-2121139168, testCruiseControlMetrics=my-topic-646189816-670248288, testReceiveSimpleMessageTls=my-topic-1007790874-1032272408, testKafkaBrokersCount=my-topic-480196974-662544545, testClusterOperatorMetrics=my-topic-174268591-1154466070, testUserOperatorMetrics=my-topic-1504927319-1084001771, testKafkaMetricsSettings=my-topic-66444693-1542984361, testMirrorMakerTlsAuthenticated=my-topic-1945138991-1836415682}
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaConnectResponse=my-cluster-1adc403d-kafka-clients, testSendSimpleMessageTls=my-cluster-b70e5342-kafka-clients, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testZookeeperWatchersCount=my-cluster-402c5b37-kafka-clients, testKafkaTopicPartitions=my-cluster-a4922820-kafka-clients, testMirrorMaker2Metrics=my-cluster-3313dd99-kafka-clients, testZookeeperAliveConnections=my-cluster-914c3bfa-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-be9a0596-kafka-clients, testSendMessagesTlsScramSha=my-cluster-380e6586-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a-kafka-clients, testZookeeperQuorumSize=my-cluster-6e3782cf-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070-kafka-clients, testKafkaBridgeMetrics=my-cluster-047c2ac1-kafka-clients, testUpdateUser=my-cluster-9b2bd187-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574-kafka-clients, testTopicOperatorMetrics=my-cluster-a794fbf8-kafka-clients, testKafkaActiveControllers=my-cluster-8ce86250-kafka-clients, testKafkaExporterDifferentSetting=my-cluster-14731430-kafka-clients, testKafkaConnectRequests=my-cluster-d11b8099-kafka-clients, testCruiseControlMetrics=my-cluster-967b129c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-e8a518cd-kafka-clients, testKafkaBrokersCount=my-cluster-b29913ee-kafka-clients, testClusterOperatorMetrics=my-cluster-f7aec375-kafka-clients, testUserOperatorMetrics=my-cluster-0ae7d432-kafka-clients, testKafkaMetricsSettings=my-cluster-cdcb75c7-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9-kafka-clients}
2022-03-30 22:35:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl exec metrics-cluster-name-kafka-exporter-8454677f49-czq98 -n infra-namespace -- cat /tmp/run.sh
2022-03-30 22:35:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for KafkaProducer metrics will be available
2022-03-30 22:35:19 [ForkJoinPool-3-worker-9] [32mINFO [m [MetricsIsolatedST:420] Looking for 'strimzi_bridge_kafka_producer_count' in bridge metrics
2022-03-30 22:35:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 22:35:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-8r49n -n infra-namespace -- curl 172.17.0.22:8080/metrics
2022-03-30 22:35:20 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsIsolatedST:608] Metrics collection for pod metrics-cluster-name-kafka-exporter-8454677f49-czq98 return code - 0
2022-03-30 22:35:20 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment metrics-cluster-name-kafka-exporter rolling update
2022-03-30 22:35:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace
2022-03-30 22:35:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-czq98=bba39fab-a50f-4e64-b17d-cc986fa51aa5}
2022-03-30 22:35:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-czq98=bba39fab-a50f-4e64-b17d-cc986fa51aa5}
2022-03-30 22:35:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {metrics-cluster-name-kafka-exporter-8454677f49-czq98=bba39fab-a50f-4e64-b17d-cc986fa51aa5}
2022-03-30 22:35:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace not ready, will try again in 5000 ms (599988ms till timeout)
2022-03-30 22:35:20 [ForkJoinPool-3-worker-13] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.24 from Pod infra-namespace-kafka-clients-748578f786-8r49n finished with return code: 0
2022-03-30 22:35:20 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 22:35:20 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 22:35:20 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:35:20 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:346] In context testKafkaConnectRequests is everything deleted.
2022-03-30 22:35:20 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:35:20 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:267] testKafkaConnectRequests - Notifies waiting test cases:[testCruiseControlBasicAPIRequests, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testUpdateUser, testSendSimpleMessageTls, testAutoRenewAllCaCertsTriggeredByAnno, testReceiveSimpleMessageTls, testSendMessagesCustomListenerTlsScramSha, testKafkaAndZookeeperScaleUpScaleDown, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testUserOperatorMetrics, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperWatchersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics, testKafkaMetricsSettings, testClusterOperatorMetrics, testCruiseControlMetrics, testTopicOperatorMetrics, testKafkaConnectRequests, testKafkaConnectResponse, testKafkaExporterDifferentSetting] to and randomly select one to start execution
2022-03-30 22:35:20 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaConnectRequests
2022-03-30 22:35:20 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 4
2022-03-30 22:35:20 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaConnectRequests-FINISHED
2022-03-30 22:35:20 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 22:35:20 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 48
2022-03-30 22:35:20 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 48
2022-03-30 22:35:20 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 48
2022-03-30 22:35:20 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 48
2022-03-30 22:35:20 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (297876ms till timeout)
2022-03-30 22:35:20 [ForkJoinPool-3-worker-11] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.24 from Pod infra-namespace-kafka-clients-748578f786-8r49n finished with return code: 0
2022-03-30 22:35:20 [ForkJoinPool-3-worker-11] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 22:35:20 [ForkJoinPool-3-worker-11] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 22:35:20 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:35:20 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:346] In context testKafkaConnectResponse is everything deleted.
2022-03-30 22:35:20 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:35:20 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:267] testKafkaConnectResponse - Notifies waiting test cases:[testCruiseControlBasicAPIRequests, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testUpdateUser, testSendSimpleMessageTls, testAutoRenewAllCaCertsTriggeredByAnno, testReceiveSimpleMessageTls, testSendMessagesCustomListenerTlsScramSha, testKafkaAndZookeeperScaleUpScaleDown, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testUserOperatorMetrics, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperWatchersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics, testKafkaMetricsSettings, testClusterOperatorMetrics, testCruiseControlMetrics, testTopicOperatorMetrics, testKafkaConnectRequests, testKafkaConnectResponse, testKafkaExporterDifferentSetting] to and randomly select one to start execution
2022-03-30 22:35:20 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaConnectResponse
2022-03-30 22:35:20 [ForkJoinPool-3-worker-11] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 3
2022-03-30 22:35:20 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaConnectResponse-FINISHED
2022-03-30 22:35:20 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 22:35:20 [ForkJoinPool-3-worker-9] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.22 from Pod infra-namespace-kafka-clients-748578f786-8r49n finished with return code: 0
2022-03-30 22:35:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaProducer metrics will be available not ready, will try again in 1000 ms (299204ms till timeout)
2022-03-30 22:35:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:35:21 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 47
2022-03-30 22:35:21 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 47
2022-03-30 22:35:21 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 47
2022-03-30 22:35:21 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 47
2022-03-30 22:35:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (296866ms till timeout)
2022-03-30 22:35:21 [ForkJoinPool-3-worker-9] [32mINFO [m [MetricsIsolatedST:420] Looking for 'strimzi_bridge_kafka_producer_count' in bridge metrics
2022-03-30 22:35:21 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 22:35:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-8r49n -n infra-namespace -- curl 172.17.0.22:8080/metrics
2022-03-30 22:35:22 [ForkJoinPool-3-worker-9] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.22 from Pod infra-namespace-kafka-clients-748578f786-8r49n finished with return code: 0
2022-03-30 22:35:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for KafkaConsumer metrics will be available
2022-03-30 22:35:22 [ForkJoinPool-3-worker-9] [32mINFO [m [MetricsIsolatedST:428] Looking for 'strimzi_bridge_kafka_consumer_connection_count' in bridge metrics
2022-03-30 22:35:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 22:35:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-8r49n -n infra-namespace -- curl 172.17.0.22:8080/metrics
2022-03-30 22:35:22 [ForkJoinPool-3-worker-9] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.22 from Pod infra-namespace-kafka-clients-748578f786-8r49n finished with return code: 0
2022-03-30 22:35:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 22:35:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 22:35:22 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:35:22 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaBridgeMetrics
2022-03-30 22:35:22 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Job bridge-consumer in namespace infra-namespace
2022-03-30 22:35:22 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Job bridge-producer in namespace infra-namespace
2022-03-30 22:35:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:bridge-consumer
2022-03-30 22:35:22 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:bridge-producer
2022-03-30 22:35:22 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:35:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:267] testKafkaBridgeMetrics - Notifies waiting test cases:[testCruiseControlBasicAPIRequests, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testUpdateUser, testSendSimpleMessageTls, testAutoRenewAllCaCertsTriggeredByAnno, testReceiveSimpleMessageTls, testSendMessagesCustomListenerTlsScramSha, testKafkaAndZookeeperScaleUpScaleDown, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testUserOperatorMetrics, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperWatchersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics, testKafkaMetricsSettings, testClusterOperatorMetrics, testCruiseControlMetrics, testTopicOperatorMetrics, testKafkaConnectRequests, testKafkaConnectResponse, testKafkaExporterDifferentSetting] to and randomly select one to start execution
2022-03-30 22:35:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaBridgeMetrics
2022-03-30 22:35:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 2
2022-03-30 22:35:22 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaBridgeMetrics-FINISHED
2022-03-30 22:35:22 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 22:35:22 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 46
2022-03-30 22:35:22 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 46
2022-03-30 22:35:22 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 46
2022-03-30 22:35:22 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 46
2022-03-30 22:35:22 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (295856ms till timeout)
2022-03-30 22:35:23 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 45
2022-03-30 22:35:23 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 45
2022-03-30 22:35:23 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 45
2022-03-30 22:35:23 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 45
2022-03-30 22:35:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (294845ms till timeout)
2022-03-30 22:35:24 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 44
2022-03-30 22:35:24 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 44
2022-03-30 22:35:24 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 44
2022-03-30 22:35:24 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 44
2022-03-30 22:35:24 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (293836ms till timeout)
2022-03-30 22:35:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-czq98=bba39fab-a50f-4e64-b17d-cc986fa51aa5}
2022-03-30 22:35:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-czq98=bba39fab-a50f-4e64-b17d-cc986fa51aa5, metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv=a8311251-12e8-4af0-b566-3d001aa49af0}
2022-03-30 22:35:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {metrics-cluster-name-kafka-exporter-8454677f49-czq98=bba39fab-a50f-4e64-b17d-cc986fa51aa5}
2022-03-30 22:35:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace not ready, will try again in 5000 ms (594980ms till timeout)
2022-03-30 22:35:25 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 43
2022-03-30 22:35:25 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 43
2022-03-30 22:35:25 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 43
2022-03-30 22:35:25 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 43
2022-03-30 22:35:25 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (292828ms till timeout)
2022-03-30 22:35:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:35:26 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 42
2022-03-30 22:35:26 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 42
2022-03-30 22:35:26 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 42
2022-03-30 22:35:26 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 42
2022-03-30 22:35:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (291819ms till timeout)
2022-03-30 22:35:27 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 41
2022-03-30 22:35:27 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 41
2022-03-30 22:35:27 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 41
2022-03-30 22:35:27 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 41
2022-03-30 22:35:27 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (290809ms till timeout)
2022-03-30 22:35:28 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 40
2022-03-30 22:35:28 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 40
2022-03-30 22:35:28 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 40
2022-03-30 22:35:28 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 40
2022-03-30 22:35:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (289800ms till timeout)
2022-03-30 22:35:29 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 39
2022-03-30 22:35:29 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 39
2022-03-30 22:35:29 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 39
2022-03-30 22:35:29 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 39
2022-03-30 22:35:29 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (288791ms till timeout)
2022-03-30 22:35:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-czq98=bba39fab-a50f-4e64-b17d-cc986fa51aa5}
2022-03-30 22:35:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-czq98=bba39fab-a50f-4e64-b17d-cc986fa51aa5, metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv=a8311251-12e8-4af0-b566-3d001aa49af0}
2022-03-30 22:35:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {metrics-cluster-name-kafka-exporter-8454677f49-czq98=bba39fab-a50f-4e64-b17d-cc986fa51aa5}
2022-03-30 22:35:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace not ready, will try again in 5000 ms (589973ms till timeout)
2022-03-30 22:35:30 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 38
2022-03-30 22:35:30 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 38
2022-03-30 22:35:30 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 38
2022-03-30 22:35:30 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 38
2022-03-30 22:35:30 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (287781ms till timeout)
2022-03-30 22:35:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:35:31 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 37
2022-03-30 22:35:31 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 37
2022-03-30 22:35:31 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 37
2022-03-30 22:35:31 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 37
2022-03-30 22:35:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (286773ms till timeout)
2022-03-30 22:35:32 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 36
2022-03-30 22:35:32 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 36
2022-03-30 22:35:32 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 36
2022-03-30 22:35:32 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 36
2022-03-30 22:35:32 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (285763ms till timeout)
2022-03-30 22:35:33 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 35
2022-03-30 22:35:33 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 35
2022-03-30 22:35:33 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 35
2022-03-30 22:35:33 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 35
2022-03-30 22:35:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (284754ms till timeout)
2022-03-30 22:35:34 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 34
2022-03-30 22:35:34 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 34
2022-03-30 22:35:34 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 34
2022-03-30 22:35:34 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 34
2022-03-30 22:35:34 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (283745ms till timeout)
2022-03-30 22:35:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-czq98=bba39fab-a50f-4e64-b17d-cc986fa51aa5}
2022-03-30 22:35:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-czq98=bba39fab-a50f-4e64-b17d-cc986fa51aa5, metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv=a8311251-12e8-4af0-b566-3d001aa49af0}
2022-03-30 22:35:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {metrics-cluster-name-kafka-exporter-8454677f49-czq98=bba39fab-a50f-4e64-b17d-cc986fa51aa5}
2022-03-30 22:35:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace not ready, will try again in 5000 ms (584966ms till timeout)
2022-03-30 22:35:35 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 33
2022-03-30 22:35:35 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 33
2022-03-30 22:35:35 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 33
2022-03-30 22:35:35 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 33
2022-03-30 22:35:35 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (282736ms till timeout)
2022-03-30 22:35:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:35:36 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 32
2022-03-30 22:35:36 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 32
2022-03-30 22:35:36 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 32
2022-03-30 22:35:36 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 32
2022-03-30 22:35:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (281728ms till timeout)
2022-03-30 22:35:37 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 31
2022-03-30 22:35:37 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 31
2022-03-30 22:35:37 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 31
2022-03-30 22:35:37 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 31
2022-03-30 22:35:37 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (280718ms till timeout)
2022-03-30 22:35:38 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 30
2022-03-30 22:35:38 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 30
2022-03-30 22:35:38 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 30
2022-03-30 22:35:38 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 30
2022-03-30 22:35:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (279709ms till timeout)
2022-03-30 22:35:39 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 29
2022-03-30 22:35:39 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 29
2022-03-30 22:35:39 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 29
2022-03-30 22:35:39 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 29
2022-03-30 22:35:39 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (278700ms till timeout)
2022-03-30 22:35:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-czq98=bba39fab-a50f-4e64-b17d-cc986fa51aa5}
2022-03-30 22:35:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-czq98=bba39fab-a50f-4e64-b17d-cc986fa51aa5, metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv=a8311251-12e8-4af0-b566-3d001aa49af0}
2022-03-30 22:35:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {metrics-cluster-name-kafka-exporter-8454677f49-czq98=bba39fab-a50f-4e64-b17d-cc986fa51aa5}
2022-03-30 22:35:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace not ready, will try again in 5000 ms (579959ms till timeout)
2022-03-30 22:35:40 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 28
2022-03-30 22:35:40 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 28
2022-03-30 22:35:40 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 28
2022-03-30 22:35:40 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 28
2022-03-30 22:35:40 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (277691ms till timeout)
2022-03-30 22:35:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:35:41 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 27
2022-03-30 22:35:41 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 27
2022-03-30 22:35:41 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 27
2022-03-30 22:35:41 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 27
2022-03-30 22:35:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (276682ms till timeout)
2022-03-30 22:35:42 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 26
2022-03-30 22:35:42 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 26
2022-03-30 22:35:42 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 26
2022-03-30 22:35:42 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 26
2022-03-30 22:35:42 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (275672ms till timeout)
2022-03-30 22:35:43 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 25
2022-03-30 22:35:43 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 25
2022-03-30 22:35:43 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 25
2022-03-30 22:35:43 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 25
2022-03-30 22:35:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (274663ms till timeout)
2022-03-30 22:35:44 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 24
2022-03-30 22:35:44 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 24
2022-03-30 22:35:44 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 24
2022-03-30 22:35:44 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 24
2022-03-30 22:35:44 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (273654ms till timeout)
2022-03-30 22:35:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-czq98=bba39fab-a50f-4e64-b17d-cc986fa51aa5}
2022-03-30 22:35:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-czq98=bba39fab-a50f-4e64-b17d-cc986fa51aa5, metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv=a8311251-12e8-4af0-b566-3d001aa49af0}
2022-03-30 22:35:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {metrics-cluster-name-kafka-exporter-8454677f49-czq98=bba39fab-a50f-4e64-b17d-cc986fa51aa5}
2022-03-30 22:35:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace not ready, will try again in 5000 ms (574952ms till timeout)
2022-03-30 22:35:45 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 23
2022-03-30 22:35:45 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 23
2022-03-30 22:35:45 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 23
2022-03-30 22:35:45 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 23
2022-03-30 22:35:45 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (272644ms till timeout)
2022-03-30 22:35:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:35:46 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 22
2022-03-30 22:35:46 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 22
2022-03-30 22:35:46 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 22
2022-03-30 22:35:46 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 22
2022-03-30 22:35:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (271636ms till timeout)
2022-03-30 22:35:47 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 21
2022-03-30 22:35:47 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 21
2022-03-30 22:35:47 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 21
2022-03-30 22:35:47 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 21
2022-03-30 22:35:47 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (270627ms till timeout)
2022-03-30 22:35:48 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 20
2022-03-30 22:35:48 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 20
2022-03-30 22:35:48 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 20
2022-03-30 22:35:48 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 20
2022-03-30 22:35:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (269618ms till timeout)
2022-03-30 22:35:49 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 19
2022-03-30 22:35:49 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 19
2022-03-30 22:35:49 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 19
2022-03-30 22:35:49 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 19
2022-03-30 22:35:49 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (268609ms till timeout)
2022-03-30 22:35:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-czq98=bba39fab-a50f-4e64-b17d-cc986fa51aa5}
2022-03-30 22:35:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-czq98=bba39fab-a50f-4e64-b17d-cc986fa51aa5, metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv=a8311251-12e8-4af0-b566-3d001aa49af0}
2022-03-30 22:35:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {metrics-cluster-name-kafka-exporter-8454677f49-czq98=bba39fab-a50f-4e64-b17d-cc986fa51aa5}
2022-03-30 22:35:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace not ready, will try again in 5000 ms (569944ms till timeout)
2022-03-30 22:35:50 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 18
2022-03-30 22:35:50 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 18
2022-03-30 22:35:50 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 18
2022-03-30 22:35:50 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 18
2022-03-30 22:35:50 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (267599ms till timeout)
2022-03-30 22:35:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:35:51 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 17
2022-03-30 22:35:51 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 17
2022-03-30 22:35:51 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 17
2022-03-30 22:35:51 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 17
2022-03-30 22:35:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (266590ms till timeout)
2022-03-30 22:35:52 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 16
2022-03-30 22:35:52 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 16
2022-03-30 22:35:52 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 16
2022-03-30 22:35:52 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 16
2022-03-30 22:35:52 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (265581ms till timeout)
2022-03-30 22:35:53 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 15
2022-03-30 22:35:53 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 15
2022-03-30 22:35:53 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 15
2022-03-30 22:35:53 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 15
2022-03-30 22:35:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (264571ms till timeout)
2022-03-30 22:35:54 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 14
2022-03-30 22:35:54 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 14
2022-03-30 22:35:54 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 14
2022-03-30 22:35:54 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 14
2022-03-30 22:35:54 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (263562ms till timeout)
2022-03-30 22:35:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-czq98=bba39fab-a50f-4e64-b17d-cc986fa51aa5}
2022-03-30 22:35:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-czq98=bba39fab-a50f-4e64-b17d-cc986fa51aa5, metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv=a8311251-12e8-4af0-b566-3d001aa49af0}
2022-03-30 22:35:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {metrics-cluster-name-kafka-exporter-8454677f49-czq98=bba39fab-a50f-4e64-b17d-cc986fa51aa5}
2022-03-30 22:35:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace not ready, will try again in 5000 ms (564937ms till timeout)
2022-03-30 22:35:55 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 13
2022-03-30 22:35:55 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 13
2022-03-30 22:35:55 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 13
2022-03-30 22:35:55 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 13
2022-03-30 22:35:55 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (262552ms till timeout)
2022-03-30 22:35:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:35:56 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 12
2022-03-30 22:35:56 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 12
2022-03-30 22:35:56 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 12
2022-03-30 22:35:56 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 12
2022-03-30 22:35:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (261543ms till timeout)
2022-03-30 22:35:57 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 11
2022-03-30 22:35:57 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 11
2022-03-30 22:35:57 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 11
2022-03-30 22:35:57 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 11
2022-03-30 22:35:57 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (260534ms till timeout)
2022-03-30 22:35:58 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 10
2022-03-30 22:35:58 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 10
2022-03-30 22:35:58 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 10
2022-03-30 22:35:58 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 10
2022-03-30 22:35:58 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (259525ms till timeout)
2022-03-30 22:35:59 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 9
2022-03-30 22:35:59 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 9
2022-03-30 22:35:59 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 9
2022-03-30 22:35:59 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 9
2022-03-30 22:35:59 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (258516ms till timeout)
2022-03-30 22:36:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-czq98=bba39fab-a50f-4e64-b17d-cc986fa51aa5}
2022-03-30 22:36:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-czq98=bba39fab-a50f-4e64-b17d-cc986fa51aa5, metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv=a8311251-12e8-4af0-b566-3d001aa49af0}
2022-03-30 22:36:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {metrics-cluster-name-kafka-exporter-8454677f49-czq98=bba39fab-a50f-4e64-b17d-cc986fa51aa5}
2022-03-30 22:36:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace not ready, will try again in 5000 ms (559930ms till timeout)
2022-03-30 22:36:00 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 8
2022-03-30 22:36:00 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 8
2022-03-30 22:36:00 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 8
2022-03-30 22:36:00 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 8
2022-03-30 22:36:00 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (257507ms till timeout)
2022-03-30 22:36:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:36:01 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 7
2022-03-30 22:36:01 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 7
2022-03-30 22:36:01 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 7
2022-03-30 22:36:01 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 7
2022-03-30 22:36:01 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (256497ms till timeout)
2022-03-30 22:36:02 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 6
2022-03-30 22:36:02 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 6
2022-03-30 22:36:02 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 6
2022-03-30 22:36:02 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 6
2022-03-30 22:36:02 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (255488ms till timeout)
2022-03-30 22:36:03 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 5
2022-03-30 22:36:03 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 5
2022-03-30 22:36:03 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 5
2022-03-30 22:36:03 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 5
2022-03-30 22:36:03 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (254479ms till timeout)
2022-03-30 22:36:04 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 4
2022-03-30 22:36:04 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 4
2022-03-30 22:36:04 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 4
2022-03-30 22:36:04 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 4
2022-03-30 22:36:04 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (253469ms till timeout)
2022-03-30 22:36:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-czq98=bba39fab-a50f-4e64-b17d-cc986fa51aa5}
2022-03-30 22:36:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-czq98=bba39fab-a50f-4e64-b17d-cc986fa51aa5, metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv=a8311251-12e8-4af0-b566-3d001aa49af0}
2022-03-30 22:36:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {metrics-cluster-name-kafka-exporter-8454677f49-czq98=bba39fab-a50f-4e64-b17d-cc986fa51aa5}
2022-03-30 22:36:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace not ready, will try again in 5000 ms (554923ms till timeout)
2022-03-30 22:36:05 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 3
2022-03-30 22:36:05 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 3
2022-03-30 22:36:05 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 3
2022-03-30 22:36:05 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 3
2022-03-30 22:36:05 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (252460ms till timeout)
2022-03-30 22:36:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:36:06 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 2
2022-03-30 22:36:06 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 2
2022-03-30 22:36:06 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 2
2022-03-30 22:36:06 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 2
2022-03-30 22:36:06 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (251451ms till timeout)
2022-03-30 22:36:07 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 1
2022-03-30 22:36:07 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 1
2022-03-30 22:36:07 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 1
2022-03-30 22:36:07 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 1
2022-03-30 22:36:07 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:335] All pods are stable second-kafka-cluster-entity-operator-5f8949dc9c-kd952 ,second-kafka-cluster-kafka-0 ,second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz ,second-kafka-cluster-zookeeper-0
2022-03-30 22:36:07 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: second-kafka-cluster are stable
2022-03-30 22:36:07 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for pod with prefixsecond-kafka-cluster is present.
2022-03-30 22:36:07 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Pods stability in phase Running
2022-03-30 22:36:07 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 50
2022-03-30 22:36:07 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 50
2022-03-30 22:36:07 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 50
2022-03-30 22:36:07 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 50
2022-03-30 22:36:07 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (299992ms till timeout)
2022-03-30 22:36:08 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 49
2022-03-30 22:36:08 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 49
2022-03-30 22:36:08 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 49
2022-03-30 22:36:08 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 49
2022-03-30 22:36:08 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (298983ms till timeout)
2022-03-30 22:36:09 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 48
2022-03-30 22:36:09 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 48
2022-03-30 22:36:09 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 48
2022-03-30 22:36:09 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 48
2022-03-30 22:36:09 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (297974ms till timeout)
2022-03-30 22:36:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-czq98=bba39fab-a50f-4e64-b17d-cc986fa51aa5}
2022-03-30 22:36:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv=a8311251-12e8-4af0-b566-3d001aa49af0}
2022-03-30 22:36:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DeploymentUtils:119] All pods seem to have rolled
2022-03-30 22:36:10 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: metrics-cluster-name-kafka-exporter will be ready
2022-03-30 22:36:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: metrics-cluster-name-kafka-exporter will be ready
2022-03-30 22:36:10 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: metrics-cluster-name-kafka-exporter is ready
2022-03-30 22:36:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready
2022-03-30 22:36:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv not ready: metrics-cluster-name-kafka-exporter)
2022-03-30 22:36:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv are ready
2022-03-30 22:36:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-30 22:36:10 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 47
2022-03-30 22:36:10 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 47
2022-03-30 22:36:10 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 47
2022-03-30 22:36:10 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 47
2022-03-30 22:36:10 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (296964ms till timeout)
2022-03-30 22:36:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv not ready: metrics-cluster-name-kafka-exporter)
2022-03-30 22:36:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv are ready
2022-03-30 22:36:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598992ms till timeout)
2022-03-30 22:36:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:36:11 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 46
2022-03-30 22:36:11 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 46
2022-03-30 22:36:11 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 46
2022-03-30 22:36:11 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 46
2022-03-30 22:36:11 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (295955ms till timeout)
2022-03-30 22:36:12 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv not ready: metrics-cluster-name-kafka-exporter)
2022-03-30 22:36:12 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv are ready
2022-03-30 22:36:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597988ms till timeout)
2022-03-30 22:36:12 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 45
2022-03-30 22:36:12 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 45
2022-03-30 22:36:12 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 45
2022-03-30 22:36:12 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 45
2022-03-30 22:36:12 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (294946ms till timeout)
2022-03-30 22:36:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv not ready: metrics-cluster-name-kafka-exporter)
2022-03-30 22:36:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv are ready
2022-03-30 22:36:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596983ms till timeout)
2022-03-30 22:36:13 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 44
2022-03-30 22:36:13 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 44
2022-03-30 22:36:13 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 44
2022-03-30 22:36:13 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 44
2022-03-30 22:36:13 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (293936ms till timeout)
2022-03-30 22:36:14 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv not ready: metrics-cluster-name-kafka-exporter)
2022-03-30 22:36:14 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv are ready
2022-03-30 22:36:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595979ms till timeout)
2022-03-30 22:36:14 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 43
2022-03-30 22:36:14 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 43
2022-03-30 22:36:14 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 43
2022-03-30 22:36:14 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 43
2022-03-30 22:36:14 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (292927ms till timeout)
2022-03-30 22:36:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv not ready: metrics-cluster-name-kafka-exporter)
2022-03-30 22:36:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv are ready
2022-03-30 22:36:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594975ms till timeout)
2022-03-30 22:36:15 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 42
2022-03-30 22:36:15 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 42
2022-03-30 22:36:15 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 42
2022-03-30 22:36:15 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 42
2022-03-30 22:36:15 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (291917ms till timeout)
2022-03-30 22:36:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv not ready: metrics-cluster-name-kafka-exporter)
2022-03-30 22:36:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv are ready
2022-03-30 22:36:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593971ms till timeout)
2022-03-30 22:36:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:36:16 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 41
2022-03-30 22:36:16 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 41
2022-03-30 22:36:16 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 41
2022-03-30 22:36:16 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 41
2022-03-30 22:36:16 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (290908ms till timeout)
2022-03-30 22:36:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv not ready: metrics-cluster-name-kafka-exporter)
2022-03-30 22:36:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv are ready
2022-03-30 22:36:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592966ms till timeout)
2022-03-30 22:36:17 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 40
2022-03-30 22:36:17 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 40
2022-03-30 22:36:17 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 40
2022-03-30 22:36:17 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 40
2022-03-30 22:36:17 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (289899ms till timeout)
2022-03-30 22:36:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv not ready: metrics-cluster-name-kafka-exporter)
2022-03-30 22:36:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv are ready
2022-03-30 22:36:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591962ms till timeout)
2022-03-30 22:36:18 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 39
2022-03-30 22:36:18 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 39
2022-03-30 22:36:18 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 39
2022-03-30 22:36:18 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 39
2022-03-30 22:36:18 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (288889ms till timeout)
2022-03-30 22:36:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv not ready: metrics-cluster-name-kafka-exporter)
2022-03-30 22:36:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv are ready
2022-03-30 22:36:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590957ms till timeout)
2022-03-30 22:36:19 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 38
2022-03-30 22:36:20 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 38
2022-03-30 22:36:20 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 38
2022-03-30 22:36:20 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 38
2022-03-30 22:36:20 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (287880ms till timeout)
2022-03-30 22:36:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv not ready: metrics-cluster-name-kafka-exporter)
2022-03-30 22:36:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv are ready
2022-03-30 22:36:20 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:141] Deployment metrics-cluster-name-kafka-exporter rolling update finished
2022-03-30 22:36:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl exec metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv -n infra-namespace -- cat /tmp/run.sh
2022-03-30 22:36:20 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsIsolatedST:608] Metrics collection for pod metrics-cluster-name-kafka-exporter-8b86874b8-zfhlv return code - 0
2022-03-30 22:36:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 22:36:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 22:36:20 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:36:20 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:346] In context testKafkaExporterDifferentSetting is everything deleted.
2022-03-30 22:36:20 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:36:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:267] testKafkaExporterDifferentSetting - Notifies waiting test cases:[testCruiseControlBasicAPIRequests, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testUpdateUser, testSendSimpleMessageTls, testAutoRenewAllCaCertsTriggeredByAnno, testReceiveSimpleMessageTls, testSendMessagesCustomListenerTlsScramSha, testKafkaAndZookeeperScaleUpScaleDown, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testUserOperatorMetrics, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperWatchersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics, testKafkaMetricsSettings, testClusterOperatorMetrics, testCruiseControlMetrics, testTopicOperatorMetrics, testKafkaConnectRequests, testKafkaConnectResponse, testKafkaExporterDifferentSetting] to and randomly select one to start execution
2022-03-30 22:36:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaExporterDifferentSetting
2022-03-30 22:36:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 1
2022-03-30 22:36:20 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaExporterDifferentSetting-FINISHED
2022-03-30 22:36:20 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 22:36:21 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 37
2022-03-30 22:36:21 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 37
2022-03-30 22:36:21 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 37
2022-03-30 22:36:21 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 37
2022-03-30 22:36:21 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (286871ms till timeout)
2022-03-30 22:36:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:36:22 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 36
2022-03-30 22:36:22 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 36
2022-03-30 22:36:22 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 36
2022-03-30 22:36:22 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 36
2022-03-30 22:36:22 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (285862ms till timeout)
2022-03-30 22:36:23 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 35
2022-03-30 22:36:23 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 35
2022-03-30 22:36:23 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 35
2022-03-30 22:36:23 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 35
2022-03-30 22:36:23 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (284852ms till timeout)
2022-03-30 22:36:24 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 34
2022-03-30 22:36:24 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 34
2022-03-30 22:36:24 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 34
2022-03-30 22:36:24 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 34
2022-03-30 22:36:24 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (283843ms till timeout)
2022-03-30 22:36:25 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 33
2022-03-30 22:36:25 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 33
2022-03-30 22:36:25 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 33
2022-03-30 22:36:25 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 33
2022-03-30 22:36:25 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (282834ms till timeout)
2022-03-30 22:36:26 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 32
2022-03-30 22:36:26 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 32
2022-03-30 22:36:26 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 32
2022-03-30 22:36:26 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 32
2022-03-30 22:36:26 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (281825ms till timeout)
2022-03-30 22:36:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:36:27 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 31
2022-03-30 22:36:27 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 31
2022-03-30 22:36:27 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 31
2022-03-30 22:36:27 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 31
2022-03-30 22:36:27 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (280816ms till timeout)
2022-03-30 22:36:28 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 30
2022-03-30 22:36:28 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 30
2022-03-30 22:36:28 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 30
2022-03-30 22:36:28 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 30
2022-03-30 22:36:28 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (279807ms till timeout)
2022-03-30 22:36:29 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 29
2022-03-30 22:36:29 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 29
2022-03-30 22:36:29 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 29
2022-03-30 22:36:29 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 29
2022-03-30 22:36:29 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (278797ms till timeout)
2022-03-30 22:36:30 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 28
2022-03-30 22:36:30 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 28
2022-03-30 22:36:30 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 28
2022-03-30 22:36:30 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 28
2022-03-30 22:36:30 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (277788ms till timeout)
2022-03-30 22:36:31 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 27
2022-03-30 22:36:31 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 27
2022-03-30 22:36:31 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 27
2022-03-30 22:36:31 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 27
2022-03-30 22:36:31 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (276778ms till timeout)
2022-03-30 22:36:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:36:32 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 26
2022-03-30 22:36:32 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 26
2022-03-30 22:36:32 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 26
2022-03-30 22:36:32 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 26
2022-03-30 22:36:32 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (275769ms till timeout)
2022-03-30 22:36:33 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 25
2022-03-30 22:36:33 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 25
2022-03-30 22:36:33 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 25
2022-03-30 22:36:33 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 25
2022-03-30 22:36:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (274760ms till timeout)
2022-03-30 22:36:34 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 24
2022-03-30 22:36:34 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 24
2022-03-30 22:36:34 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 24
2022-03-30 22:36:34 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 24
2022-03-30 22:36:34 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (273751ms till timeout)
2022-03-30 22:36:35 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 23
2022-03-30 22:36:35 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 23
2022-03-30 22:36:35 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 23
2022-03-30 22:36:35 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 23
2022-03-30 22:36:35 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (272741ms till timeout)
2022-03-30 22:36:36 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 22
2022-03-30 22:36:36 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 22
2022-03-30 22:36:36 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 22
2022-03-30 22:36:36 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 22
2022-03-30 22:36:36 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (271733ms till timeout)
2022-03-30 22:36:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:36:37 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 21
2022-03-30 22:36:37 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 21
2022-03-30 22:36:37 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 21
2022-03-30 22:36:37 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 21
2022-03-30 22:36:37 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (270724ms till timeout)
2022-03-30 22:36:38 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 20
2022-03-30 22:36:38 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 20
2022-03-30 22:36:38 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 20
2022-03-30 22:36:38 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 20
2022-03-30 22:36:38 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (269714ms till timeout)
2022-03-30 22:36:39 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 19
2022-03-30 22:36:39 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 19
2022-03-30 22:36:39 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 19
2022-03-30 22:36:39 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 19
2022-03-30 22:36:39 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (268705ms till timeout)
2022-03-30 22:36:40 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 18
2022-03-30 22:36:40 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 18
2022-03-30 22:36:40 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 18
2022-03-30 22:36:40 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 18
2022-03-30 22:36:40 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (267696ms till timeout)
2022-03-30 22:36:41 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 17
2022-03-30 22:36:41 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 17
2022-03-30 22:36:41 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 17
2022-03-30 22:36:41 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 17
2022-03-30 22:36:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (266686ms till timeout)
2022-03-30 22:36:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:36:42 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 16
2022-03-30 22:36:42 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 16
2022-03-30 22:36:42 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 16
2022-03-30 22:36:42 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 16
2022-03-30 22:36:42 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (265677ms till timeout)
2022-03-30 22:36:43 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 15
2022-03-30 22:36:43 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 15
2022-03-30 22:36:43 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 15
2022-03-30 22:36:43 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 15
2022-03-30 22:36:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (264667ms till timeout)
2022-03-30 22:36:44 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 14
2022-03-30 22:36:44 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 14
2022-03-30 22:36:44 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 14
2022-03-30 22:36:44 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 14
2022-03-30 22:36:44 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (263657ms till timeout)
2022-03-30 22:36:45 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 13
2022-03-30 22:36:45 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 13
2022-03-30 22:36:45 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 13
2022-03-30 22:36:45 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 13
2022-03-30 22:36:45 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (262647ms till timeout)
2022-03-30 22:36:46 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 12
2022-03-30 22:36:46 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 12
2022-03-30 22:36:46 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 12
2022-03-30 22:36:46 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 12
2022-03-30 22:36:46 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (261639ms till timeout)
2022-03-30 22:36:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:36:47 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 11
2022-03-30 22:36:47 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 11
2022-03-30 22:36:47 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 11
2022-03-30 22:36:47 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 11
2022-03-30 22:36:47 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (260630ms till timeout)
2022-03-30 22:36:48 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 10
2022-03-30 22:36:48 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 10
2022-03-30 22:36:48 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 10
2022-03-30 22:36:48 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 10
2022-03-30 22:36:48 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (259621ms till timeout)
2022-03-30 22:36:49 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 9
2022-03-30 22:36:49 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 9
2022-03-30 22:36:49 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 9
2022-03-30 22:36:49 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 9
2022-03-30 22:36:49 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (258611ms till timeout)
2022-03-30 22:36:50 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 8
2022-03-30 22:36:50 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 8
2022-03-30 22:36:50 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 8
2022-03-30 22:36:50 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 8
2022-03-30 22:36:50 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (257602ms till timeout)
2022-03-30 22:36:51 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 7
2022-03-30 22:36:51 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 7
2022-03-30 22:36:51 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 7
2022-03-30 22:36:51 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 7
2022-03-30 22:36:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (256593ms till timeout)
2022-03-30 22:36:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:36:52 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 6
2022-03-30 22:36:52 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 6
2022-03-30 22:36:52 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 6
2022-03-30 22:36:52 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 6
2022-03-30 22:36:52 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (255583ms till timeout)
2022-03-30 22:36:53 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 5
2022-03-30 22:36:53 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 5
2022-03-30 22:36:53 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 5
2022-03-30 22:36:53 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 5
2022-03-30 22:36:53 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (254574ms till timeout)
2022-03-30 22:36:54 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 4
2022-03-30 22:36:54 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 4
2022-03-30 22:36:54 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 4
2022-03-30 22:36:54 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 4
2022-03-30 22:36:54 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (253564ms till timeout)
2022-03-30 22:36:55 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 3
2022-03-30 22:36:55 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 3
2022-03-30 22:36:55 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 3
2022-03-30 22:36:55 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 3
2022-03-30 22:36:55 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (252555ms till timeout)
2022-03-30 22:36:56 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 2
2022-03-30 22:36:56 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 2
2022-03-30 22:36:56 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 2
2022-03-30 22:36:56 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 2
2022-03-30 22:36:56 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (251546ms till timeout)
2022-03-30 22:36:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:36:57 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-kd952 is in the Running state. Remaining seconds pod to be stable 1
2022-03-30 22:36:57 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 1
2022-03-30 22:36:57 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz is in the Running state. Remaining seconds pod to be stable 1
2022-03-30 22:36:57 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 1
2022-03-30 22:36:57 [ForkJoinPool-3-worker-17] [32mINFO [m [PodUtils:335] All pods are stable second-kafka-cluster-entity-operator-5f8949dc9c-kd952 ,second-kafka-cluster-kafka-0 ,second-kafka-cluster-kafka-exporter-6dfb7ccc69-h9tgz ,second-kafka-cluster-zookeeper-0
2022-03-30 22:36:57 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 22:36:57 [ForkJoinPool-3-worker-17] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 22:36:57 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:36:57 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:346] In context testKafkaMetricsSettings is everything deleted.
2022-03-30 22:36:57 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:36:57 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:267] testKafkaMetricsSettings - Notifies waiting test cases:[testCruiseControlBasicAPIRequests, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testUpdateUser, testSendSimpleMessageTls, testAutoRenewAllCaCertsTriggeredByAnno, testReceiveSimpleMessageTls, testSendMessagesCustomListenerTlsScramSha, testKafkaAndZookeeperScaleUpScaleDown, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testUserOperatorMetrics, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperWatchersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics, testKafkaMetricsSettings, testClusterOperatorMetrics, testCruiseControlMetrics, testTopicOperatorMetrics, testKafkaConnectRequests, testKafkaConnectResponse, testKafkaExporterDifferentSetting] to and randomly select one to start execution
2022-03-30 22:36:57 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaMetricsSettings
2022-03-30 22:36:57 [ForkJoinPool-3-worker-17] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 0
2022-03-30 22:36:57 [ForkJoinPool-3-worker-17] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaMetricsSettings-FINISHED
2022-03-30 22:36:57 [ForkJoinPool-3-worker-17] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 22:36:57 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 22:36:57 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testReconcileStateMetricInTopicOperator-STARTED
2022-03-30 22:36:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 22:36:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 22:36:57 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 22:36:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaConnectResponse=my-cluster-1adc403d, testSendSimpleMessageTls=my-cluster-b70e5342, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7, testReconcileStateMetricInTopicOperator=my-cluster-27c9c361, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testZookeeperWatchersCount=my-cluster-402c5b37, testKafkaTopicPartitions=my-cluster-a4922820, testMirrorMaker2Metrics=my-cluster-3313dd99, testZookeeperAliveConnections=my-cluster-914c3bfa, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349, testKafkaTopicUnderReplicatedPartitions=my-cluster-be9a0596, testSendMessagesTlsScramSha=my-cluster-380e6586, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a, testZookeeperQuorumSize=my-cluster-6e3782cf, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070, testKafkaBridgeMetrics=my-cluster-047c2ac1, testUpdateUser=my-cluster-9b2bd187, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574, testTopicOperatorMetrics=my-cluster-a794fbf8, testKafkaActiveControllers=my-cluster-8ce86250, testKafkaExporterDifferentSetting=my-cluster-14731430, testKafkaConnectRequests=my-cluster-d11b8099, testCruiseControlMetrics=my-cluster-967b129c, testReceiveSimpleMessageTls=my-cluster-e8a518cd, testKafkaBrokersCount=my-cluster-b29913ee, testClusterOperatorMetrics=my-cluster-f7aec375, testUserOperatorMetrics=my-cluster-0ae7d432, testKafkaMetricsSettings=my-cluster-cdcb75c7, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9}
2022-03-30 22:36:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaConnectResponse=my-user-1674486009-1911649798, testSendSimpleMessageTls=my-user-602433797-900694548, testKafkaConnectIoNetwork=my-user-1415252823-246514724, testReconcileStateMetricInTopicOperator=my-user-1490907901-1971233398, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1606351157-1815623087, testSendMessagesCustomListenerTlsScramSha=my-user-445988529-51741427, testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testZookeeperWatchersCount=my-user-262847981-879141545, testKafkaTopicPartitions=my-user-560785779-785480841, testMirrorMaker2Metrics=my-user-1961983524-1687595462, testZookeeperAliveConnections=my-user-1378509155-786076411, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1590558165-46977776, testKafkaTopicUnderReplicatedPartitions=my-user-2067443766-1667934716, testSendMessagesTlsScramSha=my-user-2039401547-533835955, testMirrorMaker2TlsAndTlsClientAuth=my-user-707354298-1518057957, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659, testRackAwareConnectCorrectDeployment=my-user-389825219-1608762022, testZookeeperQuorumSize=my-user-625293736-680311428, testKafkaAndZookeeperScaleUpScaleDown=my-user-1001559761-691587912, testKafkaBridgeMetrics=my-user-2093621856-1949785375, testUpdateUser=my-user-595127408-1604948726, testKafkaInDifferentNsThanClusterOperator=my-user-1518021213-326071957, testTopicOperatorMetrics=my-user-84445048-466060506, testKafkaActiveControllers=my-user-1625454548-1904690658, testKafkaExporterDifferentSetting=my-user-1796654337-353542127, testKafkaConnectRequests=my-user-1702280136-701036393, testCruiseControlMetrics=my-user-621328593-1412519713, testReceiveSimpleMessageTls=my-user-761922164-2019863647, testKafkaBrokersCount=my-user-161347164-197258481, testClusterOperatorMetrics=my-user-323771535-901246887, testUserOperatorMetrics=my-user-366296042-1312500013, testKafkaMetricsSettings=my-user-58143961-1414938572, testMirrorMakerTlsAuthenticated=my-user-692603309-1711649825}
2022-03-30 22:36:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaConnectResponse=my-topic-1058695284-857971809, testSendSimpleMessageTls=my-topic-1299629356-597168683, testKafkaConnectIoNetwork=my-topic-1319353181-1427134540, testReconcileStateMetricInTopicOperator=my-topic-857654796-1592770117, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1931531910-629202741, testSendMessagesCustomListenerTlsScramSha=my-topic-1039904883-1044305687, testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testZookeeperWatchersCount=my-topic-464196692-1269638121, testKafkaTopicPartitions=my-topic-554356311-897225030, testMirrorMaker2Metrics=my-topic-972857008-735153820, testZookeeperAliveConnections=my-topic-142178161-401786225, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1210910585-1669754462, testKafkaTopicUnderReplicatedPartitions=my-topic-1554151013-1160196250, testSendMessagesTlsScramSha=my-topic-988094002-1400465950, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1279769671-1440257906, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393, testRackAwareConnectCorrectDeployment=my-topic-1801184984-1468632754, testZookeeperQuorumSize=my-topic-2058761647-1946180799, testKafkaAndZookeeperScaleUpScaleDown=my-topic-741363275-285355018, testKafkaBridgeMetrics=my-topic-784030803-1986191588, testUpdateUser=my-topic-1875736194-1340884809, testKafkaInDifferentNsThanClusterOperator=my-topic-2034182350-493335345, testTopicOperatorMetrics=my-topic-1485093017-1833103632, testKafkaActiveControllers=my-topic-1892159362-1633250172, testKafkaExporterDifferentSetting=my-topic-2139827014-879026263, testKafkaConnectRequests=my-topic-250627930-2121139168, testCruiseControlMetrics=my-topic-646189816-670248288, testReceiveSimpleMessageTls=my-topic-1007790874-1032272408, testKafkaBrokersCount=my-topic-480196974-662544545, testClusterOperatorMetrics=my-topic-174268591-1154466070, testUserOperatorMetrics=my-topic-1504927319-1084001771, testKafkaMetricsSettings=my-topic-66444693-1542984361, testMirrorMakerTlsAuthenticated=my-topic-1945138991-1836415682}
2022-03-30 22:36:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaConnectResponse=my-cluster-1adc403d-kafka-clients, testSendSimpleMessageTls=my-cluster-b70e5342-kafka-clients, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7-kafka-clients, testReconcileStateMetricInTopicOperator=my-cluster-27c9c361-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testZookeeperWatchersCount=my-cluster-402c5b37-kafka-clients, testKafkaTopicPartitions=my-cluster-a4922820-kafka-clients, testMirrorMaker2Metrics=my-cluster-3313dd99-kafka-clients, testZookeeperAliveConnections=my-cluster-914c3bfa-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-be9a0596-kafka-clients, testSendMessagesTlsScramSha=my-cluster-380e6586-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a-kafka-clients, testZookeeperQuorumSize=my-cluster-6e3782cf-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070-kafka-clients, testKafkaBridgeMetrics=my-cluster-047c2ac1-kafka-clients, testUpdateUser=my-cluster-9b2bd187-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574-kafka-clients, testTopicOperatorMetrics=my-cluster-a794fbf8-kafka-clients, testKafkaActiveControllers=my-cluster-8ce86250-kafka-clients, testKafkaExporterDifferentSetting=my-cluster-14731430-kafka-clients, testKafkaConnectRequests=my-cluster-d11b8099-kafka-clients, testCruiseControlMetrics=my-cluster-967b129c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-e8a518cd-kafka-clients, testKafkaBrokersCount=my-cluster-b29913ee-kafka-clients, testClusterOperatorMetrics=my-cluster-f7aec375-kafka-clients, testUserOperatorMetrics=my-cluster-0ae7d432-kafka-clients, testKafkaMetricsSettings=my-cluster-cdcb75c7-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9-kafka-clients}
2022-03-30 22:36:57 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-metrics-cluster-test
2022-03-30 22:36:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-857654796-1592770117 in namespace second-metrics-cluster-test
2022-03-30 22:36:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-857654796-1592770117
2022-03-30 22:36:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-857654796-1592770117 will have desired state: Ready
2022-03-30 22:36:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-857654796-1592770117 will have desired state: Ready
2022-03-30 22:36:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-857654796-1592770117 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 22:36:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-857654796-1592770117 is in desired state: Ready
2022-03-30 22:36:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 22:36:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec second-metrics-cluster-test-kafka-clients-6c57d6898-wvhzw -n second-metrics-cluster-test -- curl 172.17.0.14:8080/metrics
2022-03-30 22:36:58 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.14 from Pod second-metrics-cluster-test-kafka-clients-6c57d6898-wvhzw finished with return code: 0
2022-03-30 22:36:58 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:553] Checking if resource state metric reason message is "none" and KafkaTopic is ready
2022-03-30 22:36:58 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:556] Changing topic name in spec.topicName
2022-03-30 22:36:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-857654796-1592770117 will have desired state: NotReady
2022-03-30 22:36:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-857654796-1592770117 will have desired state: NotReady
2022-03-30 22:36:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-857654796-1592770117 will have desired state: NotReady not ready, will try again in 1000 ms (179997ms till timeout)
2022-03-30 22:36:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-857654796-1592770117 is in desired state: NotReady
2022-03-30 22:36:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 22:36:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec second-metrics-cluster-test-kafka-clients-6c57d6898-wvhzw -n second-metrics-cluster-test -- curl 172.17.0.14:8080/metrics
2022-03-30 22:36:59 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.14 from Pod second-metrics-cluster-test-kafka-clients-6c57d6898-wvhzw finished with return code: 0
2022-03-30 22:36:59 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:564] Changing back to it's original name and scaling replicas to be higher number
2022-03-30 22:36:59 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:132] Waiting for KafkaTopic change my-topic-857654796-1592770117
2022-03-30 22:36:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic change my-topic-857654796-1592770117
2022-03-30 22:36:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 22:36:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec second-metrics-cluster-test-kafka-clients-6c57d6898-wvhzw -n second-metrics-cluster-test -- curl 172.17.0.14:8080/metrics
2022-03-30 22:37:00 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.14 from Pod second-metrics-cluster-test-kafka-clients-6c57d6898-wvhzw finished with return code: 0
2022-03-30 22:37:00 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:576] Scaling replicas to be higher than before
2022-03-30 22:37:00 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:132] Waiting for KafkaTopic change my-topic-857654796-1592770117
2022-03-30 22:37:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic change my-topic-857654796-1592770117
2022-03-30 22:37:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 22:37:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec second-metrics-cluster-test-kafka-clients-6c57d6898-wvhzw -n second-metrics-cluster-test -- curl 172.17.0.14:8080/metrics
2022-03-30 22:37:00 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.14 from Pod second-metrics-cluster-test-kafka-clients-6c57d6898-wvhzw finished with return code: 0
2022-03-30 22:37:00 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:584] Changing KafkaTopic's spec to correct state
2022-03-30 22:37:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-857654796-1592770117 will have desired state: Ready
2022-03-30 22:37:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-857654796-1592770117 will have desired state: Ready
2022-03-30 22:37:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-857654796-1592770117 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 22:37:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-857654796-1592770117 is in desired state: Ready
2022-03-30 22:37:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 22:37:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec second-metrics-cluster-test-kafka-clients-6c57d6898-wvhzw -n second-metrics-cluster-test -- curl 172.17.0.14:8080/metrics
2022-03-30 22:37:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:37:01 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.14 from Pod second-metrics-cluster-test-kafka-clients-6c57d6898-wvhzw finished with return code: 0
2022-03-30 22:37:01 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 22:37:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 22:37:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 22:37:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:37:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testReconcileStateMetricInTopicOperator
2022-03-30 22:37:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-857654796-1592770117 in namespace second-metrics-cluster-test
2022-03-30 22:37:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-857654796-1592770117
2022-03-30 22:37:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-857654796-1592770117 not ready, will try again in 10000 ms (179995ms till timeout)
2022-03-30 22:37:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:37:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:37:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:37:11 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testReconcileStateMetricInTopicOperator-FINISHED
2022-03-30 22:37:11 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 22:37:11 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 22:37:11 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaExporterDataAfterExchange-STARTED
2022-03-30 22:37:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 22:37:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 22:37:11 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 22:37:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaConnectResponse=my-cluster-1adc403d, testSendSimpleMessageTls=my-cluster-b70e5342, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7, testReconcileStateMetricInTopicOperator=my-cluster-27c9c361, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testZookeeperWatchersCount=my-cluster-402c5b37, testKafkaTopicPartitions=my-cluster-a4922820, testMirrorMaker2Metrics=my-cluster-3313dd99, testZookeeperAliveConnections=my-cluster-914c3bfa, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349, testKafkaTopicUnderReplicatedPartitions=my-cluster-be9a0596, testSendMessagesTlsScramSha=my-cluster-380e6586, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a, testZookeeperQuorumSize=my-cluster-6e3782cf, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070, testKafkaBridgeMetrics=my-cluster-047c2ac1, testKafkaExporterDataAfterExchange=my-cluster-ae2f1e3d, testUpdateUser=my-cluster-9b2bd187, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574, testTopicOperatorMetrics=my-cluster-a794fbf8, testKafkaActiveControllers=my-cluster-8ce86250, testKafkaExporterDifferentSetting=my-cluster-14731430, testKafkaConnectRequests=my-cluster-d11b8099, testCruiseControlMetrics=my-cluster-967b129c, testReceiveSimpleMessageTls=my-cluster-e8a518cd, testKafkaBrokersCount=my-cluster-b29913ee, testClusterOperatorMetrics=my-cluster-f7aec375, testUserOperatorMetrics=my-cluster-0ae7d432, testKafkaMetricsSettings=my-cluster-cdcb75c7, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9}
2022-03-30 22:37:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaConnectResponse=my-user-1674486009-1911649798, testSendSimpleMessageTls=my-user-602433797-900694548, testKafkaConnectIoNetwork=my-user-1415252823-246514724, testReconcileStateMetricInTopicOperator=my-user-1490907901-1971233398, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1606351157-1815623087, testSendMessagesCustomListenerTlsScramSha=my-user-445988529-51741427, testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testZookeeperWatchersCount=my-user-262847981-879141545, testKafkaTopicPartitions=my-user-560785779-785480841, testMirrorMaker2Metrics=my-user-1961983524-1687595462, testZookeeperAliveConnections=my-user-1378509155-786076411, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1590558165-46977776, testKafkaTopicUnderReplicatedPartitions=my-user-2067443766-1667934716, testSendMessagesTlsScramSha=my-user-2039401547-533835955, testMirrorMaker2TlsAndTlsClientAuth=my-user-707354298-1518057957, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659, testRackAwareConnectCorrectDeployment=my-user-389825219-1608762022, testZookeeperQuorumSize=my-user-625293736-680311428, testKafkaAndZookeeperScaleUpScaleDown=my-user-1001559761-691587912, testKafkaBridgeMetrics=my-user-2093621856-1949785375, testKafkaExporterDataAfterExchange=my-user-446996476-4688085, testUpdateUser=my-user-595127408-1604948726, testKafkaInDifferentNsThanClusterOperator=my-user-1518021213-326071957, testTopicOperatorMetrics=my-user-84445048-466060506, testKafkaActiveControllers=my-user-1625454548-1904690658, testKafkaExporterDifferentSetting=my-user-1796654337-353542127, testKafkaConnectRequests=my-user-1702280136-701036393, testCruiseControlMetrics=my-user-621328593-1412519713, testReceiveSimpleMessageTls=my-user-761922164-2019863647, testKafkaBrokersCount=my-user-161347164-197258481, testClusterOperatorMetrics=my-user-323771535-901246887, testUserOperatorMetrics=my-user-366296042-1312500013, testKafkaMetricsSettings=my-user-58143961-1414938572, testMirrorMakerTlsAuthenticated=my-user-692603309-1711649825}
2022-03-30 22:37:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaConnectResponse=my-topic-1058695284-857971809, testSendSimpleMessageTls=my-topic-1299629356-597168683, testKafkaConnectIoNetwork=my-topic-1319353181-1427134540, testReconcileStateMetricInTopicOperator=my-topic-857654796-1592770117, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1931531910-629202741, testSendMessagesCustomListenerTlsScramSha=my-topic-1039904883-1044305687, testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testZookeeperWatchersCount=my-topic-464196692-1269638121, testKafkaTopicPartitions=my-topic-554356311-897225030, testMirrorMaker2Metrics=my-topic-972857008-735153820, testZookeeperAliveConnections=my-topic-142178161-401786225, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1210910585-1669754462, testKafkaTopicUnderReplicatedPartitions=my-topic-1554151013-1160196250, testSendMessagesTlsScramSha=my-topic-988094002-1400465950, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1279769671-1440257906, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393, testRackAwareConnectCorrectDeployment=my-topic-1801184984-1468632754, testZookeeperQuorumSize=my-topic-2058761647-1946180799, testKafkaAndZookeeperScaleUpScaleDown=my-topic-741363275-285355018, testKafkaBridgeMetrics=my-topic-784030803-1986191588, testKafkaExporterDataAfterExchange=my-topic-773904850-151506781, testUpdateUser=my-topic-1875736194-1340884809, testKafkaInDifferentNsThanClusterOperator=my-topic-2034182350-493335345, testTopicOperatorMetrics=my-topic-1485093017-1833103632, testKafkaActiveControllers=my-topic-1892159362-1633250172, testKafkaExporterDifferentSetting=my-topic-2139827014-879026263, testKafkaConnectRequests=my-topic-250627930-2121139168, testCruiseControlMetrics=my-topic-646189816-670248288, testReceiveSimpleMessageTls=my-topic-1007790874-1032272408, testKafkaBrokersCount=my-topic-480196974-662544545, testClusterOperatorMetrics=my-topic-174268591-1154466070, testUserOperatorMetrics=my-topic-1504927319-1084001771, testKafkaMetricsSettings=my-topic-66444693-1542984361, testMirrorMakerTlsAuthenticated=my-topic-1945138991-1836415682}
2022-03-30 22:37:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaConnectResponse=my-cluster-1adc403d-kafka-clients, testSendSimpleMessageTls=my-cluster-b70e5342-kafka-clients, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7-kafka-clients, testReconcileStateMetricInTopicOperator=my-cluster-27c9c361-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testZookeeperWatchersCount=my-cluster-402c5b37-kafka-clients, testKafkaTopicPartitions=my-cluster-a4922820-kafka-clients, testMirrorMaker2Metrics=my-cluster-3313dd99-kafka-clients, testZookeeperAliveConnections=my-cluster-914c3bfa-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-be9a0596-kafka-clients, testSendMessagesTlsScramSha=my-cluster-380e6586-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a-kafka-clients, testZookeeperQuorumSize=my-cluster-6e3782cf-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070-kafka-clients, testKafkaBridgeMetrics=my-cluster-047c2ac1-kafka-clients, testKafkaExporterDataAfterExchange=my-cluster-ae2f1e3d-kafka-clients, testUpdateUser=my-cluster-9b2bd187-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574-kafka-clients, testTopicOperatorMetrics=my-cluster-a794fbf8-kafka-clients, testKafkaActiveControllers=my-cluster-8ce86250-kafka-clients, testKafkaExporterDifferentSetting=my-cluster-14731430-kafka-clients, testKafkaConnectRequests=my-cluster-d11b8099-kafka-clients, testCruiseControlMetrics=my-cluster-967b129c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-e8a518cd-kafka-clients, testKafkaBrokersCount=my-cluster-b29913ee-kafka-clients, testClusterOperatorMetrics=my-cluster-f7aec375-kafka-clients, testUserOperatorMetrics=my-cluster-0ae7d432-kafka-clients, testKafkaMetricsSettings=my-cluster-cdcb75c7-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9-kafka-clients}
2022-03-30 22:37:11 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-30 22:37:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@68dd5467, which are set.
2022-03-30 22:37:11 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@55c5b258, messages=[], arguments=[--topic, my-topic-1172351742-1939014984, --bootstrap-server, metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092, --max-messages, 5000], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='infra-namespace-kafka-clients-748578f786-8r49n', podNamespace='infra-namespace', bootstrapServer='metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092', topicName='my-topic-1172351742-1939014984', maxMessages=5000, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@68dd5467}
2022-03-30 22:37:11 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:94] Producing 5000 messages to metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092:my-topic-1172351742-1939014984 from pod infra-namespace-kafka-clients-748578f786-8r49n
2022-03-30 22:37:11 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec infra-namespace-kafka-clients-748578f786-8r49n -n infra-namespace -- /opt/kafka/producer.sh --topic my-topic-1172351742-1939014984 --bootstrap-server metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092 --max-messages 5000
2022-03-30 22:37:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-8r49n -n infra-namespace -- /opt/kafka/producer.sh --topic my-topic-1172351742-1939014984 --bootstrap-server metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092 --max-messages 5000
2022-03-30 22:37:14 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-03-30 22:37:14 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:101] Producer produced 5000 messages
2022-03-30 22:37:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@16661a9f, which are set.
2022-03-30 22:37:14 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@2b122afc, messages=[], arguments=[--topic, my-topic-1172351742-1939014984, --bootstrap-server, metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092, --max-messages, 5000, --group-id, my-consumer-group-444496723, --group-instance-id, instance362207974], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='infra-namespace-kafka-clients-748578f786-8r49n', podNamespace='infra-namespace', bootstrapServer='metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092', topicName='my-topic-1172351742-1939014984', maxMessages=5000, kafkaUsername='null', consumerGroupName='my-consumer-group-444496723', consumerInstanceId='instance362207974', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@16661a9f}
2022-03-30 22:37:14 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:157] Consuming 5000 messages from metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092#my-topic-1172351742-1939014984 from pod infra-namespace-kafka-clients-748578f786-8r49n
2022-03-30 22:37:14 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec infra-namespace-kafka-clients-748578f786-8r49n -n infra-namespace -- /opt/kafka/consumer.sh --topic my-topic-1172351742-1939014984 --bootstrap-server metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092 --max-messages 5000 --group-id my-consumer-group-444496723 --group-instance-id instance362207974
2022-03-30 22:37:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-8r49n -n infra-namespace -- /opt/kafka/consumer.sh --topic my-topic-1172351742-1939014984 --bootstrap-server metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092 --max-messages 5000 --group-id my-consumer-group-444496723 --group-instance-id instance362207974
2022-03-30 22:37:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:37:20 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-30 22:37:20 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 5000 messages
2022-03-30 22:37:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 22:37:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-8r49n -n infra-namespace -- curl 172.17.0.27:9404/metrics
2022-03-30 22:37:20 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.27 from Pod infra-namespace-kafka-clients-748578f786-8r49n finished with return code: 0
2022-03-30 22:37:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 22:37:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 22:37:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:37:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context testKafkaExporterDataAfterExchange is everything deleted.
2022-03-30 22:37:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:37:20 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaExporterDataAfterExchange-FINISHED
2022-03-30 22:37:20 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 22:37:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 22:37:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:690] [metrics.MetricsIsolatedST - After All] - Clean up after test suite
2022-03-30 22:37:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:37:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for MetricsIsolatedST
2022-03-30 22:37:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-284089119-285277287 in namespace infra-namespace
2022-03-30 22:37:20 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy second-kafka-cluster-entity-operator-allow in namespace second-metrics-cluster-test
2022-03-30 22:37:20 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-120969012-2034167634 in namespace infra-namespace
2022-03-30 22:37:20 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 mm2-cluster in namespace infra-namespace
2022-03-30 22:37:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-284089119-285277287
2022-03-30 22:37:20 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:second-kafka-cluster-entity-operator-allow
2022-03-30 22:37:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-120969012-2034167634
2022-03-30 22:37:20 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaMirrorMaker2:mm2-cluster
2022-03-30 22:37:20 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy second-kafka-cluster-kafka-exporter-allow in namespace second-metrics-cluster-test
2022-03-30 22:37:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-120969012-2034167634 not ready, will try again in 10000 ms (179988ms till timeout)
2022-03-30 22:37:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-284089119-285277287 not ready, will try again in 10000 ms (179981ms till timeout)
2022-03-30 22:37:20 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaMirrorMaker2:mm2-cluster not ready, will try again in 10000 ms (599982ms till timeout)
2022-03-30 22:37:20 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:second-kafka-cluster-kafka-exporter-allow
2022-03-30 22:37:20 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy metrics-cluster-name-kafka-exporter-allow in namespace infra-namespace
2022-03-30 22:37:20 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:metrics-cluster-name-kafka-exporter-allow
2022-03-30 22:37:20 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy metrics-cluster-name-entity-operator-allow in namespace infra-namespace
2022-03-30 22:37:20 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:metrics-cluster-name-entity-operator-allow
2022-03-30 22:37:20 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy cluster-operator-allow in namespace infra-namespace
2022-03-30 22:37:20 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:cluster-operator-allow
2022-03-30 22:37:20 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect metrics-cluster-name in namespace infra-namespace
2022-03-30 22:37:20 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaConnect:metrics-cluster-name
2022-03-30 22:37:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaConnect:metrics-cluster-name not ready, will try again in 10000 ms (599992ms till timeout)
2022-03-30 22:37:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:37:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:37:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-260886687-1962678096 in namespace infra-namespace
2022-03-30 22:37:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment infra-namespace-kafka-clients in namespace infra-namespace
2022-03-30 22:37:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-260886687-1962678096
2022-03-30 22:37:30 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge my-bridge in namespace infra-namespace
2022-03-30 22:37:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:infra-namespace-kafka-clients
2022-03-30 22:37:30 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaBridge:my-bridge
2022-03-30 22:37:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-260886687-1962678096 not ready, will try again in 10000 ms (179991ms till timeout)
2022-03-30 22:37:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:infra-namespace-kafka-clients not ready, will try again in 10000 ms (479989ms till timeout)
2022-03-30 22:37:30 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaBridge:my-bridge not ready, will try again in 10000 ms (479990ms till timeout)
2022-03-30 22:37:30 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Kafka second-kafka-cluster in namespace second-metrics-cluster-test
2022-03-30 22:37:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:second-kafka-cluster
2022-03-30 22:37:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:second-kafka-cluster not ready, will try again in 10000 ms (839957ms till timeout)
2022-03-30 22:37:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:37:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:37:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment second-metrics-cluster-test-kafka-clients in namespace second-metrics-cluster-test
2022-03-30 22:37:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:second-metrics-cluster-test-kafka-clients
2022-03-30 22:37:40 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1172351742-1939014984 in namespace infra-namespace
2022-03-30 22:37:40 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1172351742-1939014984
2022-03-30 22:37:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:second-metrics-cluster-test-kafka-clients not ready, will try again in 10000 ms (479989ms till timeout)
2022-03-30 22:37:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:infra-namespace-kafka-clients not ready, will try again in 10000 ms (469974ms till timeout)
2022-03-30 22:37:40 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1172351742-1939014984 not ready, will try again in 10000 ms (179993ms till timeout)
2022-03-30 22:37:40 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Kafka metrics-cluster-name in namespace infra-namespace
2022-03-30 22:37:40 [ForkJoinPool-3-worker-9] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace infra-namespace, for cruise control Kafka cluster metrics-cluster-name
2022-03-30 22:37:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:metrics-cluster-name
2022-03-30 22:37:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:metrics-cluster-name not ready, will try again in 10000 ms (839957ms till timeout)
2022-03-30 22:37:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:37:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:37:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:second-metrics-cluster-test-kafka-clients not ready, will try again in 10000 ms (469981ms till timeout)
2022-03-30 22:37:50 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1427443759-776507328 in namespace infra-namespace
2022-03-30 22:37:50 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1427443759-776507328
2022-03-30 22:37:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:infra-namespace-kafka-clients not ready, will try again in 10000 ms (459966ms till timeout)
2022-03-30 22:37:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:37:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:38:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:second-metrics-cluster-test-kafka-clients not ready, will try again in 10000 ms (459973ms till timeout)
2022-03-30 22:38:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:infra-namespace-kafka-clients not ready, will try again in 10000 ms (449958ms till timeout)
2022-03-30 22:38:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:38:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:38:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:second-metrics-cluster-test-kafka-clients not ready, will try again in 10000 ms (449965ms till timeout)
2022-03-30 22:38:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:38:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:38:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:second-metrics-cluster-test-kafka-clients not ready, will try again in 10000 ms (439957ms till timeout)
2022-03-30 22:38:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:38:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:38:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:second-metrics-cluster-test-kafka-clients not ready, will try again in 10000 ms (429949ms till timeout)
2022-03-30 22:38:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:38:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite connect.ConnectIsolatedST is waiting to lock to be released.
2022-03-30 22:38:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 20, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4,204.127 s - in io.strimzi.systemtest.metrics.MetricsIsolatedST
2022-03-30 22:38:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:136] Suite connect.ConnectIsolatedST has locked the @IsolatedSuite and other @IsolatedSuites must wait until lock is released.
2022-03-30 22:38:41 [ForkJoinPool-3-worker-15] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-30 22:38:41 [ForkJoinPool-3-worker-15] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-30 22:38:41 [ForkJoinPool-3-worker-15] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-30 22:38:41 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:38:41 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-30 22:38:41 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 22:38:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace second-metrics-cluster-test
2022-03-30 22:38:41 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 22:38:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 22:38:41 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 22:38:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 22:38:41 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 22:38:41 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 22:38:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 22:38:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 22:38:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 22:38:41 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 22:38:41 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 22:38:41 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:38:41 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 22:38:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io not ready, will try again in 10000 ms (179988ms till timeout)
2022-03-30 22:38:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 22:38:41 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 22:38:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 22:38:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 22:38:41 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace second-metrics-cluster-test
2022-03-30 22:38:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 22:38:41 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:38:41 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator not ready, will try again in 10000 ms (479985ms till timeout)
2022-03-30 22:38:41 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 22:38:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 22:38:41 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 22:38:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io not ready, will try again in 10000 ms (179984ms till timeout)
2022-03-30 22:38:41 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 22:38:41 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 22:38:41 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:38:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 22:38:41 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 22:38:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 22:38:41 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 22:38:41 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 22:38:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 22:38:41 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179947ms till timeout)
2022-03-30 22:38:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io not ready, will try again in 10000 ms (179952ms till timeout)
2022-03-30 22:38:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 22:38:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 22:38:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 22:38:51 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 22:38:51 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:38:51 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 22:38:51 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 22:38:51 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 22:38:51 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 22:38:51 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 22:38:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 22:38:51 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 22:38:51 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 22:38:51 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 22:38:51 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 22:38:51 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 22:38:51 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:38:51 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 22:38:51 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 22:38:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:38:51 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 22:38:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179876ms till timeout)
2022-03-30 22:38:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 22:38:51 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io not ready, will try again in 10000 ms (179908ms till timeout)
2022-03-30 22:38:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io not ready, will try again in 10000 ms (179909ms till timeout)
2022-03-30 22:38:51 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io not ready, will try again in 10000 ms (179886ms till timeout)
2022-03-30 22:38:51 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179712ms till timeout)
2022-03-30 22:39:01 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:39:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-30 22:39:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 22:39:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 22:39:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v111434
2022-03-30 22:39:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v111434
2022-03-30 22:39:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=111434&allowWatchBookmarks=true&watch=true...
2022-03-30 22:39:01 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 22:39:02 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 111435
2022-03-30 22:39:07 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 111646
2022-03-30 22:39:12 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 111650
2022-03-30 22:39:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: second-metrics-cluster-test
2022-03-30 22:39:12 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v111646 in namespace default
2022-03-30 22:39:12 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@5da9a1ab
2022-03-30 22:39:12 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 22:39:12 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@59963ec6
2022-03-30 22:39:12 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@59963ec6
2022-03-30 22:39:12 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@59963ec6
2022-03-30 22:39:12 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 22:39:12 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 22:39:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 22:39:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 22:39:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v111651
2022-03-30 22:39:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v111651
2022-03-30 22:39:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dsecond-metrics-cluster-test&resourceVersion=111651&allowWatchBookmarks=true&watch=true...
2022-03-30 22:39:12 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 22:39:12 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 111652
2022-03-30 22:39:17 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 111728
2022-03-30 22:39:22 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 111731
2022-03-30 22:39:22 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v111728 in namespace default
2022-03-30 22:39:22 [ForkJoinPool-3-worker-15] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@26fcd291
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-30 22:39:22 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@3d769e51
2022-03-30 22:39:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@3f2e6467, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@26fcd291, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-30 22:39:22 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 22:39:22 [ForkJoinPool-3-worker-15] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-30 22:39:22 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@2d42d4ad
2022-03-30 22:39:22 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@2d42d4ad
2022-03-30 22:39:22 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@2d42d4ad
2022-03-30 22:39:22 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 22:39:22 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 22:39:22 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-30 22:39:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-30 22:39:22 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 22:39:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 22:39:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:39:22 [ForkJoinPool-3-worker-15] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T22:39:22Z",
        "name": "infra-namespace",
        "resourceVersion": "111732",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "34bdfb40-ff07-403f-9eb1-29addf11b8ff"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 22:39:22 [ForkJoinPool-3-worker-15] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-30 22:39:22 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 22:39:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-30 22:39:22 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:39:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 22:39:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:39:22 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 22:39:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 22:39:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:39:22 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 22:39:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 22:39:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:39:22 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 22:39:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 22:39:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:39:22 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 22:39:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 22:39:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 22:39:22 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 22:39:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 22:39:23 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479995ms till timeout)
2022-03-30 22:39:24 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478992ms till timeout)
2022-03-30 22:39:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477989ms till timeout)
2022-03-30 22:39:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476986ms till timeout)
2022-03-30 22:39:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475983ms till timeout)
2022-03-30 22:39:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474980ms till timeout)
2022-03-30 22:39:29 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473977ms till timeout)
2022-03-30 22:39:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472974ms till timeout)
2022-03-30 22:39:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471970ms till timeout)
2022-03-30 22:39:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470968ms till timeout)
2022-03-30 22:39:33 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469965ms till timeout)
2022-03-30 22:39:34 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468962ms till timeout)
2022-03-30 22:39:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467959ms till timeout)
2022-03-30 22:39:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466956ms till timeout)
2022-03-30 22:39:37 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465953ms till timeout)
2022-03-30 22:39:38 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464950ms till timeout)
2022-03-30 22:39:39 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463948ms till timeout)
2022-03-30 22:39:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462945ms till timeout)
2022-03-30 22:39:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461942ms till timeout)
2022-03-30 22:39:42 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (460939ms till timeout)
2022-03-30 22:39:43 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (459936ms till timeout)
2022-03-30 22:39:44 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (458933ms till timeout)
2022-03-30 22:39:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (457930ms till timeout)
2022-03-30 22:39:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (456927ms till timeout)
2022-03-30 22:39:47 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (455924ms till timeout)
2022-03-30 22:39:48 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-30 22:39:48 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-30 22:39:48 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-30 22:39:48 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-9647k not ready: strimzi-cluster-operator)
2022-03-30 22:39:48 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-9647k are ready
2022-03-30 22:39:48 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-30 22:39:49 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-9647k not ready: strimzi-cluster-operator)
2022-03-30 22:39:49 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-9647k are ready
2022-03-30 22:39:49 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-30 22:39:50 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-9647k not ready: strimzi-cluster-operator)
2022-03-30 22:39:50 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-9647k are ready
2022-03-30 22:39:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597990ms till timeout)
2022-03-30 22:39:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-9647k not ready: strimzi-cluster-operator)
2022-03-30 22:39:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-9647k are ready
2022-03-30 22:39:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596987ms till timeout)
2022-03-30 22:39:52 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-9647k not ready: strimzi-cluster-operator)
2022-03-30 22:39:52 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-9647k are ready
2022-03-30 22:39:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595984ms till timeout)
2022-03-30 22:39:53 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-9647k not ready: strimzi-cluster-operator)
2022-03-30 22:39:53 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-9647k are ready
2022-03-30 22:39:53 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594980ms till timeout)
2022-03-30 22:39:54 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-9647k not ready: strimzi-cluster-operator)
2022-03-30 22:39:54 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-9647k are ready
2022-03-30 22:39:54 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593977ms till timeout)
2022-03-30 22:39:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-9647k not ready: strimzi-cluster-operator)
2022-03-30 22:39:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-9647k are ready
2022-03-30 22:39:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592974ms till timeout)
2022-03-30 22:39:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-9647k not ready: strimzi-cluster-operator)
2022-03-30 22:39:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-9647k are ready
2022-03-30 22:39:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591971ms till timeout)
2022-03-30 22:39:57 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-9647k not ready: strimzi-cluster-operator)
2022-03-30 22:39:57 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-9647k are ready
2022-03-30 22:39:57 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590967ms till timeout)
2022-03-30 22:39:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-9647k not ready: strimzi-cluster-operator)
2022-03-30 22:39:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-9647k are ready
2022-03-30 22:39:58 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-30 22:39:58 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 22:39:58 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testMultiNodeKafkaConnectWithConnectorCreation-STARTED
2022-03-30 22:39:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 22:39:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:659] [connect.ConnectIsolatedST - Before Each] - Setup test case environment
2022-03-30 22:39:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:77] [connect.ConnectIsolatedST] - Adding parallel test: testMultiNodeKafkaConnectWithConnectorCreation
2022-03-30 22:39:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:81] [connect.ConnectIsolatedST] - Parallel test count: 1
2022-03-30 22:39:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:230] testMultiNodeKafkaConnectWithConnectorCreation test now can proceed its execution
2022-03-30 22:39:58 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 22:39:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaConnectResponse=my-cluster-1adc403d, testSendSimpleMessageTls=my-cluster-b70e5342, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7, testReconcileStateMetricInTopicOperator=my-cluster-27c9c361, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608, testZookeeperWatchersCount=my-cluster-402c5b37, testKafkaTopicPartitions=my-cluster-a4922820, testMirrorMaker2Metrics=my-cluster-3313dd99, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-9c827741, testZookeeperAliveConnections=my-cluster-914c3bfa, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349, testKafkaTopicUnderReplicatedPartitions=my-cluster-be9a0596, testSendMessagesTlsScramSha=my-cluster-380e6586, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a, testZookeeperQuorumSize=my-cluster-6e3782cf, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070, testKafkaBridgeMetrics=my-cluster-047c2ac1, testKafkaExporterDataAfterExchange=my-cluster-ae2f1e3d, testUpdateUser=my-cluster-9b2bd187, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574, testTopicOperatorMetrics=my-cluster-a794fbf8, testKafkaActiveControllers=my-cluster-8ce86250, testKafkaExporterDifferentSetting=my-cluster-14731430, testKafkaConnectRequests=my-cluster-d11b8099, testCruiseControlMetrics=my-cluster-967b129c, testReceiveSimpleMessageTls=my-cluster-e8a518cd, testKafkaBrokersCount=my-cluster-b29913ee, testClusterOperatorMetrics=my-cluster-f7aec375, testUserOperatorMetrics=my-cluster-0ae7d432, testKafkaMetricsSettings=my-cluster-cdcb75c7, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9}
2022-03-30 22:39:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaConnectResponse=my-user-1674486009-1911649798, testSendSimpleMessageTls=my-user-602433797-900694548, testKafkaConnectIoNetwork=my-user-1415252823-246514724, testReconcileStateMetricInTopicOperator=my-user-1490907901-1971233398, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1606351157-1815623087, testSendMessagesCustomListenerTlsScramSha=my-user-445988529-51741427, testCruiseControlBasicAPIRequests=my-user-507552110-1812270342, testZookeeperWatchersCount=my-user-262847981-879141545, testKafkaTopicPartitions=my-user-560785779-785480841, testMirrorMaker2Metrics=my-user-1961983524-1687595462, testMultiNodeKafkaConnectWithConnectorCreation=my-user-2020360651-1910710512, testZookeeperAliveConnections=my-user-1378509155-786076411, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1590558165-46977776, testKafkaTopicUnderReplicatedPartitions=my-user-2067443766-1667934716, testSendMessagesTlsScramSha=my-user-2039401547-533835955, testMirrorMaker2TlsAndTlsClientAuth=my-user-707354298-1518057957, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-1699615032-1690627659, testRackAwareConnectCorrectDeployment=my-user-389825219-1608762022, testZookeeperQuorumSize=my-user-625293736-680311428, testKafkaAndZookeeperScaleUpScaleDown=my-user-1001559761-691587912, testKafkaBridgeMetrics=my-user-2093621856-1949785375, testKafkaExporterDataAfterExchange=my-user-446996476-4688085, testUpdateUser=my-user-595127408-1604948726, testKafkaInDifferentNsThanClusterOperator=my-user-1518021213-326071957, testTopicOperatorMetrics=my-user-84445048-466060506, testKafkaActiveControllers=my-user-1625454548-1904690658, testKafkaExporterDifferentSetting=my-user-1796654337-353542127, testKafkaConnectRequests=my-user-1702280136-701036393, testCruiseControlMetrics=my-user-621328593-1412519713, testReceiveSimpleMessageTls=my-user-761922164-2019863647, testKafkaBrokersCount=my-user-161347164-197258481, testClusterOperatorMetrics=my-user-323771535-901246887, testUserOperatorMetrics=my-user-366296042-1312500013, testKafkaMetricsSettings=my-user-58143961-1414938572, testMirrorMakerTlsAuthenticated=my-user-692603309-1711649825}
2022-03-30 22:39:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaConnectResponse=my-topic-1058695284-857971809, testSendSimpleMessageTls=my-topic-1299629356-597168683, testKafkaConnectIoNetwork=my-topic-1319353181-1427134540, testReconcileStateMetricInTopicOperator=my-topic-857654796-1592770117, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1931531910-629202741, testSendMessagesCustomListenerTlsScramSha=my-topic-1039904883-1044305687, testCruiseControlBasicAPIRequests=my-topic-716947866-29609027, testZookeeperWatchersCount=my-topic-464196692-1269638121, testKafkaTopicPartitions=my-topic-554356311-897225030, testMirrorMaker2Metrics=my-topic-972857008-735153820, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-1415615461-2048143239, testZookeeperAliveConnections=my-topic-142178161-401786225, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1210910585-1669754462, testKafkaTopicUnderReplicatedPartitions=my-topic-1554151013-1160196250, testSendMessagesTlsScramSha=my-topic-988094002-1400465950, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1279769671-1440257906, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1020192302-679765393, testRackAwareConnectCorrectDeployment=my-topic-1801184984-1468632754, testZookeeperQuorumSize=my-topic-2058761647-1946180799, testKafkaAndZookeeperScaleUpScaleDown=my-topic-741363275-285355018, testKafkaBridgeMetrics=my-topic-784030803-1986191588, testKafkaExporterDataAfterExchange=my-topic-773904850-151506781, testUpdateUser=my-topic-1875736194-1340884809, testKafkaInDifferentNsThanClusterOperator=my-topic-2034182350-493335345, testTopicOperatorMetrics=my-topic-1485093017-1833103632, testKafkaActiveControllers=my-topic-1892159362-1633250172, testKafkaExporterDifferentSetting=my-topic-2139827014-879026263, testKafkaConnectRequests=my-topic-250627930-2121139168, testCruiseControlMetrics=my-topic-646189816-670248288, testReceiveSimpleMessageTls=my-topic-1007790874-1032272408, testKafkaBrokersCount=my-topic-480196974-662544545, testClusterOperatorMetrics=my-topic-174268591-1154466070, testUserOperatorMetrics=my-topic-1504927319-1084001771, testKafkaMetricsSettings=my-topic-66444693-1542984361, testMirrorMakerTlsAuthenticated=my-topic-1945138991-1836415682}
2022-03-30 22:39:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaConnectResponse=my-cluster-1adc403d-kafka-clients, testSendSimpleMessageTls=my-cluster-b70e5342-kafka-clients, testKafkaConnectIoNetwork=my-cluster-b6c3aeb7-kafka-clients, testReconcileStateMetricInTopicOperator=my-cluster-27c9c361-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-b2680d3c-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-98f0c2b1-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-cdcd0608-kafka-clients, testZookeeperWatchersCount=my-cluster-402c5b37-kafka-clients, testKafkaTopicPartitions=my-cluster-a4922820-kafka-clients, testMirrorMaker2Metrics=my-cluster-3313dd99-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-9c827741-kafka-clients, testZookeeperAliveConnections=my-cluster-914c3bfa-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-58833349-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-be9a0596-kafka-clients, testSendMessagesTlsScramSha=my-cluster-380e6586-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-6e67dcda-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-7b742d09-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-bc24959a-kafka-clients, testZookeeperQuorumSize=my-cluster-6e3782cf-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-94dd1070-kafka-clients, testKafkaBridgeMetrics=my-cluster-047c2ac1-kafka-clients, testKafkaExporterDataAfterExchange=my-cluster-ae2f1e3d-kafka-clients, testUpdateUser=my-cluster-9b2bd187-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-0405d574-kafka-clients, testTopicOperatorMetrics=my-cluster-a794fbf8-kafka-clients, testKafkaActiveControllers=my-cluster-8ce86250-kafka-clients, testKafkaExporterDifferentSetting=my-cluster-14731430-kafka-clients, testKafkaConnectRequests=my-cluster-d11b8099-kafka-clients, testCruiseControlMetrics=my-cluster-967b129c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-e8a518cd-kafka-clients, testKafkaBrokersCount=my-cluster-b29913ee-kafka-clients, testClusterOperatorMetrics=my-cluster-f7aec375-kafka-clients, testUserOperatorMetrics=my-cluster-0ae7d432-kafka-clients, testKafkaMetricsSettings=my-cluster-cdcb75c7-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-a0cac5a9-kafka-clients}
2022-03-30 22:39:58 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-8 for test case:testMultiNodeKafkaConnectWithConnectorCreation
2022-03-30 22:39:58 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-8
2022-03-30 22:39:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-8
2022-03-30 22:39:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-8 -o json
2022-03-30 22:39:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-8 -o json
2022-03-30 22:39:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:39:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T22:39:58Z",
        "name": "namespace-8",
        "resourceVersion": "111833",
        "selfLink": "/api/v1/namespaces/namespace-8",
        "uid": "18406057-91d3-4e5c-b4ad-612f4dc395fb"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 22:39:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@c27758f5=[namespace-8]}
2022-03-30 22:39:58 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-8
2022-03-30 22:39:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-8, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 22:39:58 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-8
2022-03-30 22:39:58 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-13f2f3b4 in namespace namespace-8
2022-03-30 22:39:58 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-30 22:39:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-13f2f3b4
2022-03-30 22:39:58 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-13f2f3b4 will have desired state: Ready
2022-03-30 22:39:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-13f2f3b4 will have desired state: Ready
2022-03-30 22:39:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (839997ms till timeout)
2022-03-30 22:39:59 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (838995ms till timeout)
2022-03-30 22:40:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (837991ms till timeout)
2022-03-30 22:40:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (836988ms till timeout)
2022-03-30 22:40:02 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (835985ms till timeout)
2022-03-30 22:40:03 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (834982ms till timeout)
2022-03-30 22:40:04 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (833979ms till timeout)
2022-03-30 22:40:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (832976ms till timeout)
2022-03-30 22:40:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (831973ms till timeout)
2022-03-30 22:40:07 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (830970ms till timeout)
2022-03-30 22:40:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (829966ms till timeout)
2022-03-30 22:40:09 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (828962ms till timeout)
2022-03-30 22:40:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (827959ms till timeout)
2022-03-30 22:40:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (826956ms till timeout)
2022-03-30 22:40:12 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (825952ms till timeout)
2022-03-30 22:40:13 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (824949ms till timeout)
2022-03-30 22:40:14 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (823946ms till timeout)
2022-03-30 22:40:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (822943ms till timeout)
2022-03-30 22:40:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (821940ms till timeout)
2022-03-30 22:40:17 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (820937ms till timeout)
2022-03-30 22:40:18 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (819934ms till timeout)
2022-03-30 22:40:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (818931ms till timeout)
2022-03-30 22:40:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (817928ms till timeout)
2022-03-30 22:40:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (816925ms till timeout)
2022-03-30 22:40:22 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (815922ms till timeout)
2022-03-30 22:40:23 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (814919ms till timeout)
2022-03-30 22:40:24 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (813916ms till timeout)
2022-03-30 22:40:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (812912ms till timeout)
2022-03-30 22:40:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (811909ms till timeout)
2022-03-30 22:40:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (810905ms till timeout)
2022-03-30 22:40:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (809902ms till timeout)
2022-03-30 22:40:29 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (808899ms till timeout)
2022-03-30 22:40:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (807895ms till timeout)
2022-03-30 22:40:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (806892ms till timeout)
2022-03-30 22:40:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (805880ms till timeout)
2022-03-30 22:40:33 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (804875ms till timeout)
2022-03-30 22:40:34 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (803873ms till timeout)
2022-03-30 22:40:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (802869ms till timeout)
2022-03-30 22:40:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (801866ms till timeout)
2022-03-30 22:40:37 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (800863ms till timeout)
2022-03-30 22:40:38 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (799860ms till timeout)
2022-03-30 22:40:39 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (798857ms till timeout)
2022-03-30 22:40:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (797853ms till timeout)
2022-03-30 22:40:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (796850ms till timeout)
2022-03-30 22:40:42 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (795847ms till timeout)
2022-03-30 22:40:43 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (794844ms till timeout)
2022-03-30 22:40:44 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (793841ms till timeout)
2022-03-30 22:40:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (792838ms till timeout)
2022-03-30 22:40:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (791835ms till timeout)
2022-03-30 22:40:47 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (790832ms till timeout)
2022-03-30 22:40:48 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (789828ms till timeout)
2022-03-30 22:40:49 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (788825ms till timeout)
2022-03-30 22:40:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (787822ms till timeout)
2022-03-30 22:40:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (786819ms till timeout)
2022-03-30 22:40:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (785816ms till timeout)
2022-03-30 22:40:53 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (784813ms till timeout)
2022-03-30 22:40:54 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (783809ms till timeout)
2022-03-30 22:40:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (782806ms till timeout)
2022-03-30 22:40:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (781803ms till timeout)
2022-03-30 22:40:57 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (780800ms till timeout)
2022-03-30 22:40:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (779797ms till timeout)
2022-03-30 22:40:59 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (778794ms till timeout)
2022-03-30 22:41:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (777791ms till timeout)
2022-03-30 22:41:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (776787ms till timeout)
2022-03-30 22:41:02 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (775784ms till timeout)
2022-03-30 22:41:03 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (774781ms till timeout)
2022-03-30 22:41:04 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (773778ms till timeout)
2022-03-30 22:41:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (772775ms till timeout)
2022-03-30 22:41:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (771771ms till timeout)
2022-03-30 22:41:07 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (770768ms till timeout)
2022-03-30 22:41:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (769765ms till timeout)
2022-03-30 22:41:09 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (768762ms till timeout)
2022-03-30 22:41:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (767759ms till timeout)
2022-03-30 22:41:11 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-13f2f3b4 is in desired state: Ready
2022-03-30 22:41:11 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-13f2f3b4-scraper in namespace namespace-8
2022-03-30 22:41:11 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-30 22:41:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-13f2f3b4-scraper
2022-03-30 22:41:11 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-13f2f3b4-scraper will be ready
2022-03-30 22:41:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-13f2f3b4-scraper will be ready
2022-03-30 22:41:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-13f2f3b4-scraper will be ready not ready, will try again in 1000 ms (479998ms till timeout)
2022-03-30 22:41:12 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-13f2f3b4-scraper is ready
2022-03-30 22:41:12 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-13f2f3b4-scraper to be ready
2022-03-30 22:41:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-13f2f3b4-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready
2022-03-30 22:41:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-13f2f3b4-scraper-659d989f8d-gg6cm not ready: my-cluster-13f2f3b4-scraper)
2022-03-30 22:41:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-13f2f3b4-scraper-659d989f8d-gg6cm are ready
2022-03-30 22:41:12 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-13f2f3b4-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-30 22:41:13 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-13f2f3b4-scraper-659d989f8d-gg6cm not ready: my-cluster-13f2f3b4-scraper)
2022-03-30 22:41:13 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-13f2f3b4-scraper-659d989f8d-gg6cm are ready
2022-03-30 22:41:13 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-13f2f3b4-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598993ms till timeout)
2022-03-30 22:41:14 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-13f2f3b4-scraper-659d989f8d-gg6cm not ready: my-cluster-13f2f3b4-scraper)
2022-03-30 22:41:14 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-13f2f3b4-scraper-659d989f8d-gg6cm are ready
2022-03-30 22:41:14 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-13f2f3b4-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597989ms till timeout)
2022-03-30 22:41:15 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-13f2f3b4-scraper-659d989f8d-gg6cm not ready: my-cluster-13f2f3b4-scraper)
2022-03-30 22:41:15 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-13f2f3b4-scraper-659d989f8d-gg6cm are ready
2022-03-30 22:41:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-13f2f3b4-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596985ms till timeout)
2022-03-30 22:41:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-13f2f3b4-scraper-659d989f8d-gg6cm not ready: my-cluster-13f2f3b4-scraper)
2022-03-30 22:41:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-13f2f3b4-scraper-659d989f8d-gg6cm are ready
2022-03-30 22:41:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-13f2f3b4-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595981ms till timeout)
2022-03-30 22:41:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-13f2f3b4-scraper-659d989f8d-gg6cm not ready: my-cluster-13f2f3b4-scraper)
2022-03-30 22:41:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-13f2f3b4-scraper-659d989f8d-gg6cm are ready
2022-03-30 22:41:17 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-13f2f3b4-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594977ms till timeout)
2022-03-30 22:41:18 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-13f2f3b4-scraper-659d989f8d-gg6cm not ready: my-cluster-13f2f3b4-scraper)
2022-03-30 22:41:18 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-13f2f3b4-scraper-659d989f8d-gg6cm are ready
2022-03-30 22:41:18 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-13f2f3b4-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593973ms till timeout)
2022-03-30 22:41:19 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-13f2f3b4-scraper-659d989f8d-gg6cm not ready: my-cluster-13f2f3b4-scraper)
2022-03-30 22:41:19 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-13f2f3b4-scraper-659d989f8d-gg6cm are ready
2022-03-30 22:41:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-13f2f3b4-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592969ms till timeout)
2022-03-30 22:41:20 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-13f2f3b4-scraper-659d989f8d-gg6cm not ready: my-cluster-13f2f3b4-scraper)
2022-03-30 22:41:20 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-13f2f3b4-scraper-659d989f8d-gg6cm are ready
2022-03-30 22:41:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-13f2f3b4-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591965ms till timeout)
2022-03-30 22:41:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-13f2f3b4-scraper-659d989f8d-gg6cm not ready: my-cluster-13f2f3b4-scraper)
2022-03-30 22:41:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-13f2f3b4-scraper-659d989f8d-gg6cm are ready
2022-03-30 22:41:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-13f2f3b4-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590961ms till timeout)
2022-03-30 22:41:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-13f2f3b4-scraper-659d989f8d-gg6cm not ready: my-cluster-13f2f3b4-scraper)
2022-03-30 22:41:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-13f2f3b4-scraper-659d989f8d-gg6cm are ready
2022-03-30 22:41:22 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-13f2f3b4-scraper is ready
2022-03-30 22:41:22 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-13f2f3b4-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-03-30 22:41:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [NetworkPolicyResource:227] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=my-cluster-13f2f3b4-allow, namespace=namespace-8, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8083, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9404, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8080, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9999, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-13f2f3b4, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-13f2f3b4-connect}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 22:41:22 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-13f2f3b4-allow in namespace namespace-8
2022-03-30 22:41:22 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-30 22:41:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:my-cluster-13f2f3b4-allow
2022-03-30 22:41:23 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-03-30 22:41:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-13f2f3b4 in namespace namespace-8
2022-03-30 22:41:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-30 22:41:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaConnect:my-cluster-13f2f3b4
2022-03-30 22:41:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready
2022-03-30 22:41:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready
2022-03-30 22:41:23 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 22:41:24 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (598995ms till timeout)
2022-03-30 22:41:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (597992ms till timeout)
2022-03-30 22:41:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (596985ms till timeout)
2022-03-30 22:41:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (595982ms till timeout)
2022-03-30 22:41:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (594979ms till timeout)
2022-03-30 22:41:29 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (593974ms till timeout)
2022-03-30 22:41:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (592971ms till timeout)
2022-03-30 22:41:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (591967ms till timeout)
2022-03-30 22:41:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (590964ms till timeout)
2022-03-30 22:41:33 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (589960ms till timeout)
2022-03-30 22:41:34 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (588957ms till timeout)
2022-03-30 22:41:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (587954ms till timeout)
2022-03-30 22:41:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (586951ms till timeout)
2022-03-30 22:41:37 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (585948ms till timeout)
2022-03-30 22:41:38 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (584945ms till timeout)
2022-03-30 22:41:39 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (583942ms till timeout)
2022-03-30 22:41:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (582939ms till timeout)
2022-03-30 22:41:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (581936ms till timeout)
2022-03-30 22:41:42 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (580932ms till timeout)
2022-03-30 22:41:43 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (579929ms till timeout)
2022-03-30 22:41:44 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (578926ms till timeout)
2022-03-30 22:41:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (577923ms till timeout)
2022-03-30 22:41:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (576920ms till timeout)
2022-03-30 22:41:47 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (575917ms till timeout)
2022-03-30 22:41:48 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (574914ms till timeout)
2022-03-30 22:41:49 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (573911ms till timeout)
2022-03-30 22:41:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (572908ms till timeout)
2022-03-30 22:41:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (571905ms till timeout)
2022-03-30 22:41:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (570902ms till timeout)
2022-03-30 22:41:53 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (569899ms till timeout)
2022-03-30 22:41:54 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (568895ms till timeout)
2022-03-30 22:41:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (567892ms till timeout)
2022-03-30 22:41:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (566889ms till timeout)
2022-03-30 22:41:57 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (565886ms till timeout)
2022-03-30 22:41:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (564883ms till timeout)
2022-03-30 22:41:59 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (563880ms till timeout)
2022-03-30 22:42:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (562874ms till timeout)
2022-03-30 22:42:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (561871ms till timeout)
2022-03-30 22:42:02 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (560868ms till timeout)
2022-03-30 22:42:03 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (559865ms till timeout)
2022-03-30 22:42:04 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (558862ms till timeout)
2022-03-30 22:42:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (557858ms till timeout)
2022-03-30 22:42:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (556855ms till timeout)
2022-03-30 22:42:07 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (555852ms till timeout)
2022-03-30 22:42:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (554849ms till timeout)
2022-03-30 22:42:09 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (553846ms till timeout)
2022-03-30 22:42:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (552843ms till timeout)
2022-03-30 22:42:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (551840ms till timeout)
2022-03-30 22:42:12 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (550837ms till timeout)
2022-03-30 22:42:13 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (549834ms till timeout)
2022-03-30 22:42:14 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (548830ms till timeout)
2022-03-30 22:42:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (547827ms till timeout)
2022-03-30 22:42:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (546824ms till timeout)
2022-03-30 22:42:17 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (545820ms till timeout)
2022-03-30 22:42:18 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (544817ms till timeout)
2022-03-30 22:42:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (543814ms till timeout)
2022-03-30 22:42:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (542811ms till timeout)
2022-03-30 22:42:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (541808ms till timeout)
2022-03-30 22:42:22 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (540805ms till timeout)
2022-03-30 22:42:23 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (539802ms till timeout)
2022-03-30 22:42:24 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (538799ms till timeout)
2022-03-30 22:42:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (537795ms till timeout)
2022-03-30 22:42:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (536792ms till timeout)
2022-03-30 22:42:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (535789ms till timeout)
2022-03-30 22:42:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (534785ms till timeout)
2022-03-30 22:42:29 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (533783ms till timeout)
2022-03-30 22:42:30 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-13f2f3b4 is in desired state: Ready
2022-03-30 22:42:30 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector my-cluster-13f2f3b4 in namespace namespace-8
2022-03-30 22:42:30 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-30 22:42:30 [ForkJoinPool-3-worker-15] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkaconnectors' with unstable version 'v1beta2'
2022-03-30 22:42:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaConnector:my-cluster-13f2f3b4
2022-03-30 22:42:30 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-13f2f3b4 will have desired state: Ready
2022-03-30 22:42:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for KafkaConnector: my-cluster-13f2f3b4 will have desired state: Ready
2022-03-30 22:42:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaConnector: my-cluster-13f2f3b4 will have desired state: Ready not ready, will try again in 1000 ms (239998ms till timeout)
2022-03-30 22:42:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-13f2f3b4 is in desired state: Ready
2022-03-30 22:42:31 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-30 22:42:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 exec my-cluster-13f2f3b4-connect-5784459786-l24cc -- curl -X GET http://localhost:8083/connectors/my-cluster-13f2f3b4/status
2022-03-30 22:42:31 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-8 exec my-cluster-13f2f3b4-connect-5784459786-l24cc -- curl -X GET http://localhost:8083/connectors/my-cluster-13f2f3b4/status
2022-03-30 22:42:31 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-03-30 22:42:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-13f2f3b4-hello-world-producer in namespace namespace-8
2022-03-30 22:42:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-30 22:42:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-13f2f3b4-hello-world-consumer in namespace namespace-8
2022-03-30 22:42:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-30 22:42:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:my-cluster-13f2f3b4-hello-world-producer
2022-03-30 22:42:31 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-13f2f3b4-hello-world-producer will be in active state
2022-03-30 22:42:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for job active
2022-03-30 22:42:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:my-cluster-13f2f3b4-hello-world-consumer
2022-03-30 22:42:31 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-13f2f3b4-hello-world-consumer will be in active state
2022-03-30 22:42:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for job active
2022-03-30 22:42:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ClientUtils:61] Waiting till producer my-cluster-13f2f3b4-hello-world-producer and consumer my-cluster-13f2f3b4-hello-world-consumer finish
2022-03-30 22:42:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for clients finished
2022-03-30 22:42:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (219998ms till timeout)
2022-03-30 22:42:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (218995ms till timeout)
2022-03-30 22:42:33 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (217992ms till timeout)
2022-03-30 22:42:34 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (216988ms till timeout)
2022-03-30 22:42:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (215983ms till timeout)
2022-03-30 22:42:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (214978ms till timeout)
2022-03-30 22:42:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [JobUtils:37] Waiting for ReplicaSet of Deployment my-cluster-13f2f3b4-hello-world-producer deletion
2022-03-30 22:42:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for ReplicaSet my-cluster-13f2f3b4-hello-world-producer to be deleted
2022-03-30 22:42:37 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] ReplicaSet my-cluster-13f2f3b4-hello-world-producer to be deleted not ready, will try again in 5000 ms (179996ms till timeout)
2022-03-30 22:42:42 [ForkJoinPool-3-worker-15] [36mDEBUG[m [JobUtils:40] Job my-cluster-13f2f3b4-hello-world-producer was deleted
2022-03-30 22:42:42 [ForkJoinPool-3-worker-15] [36mDEBUG[m [JobUtils:37] Waiting for ReplicaSet of Deployment my-cluster-13f2f3b4-hello-world-consumer deletion
2022-03-30 22:42:42 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for ReplicaSet my-cluster-13f2f3b4-hello-world-consumer to be deleted
2022-03-30 22:42:42 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] ReplicaSet my-cluster-13f2f3b4-hello-world-consumer to be deleted not ready, will try again in 5000 ms (179996ms till timeout)
2022-03-30 22:42:47 [ForkJoinPool-3-worker-15] [36mDEBUG[m [JobUtils:40] Job my-cluster-13f2f3b4-hello-world-consumer was deleted
2022-03-30 22:42:47 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-13f2f3b4-connect-5784459786-pdt2j
2022-03-30 22:42:47 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for messages in file sink
2022-03-30 22:42:47 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 exec my-cluster-13f2f3b4-connect-5784459786-pdt2j -- /bin/bash -c cat /tmp/test-file-sink.txt
2022-03-30 22:42:47 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:421] Command: kubectl --namespace namespace-8 exec my-cluster-13f2f3b4-connect-5784459786-pdt2j -- /bin/bash -c cat /tmp/test-file-sink.txt
2022-03-30 22:42:47 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:421] Return code: 0
2022-03-30 22:42:47 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaConnectUtils:77] Expected messages are in file sink on my-cluster-13f2f3b4-connect-5784459786-pdt2j
2022-03-30 22:42:47 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 22:42:47 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:675] [connect.ConnectIsolatedST - After Each] - Clean up after test
2022-03-30 22:42:47 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:42:47 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testMultiNodeKafkaConnectWithConnectorCreation
2022-03-30 22:42:47 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector my-cluster-13f2f3b4 in namespace namespace-8
2022-03-30 22:42:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-13f2f3b4-scraper in namespace namespace-8
2022-03-30 22:42:47 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-13f2f3b4-allow in namespace namespace-8
2022-03-30 22:42:47 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-13f2f3b4-hello-world-consumer in namespace namespace-8
2022-03-30 22:42:47 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-13f2f3b4 in namespace namespace-8
2022-03-30 22:42:47 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:my-cluster-13f2f3b4-hello-world-consumer
2022-03-30 22:42:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-13f2f3b4-scraper
2022-03-30 22:42:47 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:my-cluster-13f2f3b4-allow
2022-03-30 22:42:47 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaConnector:my-cluster-13f2f3b4
2022-03-30 22:42:47 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-13f2f3b4-hello-world-producer in namespace namespace-8
2022-03-30 22:42:47 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-13f2f3b4
2022-03-30 22:42:47 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-13f2f3b4 in namespace namespace-8
2022-03-30 22:42:47 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:my-cluster-13f2f3b4-hello-world-producer
2022-03-30 22:42:47 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-13f2f3b4 not ready, will try again in 10000 ms (839989ms till timeout)
2022-03-30 22:42:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-13f2f3b4-scraper not ready, will try again in 10000 ms (479983ms till timeout)
2022-03-30 22:42:47 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaConnector:my-cluster-13f2f3b4 not ready, will try again in 10000 ms (239986ms till timeout)
2022-03-30 22:42:47 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaConnect:my-cluster-13f2f3b4
2022-03-30 22:42:47 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaConnect:my-cluster-13f2f3b4 not ready, will try again in 10000 ms (599992ms till timeout)
2022-03-30 22:42:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-13f2f3b4-scraper not ready, will try again in 10000 ms (469978ms till timeout)
2022-03-30 22:43:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-13f2f3b4-scraper not ready, will try again in 10000 ms (459972ms till timeout)
2022-03-30 22:43:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-13f2f3b4-scraper not ready, will try again in 10000 ms (449966ms till timeout)
2022-03-30 22:43:27 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:43:27 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-8 for test case:testMultiNodeKafkaConnectWithConnectorCreation
2022-03-30 22:43:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-8 removal
2022-03-30 22:43:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 22:43:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 22:43:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:43:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (479929ms till timeout)
2022-03-30 22:43:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 22:43:28 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 22:43:28 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:43:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (478858ms till timeout)
2022-03-30 22:43:29 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 22:43:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 22:43:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:43:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (477782ms till timeout)
2022-03-30 22:43:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 22:43:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 22:43:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:43:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (476707ms till timeout)
2022-03-30 22:43:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 22:43:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 22:43:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 22:43:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (475631ms till timeout)
2022-03-30 22:43:33 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 22:43:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 22:43:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 22:43:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 22:43:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-8" not found
2022-03-30 22:43:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 22:43:33 [ForkJoinPool-3-worker-15] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@c27758f5=[]}
2022-03-30 22:43:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:267] testMultiNodeKafkaConnectWithConnectorCreation - Notifies waiting test cases:[testCruiseControlBasicAPIRequests, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendMessagesTlsScramSha, testUpdateUser, testSendSimpleMessageTls, testAutoRenewAllCaCertsTriggeredByAnno, testReceiveSimpleMessageTls, testSendMessagesCustomListenerTlsScramSha, testKafkaAndZookeeperScaleUpScaleDown, testMirrorMakerTlsAuthenticated, testMirrorMaker2TlsAndTlsClientAuth, testUserOperatorMetrics, testKafkaBrokersCount, testKafkaConnectIoNetwork, testZookeeperWatchersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics, testKafkaMetricsSettings, testClusterOperatorMetrics, testCruiseControlMetrics, testTopicOperatorMetrics, testKafkaConnectRequests, testKafkaConnectResponse, testKafkaExporterDifferentSetting, testMultiNodeKafkaConnectWithConnectorCreation] to and randomly select one to start execution
2022-03-30 22:43:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:93] [connect.ConnectIsolatedST] - Removing parallel test: testMultiNodeKafkaConnectWithConnectorCreation
2022-03-30 22:43:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:97] [connect.ConnectIsolatedST] - Parallel test count: 0
2022-03-30 22:43:33 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testMultiNodeKafkaConnectWithConnectorCreation-FINISHED
2022-03-30 22:43:33 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 22:43:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 22:43:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:690] [connect.ConnectIsolatedST - After All] - Clean up after test suite
2022-03-30 22:43:33 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:43:33 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:346] In context ConnectIsolatedST is everything deleted.
2022-03-30 22:43:33 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4,031.696 s - in io.strimzi.systemtest.connect.ConnectIsolatedST
2022-03-30 22:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-30 22:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-30 22:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-30 22:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 22:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-30 22:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 22:43:33 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 22:43:33 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 22:43:33 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 22:43:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 22:43:33 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 22:43:33 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 22:43:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 22:43:33 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:43:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io not ready, will try again in 10000 ms (179984ms till timeout)
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 22:43:33 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io not ready, will try again in 10000 ms (179981ms till timeout)
2022-03-30 22:43:33 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 22:43:33 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 22:43:33 [ForkJoinPool-3-worker-11] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 22:43:33 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179954ms till timeout)
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 22:43:33 [ForkJoinPool-3-worker-11] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io not ready, will try again in 10000 ms (179965ms till timeout)
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 22:43:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179819ms till timeout)
2022-03-30 22:43:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 22:43:43 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 22:43:43 [ForkJoinPool-3-worker-17] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 22:43:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 22:43:43 [ForkJoinPool-3-worker-17] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io not ready, will try again in 10000 ms (179978ms till timeout)
2022-03-30 22:43:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179959ms till timeout)
2022-03-30 22:43:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 22:43:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-30 22:43:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 22:43:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 22:43:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v112687
2022-03-30 22:43:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v112687
2022-03-30 22:43:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=112687&allowWatchBookmarks=true&watch=true...
2022-03-30 22:43:53 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 22:43:53 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 112688
2022-03-30 22:43:58 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 112710
2022-03-30 22:44:19 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 112729
2022-03-30 22:44:19 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v112710 in namespace default
2022-03-30 22:44:19 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@24215749
2022-03-30 22:44:19 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 22:44:19 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@201ed715
2022-03-30 22:44:19 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@201ed715
2022-03-30 22:44:19 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@201ed715
2022-03-30 22:44:19 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 22:44:19 [main] [32mINFO [m [TestExecutionListener:40] =======================================================================
2022-03-30 22:44:19 [main] [32mINFO [m [TestExecutionListener:41] =======================================================================
2022-03-30 22:44:19 [main] [32mINFO [m [TestExecutionListener:42]                         Test run finished
2022-03-30 22:44:19 [main] [32mINFO [m [TestExecutionListener:43] =======================================================================
2022-03-30 22:44:19 [main] [32mINFO [m [TestExecutionListener:44] =======================================================================
2022-03-30 22:44:19 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 35, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] [1mReactor Summary for Strimzi - Apache Kafka on Kubernetes and OpenShift 0.29.0-SNAPSHOT:[m
[[1;34mINFO[m] 
[[1;34mINFO[m] Strimzi - Apache Kafka on Kubernetes and OpenShift . [1;32mSUCCESS[m [  2.551 s]
[[1;34mINFO[m] test ............................................... [1;32mSUCCESS[m [  0.979 s]
[[1;34mINFO[m] crd-annotations .................................... [1;32mSUCCESS[m [  1.008 s]
[[1;34mINFO[m] crd-generator ...................................... [1;32mSUCCESS[m [  2.546 s]
[[1;34mINFO[m] api ................................................ [1;32mSUCCESS[m [  6.755 s]
[[1;34mINFO[m] mockkube ........................................... [1;32mSUCCESS[m [  0.912 s]
[[1;34mINFO[m] config-model ....................................... [1;32mSUCCESS[m [  0.724 s]
[[1;34mINFO[m] certificate-manager ................................ [1;32mSUCCESS[m [  0.762 s]
[[1;34mINFO[m] operator-common .................................... [1;32mSUCCESS[m [  1.800 s]
[[1;34mINFO[m] systemtest ......................................... [1;32mSUCCESS[m [  01:15 h]
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] [1;32mBUILD SUCCESS[m
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] Total time:  01:16 h
[[1;34mINFO[m] Finished at: 2022-03-30T22:44:19Z
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
