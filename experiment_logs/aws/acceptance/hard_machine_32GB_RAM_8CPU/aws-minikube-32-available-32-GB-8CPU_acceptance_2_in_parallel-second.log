[[1;34mINFO[m] Scanning for projects...
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] [1mReactor Build Order:[m
[[1;34mINFO[m] 
[[1;34mINFO[m] Strimzi - Apache Kafka on Kubernetes and OpenShift                 [pom]
[[1;34mINFO[m] test                                                               [jar]
[[1;34mINFO[m] crd-annotations                                                    [jar]
[[1;34mINFO[m] crd-generator                                                      [jar]
[[1;34mINFO[m] api                                                                [jar]
[[1;34mINFO[m] mockkube                                                           [jar]
[[1;34mINFO[m] config-model                                                       [jar]
[[1;34mINFO[m] certificate-manager                                                [jar]
[[1;34mINFO[m] operator-common                                                    [jar]
[[1;34mINFO[m] systemtest                                                         [jar]
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m-------------------------< [0;36mio.strimzi:strimzi[0;1m >-------------------------[m
[[1;34mINFO[m] [1mBuilding Strimzi - Apache Kafka on Kubernetes and OpenShift 0.29.0-SNAPSHOT [1/10][m
[[1;34mINFO[m] [1m--------------------------------[ pom ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mstrimzi[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mstrimzi[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] No tests to run.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Skipping pom project
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--------------------------< [0;36mio.strimzi:test[0;1m >---------------------------[m
[[1;34mINFO[m] [1mBuilding test 0.29.0-SNAPSHOT                                     [2/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/test/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/test/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/test/src/test/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] No sources to compile
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] No tests to run.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mtest[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/test/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mtest[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/test/target/test-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] No tests to run.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m---------------------< [0;36mio.strimzi:crd-annotations[0;1m >---------------------[m
[[1;34mINFO[m] [1mBuilding crd-annotations 0.29.0-SNAPSHOT                          [3/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/crd-annotations/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/crd-annotations/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/crd-annotations/src/test/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mcrd-annotations[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/crd-annotations/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mcrd-annotations[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/crd-annotations/target/crd-annotations-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mcrd-annotations[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m----------------------< [0;36mio.strimzi:crd-generator[0;1m >----------------------[m
[[1;34mINFO[m] [1mBuilding crd-generator 0.29.0-SNAPSHOT                            [4/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/crd-generator/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/crd-generator/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 7 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mcrd-generator[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/crd-generator/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mcrd-generator[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-shade-plugin:3.1.0:shade[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Including io.strimzi:crd-annotations:jar:0.29.0-SNAPSHOT in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.core:jackson-core:jar:2.12.6 in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.core:jackson-databind:jar:2.12.6 in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:jar:2.12.6 in the shaded jar.
[[1;34mINFO[m] Including org.yaml:snakeyaml:jar:1.27 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-client:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-rbac:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-admissionregistration:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-apps:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-autoscaling:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-apiextensions:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-batch:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-certificates:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-coordination:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-discovery:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-events:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-extensions:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-flowcontrol:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-networking:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-metrics:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-policy:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-scheduling:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-storageclass:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-node:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including com.squareup.okhttp3:okhttp:jar:3.12.12 in the shaded jar.
[[1;34mINFO[m] Including com.squareup.okio:okio:jar:1.15.0 in the shaded jar.
[[1;34mINFO[m] Including com.squareup.okhttp3:logging-interceptor:jar:3.12.12 in the shaded jar.
[[1;34mINFO[m] Including org.slf4j:slf4j-api:jar:1.7.36 in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.datatype:jackson-datatype-jsr310:jar:2.13.1 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:zjsonpatch:jar:0.3.0 in the shaded jar.
[[1;34mINFO[m] Including com.github.mifmif:generex:jar:1.0.2 in the shaded jar.
[[1;34mINFO[m] Including dk.brics.automaton:automaton:jar:1.11-8 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-core:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-common:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.core:jackson-annotations:jar:2.12.6 in the shaded jar.
[[1;33mWARNING[m] Discovered module-info.class. Shading will break its strong encapsulation.
[[1;33mWARNING[m] Discovered module-info.class. Shading will break its strong encapsulation.
[[1;33mWARNING[m] Discovered module-info.class. Shading will break its strong encapsulation.
[[1;33mWARNING[m] Discovered module-info.class. Shading will break its strong encapsulation.
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, generex-1.0.2.jar define 7 overlapping classes: 
[[1;33mWARNING[m]   - com.mifmif.common.regex.GenerexIterator
[[1;33mWARNING[m]   - com.mifmif.common.regex.Generex
[[1;33mWARNING[m]   - com.mifmif.common.regex.GenerexIterator$Step
[[1;33mWARNING[m]   - com.mifmif.common.regex.Node
[[1;33mWARNING[m]   - com.mifmif.common.regex.Main
[[1;33mWARNING[m]   - com.mifmif.common.regex.util.Iterable
[[1;33mWARNING[m]   - com.mifmif.common.regex.util.Iterator
[[1;33mWARNING[m] kubernetes-model-rbac-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 80 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.SubjectBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.RoleListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingFluent$MetadataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.RoleBindingBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.AggregationRuleFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.SubjectFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.PolicyRuleFluent
[[1;33mWARNING[m]   - 70 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, jackson-annotations-2.12.6.jar define 71 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonAutoDetect
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonInclude
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.ObjectIdGenerators
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonFormat$Features
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonFormat$Feature
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonIgnore
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonSetter
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonTypeInfo$None
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonFormat$Shape
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonSubTypes
[[1;33mWARNING[m]   - 61 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-extensions-5.12.0.jar define 264 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetConditionBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicyListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.DeploymentStrategyFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.IngressListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicySpecFluent$IngressNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.IngressStatus
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetFluentImpl$SpecNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.IngressSpecFluent$RulesNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetSpecFluent$UpdateStrategyNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicyPeerBuilder
[[1;33mWARNING[m]   - 254 more...
[[1;33mWARNING[m] kubernetes-model-autoscaling-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 350 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.MetricSpecFluentImpl$ObjectNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.CrossVersionObjectReferenceBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerStatusFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerFluent$SpecNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.ContainerResourceMetricStatusFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerStatus
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.MetricStatusFluent$ObjectNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerSpecFluent$ScaleTargetRefNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerFluent$SpecNested
[[1;33mWARNING[m]   - 340 more...
[[1;33mWARNING[m] kubernetes-model-storageclass-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 172 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSIStorageCapacityListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.CSINodeFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.CSINodeDriverFluentImpl$AllocatableNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.StorageClass
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSourceBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSourceFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.TokenRequestFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSINodeDriverFluent$AllocatableNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSIDriverSpecBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSpecFluent
[[1;33mWARNING[m]   - 162 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-batch-5.12.0.jar define 112 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.JobFluentImpl$StatusNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.CronJobStatusFluentImpl$ActiveNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.JobSpecFluent$TemplateNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobSpecFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.JobSpecFluentImpl$TemplateNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.Job
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobListFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.CronJobListFluent
[[1;33mWARNING[m]   - 102 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-apiextensions-5.12.0.jar define 350 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionVersionBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrBoolBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionSpecFluent$ValidationNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionVersionFluentImpl$SchemaNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrStringArraySerDe$Deserializer$1
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceValidationFluentImpl$OpenAPIV3SchemaNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsFluentImpl$NotNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.WebhookClientConfigFluentImpl$ServiceNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrArrayFluent$SchemaNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1.JSONSchemaPropsOrBoolSerDe
[[1;33mWARNING[m]   - 340 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-discovery-5.12.0.jar define 88 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointSliceFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointSliceBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointPort
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.ForZoneBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointFluent$TargetRefNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointFluentImpl$ConditionsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointSliceListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointConditionsFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointSliceListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - 78 more...
[[1;33mWARNING[m] okhttp-3.12.12.jar, crd-generator-0.29.0-SNAPSHOT.jar define 208 overlapping classes: 
[[1;33mWARNING[m]   - okhttp3.WebSocket
[[1;33mWARNING[m]   - okhttp3.Cookie$Builder
[[1;33mWARNING[m]   - okhttp3.internal.http.HttpHeaders
[[1;33mWARNING[m]   - okhttp3.internal.http2.Http2Connection$ReaderRunnable
[[1;33mWARNING[m]   - okhttp3.internal.http2.Http2Reader$ContinuationSource
[[1;33mWARNING[m]   - okhttp3.internal.tls.OkHostnameVerifier
[[1;33mWARNING[m]   - okhttp3.Cache$Entry
[[1;33mWARNING[m]   - okhttp3.internal.http2.Http2Connection$3
[[1;33mWARNING[m]   - okhttp3.internal.ws.RealWebSocket$Streams
[[1;33mWARNING[m]   - okhttp3.CacheControl$Builder
[[1;33mWARNING[m]   - 198 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-metrics-5.12.0.jar define 30 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.ContainerMetricsFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetrics
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsFluent$MetadataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsFluentImpl$ContainersNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsListBuilder
[[1;33mWARNING[m]   - 20 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-flowcontrol-5.12.0.jar define 132 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationFluentImpl$SpecNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.FlowSchemaConditionFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.FlowDistinguisherMethodBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationReferenceBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.QueuingConfiguration
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.QueuingConfigurationFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfiguration
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationReference
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PolicyRulesWithSubjects
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationListFluent$ItemsNested
[[1;33mWARNING[m]   - 122 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-events-5.12.0.jar define 44 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$MetadataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$SeriesNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1.EventList
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1.EventListBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$RegardingNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1.EventSeriesFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - 34 more...
[[1;33mWARNING[m] automaton-1.11-8.jar, crd-generator-0.29.0-SNAPSHOT.jar define 25 overlapping classes: 
[[1;33mWARNING[m]   - dk.brics.automaton.AutomatonMatcher
[[1;33mWARNING[m]   - dk.brics.automaton.ShuffleOperations$ShuffleConfiguration
[[1;33mWARNING[m]   - dk.brics.automaton.RegExp$Kind
[[1;33mWARNING[m]   - dk.brics.automaton.RunAutomaton
[[1;33mWARNING[m]   - dk.brics.automaton.Automaton
[[1;33mWARNING[m]   - dk.brics.automaton.RegExp
[[1;33mWARNING[m]   - dk.brics.automaton.AutomatonProvider
[[1;33mWARNING[m]   - dk.brics.automaton.RegExp$1
[[1;33mWARNING[m]   - dk.brics.automaton.MinimizationOperations$StateListNode
[[1;33mWARNING[m]   - dk.brics.automaton.State
[[1;33mWARNING[m]   - 15 more...
[[1;33mWARNING[m] jackson-core-2.12.6.jar, crd-generator-0.29.0-SNAPSHOT.jar define 124 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.JsonGenerator$Feature
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.json.JsonReadFeature
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.util.ThreadLocalBufferManager$ThreadLocalBufferManagerHolder
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.util.Separators
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.io.SegmentedStringWriter
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.TreeNode
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.sym.Name
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.util.RequestPayload
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.util.JsonGeneratorDelegate
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.async.NonBlockingInputFeeder
[[1;33mWARNING[m]   - 114 more...
[[1;33mWARNING[m] kubernetes-model-networking-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 234 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.IngressServiceBackend
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.NetworkPolicyPort
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressFluentImpl$StatusNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.IngressStatus
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.IngressClassFluent$SpecNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.NetworkPolicyBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.IngressRuleFluentImpl$HttpNestedImpl
[[1;33mWARNING[m]   - 224 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-coordination-5.12.0.jar define 18 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluentImpl$SpecNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluent$SpecNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpecBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpecFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluentImpl$MetadataNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluent
[[1;33mWARNING[m]   - 8 more...
[[1;33mWARNING[m] zjsonpatch-0.3.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 24 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.sequence.InsertCommand
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.Operation
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.sequence.CommandVisitor
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.guava.Strings
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.sequence.EditCommand
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.JsonDiff$EncodePathFunction
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.sequence.SequencesComparator
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.Diff
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.ListUtils
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.JsonPatch
[[1;33mWARNING[m]   - 14 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-common-5.12.0.jar define 16 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.Plural
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.Group
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.jackson.JsonUnwrappedDeserializer$CancelUnwrapped
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.jackson.JsonUnwrappedDeserializer
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.PrinterColumn
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.jackson.UnwrappedTypeResolverBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.Singular
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.StatusReplicas
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.SpecReplicas
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.Version
[[1;33mWARNING[m]   - 6 more...
[[1;33mWARNING[m] kubernetes-model-admissionregistration-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 362 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookFluent$ObjectSelectorNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authorization.v1.SubjectAccessReviewSpecFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SubjectRulesReviewStatusBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1.ValidatingWebhookConfigurationBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authentication.TokenReviewFluentImpl$MetadataNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SelfSubjectRulesReviewSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookFluentImpl$NamespaceSelectorNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookConfigurationFluentImpl$WebhooksNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SelfSubjectAccessReviewFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1.MutatingWebhookFluent$ClientConfigNested
[[1;33mWARNING[m]   - 352 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, okio-1.15.0.jar define 44 overlapping classes: 
[[1;33mWARNING[m]   - okio.ByteString
[[1;33mWARNING[m]   - okio.Source
[[1;33mWARNING[m]   - okio.ForwardingSink
[[1;33mWARNING[m]   - okio.BufferedSource
[[1;33mWARNING[m]   - okio.Util
[[1;33mWARNING[m]   - okio.AsyncTimeout$1
[[1;33mWARNING[m]   - okio.HashingSource
[[1;33mWARNING[m]   - okio.GzipSink
[[1;33mWARNING[m]   - okio.Okio$1
[[1;33mWARNING[m]   - okio.Pipe$PipeSink
[[1;33mWARNING[m]   - 34 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-certificates-5.12.0.jar define 60 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestStatusFluent$ConditionsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluent$SpecNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluentImpl$StatusNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestStatusFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestFluent$MetadataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestConditionFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestStatusBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestStatusFluent$ConditionsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluentImpl
[[1;33mWARNING[m]   - 50 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, jackson-datatype-jsr310-2.13.1.jar define 59 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.deser.LocalDateDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.deser.key.Jsr310KeyDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.PackageVersion
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.deser.YearDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.ser.key.Jsr310NullKeySerializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.deser.key.LocalDateTimeKeyDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.util.DurationUnitConverter
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.ser.InstantSerializerBase
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.ser.LocalDateTimeSerializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.ser.OffsetDateTimeSerializer
[[1;33mWARNING[m]   - 49 more...
[[1;33mWARNING[m] crd-annotations-0.29.0-SNAPSHOT.jar, crd-generator-0.29.0-SNAPSHOT.jar define 8 overlapping classes: 
[[1;33mWARNING[m]   - io.strimzi.api.annotations.VersionRange
[[1;33mWARNING[m]   - io.strimzi.api.annotations.ApiVersion
[[1;33mWARNING[m]   - io.strimzi.api.annotations.ApiVersion$Stability
[[1;33mWARNING[m]   - io.strimzi.api.annotations.ApiVersion$1
[[1;33mWARNING[m]   - io.strimzi.api.annotations.DeprecatedType
[[1;33mWARNING[m]   - io.strimzi.api.annotations.DeprecatedProperty
[[1;33mWARNING[m]   - io.strimzi.api.annotations.VersionRange$VersionParser
[[1;33mWARNING[m]   - io.strimzi.api.annotations.KubeVersion
[[1;33mWARNING[m] kubernetes-model-apps-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 212 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.StatefulSetConditionFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.DeploymentStrategyFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ControllerRevisionFluent$DeploymentDataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetSpecFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.DeploymentFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetStatusFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ControllerRevisionFluentImpl$PersistentVolumeClaimDataNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetCondition
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.StatefulSetSpecFluent$UpdateStrategyNested
[[1;33mWARNING[m]   - 202 more...
[[1;33mWARNING[m] logging-interceptor-3.12.12.jar, crd-generator-0.29.0-SNAPSHOT.jar define 8 overlapping classes: 
[[1;33mWARNING[m]   - okhttp3.logging.HttpLoggingInterceptor$Logger$1
[[1;33mWARNING[m]   - okhttp3.logging.LoggingEventListener$Factory
[[1;33mWARNING[m]   - okhttp3.logging.HttpLoggingInterceptor$Level
[[1;33mWARNING[m]   - okhttp3.logging.HttpLoggingInterceptor
[[1;33mWARNING[m]   - okhttp3.logging.package-info
[[1;33mWARNING[m]   - okhttp3.logging.LoggingEventListener
[[1;33mWARNING[m]   - okhttp3.logging.LoggingEventListener$1
[[1;33mWARNING[m]   - okhttp3.logging.HttpLoggingInterceptor$Logger
[[1;33mWARNING[m] jackson-databind-2.12.6.jar, crd-generator-0.29.0-SNAPSHOT.jar define 700 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.introspect.AnnotationCollector$NoAnnotations
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.jsontype.BasicPolymorphicTypeValidator$Builder
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.BeanDescription
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.deser.impl.BeanAsArrayBuilderDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.introspect.AnnotatedMethodMap
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.SerializerProvider
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.introspect.AnnotationCollector$OneAnnotation
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.ser.std.StaticListSerializerBase
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.ser.std.NumberSerializers$ShortSerializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.ser.BeanSerializerFactory
[[1;33mWARNING[m]   - 690 more...
[[1;33mWARNING[m] jackson-dataformat-yaml-2.12.6.jar, crd-generator-0.29.0-SNAPSHOT.jar define 17 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLMapper$Builder
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.snakeyaml.error.Mark
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLGenerator
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.UTF8Reader
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLParser
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.util.StringQuotingChecker
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLGenerator$Feature
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.JacksonYAMLParseException
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLParser$Feature
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.util.StringQuotingChecker$Default
[[1;33mWARNING[m]   - 7 more...
[[1;33mWARNING[m] kubernetes-model-core-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 2394 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.BaseKubernetesListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.StatusBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.KubeSchemaFluentImpl$APIResourceNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.NodeListBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.ResourceQuotaListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.WatchEventFluentImpl$APIServiceStatusObjectNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.WatchEventFluent$VsphereVirtualDiskVolumeSourceObjectNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.ProbeFluentImpl$HttpGetNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.PatchOptionsFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.ServerAddressByClientCIDRFluentImpl
[[1;33mWARNING[m]   - 2384 more...
[[1;33mWARNING[m] slf4j-api-1.7.36.jar, crd-generator-0.29.0-SNAPSHOT.jar define 34 overlapping classes: 
[[1;33mWARNING[m]   - org.slf4j.helpers.SubstituteLogger
[[1;33mWARNING[m]   - org.slf4j.helpers.NamedLoggerBase
[[1;33mWARNING[m]   - org.slf4j.helpers.NOPMDCAdapter
[[1;33mWARNING[m]   - org.slf4j.MarkerFactory
[[1;33mWARNING[m]   - org.slf4j.helpers.BasicMarker
[[1;33mWARNING[m]   - org.slf4j.spi.LoggerFactoryBinder
[[1;33mWARNING[m]   - org.slf4j.MDC$MDCCloseable
[[1;33mWARNING[m]   - org.slf4j.spi.LocationAwareLogger
[[1;33mWARNING[m]   - org.slf4j.helpers.MessageFormatter
[[1;33mWARNING[m]   - org.slf4j.helpers.Util$ClassContextSecurityManager
[[1;33mWARNING[m]   - 24 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-node-5.12.0.jar define 78 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1beta1.OverheadBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1alpha1.Scheduling
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1beta1.RuntimeClassFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1beta1.RuntimeClassListBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1alpha1.SchedulingFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1alpha1.RuntimeClassSpecFluent$OverheadNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassFluentImpl
[[1;33mWARNING[m]   - 68 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, snakeyaml-1.27.jar define 216 overlapping classes: 
[[1;33mWARNING[m]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockMappingValue
[[1;33mWARNING[m]   - org.yaml.snakeyaml.parser.ParserImpl$ParseBlockNode
[[1;33mWARNING[m]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockMappingSimpleValue
[[1;33mWARNING[m]   - org.yaml.snakeyaml.emitter.Emitter$ExpectDocumentEnd
[[1;33mWARNING[m]   - org.yaml.snakeyaml.Yaml$3
[[1;33mWARNING[m]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockSequenceItem
[[1;33mWARNING[m]   - org.yaml.snakeyaml.parser.ParserImpl$ParseBlockSequenceEntry
[[1;33mWARNING[m]   - org.yaml.snakeyaml.util.ArrayUtils
[[1;33mWARNING[m]   - org.yaml.snakeyaml.tokens.Token$ID
[[1;33mWARNING[m]   - org.yaml.snakeyaml.reader.StreamReader
[[1;33mWARNING[m]   - 206 more...
[[1;33mWARNING[m] kubernetes-client-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 536 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.internal.CertUtils
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.CustomResource
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.osgi.ManagedKubernetesClient
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.V1beta1ApiextensionAPIGroupDSL
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.internal.PatchUtils$SingletonHolder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.VersionInfo$1
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.utils.ReplaceValueStream
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.dsl.CreateFromServerGettable
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.dsl.ApiextensionsAPIGroupDSL
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.dsl.Containerable
[[1;33mWARNING[m]   - 526 more...
[[1;33mWARNING[m] kubernetes-model-scheduling-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 24 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassFluentImpl$MetadataNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassList
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClass
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassFluent$MetadataNested
[[1;33mWARNING[m]   - 14 more...
[[1;33mWARNING[m] kubernetes-model-policy-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 162 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1.PodDisruptionBudgetList
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.HostPortRangeBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.EvictionFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicyFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicySpecFluentImpl$AllowedCSIDriversNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicyListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.AllowedFlexVolumeBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.IDRangeFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.SELinuxStrategyOptionsFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicySpecFluentImpl$FsGroupNestedImpl
[[1;33mWARNING[m]   - 152 more...
[[1;33mWARNING[m] maven-shade-plugin has detected that some class files are
[[1;33mWARNING[m] present in two or more JARs. When this happens, only one
[[1;33mWARNING[m] single version of the class is copied to the uber jar.
[[1;33mWARNING[m] Usually this is not harmful and you can skip these warnings,
[[1;33mWARNING[m] otherwise try to manually exclude artifacts based on
[[1;33mWARNING[m] mvn dependency:tree -Ddetail=true and the above output.
[[1;33mWARNING[m] See http://maven.apache.org/plugins/maven-shade-plugin/
[[1;34mINFO[m] Replacing original artifact with shaded artifact.
[[1;34mINFO[m] Replacing /home/ec2-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT.jar with /home/ec2-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT-shaded.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m---------------------------< [0;36mio.strimzi:api[0;1m >---------------------------[m
[[1;34mINFO[m] [1mBuilding api 0.29.0-SNAPSHOT                                      [5/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/api/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/api/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mexec-maven-plugin:1.6.0:exec[m [1m(generate-crd-co-install-v1)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mexec-maven-plugin:1.6.0:exec[m [1m(generate-crd-co-install-v1-eo)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mexec-maven-plugin:1.6.0:exec[m [1m(generate-doc)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 99 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-test-compile)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mapi[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/api/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mapi[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:test-jar[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m------------------------< [0;36mio.strimzi:mockkube[0;1m >-------------------------[m
[[1;34mINFO[m] [1mBuilding mockkube 0.29.0-SNAPSHOT                                 [6/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/mockkube/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/mockkube/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 1 resource
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mmockkube[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/mockkube/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mmockkube[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/mockkube/target/mockkube-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mmockkube[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m----------------------< [0;36mio.strimzi:config-model[0;1m >-----------------------[m
[[1;34mINFO[m] [1mBuilding config-model 0.29.0-SNAPSHOT                             [7/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/config-model/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/config-model/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/config-model/src/test/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mconfig-model[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/config-model/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mconfig-model[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/config-model/target/config-model-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mconfig-model[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m-------------------< [0;36mio.strimzi:certificate-manager[0;1m >-------------------[m
[[1;34mINFO[m] [1mBuilding certificate-manager 0.29.0-SNAPSHOT                      [8/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/certificate-manager/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 1 resource
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 1 resource
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mcertificate-manager[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/certificate-manager/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mcertificate-manager[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/certificate-manager/target/certificate-manager-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mcertificate-manager[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m---------------------< [0;36mio.strimzi:operator-common[0;1m >---------------------[m
[[1;34mINFO[m] [1mBuilding operator-common 0.29.0-SNAPSHOT                          [9/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/operator-common/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/operator-common/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 9 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36moperator-common[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/operator-common/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36moperator-common[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/operator-common/target/operator-common-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:test-jar[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m-----------------------< [0;36mio.strimzi:systemtest[0;1m >------------------------[m
[[1;34mINFO[m] [1mBuilding systemtest 0.29.0-SNAPSHOT                              [10/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/systemtest/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] Copying 2 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 32 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;33mWARNING[m] Corrupted STDOUT by directly writing to native stream in forked JVM 1. See FAQ web page and the dump file /home/ec2-user/strimzi-kafka-operator/systemtest/target/surefire-reports/2022-03-30T16-36-28_302-jvmRun1.dumpstream
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/systemtest/target/systemtest-0.29.0-SNAPSHOT.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36msystemtest[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/systemtest/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36msystemtest[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/systemtest/target/systemtest-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36msystemtest[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
2022-03-30 16:36:46 [main] [32mINFO [m [TestExecutionListener:29] =======================================================================
2022-03-30 16:36:46 [main] [32mINFO [m [TestExecutionListener:30] =======================================================================
2022-03-30 16:36:46 [main] [32mINFO [m [TestExecutionListener:31]                         Test run started
2022-03-30 16:36:46 [main] [32mINFO [m [TestExecutionListener:32] =======================================================================
2022-03-30 16:36:46 [main] [32mINFO [m [TestExecutionListener:33] =======================================================================
2022-03-30 16:36:46 [main] [32mINFO [m [TestExecutionListener:48] Following testclasses are selected for run:
2022-03-30 16:36:46 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.operators.user.UserST
2022-03-30 16:36:46 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.bridge.HttpBridgeTlsST
2022-03-30 16:36:46 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.cruisecontrol.CruiseControlApiST
2022-03-30 16:36:46 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.cruisecontrol.CruiseControlST
2022-03-30 16:36:46 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.kafka.listeners.ListenersST
2022-03-30 16:36:46 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.security.SecurityST
2022-03-30 16:36:46 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.rollingupdate.RollingUpdateST
2022-03-30 16:36:46 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.specific.SpecificIsolatedST
2022-03-30 16:36:46 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.watcher.AllNamespaceIsolatedST
2022-03-30 16:36:46 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.connect.ConnectIsolatedST
2022-03-30 16:36:46 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST
2022-03-30 16:36:46 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST
2022-03-30 16:36:46 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.metrics.MetricsIsolatedST
2022-03-30 16:36:46 [main] [32mINFO [m [TestExecutionListener:52] =======================================================================
2022-03-30 16:36:46 [main] [32mINFO [m [TestExecutionListener:53] =======================================================================
[[1;34mINFO[m] Running io.strimzi.systemtest.operators.user.UserST
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Environment:271] Json configuration is not provided or cannot be processed!
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:219] Used environment variables:
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:220] CONFIG: /home/ec2-user/strimzi-kafka-operator/systemtest/config.json
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] STRIMZI_RBAC_SCOPE: CLUSTER
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] OLM_APP_BUNDLE_PREFIX: strimzi-cluster-operator
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_CLIENTS_VERSION: 0.2.0
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] OLM_SOURCE_NAMESPACE: openshift-marketplace
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] CLUSTER_OPERATOR_INSTALL_TYPE: BUNDLE
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] STRIMZI_COMPONENTS_LOG_LEVEL: INFO
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] SKIP_TEARDOWN: false
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] LB_FINALIZERS: false
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] OLM_OPERATOR_DEPLOYMENT_NAME: strimzi-cluster-operator
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] DOCKER_ORG: strimzi
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_LOG_DIR: /home/ec2-user/strimzi-kafka-operator/systemtest/../systemtest/target/logs/
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] COMPONENTS_IMAGE_PULL_POLICY: IfNotPresent
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] DOCKER_REGISTRY: quay.io
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_CLIENT_IMAGE: quay.io/strimzi/test-client:latest-kafka-3.1.0
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] SYSTEM_TEST_STRIMZI_IMAGE_PULL_SECRET: 
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_ADMIN_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-admin:0.2.0-kafka-3.1.0
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_HTTP_PRODUCER_IMAGE: quay.io/strimzi-test-clients/test-client-http-producer:0.2.0
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] OLM_OPERATOR_NAME: strimzi-kafka-operator
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] DOCKER_TAG: latest
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] OLM_SOURCE_NAME: community-operators
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] STRIMZI_FEATURE_GATES: 
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] CLIENTS_KAFKA_VERSION: 3.1.0
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_HTTP_CONSUMER_IMAGE: quay.io/strimzi-test-clients/test-client-http-consumer:0.2.0
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] STRIMZI_LOG_LEVEL: DEBUG
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] ST_KAFKA_VERSION: 3.1.0
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] OPERATOR_IMAGE_PULL_POLICY: Always
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] DEFAULT_TO_DENY_NETWORK_POLICIES: true
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_PRODUCER_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-producer:0.2.0-kafka-3.1.0
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] BRIDGE_IMAGE: latest-released
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_STREAMS_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-streams:0.2.0-kafka-3.1.0
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_CONSUMER_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-consumer:0.2.0-kafka-3.1.0
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] OLM_OPERATOR_VERSION: 
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [BeforeAllOnce:51] ============================================================================
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [BeforeAllOnce:52] [io.strimzi.systemtest.operators.user.UserST - Before Suite] - Setup Suite environment
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Config:540] Trying to configure client from Kubernetes config...
2022-03-30 16:36:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Config:549] Found for Kubernetes config at: [/home/ec2-user/.kube/config].
2022-03-30 16:36:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Config:540] Trying to configure client from Kubernetes config...
2022-03-30 16:36:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Config:549] Found for Kubernetes config at: [/home/ec2-user/.kube/config].
2022-03-30 16:36:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [KubeCluster:71] Cluster minikube is installed
2022-03-30 16:36:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - minikube status
2022-03-30 16:36:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: minikube status
2022-03-30 16:36:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:36:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [KubeCluster:73] Cluster minikube is running
2022-03-30 16:36:47 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeCluster:87] Using cluster: minikube
2022-03-30 16:36:47 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:60] Cluster default namespace is 'default'
2022-03-30 16:36:47 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5e99e8d5
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-30 16:36:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@cbb87a8, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5e99e8d5, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-30 16:36:47 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-30 16:36:48 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-30 16:36:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-30 16:36:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace default get Namespace infra-namespace -o json
2022-03-30 16:36:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace default get Namespace infra-namespace -o json
2022-03-30 16:36:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:36:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T16:36:48Z",
        "name": "infra-namespace",
        "resourceVersion": "49331",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "8b180569-b62e-4872-891c-c245cc36bcd8"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 16:36:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-30 16:36:48 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 16:36:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-30 16:36:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 16:36:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 16:36:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 16:36:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 16:36:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 16:36:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 16:36:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 16:36:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 16:36:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 16:36:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 16:36:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 16:36:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 16:36:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 16:36:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 16:36:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 16:36:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 16:36:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 16:36:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 16:36:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 16:36:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 16:36:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 16:36:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 16:36:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 16:36:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 16:36:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 16:36:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 16:36:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 16:36:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 16:36:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 16:36:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 16:36:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 16:36:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 16:36:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 16:36:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 16:36:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 16:36:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 16:36:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 16:36:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 16:36:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 16:36:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 16:36:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 16:36:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 16:36:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 16:36:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 16:36:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 16:36:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 16:36:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 16:36:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-30 16:36:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 16:36:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 16:36:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 16:36:50 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-30 16:36:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 16:36:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 16:36:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 16:36:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 16:36:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 16:36:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 16:36:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 16:36:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 16:36:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 16:36:50 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 16:36:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 16:36:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 16:36:50 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 16:36:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 16:36:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 16:36:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 16:36:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 16:36:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 16:36:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 16:36:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 16:36:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 16:36:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 16:36:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 16:36:50 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 16:36:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 16:36:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479986ms till timeout)
2022-03-30 16:36:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478981ms till timeout)
2022-03-30 16:36:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477975ms till timeout)
2022-03-30 16:36:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476970ms till timeout)
2022-03-30 16:36:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475965ms till timeout)
2022-03-30 16:36:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474961ms till timeout)
2022-03-30 16:36:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473956ms till timeout)
2022-03-30 16:36:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472950ms till timeout)
2022-03-30 16:36:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471946ms till timeout)
2022-03-30 16:36:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470940ms till timeout)
2022-03-30 16:37:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469929ms till timeout)
2022-03-30 16:37:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468922ms till timeout)
2022-03-30 16:37:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467915ms till timeout)
2022-03-30 16:37:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466910ms till timeout)
2022-03-30 16:37:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465905ms till timeout)
2022-03-30 16:37:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464900ms till timeout)
2022-03-30 16:37:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463896ms till timeout)
2022-03-30 16:37:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462891ms till timeout)
2022-03-30 16:37:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461886ms till timeout)
2022-03-30 16:37:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (460882ms till timeout)
2022-03-30 16:37:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (459878ms till timeout)
2022-03-30 16:37:11 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-30 16:37:11 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-30 16:37:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-30 16:37:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-4gnvw not ready: strimzi-cluster-operator)
2022-03-30 16:37:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-4gnvw are ready
2022-03-30 16:37:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599982ms till timeout)
2022-03-30 16:37:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-4gnvw not ready: strimzi-cluster-operator)
2022-03-30 16:37:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-4gnvw are ready
2022-03-30 16:37:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598977ms till timeout)
2022-03-30 16:37:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-4gnvw not ready: strimzi-cluster-operator)
2022-03-30 16:37:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-4gnvw are ready
2022-03-30 16:37:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597972ms till timeout)
2022-03-30 16:37:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-4gnvw not ready: strimzi-cluster-operator)
2022-03-30 16:37:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-4gnvw are ready
2022-03-30 16:37:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596966ms till timeout)
2022-03-30 16:37:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-4gnvw not ready: strimzi-cluster-operator)
2022-03-30 16:37:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-4gnvw are ready
2022-03-30 16:37:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595960ms till timeout)
2022-03-30 16:37:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-4gnvw not ready: strimzi-cluster-operator)
2022-03-30 16:37:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-4gnvw are ready
2022-03-30 16:37:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594955ms till timeout)
2022-03-30 16:37:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-4gnvw not ready: strimzi-cluster-operator)
2022-03-30 16:37:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-4gnvw are ready
2022-03-30 16:37:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593950ms till timeout)
2022-03-30 16:37:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-4gnvw not ready: strimzi-cluster-operator)
2022-03-30 16:37:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-4gnvw are ready
2022-03-30 16:37:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592944ms till timeout)
2022-03-30 16:37:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-4gnvw not ready: strimzi-cluster-operator)
2022-03-30 16:37:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-4gnvw are ready
2022-03-30 16:37:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591939ms till timeout)
2022-03-30 16:37:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-4gnvw not ready: strimzi-cluster-operator)
2022-03-30 16:37:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-4gnvw are ready
2022-03-30 16:37:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590933ms till timeout)
2022-03-30 16:37:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-4gnvw not ready: strimzi-cluster-operator)
2022-03-30 16:37:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-4gnvw are ready
2022-03-30 16:37:21 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-30 16:37:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 16:37:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:667] [operators.user.UserST - Before All] - Setup test suite environment
2022-03-30 16:37:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:69] [operators.user.UserST] - Adding parallel suite: UserST
2022-03-30 16:37:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:73] [operators.user.UserST] - Parallel suites count: 1
2022-03-30 16:37:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:184] UserST suite now can proceed its execution
2022-03-30 16:37:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-30 16:37:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestSuiteNamespaceManager:129] Test suite `UserST` creates these additional namespaces:[user-st]
2022-03-30 16:37:21 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: user-st
2022-03-30 16:37:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace user-st
2022-03-30 16:37:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace user-st -o json
2022-03-30 16:37:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace user-st -o json
2022-03-30 16:37:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:37:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T16:37:21Z",
        "name": "user-st",
        "resourceVersion": "49386",
        "selfLink": "/api/v1/namespaces/user-st",
        "uid": "10d45579-913a-43bd-ad49-d8e0c3766772"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 16:37:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st]}
2022-03-30 16:37:21 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: user-st
2022-03-30 16:37:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=user-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 16:37:21 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: user-st
2022-03-30 16:37:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka user-cluster-name in namespace user-st
2022-03-30 16:37:21 [ForkJoinPool-3-worker-3] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkas' with unstable version 'v1beta2'
2022-03-30 16:37:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:user-cluster-name
2022-03-30 16:37:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: user-cluster-name will have desired state: Ready
2022-03-30 16:37:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: user-cluster-name will have desired state: Ready
2022-03-30 16:37:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (839997ms till timeout)
2022-03-30 16:37:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (838992ms till timeout)
2022-03-30 16:37:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (837988ms till timeout)
2022-03-30 16:37:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (836982ms till timeout)
2022-03-30 16:37:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (835977ms till timeout)
2022-03-30 16:37:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (834973ms till timeout)
2022-03-30 16:37:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (833969ms till timeout)
2022-03-30 16:37:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (832964ms till timeout)
2022-03-30 16:37:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (831959ms till timeout)
2022-03-30 16:37:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (830955ms till timeout)
2022-03-30 16:37:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (829949ms till timeout)
2022-03-30 16:37:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (828941ms till timeout)
2022-03-30 16:37:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (827935ms till timeout)
2022-03-30 16:37:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (826931ms till timeout)
2022-03-30 16:37:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (825926ms till timeout)
2022-03-30 16:37:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (824921ms till timeout)
2022-03-30 16:37:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (823915ms till timeout)
2022-03-30 16:37:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (822911ms till timeout)
2022-03-30 16:37:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (821906ms till timeout)
2022-03-30 16:37:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (820901ms till timeout)
2022-03-30 16:37:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (819897ms till timeout)
2022-03-30 16:37:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (818892ms till timeout)
2022-03-30 16:37:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (817888ms till timeout)
2022-03-30 16:37:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (816883ms till timeout)
2022-03-30 16:37:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (815879ms till timeout)
2022-03-30 16:37:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (814875ms till timeout)
2022-03-30 16:37:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (813870ms till timeout)
2022-03-30 16:37:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (812866ms till timeout)
2022-03-30 16:37:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (811861ms till timeout)
2022-03-30 16:37:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (810857ms till timeout)
2022-03-30 16:37:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (809852ms till timeout)
2022-03-30 16:37:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (808848ms till timeout)
2022-03-30 16:37:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (807843ms till timeout)
2022-03-30 16:37:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (806839ms till timeout)
2022-03-30 16:37:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (805834ms till timeout)
2022-03-30 16:37:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (804830ms till timeout)
2022-03-30 16:37:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (803826ms till timeout)
2022-03-30 16:37:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (802821ms till timeout)
2022-03-30 16:38:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (801816ms till timeout)
2022-03-30 16:38:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (800812ms till timeout)
2022-03-30 16:38:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (799807ms till timeout)
2022-03-30 16:38:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (798803ms till timeout)
2022-03-30 16:38:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (797798ms till timeout)
2022-03-30 16:38:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (796794ms till timeout)
2022-03-30 16:38:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (795789ms till timeout)
2022-03-30 16:38:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (794785ms till timeout)
2022-03-30 16:38:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (793781ms till timeout)
2022-03-30 16:38:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (792776ms till timeout)
2022-03-30 16:38:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (791772ms till timeout)
2022-03-30 16:38:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (790767ms till timeout)
2022-03-30 16:38:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (789763ms till timeout)
2022-03-30 16:38:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (788758ms till timeout)
2022-03-30 16:38:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (787754ms till timeout)
2022-03-30 16:38:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (786750ms till timeout)
2022-03-30 16:38:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (785745ms till timeout)
2022-03-30 16:38:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (784741ms till timeout)
2022-03-30 16:38:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (783736ms till timeout)
2022-03-30 16:38:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (782731ms till timeout)
2022-03-30 16:38:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (781727ms till timeout)
2022-03-30 16:38:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (780723ms till timeout)
2022-03-30 16:38:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (779719ms till timeout)
2022-03-30 16:38:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (778714ms till timeout)
2022-03-30 16:38:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (777710ms till timeout)
2022-03-30 16:38:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (776706ms till timeout)
2022-03-30 16:38:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (775701ms till timeout)
2022-03-30 16:38:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (774697ms till timeout)
2022-03-30 16:38:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (773692ms till timeout)
2022-03-30 16:38:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (772688ms till timeout)
2022-03-30 16:38:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (771684ms till timeout)
2022-03-30 16:38:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (770679ms till timeout)
2022-03-30 16:38:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (769675ms till timeout)
2022-03-30 16:38:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (768671ms till timeout)
2022-03-30 16:38:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (767667ms till timeout)
2022-03-30 16:38:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (766663ms till timeout)
2022-03-30 16:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: user-cluster-name is in desired state: Ready
2022-03-30 16:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 16:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testUpdateUser-STARTED
2022-03-30 16:38:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 16:38:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-30 16:38:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testUpdateUser
2022-03-30 16:38:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 1
2022-03-30 16:38:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:230] testUpdateUser test now can proceed its execution
2022-03-30 16:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 16:38:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testUpdateUser=my-cluster-75998b92}
2022-03-30 16:38:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testUpdateUser=my-user-275427717-1183444028}
2022-03-30 16:38:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testUpdateUser=my-topic-275186280-765877519}
2022-03-30 16:38:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testUpdateUser=my-cluster-75998b92-kafka-clients}
2022-03-30 16:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-275427717-1183444028 in namespace user-st
2022-03-30 16:38:36 [ForkJoinPool-3-worker-3] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkausers' with unstable version 'v1beta2'
2022-03-30 16:38:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-275427717-1183444028
2022-03-30 16:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-275427717-1183444028 will have desired state: Ready
2022-03-30 16:38:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-275427717-1183444028 will have desired state: Ready
2022-03-30 16:38:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-275427717-1183444028 will have desired state: Ready not ready, will try again in 1000 ms (179997ms till timeout)
2022-03-30 16:38:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-275427717-1183444028 is in desired state: Ready
2022-03-30 16:38:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['data']['ca.crt']
2022-03-30 16:38:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['data']['user.crt']
2022-03-30 16:38:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['data']['user.key']
2022-03-30 16:38:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['metadata']['name']
2022-03-30 16:38:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['metadata']['namespace']
2022-03-30 16:38:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['metadata']['name']
2022-03-30 16:38:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['metadata']['namespace']
2022-03-30 16:38:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['spec']['authentication']['type']
2022-03-30 16:38:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for increase observation generation from 1 for user my-user-275427717-1183444028
2022-03-30 16:38:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] increase observation generation from 1 for user my-user-275427717-1183444028 not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 16:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:46] Waiting for Secret my-user-275427717-1183444028
2022-03-30 16:38:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Expected secret my-user-275427717-1183444028 exists
2022-03-30 16:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:50] Secret my-user-275427717-1183444028 created
2022-03-30 16:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-275427717-1183444028 will have desired state: Ready
2022-03-30 16:38:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-275427717-1183444028 will have desired state: Ready
2022-03-30 16:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-275427717-1183444028 is in desired state: Ready
2022-03-30 16:38:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['data']['password']
2022-03-30 16:38:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['metadata']['name']
2022-03-30 16:38:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['metadata']['namespace']
2022-03-30 16:38:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['spec']['authentication']['type']
2022-03-30 16:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUserUtils:62] Waiting for KafkaUser deletion my-user-275427717-1183444028
2022-03-30 16:38:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser deletion my-user-275427717-1183444028
2022-03-30 16:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUserUtils:75] KafkaUser my-user-275427717-1183444028 deleted
2022-03-30 16:38:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 16:38:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-30 16:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 16:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testUpdateUser
2022-03-30 16:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-275427717-1183444028 in namespace user-st
2022-03-30 16:38:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-275427717-1183444028
2022-03-30 16:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 16:38:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:267] testUpdateUser - Notifies waiting test cases:[testUpdateUser] to and randomly select one to start execution
2022-03-30 16:38:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testUpdateUser
2022-03-30 16:38:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 0
2022-03-30 16:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testUpdateUser-FINISHED
2022-03-30 16:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 16:38:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 16:38:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:690] [operators.user.UserST - After All] - Clean up after test suite
2022-03-30 16:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 16:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for UserST
2022-03-30 16:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka user-cluster-name in namespace user-st
2022-03-30 16:38:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:user-cluster-name
2022-03-30 16:38:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:user-cluster-name not ready, will try again in 10000 ms (839994ms till timeout)
2022-03-30 16:38:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 16:38:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace user-st removal
2022-03-30 16:38:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:38:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:38:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:38:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (479927ms till timeout)
2022-03-30 16:38:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:38:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:38:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:38:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (478818ms till timeout)
2022-03-30 16:38:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:38:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:38:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:38:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (477742ms till timeout)
2022-03-30 16:38:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:38:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:38:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:38:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (476668ms till timeout)
2022-03-30 16:38:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:38:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:38:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:38:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (475593ms till timeout)
2022-03-30 16:38:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:38:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:38:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:38:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (474521ms till timeout)
2022-03-30 16:38:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:38:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:38:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:38:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (473447ms till timeout)
2022-03-30 16:38:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:38:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:38:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:38:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (472379ms till timeout)
2022-03-30 16:38:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:38:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:38:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:38:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (471303ms till timeout)
2022-03-30 16:38:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:38:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:38:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:38:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (470220ms till timeout)
2022-03-30 16:38:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:38:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:38:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:38:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (469146ms till timeout)
2022-03-30 16:39:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:39:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:39:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:39:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (468063ms till timeout)
2022-03-30 16:39:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:39:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:39:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:39:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (466988ms till timeout)
2022-03-30 16:39:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:39:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:39:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:39:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (465915ms till timeout)
2022-03-30 16:39:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:39:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:39:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:39:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (464846ms till timeout)
2022-03-30 16:39:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:39:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:39:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:39:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (463776ms till timeout)
2022-03-30 16:39:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:39:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:39:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:39:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (462707ms till timeout)
2022-03-30 16:39:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:39:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:39:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:39:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (461634ms till timeout)
2022-03-30 16:39:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:39:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:39:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:39:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (460561ms till timeout)
2022-03-30 16:39:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:39:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:39:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:39:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (459491ms till timeout)
2022-03-30 16:39:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:39:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:39:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:39:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (458411ms till timeout)
2022-03-30 16:39:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:39:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:39:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:39:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (457331ms till timeout)
2022-03-30 16:39:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:39:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:39:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:39:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (456252ms till timeout)
2022-03-30 16:39:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:39:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:39:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:39:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (455180ms till timeout)
2022-03-30 16:39:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:39:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:39:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:39:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (454105ms till timeout)
2022-03-30 16:39:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:39:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace user-st get Namespace user-st -o yaml
2022-03-30 16:39:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 16:39:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 16:39:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "user-st" not found
2022-03-30 16:39:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 16:39:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[]}
2022-03-30 16:39:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:254] UserST - Notifies waiting test suites:[UserST] to and randomly select one to start execution
2022-03-30 16:39:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:85] [operators.user.UserST] - Removing parallel suite: UserST
2022-03-30 16:39:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:89] [operators.user.UserST] - Parallel suites count: 0
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 149.049 s - in io.strimzi.systemtest.operators.user.UserST
[[1;34mINFO[m] Running io.strimzi.systemtest.bridge.HttpBridgeTlsST
2022-03-30 16:39:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 16:39:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:667] [bridge.HttpBridgeTlsST - Before All] - Setup test suite environment
2022-03-30 16:39:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:69] [bridge.HttpBridgeTlsST] - Adding parallel suite: HttpBridgeTlsST
2022-03-30 16:39:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:73] [bridge.HttpBridgeTlsST] - Parallel suites count: 1
2022-03-30 16:39:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:184] HttpBridgeTlsST suite now can proceed its execution
2022-03-30 16:39:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-30 16:39:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestSuiteNamespaceManager:129] Test suite `HttpBridgeTlsST` creates these additional namespaces:[http-bridge-tls-st]
2022-03-30 16:39:15 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: http-bridge-tls-st
2022-03-30 16:39:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace http-bridge-tls-st
2022-03-30 16:39:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace user-st get Namespace http-bridge-tls-st -o json
2022-03-30 16:39:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace user-st get Namespace http-bridge-tls-st -o json
2022-03-30 16:39:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:39:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T16:39:15Z",
        "name": "http-bridge-tls-st",
        "resourceVersion": "49790",
        "selfLink": "/api/v1/namespaces/http-bridge-tls-st",
        "uid": "b7f73c22-1b5d-40ba-92d5-1c0f3864d694"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 16:39:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st]}
2022-03-30 16:39:15 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: http-bridge-tls-st
2022-03-30 16:39:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=http-bridge-tls-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 16:39:15 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: http-bridge-tls-st
2022-03-30 16:39:15 [ForkJoinPool-3-worker-3] [32mINFO [m [HttpBridgeTlsST:129] Deploy Kafka and KafkaBridge before tests
2022-03-30 16:39:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-30 16:39:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:http-bridge-tls-cluster-name
2022-03-30 16:39:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-30 16:39:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-30 16:39:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (839995ms till timeout)
2022-03-30 16:39:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (838992ms till timeout)
2022-03-30 16:39:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (837988ms till timeout)
2022-03-30 16:39:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (836984ms till timeout)
2022-03-30 16:39:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (835980ms till timeout)
2022-03-30 16:39:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (834975ms till timeout)
2022-03-30 16:39:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (833971ms till timeout)
2022-03-30 16:39:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (832967ms till timeout)
2022-03-30 16:39:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (831963ms till timeout)
2022-03-30 16:39:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (830958ms till timeout)
2022-03-30 16:39:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (829954ms till timeout)
2022-03-30 16:39:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (828950ms till timeout)
2022-03-30 16:39:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (827946ms till timeout)
2022-03-30 16:39:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (826941ms till timeout)
2022-03-30 16:39:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (825937ms till timeout)
2022-03-30 16:39:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (824932ms till timeout)
2022-03-30 16:39:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (823928ms till timeout)
2022-03-30 16:39:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (822924ms till timeout)
2022-03-30 16:39:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (821920ms till timeout)
2022-03-30 16:39:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (820916ms till timeout)
2022-03-30 16:39:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (819912ms till timeout)
2022-03-30 16:39:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (818907ms till timeout)
2022-03-30 16:39:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (817903ms till timeout)
2022-03-30 16:39:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (816899ms till timeout)
2022-03-30 16:39:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (815895ms till timeout)
2022-03-30 16:39:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (814890ms till timeout)
2022-03-30 16:39:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (813886ms till timeout)
2022-03-30 16:39:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (812882ms till timeout)
2022-03-30 16:39:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (811878ms till timeout)
2022-03-30 16:39:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (810873ms till timeout)
2022-03-30 16:39:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (809869ms till timeout)
2022-03-30 16:39:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (808864ms till timeout)
2022-03-30 16:39:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (807860ms till timeout)
2022-03-30 16:39:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (806855ms till timeout)
2022-03-30 16:39:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (805851ms till timeout)
2022-03-30 16:39:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (804847ms till timeout)
2022-03-30 16:39:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (803842ms till timeout)
2022-03-30 16:39:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (802838ms till timeout)
2022-03-30 16:39:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (801834ms till timeout)
2022-03-30 16:39:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (800831ms till timeout)
2022-03-30 16:39:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (799827ms till timeout)
2022-03-30 16:39:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (798823ms till timeout)
2022-03-30 16:39:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (797819ms till timeout)
2022-03-30 16:39:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (796815ms till timeout)
2022-03-30 16:39:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (795812ms till timeout)
2022-03-30 16:40:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (794807ms till timeout)
2022-03-30 16:40:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (793803ms till timeout)
2022-03-30 16:40:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (792799ms till timeout)
2022-03-30 16:40:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (791795ms till timeout)
2022-03-30 16:40:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (790790ms till timeout)
2022-03-30 16:40:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (789786ms till timeout)
2022-03-30 16:40:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (788782ms till timeout)
2022-03-30 16:40:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (787778ms till timeout)
2022-03-30 16:40:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (786774ms till timeout)
2022-03-30 16:40:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (785770ms till timeout)
2022-03-30 16:40:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (784766ms till timeout)
2022-03-30 16:40:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (783762ms till timeout)
2022-03-30 16:40:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (782757ms till timeout)
2022-03-30 16:40:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (781754ms till timeout)
2022-03-30 16:40:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (780750ms till timeout)
2022-03-30 16:40:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (779746ms till timeout)
2022-03-30 16:40:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (778742ms till timeout)
2022-03-30 16:40:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (777738ms till timeout)
2022-03-30 16:40:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (776734ms till timeout)
2022-03-30 16:40:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (775730ms till timeout)
2022-03-30 16:40:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (774726ms till timeout)
2022-03-30 16:40:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (773723ms till timeout)
2022-03-30 16:40:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (772719ms till timeout)
2022-03-30 16:40:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (771715ms till timeout)
2022-03-30 16:40:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: http-bridge-tls-cluster-name is in desired state: Ready
2022-03-30 16:40:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1215640632-764221849 in namespace http-bridge-tls-st
2022-03-30 16:40:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1215640632-764221849
2022-03-30 16:40:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1215640632-764221849 will have desired state: Ready
2022-03-30 16:40:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-1215640632-764221849 will have desired state: Ready
2022-03-30 16:40:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-1215640632-764221849 will have desired state: Ready not ready, will try again in 1000 ms (179997ms till timeout)
2022-03-30 16:40:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1215640632-764221849 is in desired state: Ready
2022-03-30 16:40:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment http-bridge-tls-st-kafka-clients in namespace http-bridge-tls-st
2022-03-30 16:40:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients
2022-03-30 16:40:26 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready
2022-03-30 16:40:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready
2022-03-30 16:40:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready not ready, will try again in 1000 ms (479998ms till timeout)
2022-03-30 16:40:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready not ready, will try again in 1000 ms (478994ms till timeout)
2022-03-30 16:40:28 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: http-bridge-tls-st-kafka-clients is ready
2022-03-30 16:40:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-30 16:40:28 [ForkJoinPool-3-worker-3] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkabridges' with unstable version 'v1beta2'
2022-03-30 16:40:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaBridge:http-bridge-tls-cluster-name
2022-03-30 16:40:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-30 16:40:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-30 16:40:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (479998ms till timeout)
2022-03-30 16:40:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (478994ms till timeout)
2022-03-30 16:40:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (477990ms till timeout)
2022-03-30 16:40:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (476986ms till timeout)
2022-03-30 16:40:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (475982ms till timeout)
2022-03-30 16:40:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (474977ms till timeout)
2022-03-30 16:40:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (473973ms till timeout)
2022-03-30 16:40:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (472969ms till timeout)
2022-03-30 16:40:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (471965ms till timeout)
2022-03-30 16:40:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (470961ms till timeout)
2022-03-30 16:40:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (469957ms till timeout)
2022-03-30 16:40:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (468951ms till timeout)
2022-03-30 16:40:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (467945ms till timeout)
2022-03-30 16:40:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (466941ms till timeout)
2022-03-30 16:40:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (465937ms till timeout)
2022-03-30 16:40:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (464933ms till timeout)
2022-03-30 16:40:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (463929ms till timeout)
2022-03-30 16:40:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (462925ms till timeout)
2022-03-30 16:40:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (461921ms till timeout)
2022-03-30 16:40:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (460917ms till timeout)
2022-03-30 16:40:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: http-bridge-tls-cluster-name is in desired state: Ready
2022-03-30 16:40:48 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-30 16:40:48 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 16:40:48 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 16:40:48 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testSendSimpleMessageTls-STARTED
2022-03-30 16:40:48 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testReceiveSimpleMessageTls-STARTED
2022-03-30 16:40:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 16:40:48 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 16:40:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:659] [bridge.HttpBridgeTlsST - Before Each] - Setup test case environment
2022-03-30 16:40:48 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:659] [bridge.HttpBridgeTlsST - Before Each] - Setup test case environment
2022-03-30 16:40:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:77] [bridge.HttpBridgeTlsST] - Adding parallel test: testSendSimpleMessageTls
2022-03-30 16:40:48 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:77] [bridge.HttpBridgeTlsST] - Adding parallel test: testReceiveSimpleMessageTls
2022-03-30 16:40:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:81] [bridge.HttpBridgeTlsST] - Parallel test count: 1
2022-03-30 16:40:48 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:81] [bridge.HttpBridgeTlsST] - Parallel test count: 2
2022-03-30 16:40:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:230] testSendSimpleMessageTls test now can proceed its execution
2022-03-30 16:40:48 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:230] testReceiveSimpleMessageTls test now can proceed its execution
2022-03-30 16:40:48 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 16:40:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-128d9a40, testUpdateUser=my-cluster-75998b92}
2022-03-30 16:40:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-323988616-1355569855, testUpdateUser=my-user-275427717-1183444028}
2022-03-30 16:40:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-1040221322-670398147, testUpdateUser=my-topic-275186280-765877519}
2022-03-30 16:40:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients}
2022-03-30 16:40:48 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 16:40:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-128d9a40, testUpdateUser=my-cluster-75998b92, testReceiveSimpleMessageTls=my-cluster-90021a9f}
2022-03-30 16:40:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-323988616-1355569855, testUpdateUser=my-user-275427717-1183444028, testReceiveSimpleMessageTls=my-user-491397521-1196361913}
2022-03-30 16:40:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-1040221322-670398147, testUpdateUser=my-topic-275186280-765877519, testReceiveSimpleMessageTls=my-topic-941665209-436577257}
2022-03-30 16:40:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients, testReceiveSimpleMessageTls=my-cluster-90021a9f-kafka-clients}
2022-03-30 16:40:48 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-413111297-1013135812 in namespace http-bridge-tls-st
2022-03-30 16:40:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-420199869-549061721 in namespace http-bridge-tls-st
2022-03-30 16:40:48 [ForkJoinPool-3-worker-3] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkatopics' with unstable version 'v1beta2'
2022-03-30 16:40:48 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-413111297-1013135812
2022-03-30 16:40:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-420199869-549061721
2022-03-30 16:40:48 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-413111297-1013135812 will have desired state: Ready
2022-03-30 16:40:48 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-413111297-1013135812 will have desired state: Ready
2022-03-30 16:40:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-420199869-549061721 will have desired state: Ready
2022-03-30 16:40:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-420199869-549061721 will have desired state: Ready
2022-03-30 16:40:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-413111297-1013135812 will have desired state: Ready not ready, will try again in 1000 ms (179997ms till timeout)
2022-03-30 16:40:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-420199869-549061721 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 16:40:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-413111297-1013135812 is in desired state: Ready
2022-03-30 16:40:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-420199869-549061721 is in desired state: Ready
2022-03-30 16:40:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job producer-1467030109 in namespace http-bridge-tls-st
2022-03-30 16:40:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job consumer-1455809850 in namespace http-bridge-tls-st
2022-03-30 16:40:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:consumer-1455809850
2022-03-30 16:40:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:producer-1467030109
2022-03-30 16:40:49 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: consumer-1455809850 will be in active state
2022-03-30 16:40:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for job active
2022-03-30 16:40:49 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: producer-1467030109 will be in active state
2022-03-30 16:40:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for job active
2022-03-30 16:40:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] job active not ready, will try again in 1000 ms (179996ms till timeout)
2022-03-30 16:40:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:producer-1467030109 to finished
2022-03-30 16:40:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for job finished
2022-03-30 16:40:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [ClientUtils:79] Job producer-1467030109 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T16:40:49Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 16:40:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (219995ms till timeout)
2022-03-30 16:40:50 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-30 16:40:50 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job producer-1331129449 in namespace http-bridge-tls-st
2022-03-30 16:40:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [ClientUtils:79] Job producer-1467030109 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T16:40:49Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 16:40:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (218990ms till timeout)
2022-03-30 16:40:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:producer-1331129449
2022-03-30 16:40:50 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: producer-1331129449 will be in active state
2022-03-30 16:40:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for job active
2022-03-30 16:40:50 [ForkJoinPool-3-worker-1] [32mINFO [m [ClientUtils:61] Waiting till producer producer-1331129449 and consumer consumer-1455809850 finish
2022-03-30 16:40:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for clients finished
2022-03-30 16:40:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (219998ms till timeout)
2022-03-30 16:40:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [ClientUtils:79] Job producer-1467030109 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T16:40:49Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 16:40:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (217971ms till timeout)
2022-03-30 16:40:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (218989ms till timeout)
2022-03-30 16:40:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [ClientUtils:79] Job producer-1467030109 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T16:40:49Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 16:40:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (216964ms till timeout)
2022-03-30 16:40:52 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (217987ms till timeout)
2022-03-30 16:40:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [ClientUtils:79] Job producer-1467030109 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T16:40:49Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 16:40:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (215958ms till timeout)
2022-03-30 16:40:53 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (216983ms till timeout)
2022-03-30 16:40:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [ClientUtils:79] Job producer-1467030109 in namespace http-bridge-tls-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-30T16:40:53Z, conditions=[JobCondition(lastProbeTime=2022-03-30T16:40:53Z, lastTransitionTime=2022-03-30T16:40:53Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-30T16:40:49Z, succeeded=1, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 16:40:54 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (215979ms till timeout)
2022-03-30 16:40:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [JobUtils:37] Waiting for ReplicaSet of Deployment producer-1467030109 deletion
2022-03-30 16:40:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for ReplicaSet producer-1467030109 to be deleted
2022-03-30 16:40:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] ReplicaSet producer-1467030109 to be deleted not ready, will try again in 5000 ms (179992ms till timeout)
2022-03-30 16:40:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (214973ms till timeout)
2022-03-30 16:40:56 [ForkJoinPool-3-worker-1] [36mDEBUG[m [JobUtils:37] Waiting for ReplicaSet of Deployment producer-1331129449 deletion
2022-03-30 16:40:56 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for ReplicaSet producer-1331129449 to be deleted
2022-03-30 16:40:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] ReplicaSet producer-1331129449 to be deleted not ready, will try again in 5000 ms (179996ms till timeout)
2022-03-30 16:40:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [JobUtils:40] Job producer-1467030109 was deleted
2022-03-30 16:40:59 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-30 16:40:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job consumer-2092439542 in namespace http-bridge-tls-st
2022-03-30 16:40:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:consumer-2092439542
2022-03-30 16:40:59 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: consumer-2092439542 will be in active state
2022-03-30 16:40:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for job active
2022-03-30 16:40:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] job active not ready, will try again in 1000 ms (179996ms till timeout)
2022-03-30 16:41:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:consumer-2092439542 to finished
2022-03-30 16:41:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for job finished
2022-03-30 16:41:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [ClientUtils:79] Job consumer-2092439542 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T16:40:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 16:41:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (219995ms till timeout)
2022-03-30 16:41:01 [ForkJoinPool-3-worker-1] [36mDEBUG[m [JobUtils:40] Job producer-1331129449 was deleted
2022-03-30 16:41:01 [ForkJoinPool-3-worker-1] [36mDEBUG[m [JobUtils:37] Waiting for ReplicaSet of Deployment consumer-1455809850 deletion
2022-03-30 16:41:01 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for ReplicaSet consumer-1455809850 to be deleted
2022-03-30 16:41:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] ReplicaSet consumer-1455809850 to be deleted not ready, will try again in 5000 ms (179995ms till timeout)
2022-03-30 16:41:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [ClientUtils:79] Job consumer-2092439542 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T16:40:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 16:41:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (218990ms till timeout)
2022-03-30 16:41:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [ClientUtils:79] Job consumer-2092439542 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T16:40:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 16:41:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (217984ms till timeout)
2022-03-30 16:41:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [ClientUtils:79] Job consumer-2092439542 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T16:40:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 16:41:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (216978ms till timeout)
2022-03-30 16:41:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [ClientUtils:79] Job consumer-2092439542 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T16:40:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 16:41:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (215972ms till timeout)
2022-03-30 16:41:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [ClientUtils:79] Job consumer-2092439542 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T16:40:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 16:41:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (214966ms till timeout)
2022-03-30 16:41:06 [ForkJoinPool-3-worker-1] [36mDEBUG[m [JobUtils:40] Job consumer-1455809850 was deleted
2022-03-30 16:41:06 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 16:41:06 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:675] [bridge.HttpBridgeTlsST - After Each] - Clean up after test
2022-03-30 16:41:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 16:41:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testReceiveSimpleMessageTls
2022-03-30 16:41:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job consumer-1455809850 in namespace http-bridge-tls-st
2022-03-30 16:41:06 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:consumer-1455809850
2022-03-30 16:41:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job producer-1331129449 in namespace http-bridge-tls-st
2022-03-30 16:41:06 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:producer-1331129449
2022-03-30 16:41:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-413111297-1013135812 in namespace http-bridge-tls-st
2022-03-30 16:41:06 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-413111297-1013135812
2022-03-30 16:41:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-413111297-1013135812 not ready, will try again in 10000 ms (179993ms till timeout)
2022-03-30 16:41:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [ClientUtils:79] Job consumer-2092439542 in namespace http-bridge-tls-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-30T16:41:06Z, conditions=[JobCondition(lastProbeTime=2022-03-30T16:41:06Z, lastTransitionTime=2022-03-30T16:41:06Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-30T16:40:59Z, succeeded=1, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 16:41:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [JobUtils:37] Waiting for ReplicaSet of Deployment consumer-2092439542 deletion
2022-03-30 16:41:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for ReplicaSet consumer-2092439542 to be deleted
2022-03-30 16:41:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] ReplicaSet consumer-2092439542 to be deleted not ready, will try again in 5000 ms (179995ms till timeout)
2022-03-30 16:41:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [JobUtils:40] Job consumer-2092439542 was deleted
2022-03-30 16:41:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 16:41:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:675] [bridge.HttpBridgeTlsST - After Each] - Clean up after test
2022-03-30 16:41:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 16:41:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testSendSimpleMessageTls
2022-03-30 16:41:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job producer-1467030109 in namespace http-bridge-tls-st
2022-03-30 16:41:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:producer-1467030109
2022-03-30 16:41:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job consumer-2092439542 in namespace http-bridge-tls-st
2022-03-30 16:41:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:consumer-2092439542
2022-03-30 16:41:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-420199869-549061721 in namespace http-bridge-tls-st
2022-03-30 16:41:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-420199869-549061721
2022-03-30 16:41:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-420199869-549061721 not ready, will try again in 10000 ms (179992ms till timeout)
2022-03-30 16:41:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 16:41:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:267] testReceiveSimpleMessageTls - Notifies waiting test cases:[testUpdateUser, testSendSimpleMessageTls, testReceiveSimpleMessageTls] to and randomly select one to start execution
2022-03-30 16:41:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:93] [bridge.HttpBridgeTlsST] - Removing parallel test: testReceiveSimpleMessageTls
2022-03-30 16:41:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:97] [bridge.HttpBridgeTlsST] - Parallel test count: 1
2022-03-30 16:41:16 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testReceiveSimpleMessageTls-FINISHED
2022-03-30 16:41:16 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 16:41:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 16:41:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:267] testSendSimpleMessageTls - Notifies waiting test cases:[testUpdateUser, testSendSimpleMessageTls, testReceiveSimpleMessageTls] to and randomly select one to start execution
2022-03-30 16:41:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:93] [bridge.HttpBridgeTlsST] - Removing parallel test: testSendSimpleMessageTls
2022-03-30 16:41:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:97] [bridge.HttpBridgeTlsST] - Parallel test count: 0
2022-03-30 16:41:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testSendSimpleMessageTls-FINISHED
2022-03-30 16:41:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 16:41:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 16:41:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:690] [bridge.HttpBridgeTlsST - After All] - Clean up after test suite
2022-03-30 16:41:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 16:41:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for HttpBridgeTlsST
2022-03-30 16:41:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment http-bridge-tls-st-kafka-clients in namespace http-bridge-tls-st
2022-03-30 16:41:21 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1215640632-764221849 in namespace http-bridge-tls-st
2022-03-30 16:41:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients
2022-03-30 16:41:21 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1215640632-764221849
2022-03-30 16:41:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1215640632-764221849 not ready, will try again in 10000 ms (179983ms till timeout)
2022-03-30 16:41:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (479978ms till timeout)
2022-03-30 16:41:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-30 16:41:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:http-bridge-tls-cluster-name
2022-03-30 16:41:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (469968ms till timeout)
2022-03-30 16:41:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:http-bridge-tls-cluster-name not ready, will try again in 10000 ms (839997ms till timeout)
2022-03-30 16:41:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-30 16:41:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaBridge:http-bridge-tls-cluster-name
2022-03-30 16:41:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (459957ms till timeout)
2022-03-30 16:41:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaBridge:http-bridge-tls-cluster-name not ready, will try again in 10000 ms (479988ms till timeout)
2022-03-30 16:41:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (449947ms till timeout)
2022-03-30 16:42:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 16:42:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace http-bridge-tls-st removal
2022-03-30 16:42:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace http-bridge-tls-st get Namespace http-bridge-tls-st -o yaml
2022-03-30 16:42:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace http-bridge-tls-st get Namespace http-bridge-tls-st -o yaml
2022-03-30 16:42:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:42:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (479919ms till timeout)
2022-03-30 16:42:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace http-bridge-tls-st get Namespace http-bridge-tls-st -o yaml
2022-03-30 16:42:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace http-bridge-tls-st get Namespace http-bridge-tls-st -o yaml
2022-03-30 16:42:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:42:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (478841ms till timeout)
2022-03-30 16:42:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace http-bridge-tls-st get Namespace http-bridge-tls-st -o yaml
2022-03-30 16:42:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace http-bridge-tls-st get Namespace http-bridge-tls-st -o yaml
2022-03-30 16:42:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:42:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (477767ms till timeout)
2022-03-30 16:42:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace http-bridge-tls-st get Namespace http-bridge-tls-st -o yaml
2022-03-30 16:42:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace http-bridge-tls-st get Namespace http-bridge-tls-st -o yaml
2022-03-30 16:42:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:42:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (476696ms till timeout)
2022-03-30 16:42:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace http-bridge-tls-st get Namespace http-bridge-tls-st -o yaml
2022-03-30 16:42:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace http-bridge-tls-st get Namespace http-bridge-tls-st -o yaml
2022-03-30 16:42:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:42:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (475619ms till timeout)
2022-03-30 16:42:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace http-bridge-tls-st get Namespace http-bridge-tls-st -o yaml
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace http-bridge-tls-st get Namespace http-bridge-tls-st -o yaml
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "http-bridge-tls-st" not found
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[]}
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:254] HttpBridgeTlsST - Notifies waiting test suites:[UserST, HttpBridgeTlsST] to and randomly select one to start execution
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:85] [bridge.HttpBridgeTlsST] - Removing parallel suite: HttpBridgeTlsST
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:89] [bridge.HttpBridgeTlsST] - Parallel suites count: 0
[[1;34mINFO[m] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 171.446 s - in io.strimzi.systemtest.bridge.HttpBridgeTlsST
[[1;34mINFO[m] Running io.strimzi.systemtest.cruisecontrol.CruiseControlApiST
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:667] [cruisecontrol.CruiseControlApiST - Before All] - Setup test suite environment
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:69] [cruisecontrol.CruiseControlApiST] - Adding parallel suite: CruiseControlApiST
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:73] [cruisecontrol.CruiseControlApiST] - Parallel suites count: 1
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:184] CruiseControlApiST suite now can proceed its execution
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestSuiteNamespaceManager:129] Test suite `CruiseControlApiST` creates these additional namespaces:[cruise-control-api-st]
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: cruise-control-api-st
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace cruise-control-api-st
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace http-bridge-tls-st get Namespace cruise-control-api-st -o json
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace http-bridge-tls-st get Namespace cruise-control-api-st -o json
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T16:42:07Z",
        "name": "cruise-control-api-st",
        "resourceVersion": "50393",
        "selfLink": "/api/v1/namespaces/cruise-control-api-st",
        "uid": "f98fe4ad-d915-49c0-8b15-3a068d93c013"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st]}
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: cruise-control-api-st
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=cruise-control-api-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: cruise-control-api-st
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 16:42:07 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlApiST.testCruiseControlBasicAPIRequestsWithSecurityDisabled-STARTED
2022-03-30 16:42:07 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlApiST.testCruiseControlBasicAPIRequests-STARTED
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 16:42:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:659] [cruisecontrol.CruiseControlApiST - Before Each] - Setup test case environment
2022-03-30 16:42:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:659] [cruisecontrol.CruiseControlApiST - Before Each] - Setup test case environment
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:77] [cruisecontrol.CruiseControlApiST] - Adding parallel test: testCruiseControlBasicAPIRequestsWithSecurityDisabled
2022-03-30 16:42:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:77] [cruisecontrol.CruiseControlApiST] - Adding parallel test: testCruiseControlBasicAPIRequests
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:81] [cruisecontrol.CruiseControlApiST] - Parallel test count: 1
2022-03-30 16:42:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:81] [cruisecontrol.CruiseControlApiST] - Parallel test count: 2
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:230] testCruiseControlBasicAPIRequestsWithSecurityDisabled test now can proceed its execution
2022-03-30 16:42:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:230] testCruiseControlBasicAPIRequests test now can proceed its execution
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-128d9a40, testUpdateUser=my-cluster-75998b92, testReceiveSimpleMessageTls=my-cluster-90021a9f, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de}
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-323988616-1355569855, testUpdateUser=my-user-275427717-1183444028, testReceiveSimpleMessageTls=my-user-491397521-1196361913, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-83332781-1273282344}
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-1040221322-670398147, testUpdateUser=my-topic-275186280-765877519, testReceiveSimpleMessageTls=my-topic-941665209-436577257, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1536476030-1482182833}
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients, testReceiveSimpleMessageTls=my-cluster-90021a9f-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de-kafka-clients}
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-0 for test case:testCruiseControlBasicAPIRequestsWithSecurityDisabled
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-0
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-0
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-api-st get Namespace namespace-0 -o json
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-api-st get Namespace namespace-0 -o json
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T16:42:07Z",
        "name": "namespace-0",
        "resourceVersion": "50398",
        "selfLink": "/api/v1/namespaces/namespace-0",
        "uid": "62f82158-333d-4483-b118-ae40582de4bc"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[namespace-0], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st]}
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-0
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-0, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-0
2022-03-30 16:42:07 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 16:42:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-128d9a40, testUpdateUser=my-cluster-75998b92, testReceiveSimpleMessageTls=my-cluster-90021a9f, testCruiseControlBasicAPIRequests=my-cluster-6da231a3, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de}
2022-03-30 16:42:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-323988616-1355569855, testUpdateUser=my-user-275427717-1183444028, testReceiveSimpleMessageTls=my-user-491397521-1196361913, testCruiseControlBasicAPIRequests=my-user-918568860-36694764, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-83332781-1273282344}
2022-03-30 16:42:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-1040221322-670398147, testUpdateUser=my-topic-275186280-765877519, testReceiveSimpleMessageTls=my-topic-941665209-436577257, testCruiseControlBasicAPIRequests=my-topic-710218582-658397020, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1536476030-1482182833}
2022-03-30 16:42:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients, testReceiveSimpleMessageTls=my-cluster-90021a9f-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-6da231a3-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de-kafka-clients}
2022-03-30 16:42:07 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-1 for test case:testCruiseControlBasicAPIRequests
2022-03-30 16:42:07 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-1
2022-03-30 16:42:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-1
2022-03-30 16:42:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 get Namespace namespace-1 -o json
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka cruise-control-api-cluster-name in namespace namespace-0
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-0
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:cruise-control-api-cluster-name
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: cruise-control-api-cluster-name will have desired state: Ready
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: cruise-control-api-cluster-name will have desired state: Ready
2022-03-30 16:42:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1319997ms till timeout)
2022-03-30 16:42:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 get Namespace namespace-1 -o json
2022-03-30 16:42:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:42:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T16:42:07Z",
        "name": "namespace-1",
        "resourceVersion": "50402",
        "selfLink": "/api/v1/namespaces/namespace-1",
        "uid": "66165ec7-0296-4056-bd41-a2e36278c4ac"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 16:42:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[namespace-0], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-1]}
2022-03-30 16:42:07 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-1
2022-03-30 16:42:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-1, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 16:42:07 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-1
2022-03-30 16:42:07 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-9a739e30 in namespace namespace-1
2022-03-30 16:42:07 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-1
2022-03-30 16:42:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-9a739e30
2022-03-30 16:42:07 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-9a739e30 will have desired state: Ready
2022-03-30 16:42:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-9a739e30 will have desired state: Ready
2022-03-30 16:42:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1319991ms till timeout)
2022-03-30 16:42:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1318994ms till timeout)
2022-03-30 16:42:08 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1318987ms till timeout)
2022-03-30 16:42:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1317990ms till timeout)
2022-03-30 16:42:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1317984ms till timeout)
2022-03-30 16:42:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1316986ms till timeout)
2022-03-30 16:42:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1316980ms till timeout)
2022-03-30 16:42:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1315982ms till timeout)
2022-03-30 16:42:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1315977ms till timeout)
2022-03-30 16:42:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1314979ms till timeout)
2022-03-30 16:42:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1314974ms till timeout)
2022-03-30 16:42:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1313974ms till timeout)
2022-03-30 16:42:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1313971ms till timeout)
2022-03-30 16:42:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1312971ms till timeout)
2022-03-30 16:42:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1312968ms till timeout)
2022-03-30 16:42:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1311967ms till timeout)
2022-03-30 16:42:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1311965ms till timeout)
2022-03-30 16:42:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1310963ms till timeout)
2022-03-30 16:42:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1310962ms till timeout)
2022-03-30 16:42:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1309959ms till timeout)
2022-03-30 16:42:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1309958ms till timeout)
2022-03-30 16:42:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1308947ms till timeout)
2022-03-30 16:42:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1308954ms till timeout)
2022-03-30 16:42:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1307942ms till timeout)
2022-03-30 16:42:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1307950ms till timeout)
2022-03-30 16:42:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1306937ms till timeout)
2022-03-30 16:42:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1306946ms till timeout)
2022-03-30 16:42:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1305925ms till timeout)
2022-03-30 16:42:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1305927ms till timeout)
2022-03-30 16:42:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1304916ms till timeout)
2022-03-30 16:42:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1304922ms till timeout)
2022-03-30 16:42:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1303912ms till timeout)
2022-03-30 16:42:23 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1303915ms till timeout)
2022-03-30 16:42:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1302909ms till timeout)
2022-03-30 16:42:24 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1302912ms till timeout)
2022-03-30 16:42:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1301905ms till timeout)
2022-03-30 16:42:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1301909ms till timeout)
2022-03-30 16:42:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1300902ms till timeout)
2022-03-30 16:42:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1300905ms till timeout)
2022-03-30 16:42:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1299898ms till timeout)
2022-03-30 16:42:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1299902ms till timeout)
2022-03-30 16:42:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1298894ms till timeout)
2022-03-30 16:42:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1298899ms till timeout)
2022-03-30 16:42:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1297891ms till timeout)
2022-03-30 16:42:29 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1297895ms till timeout)
2022-03-30 16:42:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1296887ms till timeout)
2022-03-30 16:42:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1296892ms till timeout)
2022-03-30 16:42:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1295883ms till timeout)
2022-03-30 16:42:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1295888ms till timeout)
2022-03-30 16:42:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1294879ms till timeout)
2022-03-30 16:42:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1294885ms till timeout)
2022-03-30 16:42:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1293875ms till timeout)
2022-03-30 16:42:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1293882ms till timeout)
2022-03-30 16:42:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1292872ms till timeout)
2022-03-30 16:42:34 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1292879ms till timeout)
2022-03-30 16:42:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1291868ms till timeout)
2022-03-30 16:42:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1291876ms till timeout)
2022-03-30 16:42:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1290865ms till timeout)
2022-03-30 16:42:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1290873ms till timeout)
2022-03-30 16:42:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1289861ms till timeout)
2022-03-30 16:42:37 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1289870ms till timeout)
2022-03-30 16:42:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1288858ms till timeout)
2022-03-30 16:42:38 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1288866ms till timeout)
2022-03-30 16:42:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1287854ms till timeout)
2022-03-30 16:42:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1287863ms till timeout)
2022-03-30 16:42:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1286850ms till timeout)
2022-03-30 16:42:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1286860ms till timeout)
2022-03-30 16:42:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1285847ms till timeout)
2022-03-30 16:42:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1285857ms till timeout)
2022-03-30 16:42:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1284843ms till timeout)
2022-03-30 16:42:42 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1284853ms till timeout)
2022-03-30 16:42:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1283840ms till timeout)
2022-03-30 16:42:43 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1283850ms till timeout)
2022-03-30 16:42:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1282836ms till timeout)
2022-03-30 16:42:44 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1282847ms till timeout)
2022-03-30 16:42:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1281833ms till timeout)
2022-03-30 16:42:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1281844ms till timeout)
2022-03-30 16:42:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1280829ms till timeout)
2022-03-30 16:42:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1280841ms till timeout)
2022-03-30 16:42:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1279824ms till timeout)
2022-03-30 16:42:47 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1279835ms till timeout)
2022-03-30 16:42:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1278820ms till timeout)
2022-03-30 16:42:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1278831ms till timeout)
2022-03-30 16:42:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1277815ms till timeout)
2022-03-30 16:42:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1277826ms till timeout)
2022-03-30 16:42:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1276810ms till timeout)
2022-03-30 16:42:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1276822ms till timeout)
2022-03-30 16:42:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1275806ms till timeout)
2022-03-30 16:42:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1275817ms till timeout)
2022-03-30 16:42:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1274802ms till timeout)
2022-03-30 16:42:52 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1274811ms till timeout)
2022-03-30 16:42:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1273798ms till timeout)
2022-03-30 16:42:53 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1273804ms till timeout)
2022-03-30 16:42:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1272790ms till timeout)
2022-03-30 16:42:54 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1272801ms till timeout)
2022-03-30 16:42:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1271786ms till timeout)
2022-03-30 16:42:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1271797ms till timeout)
2022-03-30 16:42:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1270782ms till timeout)
2022-03-30 16:42:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1270793ms till timeout)
2022-03-30 16:42:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1269779ms till timeout)
2022-03-30 16:42:57 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1269790ms till timeout)
2022-03-30 16:42:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1268775ms till timeout)
2022-03-30 16:42:58 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1268787ms till timeout)
2022-03-30 16:42:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1267772ms till timeout)
2022-03-30 16:42:59 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1267783ms till timeout)
2022-03-30 16:43:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1266768ms till timeout)
2022-03-30 16:43:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1266780ms till timeout)
2022-03-30 16:43:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1265765ms till timeout)
2022-03-30 16:43:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1265776ms till timeout)
2022-03-30 16:43:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1264761ms till timeout)
2022-03-30 16:43:02 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1264772ms till timeout)
2022-03-30 16:43:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1263758ms till timeout)
2022-03-30 16:43:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1263769ms till timeout)
2022-03-30 16:43:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1262754ms till timeout)
2022-03-30 16:43:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1262766ms till timeout)
2022-03-30 16:43:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1261751ms till timeout)
2022-03-30 16:43:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1261762ms till timeout)
2022-03-30 16:43:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1260748ms till timeout)
2022-03-30 16:43:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1260759ms till timeout)
2022-03-30 16:43:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1259744ms till timeout)
2022-03-30 16:43:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1259755ms till timeout)
2022-03-30 16:43:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1258741ms till timeout)
2022-03-30 16:43:08 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1258751ms till timeout)
2022-03-30 16:43:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1257737ms till timeout)
2022-03-30 16:43:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1257748ms till timeout)
2022-03-30 16:43:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1256734ms till timeout)
2022-03-30 16:43:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1256745ms till timeout)
2022-03-30 16:43:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1255730ms till timeout)
2022-03-30 16:43:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1255741ms till timeout)
2022-03-30 16:43:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1254726ms till timeout)
2022-03-30 16:43:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1254738ms till timeout)
2022-03-30 16:43:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1253722ms till timeout)
2022-03-30 16:43:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1253734ms till timeout)
2022-03-30 16:43:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1252716ms till timeout)
2022-03-30 16:43:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1252730ms till timeout)
2022-03-30 16:43:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1251711ms till timeout)
2022-03-30 16:43:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1251726ms till timeout)
2022-03-30 16:43:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1250707ms till timeout)
2022-03-30 16:43:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1250723ms till timeout)
2022-03-30 16:43:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1249701ms till timeout)
2022-03-30 16:43:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1249719ms till timeout)
2022-03-30 16:43:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1248697ms till timeout)
2022-03-30 16:43:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1248715ms till timeout)
2022-03-30 16:43:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1247694ms till timeout)
2022-03-30 16:43:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1247711ms till timeout)
2022-03-30 16:43:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1246690ms till timeout)
2022-03-30 16:43:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1246707ms till timeout)
2022-03-30 16:43:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1245686ms till timeout)
2022-03-30 16:43:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1245702ms till timeout)
2022-03-30 16:43:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1244681ms till timeout)
2022-03-30 16:43:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1244696ms till timeout)
2022-03-30 16:43:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1243668ms till timeout)
2022-03-30 16:43:23 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1243691ms till timeout)
2022-03-30 16:43:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1242664ms till timeout)
2022-03-30 16:43:24 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1242688ms till timeout)
2022-03-30 16:43:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1241661ms till timeout)
2022-03-30 16:43:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1241684ms till timeout)
2022-03-30 16:43:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1240658ms till timeout)
2022-03-30 16:43:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1240680ms till timeout)
2022-03-30 16:43:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1239654ms till timeout)
2022-03-30 16:43:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1239677ms till timeout)
2022-03-30 16:43:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1238651ms till timeout)
2022-03-30 16:43:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1238674ms till timeout)
2022-03-30 16:43:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1237648ms till timeout)
2022-03-30 16:43:29 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1237671ms till timeout)
2022-03-30 16:43:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1236644ms till timeout)
2022-03-30 16:43:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1236667ms till timeout)
2022-03-30 16:43:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1235641ms till timeout)
2022-03-30 16:43:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1235663ms till timeout)
2022-03-30 16:43:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1234637ms till timeout)
2022-03-30 16:43:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1234660ms till timeout)
2022-03-30 16:43:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1233634ms till timeout)
2022-03-30 16:43:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1233657ms till timeout)
2022-03-30 16:43:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1232631ms till timeout)
2022-03-30 16:43:34 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1232654ms till timeout)
2022-03-30 16:43:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1231627ms till timeout)
2022-03-30 16:43:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1231650ms till timeout)
2022-03-30 16:43:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1230624ms till timeout)
2022-03-30 16:43:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1230647ms till timeout)
2022-03-30 16:43:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1229620ms till timeout)
2022-03-30 16:43:37 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1229644ms till timeout)
2022-03-30 16:43:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1228617ms till timeout)
2022-03-30 16:43:38 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1228641ms till timeout)
2022-03-30 16:43:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1227614ms till timeout)
2022-03-30 16:43:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1227637ms till timeout)
2022-03-30 16:43:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1226610ms till timeout)
2022-03-30 16:43:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1226633ms till timeout)
2022-03-30 16:43:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1225607ms till timeout)
2022-03-30 16:43:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1225630ms till timeout)
2022-03-30 16:43:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1224604ms till timeout)
2022-03-30 16:43:42 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1224626ms till timeout)
2022-03-30 16:43:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1223600ms till timeout)
2022-03-30 16:43:43 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1223622ms till timeout)
2022-03-30 16:43:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1222597ms till timeout)
2022-03-30 16:43:44 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1222618ms till timeout)
2022-03-30 16:43:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1221592ms till timeout)
2022-03-30 16:43:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1221614ms till timeout)
2022-03-30 16:43:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1220589ms till timeout)
2022-03-30 16:43:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1220611ms till timeout)
2022-03-30 16:43:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1219585ms till timeout)
2022-03-30 16:43:47 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1219608ms till timeout)
2022-03-30 16:43:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1218580ms till timeout)
2022-03-30 16:43:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1218603ms till timeout)
2022-03-30 16:43:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1217576ms till timeout)
2022-03-30 16:43:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1217600ms till timeout)
2022-03-30 16:43:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1216572ms till timeout)
2022-03-30 16:43:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1216596ms till timeout)
2022-03-30 16:43:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1215568ms till timeout)
2022-03-30 16:43:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1215590ms till timeout)
2022-03-30 16:43:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1214563ms till timeout)
2022-03-30 16:43:52 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1214586ms till timeout)
2022-03-30 16:43:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1213559ms till timeout)
2022-03-30 16:43:53 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-9a739e30 will have desired state: Ready not ready, will try again in 1000 ms (1213582ms till timeout)
2022-03-30 16:43:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1212550ms till timeout)
2022-03-30 16:43:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-9a739e30 is in desired state: Ready
2022-03-30 16:43:54 [ForkJoinPool-3-worker-1] [32mINFO [m [CruiseControlApiST:48] ----> CRUISE CONTROL DEPLOYMENT STATE ENDPOINT <----
2022-03-30 16:43:54 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/state
2022-03-30 16:43:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/state
2022-03-30 16:43:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:43:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XGET --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/state
2022-03-30 16:43:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1211546ms till timeout)
2022-03-30 16:43:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XGET --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/state
2022-03-30 16:43:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:43:55 [ForkJoinPool-3-worker-1] [32mINFO [m [CruiseControlApiST:58] Verifying that Cruise Control REST API is available
2022-03-30 16:43:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-30 16:43:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-30 16:43:55 [ForkJoinPool-3-worker-1] [32mINFO [m [CruiseControlApiST:66] ----> KAFKA REBALANCE <----
2022-03-30 16:43:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XGET --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:43:56 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XGET --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:43:56 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:43:56 [ForkJoinPool-3-worker-1] [32mINFO [m [CruiseControlApiST:73] Waiting for CC will have for enough metrics to be recorded to make a proposal 
2022-03-30 16:43:56 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Wait for rebalance endpoint is ready
2022-03-30 16:43:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:43:56 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:43:56 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:43:56 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648658636513] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-30 16:43:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (599573ms till timeout)
2022-03-30 16:43:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1210542ms till timeout)
2022-03-30 16:43:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1209538ms till timeout)
2022-03-30 16:43:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1208535ms till timeout)
2022-03-30 16:43:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1207531ms till timeout)
2022-03-30 16:44:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1206528ms till timeout)
2022-03-30 16:44:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:44:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1205525ms till timeout)
2022-03-30 16:44:01 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:44:01 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:44:01 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648658641778] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-30 16:44:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (594309ms till timeout)
2022-03-30 16:44:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1204521ms till timeout)
2022-03-30 16:44:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1203518ms till timeout)
2022-03-30 16:44:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1202512ms till timeout)
2022-03-30 16:44:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1201508ms till timeout)
2022-03-30 16:44:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1200505ms till timeout)
2022-03-30 16:44:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:44:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:44:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:44:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648658647045] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-30 16:44:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (589042ms till timeout)
2022-03-30 16:44:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1199501ms till timeout)
2022-03-30 16:44:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1198497ms till timeout)
2022-03-30 16:44:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1197493ms till timeout)
2022-03-30 16:44:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1196488ms till timeout)
2022-03-30 16:44:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1195484ms till timeout)
2022-03-30 16:44:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:44:12 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:44:12 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:44:12 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648658652304] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-30 16:44:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (583779ms till timeout)
2022-03-30 16:44:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1194481ms till timeout)
2022-03-30 16:44:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1193477ms till timeout)
2022-03-30 16:44:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1192473ms till timeout)
2022-03-30 16:44:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1191469ms till timeout)
2022-03-30 16:44:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1190465ms till timeout)
2022-03-30 16:44:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:44:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:44:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:44:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648658657542] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-30 16:44:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (578540ms till timeout)
2022-03-30 16:44:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1189461ms till timeout)
2022-03-30 16:44:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1188457ms till timeout)
2022-03-30 16:44:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1187453ms till timeout)
2022-03-30 16:44:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1186450ms till timeout)
2022-03-30 16:44:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1185446ms till timeout)
2022-03-30 16:44:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:44:22 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:44:22 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:44:22 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648658662784] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-30 16:44:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (573300ms till timeout)
2022-03-30 16:44:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1184441ms till timeout)
2022-03-30 16:44:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1183438ms till timeout)
2022-03-30 16:44:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1182434ms till timeout)
2022-03-30 16:44:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1181430ms till timeout)
2022-03-30 16:44:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1180427ms till timeout)
2022-03-30 16:44:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:44:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1179422ms till timeout)
2022-03-30 16:44:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:44:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:44:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648658668033] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-30 16:44:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (568038ms till timeout)
2022-03-30 16:44:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1178419ms till timeout)
2022-03-30 16:44:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1177415ms till timeout)
2022-03-30 16:44:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1176411ms till timeout)
2022-03-30 16:44:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1175407ms till timeout)
2022-03-30 16:44:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1174403ms till timeout)
2022-03-30 16:44:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:44:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:44:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:44:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648658673293] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-30 16:44:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (562795ms till timeout)
2022-03-30 16:44:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1173400ms till timeout)
2022-03-30 16:44:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1172396ms till timeout)
2022-03-30 16:44:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1171392ms till timeout)
2022-03-30 16:44:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1170389ms till timeout)
2022-03-30 16:44:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1169385ms till timeout)
2022-03-30 16:44:38 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:44:38 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:44:38 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:44:38 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648658678534] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-30 16:44:38 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (557549ms till timeout)
2022-03-30 16:44:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1168381ms till timeout)
2022-03-30 16:44:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1167377ms till timeout)
2022-03-30 16:44:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1166374ms till timeout)
2022-03-30 16:44:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1165370ms till timeout)
2022-03-30 16:44:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1164366ms till timeout)
2022-03-30 16:44:43 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:44:43 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:44:43 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:44:43 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648658683786] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-30 16:44:43 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (552302ms till timeout)
2022-03-30 16:44:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1163363ms till timeout)
2022-03-30 16:44:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1162359ms till timeout)
2022-03-30 16:44:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1161356ms till timeout)
2022-03-30 16:44:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1160353ms till timeout)
2022-03-30 16:44:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: cruise-control-api-cluster-name is in desired state: Ready
2022-03-30 16:44:47 [ForkJoinPool-3-worker-3] [32mINFO [m [CruiseControlApiST:153] ----> CRUISE CONTROL DEPLOYMENT STATE ENDPOINT <----
2022-03-30 16:44:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec cruise-control-api-cluster-name-cruise-control-5fc9f6b9b4-zklvl -c cruise-control -- /bin/bash -c curl -XGET -k  HTTP://localhost:9090/kafkacruisecontrol/state
2022-03-30 16:44:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec cruise-control-api-cluster-name-cruise-control-5fc9f6b9b4-zklvl -c cruise-control -- /bin/bash -c curl -XGET -k  HTTP://localhost:9090/kafkacruisecontrol/state
2022-03-30 16:44:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:44:48 [ForkJoinPool-3-worker-3] [32mINFO [m [CruiseControlApiST:157] Verifying that Cruise Control REST API is available using HTTP request without credentials
2022-03-30 16:44:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 16:44:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:675] [cruisecontrol.CruiseControlApiST - After Each] - Clean up after test
2022-03-30 16:44:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 16:44:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testCruiseControlBasicAPIRequestsWithSecurityDisabled
2022-03-30 16:44:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka cruise-control-api-cluster-name in namespace namespace-0
2022-03-30 16:44:48 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-0, for cruise control Kafka cluster cruise-control-api-cluster-name
2022-03-30 16:44:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:cruise-control-api-cluster-name
2022-03-30 16:44:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:cruise-control-api-cluster-name not ready, will try again in 10000 ms (839996ms till timeout)
2022-03-30 16:44:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:44:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:44:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:44:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648658689025] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 16:44:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (547057ms till timeout)
2022-03-30 16:44:54 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:44:54 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:44:54 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:44:54 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648658694273] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 16:44:54 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (541815ms till timeout)
2022-03-30 16:44:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 16:44:58 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-0 for test case:testCruiseControlBasicAPIRequestsWithSecurityDisabled
2022-03-30 16:44:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-0 removal
2022-03-30 16:44:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:44:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:44:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:44:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (479927ms till timeout)
2022-03-30 16:44:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:44:59 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:44:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:44:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:44:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (478852ms till timeout)
2022-03-30 16:44:59 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:44:59 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:44:59 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648658699509] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 16:44:59 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (536574ms till timeout)
2022-03-30 16:45:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (477768ms till timeout)
2022-03-30 16:45:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (476691ms till timeout)
2022-03-30 16:45:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (475609ms till timeout)
2022-03-30 16:45:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (474532ms till timeout)
2022-03-30 16:45:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:45:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (473446ms till timeout)
2022-03-30 16:45:04 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:45:04 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:04 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648658704774] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 16:45:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (531312ms till timeout)
2022-03-30 16:45:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (472375ms till timeout)
2022-03-30 16:45:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (471299ms till timeout)
2022-03-30 16:45:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (470222ms till timeout)
2022-03-30 16:45:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (469145ms till timeout)
2022-03-30 16:45:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:45:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:45:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648658710002] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 16:45:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (526082ms till timeout)
2022-03-30 16:45:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (468066ms till timeout)
2022-03-30 16:45:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (466987ms till timeout)
2022-03-30 16:45:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (465912ms till timeout)
2022-03-30 16:45:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (464835ms till timeout)
2022-03-30 16:45:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (463757ms till timeout)
2022-03-30 16:45:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:45:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:45:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648658715262] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 16:45:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (520826ms till timeout)
2022-03-30 16:45:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (462682ms till timeout)
2022-03-30 16:45:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (461612ms till timeout)
2022-03-30 16:45:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (460540ms till timeout)
2022-03-30 16:45:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (459471ms till timeout)
2022-03-30 16:45:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (458394ms till timeout)
2022-03-30 16:45:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:45:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:45:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648658720495] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 16:45:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (515589ms till timeout)
2022-03-30 16:45:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (457324ms till timeout)
2022-03-30 16:45:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (456253ms till timeout)
2022-03-30 16:45:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (455168ms till timeout)
2022-03-30 16:45:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (454091ms till timeout)
2022-03-30 16:45:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (453011ms till timeout)
2022-03-30 16:45:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:45:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:45:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648658725729] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 16:45:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (510357ms till timeout)
2022-03-30 16:45:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (451928ms till timeout)
2022-03-30 16:45:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (450851ms till timeout)
2022-03-30 16:45:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (449769ms till timeout)
2022-03-30 16:45:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (448690ms till timeout)
2022-03-30 16:45:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (447612ms till timeout)
2022-03-30 16:45:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:45:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:45:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648658730969] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 16:45:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (505115ms till timeout)
2022-03-30 16:45:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (446531ms till timeout)
2022-03-30 16:45:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (445457ms till timeout)
2022-03-30 16:45:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (444378ms till timeout)
2022-03-30 16:45:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (443300ms till timeout)
2022-03-30 16:45:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (442226ms till timeout)
2022-03-30 16:45:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:45:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:45:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648658736193] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 16:45:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (499893ms till timeout)
2022-03-30 16:45:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (441156ms till timeout)
2022-03-30 16:45:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (440078ms till timeout)
2022-03-30 16:45:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (439005ms till timeout)
2022-03-30 16:45:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (437926ms till timeout)
2022-03-30 16:45:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:45:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 16:45:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 16:45:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 16:45:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-0" not found
2022-03-30 16:45:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 16:45:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-1]}
2022-03-30 16:45:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:267] testCruiseControlBasicAPIRequestsWithSecurityDisabled - Notifies waiting test cases:[testUpdateUser, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testCruiseControlBasicAPIRequests] to and randomly select one to start execution
2022-03-30 16:45:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:93] [cruisecontrol.CruiseControlApiST] - Removing parallel test: testCruiseControlBasicAPIRequestsWithSecurityDisabled
2022-03-30 16:45:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:97] [cruisecontrol.CruiseControlApiST] - Parallel test count: 1
2022-03-30 16:45:41 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlApiST.testCruiseControlBasicAPIRequestsWithSecurityDisabled-FINISHED
2022-03-30 16:45:41 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 16:45:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:45:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648658741428] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 16:45:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (494654ms till timeout)
2022-03-30 16:45:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:45:46 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:45:46 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:46 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648658746669] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 16:45:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (489413ms till timeout)
2022-03-30 16:45:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:45:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:45:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648658751903] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 16:45:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (484181ms till timeout)
2022-03-30 16:45:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:45:57 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:45:57 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:45:57 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648658757164] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 16:45:57 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (478917ms till timeout)
2022-03-30 16:46:02 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:46:02 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:46:02 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:46:02 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648658762390] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 16:46:02 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (473686ms till timeout)
2022-03-30 16:46:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:46:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:46:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:46:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648658767631] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 16:46:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (468457ms till timeout)
2022-03-30 16:46:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:46:12 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:46:12 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:46:12 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648658772856] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 16:46:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (463196ms till timeout)
2022-03-30 16:46:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:46:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:46:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:46:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648658778147] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 16:46:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (457941ms till timeout)
2022-03-30 16:46:23 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:46:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:46:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:46:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648658783382] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 16:46:23 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (452704ms till timeout)
2022-03-30 16:46:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:46:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:46:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:46:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648658788627] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 16:46:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (447459ms till timeout)
2022-03-30 16:46:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:46:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:46:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:46:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648658793857] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 16:46:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (442231ms till timeout)
2022-03-30 16:46:38 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:46:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:46:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:46:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648658799075] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 16:46:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (437012ms till timeout)
2022-03-30 16:46:44 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:46:44 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:46:44 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:46:44 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648658804313] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 16:46:44 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (431772ms till timeout)
2022-03-30 16:46:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:46:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:46:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:46:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [CruiseControlUtils:175] API response 

Optimization has 8 inter-broker replica(0 MB) moves, 0 intra-broker replica(0 MB) moves and 4 leadership moves with a cluster model of 1 recent windows and 100.000% of the partitions covered.
Excluded Topics: [].
Excluded Brokers For Leadership: [].
Excluded Brokers For Replica Move: [].
Counts: 3 brokers 287 replicas 6 topics.
On-demand Balancedness Score Before (87.919) After(87.919).
Provision Status: RIGHT_SIZED.

[     1 ms] Stats for RackAwareGoal(NO-ACTION):
AVG:{cpu:       1.265 networkInbound:       0.575 networkOutbound:       0.557 disk:       0.010 potentialNwOut:       1.608 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       3.118 networkInbound:       0.613 networkOutbound:       1.280 disk:       0.011 potentialNwOut:       1.645 replicas:96 leaderReplicas:41 topicReplicas:50}
MIN:{cpu:       0.335 networkInbound:       0.546 networkOutbound:       0.000 disk:       0.010 potentialNwOut:       1.579 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       1.310 networkInbound:       0.028 networkOutbound:       0.535 disk:       0.001 potentialNwOut:       0.028 replicas:0.4714045207910317 leaderReplicas:1.4142135623730951 topicReplicas:0.15713484026367722

[     0 ms] Stats for MinTopicLeadersPerBrokerGoal(NO-ACTION):
AVG:{cpu:       1.265 networkInbound:       0.575 networkOutbound:       0.557 disk:       0.010 potentialNwOut:       1.608 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       3.118 networkInbound:       0.613 networkOutbound:       1.280 disk:       0.011 potentialNwOut:       1.645 replicas:96 leaderReplicas:41 topicReplicas:50}
MIN:{cpu:       0.335 networkInbound:       0.546 networkOutbound:       0.000 disk:       0.010 potentialNwOut:       1.579 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       1.310 networkInbound:       0.028 networkOutbound:       0.535 disk:       0.001 potentialNwOut:       0.028 replicas:0.4714045207910317 leaderReplicas:1.4142135623730951 topicReplicas:0.15713484026367722

[     0 ms] Stats for ReplicaCapacityGoal(NO-ACTION):
AVG:{cpu:       1.265 networkInbound:       0.575 networkOutbound:       0.557 disk:       0.010 potentialNwOut:       1.608 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       3.118 networkInbound:       0.613 networkOutbound:       1.280 disk:       0.011 potentialNwOut:       1.645 replicas:96 leaderReplicas:41 topicReplicas:50}
MIN:{cpu:       0.335 networkInbound:       0.546 networkOutbound:       0.000 disk:       0.010 potentialNwOut:       1.579 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       1.310 networkInbound:       0.028 networkOutbound:       0.535 disk:       0.001 potentialNwOut:       0.028 replicas:0.4714045207910317 leaderReplicas:1.4142135623730951 topicReplicas:0.15713484026367722

[     1 ms] Stats for DiskCapacityGoal(NO-ACTION):
AVG:{cpu:       1.265 networkInbound:       0.575 networkOutbound:       0.557 disk:       0.010 potentialNwOut:       1.608 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       3.118 networkInbound:       0.613 networkOutbound:       1.280 disk:       0.011 potentialNwOut:       1.645 replicas:96 leaderReplicas:41 topicReplicas:50}
MIN:{cpu:       0.335 networkInbound:       0.546 networkOutbound:       0.000 disk:       0.010 potentialNwOut:       1.579 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       1.310 networkInbound:       0.028 networkOutbound:       0.535 disk:       0.001 potentialNwOut:       0.028 replicas:0.4714045207910317 leaderReplicas:1.4142135623730951 topicReplicas:0.15713484026367722

[     1 ms] Stats for NetworkInboundCapacityGoal(NO-ACTION):
AVG:{cpu:       1.265 networkInbound:       0.575 networkOutbound:       0.557 disk:       0.010 potentialNwOut:       1.608 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       3.118 networkInbound:       0.613 networkOutbound:       1.280 disk:       0.011 potentialNwOut:       1.645 replicas:96 leaderReplicas:41 topicReplicas:50}
MIN:{cpu:       0.335 networkInbound:       0.546 networkOutbound:       0.000 disk:       0.010 potentialNwOut:       1.579 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       1.310 networkInbound:       0.028 networkOutbound:       0.535 disk:       0.001 potentialNwOut:       0.028 replicas:0.4714045207910317 leaderReplicas:1.4142135623730951 topicReplicas:0.15713484026367722

[     0 ms] Stats for NetworkOutboundCapacityGoal(NO-ACTION):
AVG:{cpu:       1.265 networkInbound:       0.575 networkOutbound:       0.557 disk:       0.010 potentialNwOut:       1.608 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       3.118 networkInbound:       0.613 networkOutbound:       1.280 disk:       0.011 potentialNwOut:       1.645 replicas:96 leaderReplicas:41 topicReplicas:50}
MIN:{cpu:       0.335 networkInbound:       0.546 networkOutbound:       0.000 disk:       0.010 potentialNwOut:       1.579 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       1.310 networkInbound:       0.028 networkOutbound:       0.535 disk:       0.001 potentialNwOut:       0.028 replicas:0.4714045207910317 leaderReplicas:1.4142135623730951 topicReplicas:0.15713484026367722

[     0 ms] Stats for CpuCapacityGoal(NO-ACTION):
AVG:{cpu:       1.265 networkInbound:       0.575 networkOutbound:       0.557 disk:       0.010 potentialNwOut:       1.608 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       3.118 networkInbound:       0.613 networkOutbound:       1.280 disk:       0.011 potentialNwOut:       1.645 replicas:96 leaderReplicas:41 topicReplicas:50}
MIN:{cpu:       0.335 networkInbound:       0.546 networkOutbound:       0.000 disk:       0.010 potentialNwOut:       1.579 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       1.310 networkInbound:       0.028 networkOutbound:       0.535 disk:       0.001 potentialNwOut:       0.028 replicas:0.4714045207910317 leaderReplicas:1.4142135623730951 topicReplicas:0.15713484026367722

[     1 ms] Stats for ReplicaDistributionGoal(NO-ACTION):
AVG:{cpu:       1.265 networkInbound:       0.575 networkOutbound:       0.557 disk:       0.010 potentialNwOut:       1.608 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       3.118 networkInbound:       0.613 networkOutbound:       1.280 disk:       0.011 potentialNwOut:       1.645 replicas:96 leaderReplicas:41 topicReplicas:50}
MIN:{cpu:       0.335 networkInbound:       0.546 networkOutbound:       0.000 disk:       0.010 potentialNwOut:       1.579 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       1.310 networkInbound:       0.028 networkOutbound:       0.535 disk:       0.001 potentialNwOut:       0.028 replicas:0.4714045207910317 leaderReplicas:1.4142135623730951 topicReplicas:0.15713484026367722

[     1 ms] Stats for PotentialNwOutGoal(NO-ACTION):
AVG:{cpu:       1.265 networkInbound:       0.575 networkOutbound:       0.557 disk:       0.010 potentialNwOut:       1.608 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       3.118 networkInbound:       0.613 networkOutbound:       1.280 disk:       0.011 potentialNwOut:       1.645 replicas:96 leaderReplicas:41 topicReplicas:50}
MIN:{cpu:       0.335 networkInbound:       0.546 networkOutbound:       0.000 disk:       0.010 potentialNwOut:       1.579 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       1.310 networkInbound:       0.028 networkOutbound:       0.535 disk:       0.001 potentialNwOut:       0.028 replicas:0.4714045207910317 leaderReplicas:1.4142135623730951 topicReplicas:0.15713484026367722

[     1 ms] Stats for DiskUsageDistributionGoal(NO-ACTION):
AVG:{cpu:       1.265 networkInbound:       0.575 networkOutbound:       0.557 disk:       0.010 potentialNwOut:       1.608 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       3.118 networkInbound:       0.613 networkOutbound:       1.280 disk:       0.011 potentialNwOut:       1.645 replicas:96 leaderReplicas:41 topicReplicas:50}
MIN:{cpu:       0.335 networkInbound:       0.546 networkOutbound:       0.000 disk:       0.010 potentialNwOut:       1.579 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       1.310 networkInbound:       0.028 networkOutbound:       0.535 disk:       0.001 potentialNwOut:       0.028 replicas:0.4714045207910317 leaderReplicas:1.4142135623730951 topicReplicas:0.15713484026367722

[     0 ms] Stats for NetworkInboundUsageDistributionGoal(NO-ACTION):
AVG:{cpu:       1.265 networkInbound:       0.575 networkOutbound:       0.557 disk:       0.010 potentialNwOut:       1.608 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       3.118 networkInbound:       0.613 networkOutbound:       1.280 disk:       0.011 potentialNwOut:       1.645 replicas:96 leaderReplicas:41 topicReplicas:50}
MIN:{cpu:       0.335 networkInbound:       0.546 networkOutbound:       0.000 disk:       0.010 potentialNwOut:       1.579 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       1.310 networkInbound:       0.028 networkOutbound:       0.535 disk:       0.001 potentialNwOut:       0.028 replicas:0.4714045207910317 leaderReplicas:1.4142135623730951 topicReplicas:0.15713484026367722

[     8 ms] Stats for NetworkOutboundUsageDistributionGoal(VIOLATED):
AVG:{cpu:       1.265 networkInbound:       0.575 networkOutbound:       0.557 disk:       0.010 potentialNwOut:       1.608 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       2.359 networkInbound:       0.624 networkOutbound:       0.805 disk:       0.011 potentialNwOut:       1.656 replicas:98 leaderReplicas:46 topicReplicas:50}
MIN:{cpu:       0.645 networkInbound:       0.546 networkOutbound:       0.432 disk:       0.010 potentialNwOut:       1.579 replicas:94 leaderReplicas:29 topicReplicas:1}
STD:{cpu:       0.776 networkInbound:       0.034 networkOutbound:       0.175 disk:       0.001 potentialNwOut:       0.034 replicas:1.699673171197595 leaderReplicas:7.2571803523590805 topicReplicas:0.2864372749526131

[     7 ms] Stats for CpuUsageDistributionGoal(VIOLATED):
AVG:{cpu:       1.265 networkInbound:       0.575 networkOutbound:       0.557 disk:       0.010 potentialNwOut:       1.608 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       2.313 networkInbound:       0.624 networkOutbound:       0.805 disk:       0.011 potentialNwOut:       1.656 replicas:101 leaderReplicas:46 topicReplicas:50}
MIN:{cpu:       0.645 networkInbound:       0.524 networkOutbound:       0.432 disk:       0.010 potentialNwOut:       1.556 replicas:88 leaderReplicas:29 topicReplicas:1}
STD:{cpu:       0.745 networkInbound:       0.041 networkOutbound:       0.175 disk:       0.001 potentialNwOut:       0.041 replicas:5.557777333511022 leaderReplicas:7.2571803523590805 topicReplicas:0.9846511107040659

[     1 ms] Stats for TopicReplicaDistributionGoal(NO-ACTION):
AVG:{cpu:       1.265 networkInbound:       0.575 networkOutbound:       0.557 disk:       0.010 potentialNwOut:       1.608 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       2.313 networkInbound:       0.624 networkOutbound:       0.805 disk:       0.011 potentialNwOut:       1.656 replicas:101 leaderReplicas:46 topicReplicas:50}
MIN:{cpu:       0.645 networkInbound:       0.524 networkOutbound:       0.432 disk:       0.010 potentialNwOut:       1.556 replicas:88 leaderReplicas:29 topicReplicas:1}
STD:{cpu:       0.745 networkInbound:       0.041 networkOutbound:       0.175 disk:       0.001 potentialNwOut:       0.041 replicas:5.557777333511022 leaderReplicas:7.2571803523590805 topicReplicas:0.9846511107040659

[     3 ms] Stats for LeaderReplicaDistributionGoal(VIOLATED):
AVG:{cpu:       1.265 networkInbound:       0.575 networkOutbound:       0.557 disk:       0.010 potentialNwOut:       1.608 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       2.313 networkInbound:       0.624 networkOutbound:       0.805 disk:       0.011 potentialNwOut:       1.656 replicas:101 leaderReplicas:46 topicReplicas:50}
MIN:{cpu:       0.645 networkInbound:       0.524 networkOutbound:       0.432 disk:       0.010 potentialNwOut:       1.556 replicas:88 leaderReplicas:29 topicReplicas:1}
STD:{cpu:       0.745 networkInbound:       0.041 networkOutbound:       0.175 disk:       0.001 potentialNwOut:       0.041 replicas:5.557777333511022 leaderReplicas:7.2571803523590805 topicReplicas:0.9846511107040659

[     2 ms] Stats for LeaderBytesInDistributionGoal(VIOLATED):
AVG:{cpu:       1.265 networkInbound:       0.575 networkOutbound:       0.557 disk:       0.010 potentialNwOut:       1.608 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       2.313 networkInbound:       0.624 networkOutbound:       0.805 disk:       0.011 potentialNwOut:       1.656 replicas:101 leaderReplicas:46 topicReplicas:50}
MIN:{cpu:       0.645 networkInbound:       0.524 networkOutbound:       0.432 disk:       0.010 potentialNwOut:       1.556 replicas:88 leaderReplicas:29 topicReplicas:1}
STD:{cpu:       0.745 networkInbound:       0.041 networkOutbound:       0.175 disk:       0.001 potentialNwOut:       0.041 replicas:5.557777333511022 leaderReplicas:7.2571803523590805 topicReplicas:0.9846511107040659

[     0 ms] Stats for PreferredLeaderElectionGoal(VIOLATED):
AVG:{cpu:       1.265 networkInbound:       0.575 networkOutbound:       0.557 disk:       0.010 potentialNwOut:       1.608 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       2.313 networkInbound:       0.624 networkOutbound:       0.805 disk:       0.011 potentialNwOut:       1.656 replicas:101 leaderReplicas:46 topicReplicas:50}
MIN:{cpu:       0.645 networkInbound:       0.524 networkOutbound:       0.432 disk:       0.010 potentialNwOut:       1.556 replicas:88 leaderReplicas:29 topicReplicas:1}
STD:{cpu:       0.745 networkInbound:       0.041 networkOutbound:       0.175 disk:       0.001 potentialNwOut:       0.041 replicas:5.557777333511022 leaderReplicas:7.2571803523590805 topicReplicas:0.9846511107040659

Cluster load after rebalance:


                                                                         HOST         BROKER                                                                         RACK         DISK_CAP(MB)            DISK(MB)/_(%)_            CORE_NUM         CPU(%)          NW_IN_CAP(KB/s)       LEADER_NW_IN(KB/s)     FOLLOWER_NW_IN(KB/s)         NW_OUT_CAP(KB/s)        NW_OUT(KB/s)       PNW_OUT(KB/s)    LEADERS/REPLICAS
my-cluster-9a739e30-kafka-0.my-cluster-9a739e30-kafka-brokers.namespace-1.svc,             0,my-cluster-9a739e30-kafka-0.my-cluster-9a739e30-kafka-brokers.namespace-1.svc,          100000.000,              0.010/00.00,                  1,         0.645,               10000.000,                   0.306,                   0.318,               10000.000,              0.435,              1.656,            46/98
my-cluster-9a739e30-kafka-1.my-cluster-9a739e30-kafka-brokers.namespace-1.svc,             1,my-cluster-9a739e30-kafka-1.my-cluster-9a739e30-kafka-brokers.namespace-1.svc,          100000.000,              0.011/00.00,                  1,         0.838,               10000.000,                   0.155,                   0.424,               10000.000,              0.432,              1.611,            42/101
my-cluster-9a739e30-kafka-2.my-cluster-9a739e30-kafka-brokers.namespace-1.svc,             2,my-cluster-9a739e30-kafka-2.my-cluster-9a739e30-kafka-brokers.namespace-1.svc,          100000.000,              0.010/00.00,                  1,         2.313,               10000.000,                   0.179,                   0.345,               10000.000,              0.805,              1.556,            29/88

2022-03-30 16:46:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:46:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 16:46:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:46:49 [ForkJoinPool-3-worker-1] [32mINFO [m [CruiseControlApiST:97] ----> EXECUTION OF STOP PROPOSAL <----
2022-03-30 16:46:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XGET --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/stop_proposal_execution
2022-03-30 16:46:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XGET --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/stop_proposal_execution
2022-03-30 16:46:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:46:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/stop_proposal_execution
2022-03-30 16:46:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/stop_proposal_execution
2022-03-30 16:46:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:46:50 [ForkJoinPool-3-worker-1] [32mINFO [m [CruiseControlApiST:108] ----> USER TASKS <----
2022-03-30 16:46:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/user_tasks
2022-03-30 16:46:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/user_tasks
2022-03-30 16:46:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:46:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XGET --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/user_tasks
2022-03-30 16:46:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XGET --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/user_tasks
2022-03-30 16:46:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:46:50 [ForkJoinPool-3-worker-1] [32mINFO [m [CruiseControlApiST:126] Verifying that Cruise Control REST API doesn't allow HTTP requests
2022-03-30 16:46:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XGET -k  HTTP://localhost:9090/kafkacruisecontrol/state
2022-03-30 16:46:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XGET -k  HTTP://localhost:9090/kafkacruisecontrol/state
2022-03-30 16:46:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:46:51 [ForkJoinPool-3-worker-1] [32mINFO [m [CruiseControlApiST:132] Verifying that Cruise Control REST API doesn't allow unauthenticated requests
2022-03-30 16:46:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XGET -k  HTTPS://localhost:9090/kafkacruisecontrol/state
2022-03-30 16:46:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-9a739e30-cruise-control-684d99f76b-8lgdl -c cruise-control -- /bin/bash -c curl -XGET -k  HTTPS://localhost:9090/kafkacruisecontrol/state
2022-03-30 16:46:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:46:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 16:46:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:675] [cruisecontrol.CruiseControlApiST - After Each] - Clean up after test
2022-03-30 16:46:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 16:46:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testCruiseControlBasicAPIRequests
2022-03-30 16:46:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-9a739e30 in namespace namespace-1
2022-03-30 16:46:51 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-1, for cruise control Kafka cluster my-cluster-9a739e30
2022-03-30 16:46:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-9a739e30
2022-03-30 16:46:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-9a739e30 not ready, will try again in 10000 ms (839990ms till timeout)
2022-03-30 16:47:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 16:47:01 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-1 for test case:testCruiseControlBasicAPIRequests
2022-03-30 16:47:01 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-1 removal
2022-03-30 16:47:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:01 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:01 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (479925ms till timeout)
2022-03-30 16:47:02 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:02 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:02 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:02 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (478857ms till timeout)
2022-03-30 16:47:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:03 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:03 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (477779ms till timeout)
2022-03-30 16:47:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:04 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:04 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (476706ms till timeout)
2022-03-30 16:47:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (475637ms till timeout)
2022-03-30 16:47:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:06 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:06 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (474563ms till timeout)
2022-03-30 16:47:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (473487ms till timeout)
2022-03-30 16:47:08 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:08 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:08 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:08 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (472407ms till timeout)
2022-03-30 16:47:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:09 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:09 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (471333ms till timeout)
2022-03-30 16:47:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (470259ms till timeout)
2022-03-30 16:47:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:12 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:12 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (469191ms till timeout)
2022-03-30 16:47:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (468120ms till timeout)
2022-03-30 16:47:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:14 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:14 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (467037ms till timeout)
2022-03-30 16:47:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (465964ms till timeout)
2022-03-30 16:47:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (464885ms till timeout)
2022-03-30 16:47:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (463817ms till timeout)
2022-03-30 16:47:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (462741ms till timeout)
2022-03-30 16:47:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (461667ms till timeout)
2022-03-30 16:47:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (460593ms till timeout)
2022-03-30 16:47:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:21 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:21 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (459514ms till timeout)
2022-03-30 16:47:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:22 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:22 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (458438ms till timeout)
2022-03-30 16:47:23 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:23 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (457364ms till timeout)
2022-03-30 16:47:24 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (456288ms till timeout)
2022-03-30 16:47:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:26 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:26 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (455210ms till timeout)
2022-03-30 16:47:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (454136ms till timeout)
2022-03-30 16:47:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (453059ms till timeout)
2022-03-30 16:47:29 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:29 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:29 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:29 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (451984ms till timeout)
2022-03-30 16:47:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (450916ms till timeout)
2022-03-30 16:47:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (449840ms till timeout)
2022-03-30 16:47:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (448753ms till timeout)
2022-03-30 16:47:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (447676ms till timeout)
2022-03-30 16:47:34 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:34 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:34 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:34 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (446600ms till timeout)
2022-03-30 16:47:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (445524ms till timeout)
2022-03-30 16:47:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (444451ms till timeout)
2022-03-30 16:47:37 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:37 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (443375ms till timeout)
2022-03-30 16:47:38 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (442301ms till timeout)
2022-03-30 16:47:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (441228ms till timeout)
2022-03-30 16:47:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (440150ms till timeout)
2022-03-30 16:47:42 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:42 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:42 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:42 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (439077ms till timeout)
2022-03-30 16:47:43 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:43 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:43 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:43 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (438006ms till timeout)
2022-03-30 16:47:44 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:44 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-1 get Namespace namespace-1 -o yaml
2022-03-30 16:47:44 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 16:47:44 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 16:47:44 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-1" not found
2022-03-30 16:47:44 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 16:47:44 [ForkJoinPool-3-worker-1] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[]}
2022-03-30 16:47:44 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:267] testCruiseControlBasicAPIRequests - Notifies waiting test cases:[testUpdateUser, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testCruiseControlBasicAPIRequests] to and randomly select one to start execution
2022-03-30 16:47:44 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:93] [cruisecontrol.CruiseControlApiST] - Removing parallel test: testCruiseControlBasicAPIRequests
2022-03-30 16:47:44 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:97] [cruisecontrol.CruiseControlApiST] - Parallel test count: 0
2022-03-30 16:47:44 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlApiST.testCruiseControlBasicAPIRequests-FINISHED
2022-03-30 16:47:44 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 16:47:44 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 16:47:44 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:690] [cruisecontrol.CruiseControlApiST - After All] - Clean up after test suite
2022-03-30 16:47:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 16:47:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context CruiseControlApiST is everything deleted.
2022-03-30 16:47:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 16:47:44 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace cruise-control-api-st removal
2022-03-30 16:47:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace cruise-control-api-st -o yaml
2022-03-30 16:47:44 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace cruise-control-api-st -o yaml
2022-03-30 16:47:44 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-api-st removal not ready, will try again in 1000 ms (479930ms till timeout)
2022-03-30 16:47:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace cruise-control-api-st -o yaml
2022-03-30 16:47:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace cruise-control-api-st -o yaml
2022-03-30 16:47:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-api-st removal not ready, will try again in 1000 ms (478861ms till timeout)
2022-03-30 16:47:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace cruise-control-api-st -o yaml
2022-03-30 16:47:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace cruise-control-api-st -o yaml
2022-03-30 16:47:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-api-st removal not ready, will try again in 1000 ms (477790ms till timeout)
2022-03-30 16:47:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace cruise-control-api-st -o yaml
2022-03-30 16:47:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace cruise-control-api-st -o yaml
2022-03-30 16:47:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-api-st removal not ready, will try again in 1000 ms (476710ms till timeout)
2022-03-30 16:47:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace cruise-control-api-st -o yaml
2022-03-30 16:47:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace cruise-control-api-st -o yaml
2022-03-30 16:47:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-api-st removal not ready, will try again in 1000 ms (475642ms till timeout)
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace cruise-control-api-st -o yaml
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-1 get Namespace cruise-control-api-st -o yaml
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "cruise-control-api-st" not found
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@cf7c9d53=[]}
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:254] CruiseControlApiST - Notifies waiting test suites:[UserST, HttpBridgeTlsST, CruiseControlApiST] to and randomly select one to start execution
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:85] [cruisecontrol.CruiseControlApiST] - Removing parallel suite: CruiseControlApiST
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:89] [cruisecontrol.CruiseControlApiST] - Parallel suites count: 0
[[1;34mINFO[m] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 342.767 s - in io.strimzi.systemtest.cruisecontrol.CruiseControlApiST
[[1;34mINFO[m] Running io.strimzi.systemtest.cruisecontrol.CruiseControlST
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:667] [cruisecontrol.CruiseControlST - Before All] - Setup test suite environment
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:69] [cruisecontrol.CruiseControlST] - Adding parallel suite: CruiseControlST
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:73] [cruisecontrol.CruiseControlST] - Parallel suites count: 1
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:184] CruiseControlST suite now can proceed its execution
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestSuiteNamespaceManager:129] Test suite `CruiseControlST` creates these additional namespaces:[cruise-control-st]
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: cruise-control-st
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace cruise-control-st
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace cruise-control-st -o json
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace cruise-control-st -o json
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T16:47:49Z",
        "name": "cruise-control-st",
        "resourceVersion": "51658",
        "selfLink": "/api/v1/namespaces/cruise-control-st",
        "uid": "2180546e-849e-4659-8763-19073594e840"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@cf7c9d53=[]}
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: cruise-control-st
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=cruise-control-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: cruise-control-st
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlWithRebalanceResourceAndRefreshAnnotation-STARTED
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:659] [cruisecontrol.CruiseControlST - Before Each] - Setup test case environment
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-128d9a40, testUpdateUser=my-cluster-75998b92, testReceiveSimpleMessageTls=my-cluster-90021a9f, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5, testCruiseControlBasicAPIRequests=my-cluster-6da231a3, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de}
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-323988616-1355569855, testUpdateUser=my-user-275427717-1183444028, testReceiveSimpleMessageTls=my-user-491397521-1196361913, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-695287754-1004738297, testCruiseControlBasicAPIRequests=my-user-918568860-36694764, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-83332781-1273282344}
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-1040221322-670398147, testUpdateUser=my-topic-275186280-765877519, testReceiveSimpleMessageTls=my-topic-941665209-436577257, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1609923788-1785060017, testCruiseControlBasicAPIRequests=my-topic-710218582-658397020, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1536476030-1482182833}
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients, testReceiveSimpleMessageTls=my-cluster-90021a9f-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-6da231a3-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de-kafka-clients}
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-8d8daaa5 in namespace cruise-control-st
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-8d8daaa5
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-8d8daaa5 will have desired state: Ready
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-8d8daaa5 will have desired state: Ready
2022-03-30 16:47:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1319997ms till timeout)
2022-03-30 16:47:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1318993ms till timeout)
2022-03-30 16:47:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1317989ms till timeout)
2022-03-30 16:47:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1316986ms till timeout)
2022-03-30 16:47:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1315981ms till timeout)
2022-03-30 16:47:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1314976ms till timeout)
2022-03-30 16:47:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1313971ms till timeout)
2022-03-30 16:47:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1312967ms till timeout)
2022-03-30 16:47:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1311963ms till timeout)
2022-03-30 16:47:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1310960ms till timeout)
2022-03-30 16:48:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1309956ms till timeout)
2022-03-30 16:48:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1308952ms till timeout)
2022-03-30 16:48:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1307948ms till timeout)
2022-03-30 16:48:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1306944ms till timeout)
2022-03-30 16:48:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1305940ms till timeout)
2022-03-30 16:48:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1304935ms till timeout)
2022-03-30 16:48:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1303932ms till timeout)
2022-03-30 16:48:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1302928ms till timeout)
2022-03-30 16:48:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1301924ms till timeout)
2022-03-30 16:48:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1300920ms till timeout)
2022-03-30 16:48:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1299917ms till timeout)
2022-03-30 16:48:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1298913ms till timeout)
2022-03-30 16:48:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1297909ms till timeout)
2022-03-30 16:48:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1296905ms till timeout)
2022-03-30 16:48:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1295902ms till timeout)
2022-03-30 16:48:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1294898ms till timeout)
2022-03-30 16:48:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1293894ms till timeout)
2022-03-30 16:48:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1292891ms till timeout)
2022-03-30 16:48:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1291872ms till timeout)
2022-03-30 16:48:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1290868ms till timeout)
2022-03-30 16:48:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1289864ms till timeout)
2022-03-30 16:48:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1288860ms till timeout)
2022-03-30 16:48:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1287854ms till timeout)
2022-03-30 16:48:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1286850ms till timeout)
2022-03-30 16:48:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1285846ms till timeout)
2022-03-30 16:48:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1284839ms till timeout)
2022-03-30 16:48:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1283835ms till timeout)
2022-03-30 16:48:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1282832ms till timeout)
2022-03-30 16:48:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1281828ms till timeout)
2022-03-30 16:48:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1280825ms till timeout)
2022-03-30 16:48:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1279821ms till timeout)
2022-03-30 16:48:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1278817ms till timeout)
2022-03-30 16:48:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1277814ms till timeout)
2022-03-30 16:48:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1276811ms till timeout)
2022-03-30 16:48:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1275807ms till timeout)
2022-03-30 16:48:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1274804ms till timeout)
2022-03-30 16:48:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1273800ms till timeout)
2022-03-30 16:48:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1272796ms till timeout)
2022-03-30 16:48:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1271793ms till timeout)
2022-03-30 16:48:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1270789ms till timeout)
2022-03-30 16:48:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1269786ms till timeout)
2022-03-30 16:48:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1268782ms till timeout)
2022-03-30 16:48:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1267778ms till timeout)
2022-03-30 16:48:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1266775ms till timeout)
2022-03-30 16:48:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1265771ms till timeout)
2022-03-30 16:48:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1264767ms till timeout)
2022-03-30 16:48:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1263763ms till timeout)
2022-03-30 16:48:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1262760ms till timeout)
2022-03-30 16:48:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1261756ms till timeout)
2022-03-30 16:48:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1260752ms till timeout)
2022-03-30 16:48:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1259748ms till timeout)
2022-03-30 16:48:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1258744ms till timeout)
2022-03-30 16:48:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1257741ms till timeout)
2022-03-30 16:48:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1256737ms till timeout)
2022-03-30 16:48:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1255733ms till timeout)
2022-03-30 16:48:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1254729ms till timeout)
2022-03-30 16:48:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1253725ms till timeout)
2022-03-30 16:48:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1252721ms till timeout)
2022-03-30 16:48:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1251717ms till timeout)
2022-03-30 16:48:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1250714ms till timeout)
2022-03-30 16:49:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1249710ms till timeout)
2022-03-30 16:49:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1248706ms till timeout)
2022-03-30 16:49:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1247702ms till timeout)
2022-03-30 16:49:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1246699ms till timeout)
2022-03-30 16:49:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1245695ms till timeout)
2022-03-30 16:49:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1244691ms till timeout)
2022-03-30 16:49:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1243687ms till timeout)
2022-03-30 16:49:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1242684ms till timeout)
2022-03-30 16:49:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1241680ms till timeout)
2022-03-30 16:49:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1240676ms till timeout)
2022-03-30 16:49:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1239672ms till timeout)
2022-03-30 16:49:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1238669ms till timeout)
2022-03-30 16:49:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1237665ms till timeout)
2022-03-30 16:49:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1236661ms till timeout)
2022-03-30 16:49:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1235658ms till timeout)
2022-03-30 16:49:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1234654ms till timeout)
2022-03-30 16:49:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1233651ms till timeout)
2022-03-30 16:49:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1232647ms till timeout)
2022-03-30 16:49:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1231640ms till timeout)
2022-03-30 16:49:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1230636ms till timeout)
2022-03-30 16:49:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1229633ms till timeout)
2022-03-30 16:49:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1228629ms till timeout)
2022-03-30 16:49:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1227625ms till timeout)
2022-03-30 16:49:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1226622ms till timeout)
2022-03-30 16:49:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1225618ms till timeout)
2022-03-30 16:49:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1224614ms till timeout)
2022-03-30 16:49:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1223611ms till timeout)
2022-03-30 16:49:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1222607ms till timeout)
2022-03-30 16:49:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1221604ms till timeout)
2022-03-30 16:49:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (1220600ms till timeout)
2022-03-30 16:49:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-8d8daaa5 is in desired state: Ready
2022-03-30 16:49:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaRebalance my-cluster-8d8daaa5 in namespace cruise-control-st
2022-03-30 16:49:30 [ForkJoinPool-3-worker-3] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkarebalances' with unstable version 'v1beta2'
2022-03-30 16:49:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaRebalance:my-cluster-8d8daaa5
2022-03-30 16:49:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-8d8daaa5 will have desired state: PendingProposal
2022-03-30 16:49:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaRebalance: my-cluster-8d8daaa5 will have desired state: PendingProposal
2022-03-30 16:49:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: PendingProposal not ready, will try again in 1000 ms (359998ms till timeout)
2022-03-30 16:49:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-8d8daaa5 is in desired state: PendingProposal
2022-03-30 16:49:31 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:75] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-8d8daaa5): ============================================================================
2022-03-30 16:49:31 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:76] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-8d8daaa5): PendingProposal
2022-03-30 16:49:31 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:77] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-8d8daaa5): ============================================================================
2022-03-30 16:49:31 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:81] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-8d8daaa5): Verifying that KafkaRebalance resource is in PendingProposal state
2022-03-30 16:49:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-8d8daaa5 will have desired state: PendingProposal
2022-03-30 16:49:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaRebalance: my-cluster-8d8daaa5 will have desired state: PendingProposal
2022-03-30 16:49:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-8d8daaa5 is in desired state: PendingProposal
2022-03-30 16:49:31 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:85] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-8d8daaa5): Verifying that KafkaRebalance resource is in ProposalReady state
2022-03-30 16:49:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady
2022-03-30 16:49:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady
2022-03-30 16:49:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-30 16:49:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-30 16:49:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (597991ms till timeout)
2022-03-30 16:49:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (596987ms till timeout)
2022-03-30 16:49:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (595983ms till timeout)
2022-03-30 16:49:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (594980ms till timeout)
2022-03-30 16:49:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (593977ms till timeout)
2022-03-30 16:49:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (592974ms till timeout)
2022-03-30 16:49:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (591970ms till timeout)
2022-03-30 16:49:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (590967ms till timeout)
2022-03-30 16:49:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (589963ms till timeout)
2022-03-30 16:49:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (588960ms till timeout)
2022-03-30 16:49:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (587957ms till timeout)
2022-03-30 16:49:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (586954ms till timeout)
2022-03-30 16:49:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (585951ms till timeout)
2022-03-30 16:49:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (584948ms till timeout)
2022-03-30 16:49:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (583945ms till timeout)
2022-03-30 16:49:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (582942ms till timeout)
2022-03-30 16:49:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (581938ms till timeout)
2022-03-30 16:49:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (580935ms till timeout)
2022-03-30 16:49:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (579931ms till timeout)
2022-03-30 16:49:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (578928ms till timeout)
2022-03-30 16:49:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (577925ms till timeout)
2022-03-30 16:49:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (576922ms till timeout)
2022-03-30 16:49:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (575918ms till timeout)
2022-03-30 16:49:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (574915ms till timeout)
2022-03-30 16:49:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (573912ms till timeout)
2022-03-30 16:49:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (572909ms till timeout)
2022-03-30 16:49:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (571905ms till timeout)
2022-03-30 16:50:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (570902ms till timeout)
2022-03-30 16:50:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (569899ms till timeout)
2022-03-30 16:50:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (568896ms till timeout)
2022-03-30 16:50:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (567893ms till timeout)
2022-03-30 16:50:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (566890ms till timeout)
2022-03-30 16:50:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (565887ms till timeout)
2022-03-30 16:50:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (564883ms till timeout)
2022-03-30 16:50:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (563880ms till timeout)
2022-03-30 16:50:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (562877ms till timeout)
2022-03-30 16:50:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (561873ms till timeout)
2022-03-30 16:50:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (560870ms till timeout)
2022-03-30 16:50:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (559867ms till timeout)
2022-03-30 16:50:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (558864ms till timeout)
2022-03-30 16:50:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (557860ms till timeout)
2022-03-30 16:50:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (556857ms till timeout)
2022-03-30 16:50:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (555854ms till timeout)
2022-03-30 16:50:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (554851ms till timeout)
2022-03-30 16:50:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (553847ms till timeout)
2022-03-30 16:50:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (552844ms till timeout)
2022-03-30 16:50:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (551841ms till timeout)
2022-03-30 16:50:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (550838ms till timeout)
2022-03-30 16:50:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (549835ms till timeout)
2022-03-30 16:50:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (548832ms till timeout)
2022-03-30 16:50:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (547828ms till timeout)
2022-03-30 16:50:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (546824ms till timeout)
2022-03-30 16:50:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (545821ms till timeout)
2022-03-30 16:50:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (544818ms till timeout)
2022-03-30 16:50:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (543815ms till timeout)
2022-03-30 16:50:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (542812ms till timeout)
2022-03-30 16:50:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (541808ms till timeout)
2022-03-30 16:50:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (540805ms till timeout)
2022-03-30 16:50:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (539802ms till timeout)
2022-03-30 16:50:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (538798ms till timeout)
2022-03-30 16:50:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (537795ms till timeout)
2022-03-30 16:50:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (536791ms till timeout)
2022-03-30 16:50:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (535788ms till timeout)
2022-03-30 16:50:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (534784ms till timeout)
2022-03-30 16:50:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (533781ms till timeout)
2022-03-30 16:50:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (532777ms till timeout)
2022-03-30 16:50:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (531774ms till timeout)
2022-03-30 16:50:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (530770ms till timeout)
2022-03-30 16:50:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (529766ms till timeout)
2022-03-30 16:50:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (528763ms till timeout)
2022-03-30 16:50:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (527760ms till timeout)
2022-03-30 16:50:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (526756ms till timeout)
2022-03-30 16:50:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (525752ms till timeout)
2022-03-30 16:50:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (524749ms till timeout)
2022-03-30 16:50:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (523745ms till timeout)
2022-03-30 16:50:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (522742ms till timeout)
2022-03-30 16:50:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (521738ms till timeout)
2022-03-30 16:50:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (520735ms till timeout)
2022-03-30 16:50:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (519731ms till timeout)
2022-03-30 16:50:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (518728ms till timeout)
2022-03-30 16:50:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (517724ms till timeout)
2022-03-30 16:50:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (516721ms till timeout)
2022-03-30 16:50:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (515718ms till timeout)
2022-03-30 16:50:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (514715ms till timeout)
2022-03-30 16:50:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (513711ms till timeout)
2022-03-30 16:50:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (512708ms till timeout)
2022-03-30 16:50:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (511704ms till timeout)
2022-03-30 16:51:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (510701ms till timeout)
2022-03-30 16:51:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (509697ms till timeout)
2022-03-30 16:51:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (508694ms till timeout)
2022-03-30 16:51:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (507691ms till timeout)
2022-03-30 16:51:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (506687ms till timeout)
2022-03-30 16:51:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (505684ms till timeout)
2022-03-30 16:51:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (504680ms till timeout)
2022-03-30 16:51:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (503677ms till timeout)
2022-03-30 16:51:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (502673ms till timeout)
2022-03-30 16:51:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (501670ms till timeout)
2022-03-30 16:51:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (500666ms till timeout)
2022-03-30 16:51:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (499663ms till timeout)
2022-03-30 16:51:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (498659ms till timeout)
2022-03-30 16:51:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (497656ms till timeout)
2022-03-30 16:51:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (496652ms till timeout)
2022-03-30 16:51:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (495649ms till timeout)
2022-03-30 16:51:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (494645ms till timeout)
2022-03-30 16:51:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (493642ms till timeout)
2022-03-30 16:51:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (492638ms till timeout)
2022-03-30 16:51:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (491635ms till timeout)
2022-03-30 16:51:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (490631ms till timeout)
2022-03-30 16:51:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-8d8daaa5 is in desired state: ProposalReady
2022-03-30 16:51:21 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:90] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-8d8daaa5): ============================================================================
2022-03-30 16:51:21 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:91] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-8d8daaa5): ProposalReady
2022-03-30 16:51:21 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:92] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-8d8daaa5): ============================================================================
2022-03-30 16:51:21 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:94] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-8d8daaa5): Triggering the rebalance with annotation strimzi.io/rebalance=approve of KafkaRebalance resource
2022-03-30 16:51:21 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:63] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-8d8daaa5): Annotating KafkaRebalance:my-cluster-8d8daaa5 with annotation approve
2022-03-30 16:51:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st annotate kafkarebalance my-cluster-8d8daaa5 strimzi.io/rebalance=approve
2022-03-30 16:51:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st annotate kafkarebalance my-cluster-8d8daaa5 strimzi.io/rebalance=approve
2022-03-30 16:51:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:51:21 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:98] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-8d8daaa5): Response from the annotation process kafkarebalance.kafka.strimzi.io/my-cluster-8d8daaa5 annotated
2022-03-30 16:51:21 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:100] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-8d8daaa5): Verifying that annotation triggers the Rebalancing state
2022-03-30 16:51:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Rebalancing
2022-03-30 16:51:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Rebalancing
2022-03-30 16:51:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Rebalancing not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 16:51:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-8d8daaa5 is in desired state: Rebalancing
2022-03-30 16:51:22 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:104] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-8d8daaa5): Verifying that KafkaRebalance is in the Ready state
2022-03-30 16:51:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready
2022-03-30 16:51:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready
2022-03-30 16:51:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 16:51:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-30 16:51:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (597990ms till timeout)
2022-03-30 16:51:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (596987ms till timeout)
2022-03-30 16:51:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (595983ms till timeout)
2022-03-30 16:51:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (594980ms till timeout)
2022-03-30 16:51:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (593977ms till timeout)
2022-03-30 16:51:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (592973ms till timeout)
2022-03-30 16:51:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (591969ms till timeout)
2022-03-30 16:51:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (590963ms till timeout)
2022-03-30 16:51:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (589959ms till timeout)
2022-03-30 16:51:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (588955ms till timeout)
2022-03-30 16:51:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (587951ms till timeout)
2022-03-30 16:51:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (586948ms till timeout)
2022-03-30 16:51:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (585944ms till timeout)
2022-03-30 16:51:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (584941ms till timeout)
2022-03-30 16:51:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (583937ms till timeout)
2022-03-30 16:51:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (582933ms till timeout)
2022-03-30 16:51:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (581930ms till timeout)
2022-03-30 16:51:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (580926ms till timeout)
2022-03-30 16:51:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (579923ms till timeout)
2022-03-30 16:51:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (578919ms till timeout)
2022-03-30 16:51:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (577915ms till timeout)
2022-03-30 16:51:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (576912ms till timeout)
2022-03-30 16:51:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (575908ms till timeout)
2022-03-30 16:51:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-8d8daaa5 is in desired state: Ready
2022-03-30 16:51:47 [ForkJoinPool-3-worker-3] [32mINFO [m [CruiseControlST:152] Annotating KafkaRebalance: my-cluster-8d8daaa5 with 'refresh' anno
2022-03-30 16:51:47 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:63] Reconciliation #2(test) KafkaRebalance(cruise-control-st/my-cluster-8d8daaa5): Annotating KafkaRebalance:my-cluster-8d8daaa5 with annotation refresh
2022-03-30 16:51:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st annotate kafkarebalance my-cluster-8d8daaa5 strimzi.io/rebalance=refresh
2022-03-30 16:51:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st annotate kafkarebalance my-cluster-8d8daaa5 strimzi.io/rebalance=refresh
2022-03-30 16:51:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:51:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady
2022-03-30 16:51:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady
2022-03-30 16:51:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: ProposalReady not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 16:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-8d8daaa5 is in desired state: ProposalReady
2022-03-30 16:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [CruiseControlST:156] Trying rebalancing process again
2022-03-30 16:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:75] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-8d8daaa5): ============================================================================
2022-03-30 16:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:76] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-8d8daaa5): ProposalReady
2022-03-30 16:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:77] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-8d8daaa5): ============================================================================
2022-03-30 16:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:90] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-8d8daaa5): ============================================================================
2022-03-30 16:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:91] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-8d8daaa5): ProposalReady
2022-03-30 16:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:92] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-8d8daaa5): ============================================================================
2022-03-30 16:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:94] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-8d8daaa5): Triggering the rebalance with annotation strimzi.io/rebalance=approve of KafkaRebalance resource
2022-03-30 16:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:63] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-8d8daaa5): Annotating KafkaRebalance:my-cluster-8d8daaa5 with annotation approve
2022-03-30 16:51:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st annotate kafkarebalance my-cluster-8d8daaa5 strimzi.io/rebalance=approve
2022-03-30 16:51:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st annotate kafkarebalance my-cluster-8d8daaa5 strimzi.io/rebalance=approve
2022-03-30 16:51:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:98] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-8d8daaa5): Response from the annotation process kafkarebalance.kafka.strimzi.io/my-cluster-8d8daaa5 annotated
2022-03-30 16:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:100] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-8d8daaa5): Verifying that annotation triggers the Rebalancing state
2022-03-30 16:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Rebalancing
2022-03-30 16:51:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Rebalancing
2022-03-30 16:51:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Rebalancing not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 16:51:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-8d8daaa5 is in desired state: Rebalancing
2022-03-30 16:51:50 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:104] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-8d8daaa5): Verifying that KafkaRebalance is in the Ready state
2022-03-30 16:51:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready
2022-03-30 16:51:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready
2022-03-30 16:51:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-30 16:51:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-30 16:51:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (597990ms till timeout)
2022-03-30 16:51:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (596986ms till timeout)
2022-03-30 16:51:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-8d8daaa5 will have desired state: Ready not ready, will try again in 1000 ms (595983ms till timeout)
2022-03-30 16:51:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-8d8daaa5 is in desired state: Ready
2022-03-30 16:51:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 16:51:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:675] [cruisecontrol.CruiseControlST - After Each] - Clean up after test
2022-03-30 16:51:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 16:51:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testCruiseControlWithRebalanceResourceAndRefreshAnnotation
2022-03-30 16:51:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaRebalance my-cluster-8d8daaa5 in namespace cruise-control-st
2022-03-30 16:51:55 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-8d8daaa5 in namespace cruise-control-st
2022-03-30 16:51:55 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace cruise-control-st, for cruise control Kafka cluster my-cluster-8d8daaa5
2022-03-30 16:51:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaRebalance:my-cluster-8d8daaa5
2022-03-30 16:51:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaRebalance:my-cluster-8d8daaa5 not ready, will try again in 10000 ms (179993ms till timeout)
2022-03-30 16:51:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-8d8daaa5
2022-03-30 16:51:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-8d8daaa5 not ready, will try again in 10000 ms (839996ms till timeout)
2022-03-30 16:52:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 16:52:05 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlWithRebalanceResourceAndRefreshAnnotation-FINISHED
2022-03-30 16:52:05 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 16:52:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 16:52:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:690] [cruisecontrol.CruiseControlST - After All] - Clean up after test suite
2022-03-30 16:52:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 16:52:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context CruiseControlST is everything deleted.
2022-03-30 16:52:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 16:52:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace cruise-control-st removal
2022-03-30 16:52:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (479923ms till timeout)
2022-03-30 16:52:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (478847ms till timeout)
2022-03-30 16:52:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (477771ms till timeout)
2022-03-30 16:52:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (476694ms till timeout)
2022-03-30 16:52:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (475619ms till timeout)
2022-03-30 16:52:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (474536ms till timeout)
2022-03-30 16:52:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (473453ms till timeout)
2022-03-30 16:52:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (472376ms till timeout)
2022-03-30 16:52:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (471291ms till timeout)
2022-03-30 16:52:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (470213ms till timeout)
2022-03-30 16:52:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (469134ms till timeout)
2022-03-30 16:52:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (468057ms till timeout)
2022-03-30 16:52:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (466980ms till timeout)
2022-03-30 16:52:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (465911ms till timeout)
2022-03-30 16:52:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (464832ms till timeout)
2022-03-30 16:52:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (463752ms till timeout)
2022-03-30 16:52:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (462675ms till timeout)
2022-03-30 16:52:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (461601ms till timeout)
2022-03-30 16:52:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (460520ms till timeout)
2022-03-30 16:52:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (459443ms till timeout)
2022-03-30 16:52:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (458370ms till timeout)
2022-03-30 16:52:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (457284ms till timeout)
2022-03-30 16:52:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (456209ms till timeout)
2022-03-30 16:52:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (455132ms till timeout)
2022-03-30 16:52:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (454061ms till timeout)
2022-03-30 16:52:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (452989ms till timeout)
2022-03-30 16:52:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (451907ms till timeout)
2022-03-30 16:52:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (450834ms till timeout)
2022-03-30 16:52:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (449751ms till timeout)
2022-03-30 16:52:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (448681ms till timeout)
2022-03-30 16:52:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (447607ms till timeout)
2022-03-30 16:52:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (446534ms till timeout)
2022-03-30 16:52:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (445466ms till timeout)
2022-03-30 16:52:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (444394ms till timeout)
2022-03-30 16:52:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (443310ms till timeout)
2022-03-30 16:52:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (442242ms till timeout)
2022-03-30 16:52:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:44 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:44 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (441173ms till timeout)
2022-03-30 16:52:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (440105ms till timeout)
2022-03-30 16:52:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (439030ms till timeout)
2022-03-30 16:52:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (437962ms till timeout)
2022-03-30 16:52:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace cruise-control-st get Namespace cruise-control-st -o yaml
2022-03-30 16:52:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 16:52:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 16:52:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "cruise-control-st" not found
2022-03-30 16:52:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 16:52:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@afd54539=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@cf7c9d53=[]}
2022-03-30 16:52:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:254] CruiseControlST - Notifies waiting test suites:[UserST, HttpBridgeTlsST, CruiseControlApiST, CruiseControlST] to and randomly select one to start execution
2022-03-30 16:52:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:85] [cruisecontrol.CruiseControlST] - Removing parallel suite: CruiseControlST
2022-03-30 16:52:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:89] [cruisecontrol.CruiseControlST] - Parallel suites count: 0
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 298.552 s - in io.strimzi.systemtest.cruisecontrol.CruiseControlST
[[1;34mINFO[m] Running io.strimzi.systemtest.kafka.listeners.ListenersST
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:667] [kafka.listeners.ListenersST - Before All] - Setup test suite environment
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:69] [kafka.listeners.ListenersST] - Adding parallel suite: ListenersST
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:73] [kafka.listeners.ListenersST] - Parallel suites count: 1
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:184] ListenersST suite now can proceed its execution
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestSuiteNamespaceManager:129] Test suite `ListenersST` creates these additional namespaces:[listeners-st]
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: listeners-st
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace listeners-st
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace listeners-st -o json
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace listeners-st -o json
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T16:52:49Z",
        "name": "listeners-st",
        "resourceVersion": "52401",
        "selfLink": "/api/v1/namespaces/listeners-st",
        "uid": "c701a2eb-7a5b-4060-bf82-36e8f4a20f69"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@afd54539=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@cf7c9d53=[]}
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: listeners-st
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=listeners-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: listeners-st
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 16:52:49 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesTlsScramSha-STARTED
2022-03-30 16:52:49 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesCustomListenerTlsScramSha-STARTED
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 16:52:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:659] [kafka.listeners.ListenersST - Before Each] - Setup test case environment
2022-03-30 16:52:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:659] [kafka.listeners.ListenersST - Before Each] - Setup test case environment
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:77] [kafka.listeners.ListenersST] - Adding parallel test: testSendMessagesTlsScramSha
2022-03-30 16:52:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:77] [kafka.listeners.ListenersST] - Adding parallel test: testSendMessagesCustomListenerTlsScramSha
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:81] [kafka.listeners.ListenersST] - Parallel test count: 1
2022-03-30 16:52:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:81] [kafka.listeners.ListenersST] - Parallel test count: 2
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:230] testSendMessagesTlsScramSha test now can proceed its execution
2022-03-30 16:52:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:230] testSendMessagesCustomListenerTlsScramSha test now can proceed its execution
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-128d9a40, testUpdateUser=my-cluster-75998b92, testSendMessagesTlsScramSha=my-cluster-a11b9173, testReceiveSimpleMessageTls=my-cluster-90021a9f, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5, testCruiseControlBasicAPIRequests=my-cluster-6da231a3, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de}
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-323988616-1355569855, testUpdateUser=my-user-275427717-1183444028, testSendMessagesTlsScramSha=my-user-1791509549-1350003609, testReceiveSimpleMessageTls=my-user-491397521-1196361913, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-695287754-1004738297, testCruiseControlBasicAPIRequests=my-user-918568860-36694764, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-83332781-1273282344}
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-1040221322-670398147, testUpdateUser=my-topic-275186280-765877519, testSendMessagesTlsScramSha=my-topic-1482193652-649135537, testReceiveSimpleMessageTls=my-topic-941665209-436577257, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1609923788-1785060017, testCruiseControlBasicAPIRequests=my-topic-710218582-658397020, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1536476030-1482182833}
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients, testSendMessagesTlsScramSha=my-cluster-a11b9173-kafka-clients, testReceiveSimpleMessageTls=my-cluster-90021a9f-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-6da231a3-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de-kafka-clients}
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-2 for test case:testSendMessagesTlsScramSha
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-2
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-2
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace listeners-st get Namespace namespace-2 -o json
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace listeners-st get Namespace namespace-2 -o json
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T16:52:49Z",
        "name": "namespace-2",
        "resourceVersion": "52405",
        "selfLink": "/api/v1/namespaces/namespace-2",
        "uid": "f28eebcf-a6c9-4f41-aba4-e3adecca180c"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[namespace-2], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@afd54539=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@cf7c9d53=[]}
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-2
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-2, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 16:52:49 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-2
2022-03-30 16:52:49 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 16:52:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-128d9a40, testUpdateUser=my-cluster-75998b92, testSendMessagesTlsScramSha=my-cluster-a11b9173, testReceiveSimpleMessageTls=my-cluster-90021a9f, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e, testCruiseControlBasicAPIRequests=my-cluster-6da231a3, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de}
2022-03-30 16:52:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-323988616-1355569855, testUpdateUser=my-user-275427717-1183444028, testSendMessagesTlsScramSha=my-user-1791509549-1350003609, testReceiveSimpleMessageTls=my-user-491397521-1196361913, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-695287754-1004738297, testSendMessagesCustomListenerTlsScramSha=my-user-1653975062-1727371851, testCruiseControlBasicAPIRequests=my-user-918568860-36694764, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-83332781-1273282344}
2022-03-30 16:52:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-1040221322-670398147, testUpdateUser=my-topic-275186280-765877519, testSendMessagesTlsScramSha=my-topic-1482193652-649135537, testReceiveSimpleMessageTls=my-topic-941665209-436577257, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1609923788-1785060017, testSendMessagesCustomListenerTlsScramSha=my-topic-629448478-708445494, testCruiseControlBasicAPIRequests=my-topic-710218582-658397020, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1536476030-1482182833}
2022-03-30 16:52:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients, testSendMessagesTlsScramSha=my-cluster-a11b9173-kafka-clients, testReceiveSimpleMessageTls=my-cluster-90021a9f-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-6da231a3-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de-kafka-clients}
2022-03-30 16:52:49 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-3 for test case:testSendMessagesCustomListenerTlsScramSha
2022-03-30 16:52:49 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-3
2022-03-30 16:52:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-3
2022-03-30 16:52:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace namespace-3 -o json
2022-03-30 16:52:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-a11b9173 in namespace namespace-2
2022-03-30 16:52:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-2
2022-03-30 16:52:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-a11b9173
2022-03-30 16:52:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-a11b9173 will have desired state: Ready
2022-03-30 16:52:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-a11b9173 will have desired state: Ready
2022-03-30 16:52:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (839998ms till timeout)
2022-03-30 16:52:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace namespace-3 -o json
2022-03-30 16:52:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:52:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T16:52:49Z",
        "name": "namespace-3",
        "resourceVersion": "52409",
        "selfLink": "/api/v1/namespaces/namespace-3",
        "uid": "00d7e69a-2871-422f-9cde-b6612a63b962"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 16:52:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[namespace-3], io.strimzi.test.logs.CollectorElement@6e85bf86=[namespace-2], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@afd54539=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@cf7c9d53=[]}
2022-03-30 16:52:50 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-3
2022-03-30 16:52:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-3, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 16:52:50 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-3
2022-03-30 16:52:50 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-31e27e7e in namespace namespace-3
2022-03-30 16:52:50 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-3
2022-03-30 16:52:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-31e27e7e
2022-03-30 16:52:50 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-31e27e7e will have desired state: Ready
2022-03-30 16:52:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-31e27e7e will have desired state: Ready
2022-03-30 16:52:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (839996ms till timeout)
2022-03-30 16:52:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (838995ms till timeout)
2022-03-30 16:52:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (838993ms till timeout)
2022-03-30 16:52:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (837991ms till timeout)
2022-03-30 16:52:52 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (837990ms till timeout)
2022-03-30 16:52:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (836988ms till timeout)
2022-03-30 16:52:53 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (836988ms till timeout)
2022-03-30 16:52:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (835984ms till timeout)
2022-03-30 16:52:54 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (835985ms till timeout)
2022-03-30 16:52:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (834981ms till timeout)
2022-03-30 16:52:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (834982ms till timeout)
2022-03-30 16:52:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (833977ms till timeout)
2022-03-30 16:52:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (833979ms till timeout)
2022-03-30 16:52:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (832974ms till timeout)
2022-03-30 16:52:57 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (832976ms till timeout)
2022-03-30 16:52:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (831971ms till timeout)
2022-03-30 16:52:58 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (831972ms till timeout)
2022-03-30 16:52:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (830967ms till timeout)
2022-03-30 16:52:59 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (830969ms till timeout)
2022-03-30 16:53:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (829962ms till timeout)
2022-03-30 16:53:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (829963ms till timeout)
2022-03-30 16:53:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (828958ms till timeout)
2022-03-30 16:53:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (828959ms till timeout)
2022-03-30 16:53:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (827952ms till timeout)
2022-03-30 16:53:02 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (827952ms till timeout)
2022-03-30 16:53:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (826941ms till timeout)
2022-03-30 16:53:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (826942ms till timeout)
2022-03-30 16:53:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (825927ms till timeout)
2022-03-30 16:53:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (825938ms till timeout)
2022-03-30 16:53:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (824924ms till timeout)
2022-03-30 16:53:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (824935ms till timeout)
2022-03-30 16:53:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (823920ms till timeout)
2022-03-30 16:53:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (823932ms till timeout)
2022-03-30 16:53:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (822916ms till timeout)
2022-03-30 16:53:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (822929ms till timeout)
2022-03-30 16:53:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (821913ms till timeout)
2022-03-30 16:53:08 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (821926ms till timeout)
2022-03-30 16:53:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (820909ms till timeout)
2022-03-30 16:53:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (820923ms till timeout)
2022-03-30 16:53:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (819905ms till timeout)
2022-03-30 16:53:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (819919ms till timeout)
2022-03-30 16:53:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (818902ms till timeout)
2022-03-30 16:53:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (818917ms till timeout)
2022-03-30 16:53:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (817898ms till timeout)
2022-03-30 16:53:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (817914ms till timeout)
2022-03-30 16:53:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (816895ms till timeout)
2022-03-30 16:53:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (816911ms till timeout)
2022-03-30 16:53:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (815891ms till timeout)
2022-03-30 16:53:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (815908ms till timeout)
2022-03-30 16:53:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (814888ms till timeout)
2022-03-30 16:53:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (814905ms till timeout)
2022-03-30 16:53:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (813884ms till timeout)
2022-03-30 16:53:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (813901ms till timeout)
2022-03-30 16:53:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (812881ms till timeout)
2022-03-30 16:53:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (812898ms till timeout)
2022-03-30 16:53:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (811877ms till timeout)
2022-03-30 16:53:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (811895ms till timeout)
2022-03-30 16:53:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (810873ms till timeout)
2022-03-30 16:53:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (810891ms till timeout)
2022-03-30 16:53:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (809870ms till timeout)
2022-03-30 16:53:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (809887ms till timeout)
2022-03-30 16:53:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (808866ms till timeout)
2022-03-30 16:53:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (808884ms till timeout)
2022-03-30 16:53:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (807860ms till timeout)
2022-03-30 16:53:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (807879ms till timeout)
2022-03-30 16:53:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (806857ms till timeout)
2022-03-30 16:53:23 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (806875ms till timeout)
2022-03-30 16:53:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (805851ms till timeout)
2022-03-30 16:53:24 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (805871ms till timeout)
2022-03-30 16:53:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (804846ms till timeout)
2022-03-30 16:53:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (804868ms till timeout)
2022-03-30 16:53:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (803843ms till timeout)
2022-03-30 16:53:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (803864ms till timeout)
2022-03-30 16:53:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (802839ms till timeout)
2022-03-30 16:53:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (802862ms till timeout)
2022-03-30 16:53:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (801836ms till timeout)
2022-03-30 16:53:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (801859ms till timeout)
2022-03-30 16:53:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (800832ms till timeout)
2022-03-30 16:53:29 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (800856ms till timeout)
2022-03-30 16:53:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (799828ms till timeout)
2022-03-30 16:53:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (799853ms till timeout)
2022-03-30 16:53:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (798825ms till timeout)
2022-03-30 16:53:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (798850ms till timeout)
2022-03-30 16:53:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (797821ms till timeout)
2022-03-30 16:53:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (797848ms till timeout)
2022-03-30 16:53:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (796818ms till timeout)
2022-03-30 16:53:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (796845ms till timeout)
2022-03-30 16:53:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (795814ms till timeout)
2022-03-30 16:53:34 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (795842ms till timeout)
2022-03-30 16:53:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (794808ms till timeout)
2022-03-30 16:53:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (794839ms till timeout)
2022-03-30 16:53:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (793805ms till timeout)
2022-03-30 16:53:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (793836ms till timeout)
2022-03-30 16:53:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (792801ms till timeout)
2022-03-30 16:53:37 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (792833ms till timeout)
2022-03-30 16:53:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (791797ms till timeout)
2022-03-30 16:53:38 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (791830ms till timeout)
2022-03-30 16:53:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (790794ms till timeout)
2022-03-30 16:53:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (790827ms till timeout)
2022-03-30 16:53:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (789790ms till timeout)
2022-03-30 16:53:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (789824ms till timeout)
2022-03-30 16:53:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (788787ms till timeout)
2022-03-30 16:53:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (788821ms till timeout)
2022-03-30 16:53:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (787783ms till timeout)
2022-03-30 16:53:42 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (787817ms till timeout)
2022-03-30 16:53:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (786779ms till timeout)
2022-03-30 16:53:43 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (786814ms till timeout)
2022-03-30 16:53:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (785775ms till timeout)
2022-03-30 16:53:44 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (785811ms till timeout)
2022-03-30 16:53:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (784772ms till timeout)
2022-03-30 16:53:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (784808ms till timeout)
2022-03-30 16:53:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (783768ms till timeout)
2022-03-30 16:53:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (783805ms till timeout)
2022-03-30 16:53:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (782765ms till timeout)
2022-03-30 16:53:47 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (782801ms till timeout)
2022-03-30 16:53:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (781758ms till timeout)
2022-03-30 16:53:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (781796ms till timeout)
2022-03-30 16:53:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (780754ms till timeout)
2022-03-30 16:53:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (780793ms till timeout)
2022-03-30 16:53:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (779750ms till timeout)
2022-03-30 16:53:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (779790ms till timeout)
2022-03-30 16:53:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (778747ms till timeout)
2022-03-30 16:53:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (778788ms till timeout)
2022-03-30 16:53:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (777743ms till timeout)
2022-03-30 16:53:52 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (777784ms till timeout)
2022-03-30 16:53:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (776739ms till timeout)
2022-03-30 16:53:53 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (776780ms till timeout)
2022-03-30 16:53:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (775734ms till timeout)
2022-03-30 16:53:54 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (775776ms till timeout)
2022-03-30 16:53:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (774731ms till timeout)
2022-03-30 16:53:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (774772ms till timeout)
2022-03-30 16:53:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (773725ms till timeout)
2022-03-30 16:53:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (773767ms till timeout)
2022-03-30 16:53:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (772722ms till timeout)
2022-03-30 16:53:57 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (772758ms till timeout)
2022-03-30 16:53:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (771717ms till timeout)
2022-03-30 16:53:58 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (771754ms till timeout)
2022-03-30 16:53:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (770713ms till timeout)
2022-03-30 16:53:59 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (770744ms till timeout)
2022-03-30 16:54:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (769710ms till timeout)
2022-03-30 16:54:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (769740ms till timeout)
2022-03-30 16:54:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (768704ms till timeout)
2022-03-30 16:54:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (768736ms till timeout)
2022-03-30 16:54:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-a11b9173 will have desired state: Ready not ready, will try again in 1000 ms (767700ms till timeout)
2022-03-30 16:54:02 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (767733ms till timeout)
2022-03-30 16:54:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-a11b9173 is in desired state: Ready
2022-03-30 16:54:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1482193652-649135537 in namespace namespace-3
2022-03-30 16:54:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-2
2022-03-30 16:54:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1482193652-649135537
2022-03-30 16:54:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1482193652-649135537 will have desired state: Ready
2022-03-30 16:54:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1482193652-649135537 will have desired state: Ready
2022-03-30 16:54:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1482193652-649135537 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 16:54:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (766730ms till timeout)
2022-03-30 16:54:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1482193652-649135537 is in desired state: Ready
2022-03-30 16:54:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1791509549-1350003609 in namespace namespace-3
2022-03-30 16:54:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-2
2022-03-30 16:54:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1791509549-1350003609
2022-03-30 16:54:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1791509549-1350003609 will have desired state: Ready
2022-03-30 16:54:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-1791509549-1350003609 will have desired state: Ready
2022-03-30 16:54:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-1791509549-1350003609 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 16:54:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (765728ms till timeout)
2022-03-30 16:54:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1791509549-1350003609 is in desired state: Ready
2022-03-30 16:54:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (764725ms till timeout)
2022-03-30 16:54:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-a11b9173-kafka-clients in namespace namespace-2
2022-03-30 16:54:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-2
2022-03-30 16:54:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-a11b9173-kafka-clients
2022-03-30 16:54:05 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-a11b9173-kafka-clients will be ready
2022-03-30 16:54:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-a11b9173-kafka-clients will be ready
2022-03-30 16:54:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-a11b9173-kafka-clients will be ready not ready, will try again in 1000 ms (479995ms till timeout)
2022-03-30 16:54:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (763722ms till timeout)
2022-03-30 16:54:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-a11b9173-kafka-clients will be ready not ready, will try again in 1000 ms (478993ms till timeout)
2022-03-30 16:54:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (762718ms till timeout)
2022-03-30 16:54:07 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-a11b9173-kafka-clients is ready
2022-03-30 16:54:07 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-30 16:54:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ListenersST:370] Checking produced and consumed messages to pod:my-cluster-a11b9173-kafka-clients-95d87874b-mnrbs
2022-03-30 16:54:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@747fb271, which are set.
2022-03-30 16:54:07 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@321cf0ed, messages=[], arguments=[--topic, my-topic-1482193652-649135537, --bootstrap-server, my-cluster-a11b9173-kafka-bootstrap.namespace-2.svc:9096, USER=my_user_1791509549_1350003609, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-a11b9173-kafka-clients-95d87874b-mnrbs', podNamespace='namespace-2', bootstrapServer='my-cluster-a11b9173-kafka-bootstrap.namespace-2.svc:9096', topicName='my-topic-1482193652-649135537', maxMessages=100, kafkaUsername='my-user-1791509549-1350003609', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@747fb271}
2022-03-30 16:54:07 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-a11b9173-kafka-bootstrap.namespace-2.svc:9096:my-topic-1482193652-649135537 from pod my-cluster-a11b9173-kafka-clients-95d87874b-mnrbs
2022-03-30 16:54:07 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-a11b9173-kafka-clients-95d87874b-mnrbs -n namespace-2 -- /opt/kafka/producer.sh --topic my-topic-1482193652-649135537 --bootstrap-server my-cluster-a11b9173-kafka-bootstrap.namespace-2.svc:9096 USER=my_user_1791509549_1350003609 --max-messages 100
2022-03-30 16:54:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-a11b9173-kafka-clients-95d87874b-mnrbs -n namespace-2 -- /opt/kafka/producer.sh --topic my-topic-1482193652-649135537 --bootstrap-server my-cluster-a11b9173-kafka-bootstrap.namespace-2.svc:9096 USER=my_user_1791509549_1350003609 --max-messages 100
2022-03-30 16:54:08 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (761715ms till timeout)
2022-03-30 16:54:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (760711ms till timeout)
2022-03-30 16:54:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (759707ms till timeout)
2022-03-30 16:54:11 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-30 16:54:11 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-03-30 16:54:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6828f78e, which are set.
2022-03-30 16:54:11 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@6f870be7, messages=[], arguments=[--group-id, my-consumer-group-1986972933, --topic, my-topic-1482193652-649135537, --bootstrap-server, my-cluster-a11b9173-kafka-bootstrap.namespace-2.svc:9096, USER=my_user_1791509549_1350003609, --max-messages, 100, --group-instance-id, instance1208130930], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-a11b9173-kafka-clients-95d87874b-mnrbs', podNamespace='namespace-2', bootstrapServer='my-cluster-a11b9173-kafka-bootstrap.namespace-2.svc:9096', topicName='my-topic-1482193652-649135537', maxMessages=100, kafkaUsername='my-user-1791509549-1350003609', consumerGroupName='my-consumer-group-1986972933', consumerInstanceId='instance1208130930', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6828f78e}
2022-03-30 16:54:11 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-a11b9173-kafka-bootstrap.namespace-2.svc:9096:my-topic-1482193652-649135537 from pod my-cluster-a11b9173-kafka-clients-95d87874b-mnrbs
2022-03-30 16:54:11 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-a11b9173-kafka-clients-95d87874b-mnrbs -n namespace-2 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1986972933 --topic my-topic-1482193652-649135537 --bootstrap-server my-cluster-a11b9173-kafka-bootstrap.namespace-2.svc:9096 USER=my_user_1791509549_1350003609 --max-messages 100 --group-instance-id instance1208130930
2022-03-30 16:54:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-a11b9173-kafka-clients-95d87874b-mnrbs -n namespace-2 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1986972933 --topic my-topic-1482193652-649135537 --bootstrap-server my-cluster-a11b9173-kafka-bootstrap.namespace-2.svc:9096 USER=my_user_1791509549_1350003609 --max-messages 100 --group-instance-id instance1208130930
2022-03-30 16:54:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (758704ms till timeout)
2022-03-30 16:54:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (757700ms till timeout)
2022-03-30 16:54:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (756697ms till timeout)
2022-03-30 16:54:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (755693ms till timeout)
2022-03-30 16:54:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (754690ms till timeout)
2022-03-30 16:54:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (753686ms till timeout)
2022-03-30 16:54:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (752683ms till timeout)
2022-03-30 16:54:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (751679ms till timeout)
2022-03-30 16:54:18 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 16:54:18 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-03-30 16:54:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ListenersST:377] Checking if generated password has 25 characters
2022-03-30 16:54:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 16:54:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:675] [kafka.listeners.ListenersST - After Each] - Clean up after test
2022-03-30 16:54:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 16:54:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testSendMessagesTlsScramSha
2022-03-30 16:54:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1791509549-1350003609 in namespace namespace-2
2022-03-30 16:54:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1791509549-1350003609
2022-03-30 16:54:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1791509549-1350003609 not ready, will try again in 10000 ms (179994ms till timeout)
2022-03-30 16:54:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (750676ms till timeout)
2022-03-30 16:54:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (749671ms till timeout)
2022-03-30 16:54:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (748667ms till timeout)
2022-03-30 16:54:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (747664ms till timeout)
2022-03-30 16:54:23 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (746659ms till timeout)
2022-03-30 16:54:24 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (745655ms till timeout)
2022-03-30 16:54:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (744651ms till timeout)
2022-03-30 16:54:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (743647ms till timeout)
2022-03-30 16:54:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (742644ms till timeout)
2022-03-30 16:54:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (741640ms till timeout)
2022-03-30 16:54:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-a11b9173-kafka-clients in namespace namespace-2
2022-03-30 16:54:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-a11b9173-kafka-clients
2022-03-30 16:54:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-a11b9173-kafka-clients not ready, will try again in 10000 ms (479991ms till timeout)
2022-03-30 16:54:29 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (740637ms till timeout)
2022-03-30 16:54:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (739633ms till timeout)
2022-03-30 16:54:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (738630ms till timeout)
2022-03-30 16:54:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (737626ms till timeout)
2022-03-30 16:54:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (736623ms till timeout)
2022-03-30 16:54:34 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (735619ms till timeout)
2022-03-30 16:54:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (734615ms till timeout)
2022-03-30 16:54:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (733612ms till timeout)
2022-03-30 16:54:37 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (732608ms till timeout)
2022-03-30 16:54:38 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (731605ms till timeout)
2022-03-30 16:54:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-a11b9173-kafka-clients not ready, will try again in 10000 ms (469982ms till timeout)
2022-03-30 16:54:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (730601ms till timeout)
2022-03-30 16:54:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (729598ms till timeout)
2022-03-30 16:54:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-31e27e7e will have desired state: Ready not ready, will try again in 1000 ms (728594ms till timeout)
2022-03-30 16:54:42 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-31e27e7e is in desired state: Ready
2022-03-30 16:54:42 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-629448478-708445494 in namespace namespace-3
2022-03-30 16:54:42 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-3
2022-03-30 16:54:42 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-629448478-708445494
2022-03-30 16:54:42 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-629448478-708445494 will have desired state: Ready
2022-03-30 16:54:42 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-629448478-708445494 will have desired state: Ready
2022-03-30 16:54:42 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-629448478-708445494 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 16:54:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-629448478-708445494 is in desired state: Ready
2022-03-30 16:54:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1653975062-1727371851 in namespace namespace-3
2022-03-30 16:54:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-3
2022-03-30 16:54:43 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1653975062-1727371851
2022-03-30 16:54:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1653975062-1727371851 will have desired state: Ready
2022-03-30 16:54:43 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-1653975062-1727371851 will have desired state: Ready
2022-03-30 16:54:43 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-1653975062-1727371851 will have desired state: Ready not ready, will try again in 1000 ms (179997ms till timeout)
2022-03-30 16:54:44 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1653975062-1727371851 is in desired state: Ready
2022-03-30 16:54:44 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-31e27e7e-kafka-clients in namespace namespace-3
2022-03-30 16:54:44 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-3
2022-03-30 16:54:44 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-31e27e7e-kafka-clients
2022-03-30 16:54:44 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-31e27e7e-kafka-clients will be ready
2022-03-30 16:54:44 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-31e27e7e-kafka-clients will be ready
2022-03-30 16:54:44 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-31e27e7e-kafka-clients will be ready not ready, will try again in 1000 ms (479997ms till timeout)
2022-03-30 16:54:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-31e27e7e-kafka-clients will be ready not ready, will try again in 1000 ms (478994ms till timeout)
2022-03-30 16:54:46 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-31e27e7e-kafka-clients is ready
2022-03-30 16:54:46 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-30 16:54:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ListenersST:442] Checking produced and consumed messages to pod:my-cluster-31e27e7e-kafka-clients-648d89b788-n92xv
2022-03-30 16:54:46 [ForkJoinPool-3-worker-1] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@21ea67c, which are set.
2022-03-30 16:54:46 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@1c662d03, messages=[], arguments=[--topic, my-topic-629448478-708445494, --bootstrap-server, my-cluster-31e27e7e-kafka-bootstrap.namespace-3.svc:9122, USER=my_user_1653975062_1727371851, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-31e27e7e-kafka-clients-648d89b788-n92xv', podNamespace='namespace-3', bootstrapServer='my-cluster-31e27e7e-kafka-bootstrap.namespace-3.svc:9122', topicName='my-topic-629448478-708445494', maxMessages=100, kafkaUsername='my-user-1653975062-1727371851', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@21ea67c}
2022-03-30 16:54:46 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-31e27e7e-kafka-bootstrap.namespace-3.svc:9122:my-topic-629448478-708445494 from pod my-cluster-31e27e7e-kafka-clients-648d89b788-n92xv
2022-03-30 16:54:46 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-31e27e7e-kafka-clients-648d89b788-n92xv -n namespace-3 -- /opt/kafka/producer.sh --topic my-topic-629448478-708445494 --bootstrap-server my-cluster-31e27e7e-kafka-bootstrap.namespace-3.svc:9122 USER=my_user_1653975062_1727371851 --max-messages 100
2022-03-30 16:54:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-31e27e7e-kafka-clients-648d89b788-n92xv -n namespace-3 -- /opt/kafka/producer.sh --topic my-topic-629448478-708445494 --bootstrap-server my-cluster-31e27e7e-kafka-bootstrap.namespace-3.svc:9122 USER=my_user_1653975062_1727371851 --max-messages 100
2022-03-30 16:54:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-a11b9173-kafka-clients not ready, will try again in 10000 ms (459972ms till timeout)
2022-03-30 16:54:50 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-30 16:54:50 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-03-30 16:54:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@39964f2c, which are set.
2022-03-30 16:54:50 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@2ccd4fe6, messages=[], arguments=[--group-id, my-consumer-group-1414341973, --topic, my-topic-629448478-708445494, --bootstrap-server, my-cluster-31e27e7e-kafka-bootstrap.namespace-3.svc:9122, USER=my_user_1653975062_1727371851, --max-messages, 100, --group-instance-id, instance166597187], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-31e27e7e-kafka-clients-648d89b788-n92xv', podNamespace='namespace-3', bootstrapServer='my-cluster-31e27e7e-kafka-bootstrap.namespace-3.svc:9122', topicName='my-topic-629448478-708445494', maxMessages=100, kafkaUsername='my-user-1653975062-1727371851', consumerGroupName='my-consumer-group-1414341973', consumerInstanceId='instance166597187', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@39964f2c}
2022-03-30 16:54:50 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-31e27e7e-kafka-bootstrap.namespace-3.svc:9122:my-topic-629448478-708445494 from pod my-cluster-31e27e7e-kafka-clients-648d89b788-n92xv
2022-03-30 16:54:50 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-31e27e7e-kafka-clients-648d89b788-n92xv -n namespace-3 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1414341973 --topic my-topic-629448478-708445494 --bootstrap-server my-cluster-31e27e7e-kafka-bootstrap.namespace-3.svc:9122 USER=my_user_1653975062_1727371851 --max-messages 100 --group-instance-id instance166597187
2022-03-30 16:54:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-31e27e7e-kafka-clients-648d89b788-n92xv -n namespace-3 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1414341973 --topic my-topic-629448478-708445494 --bootstrap-server my-cluster-31e27e7e-kafka-bootstrap.namespace-3.svc:9122 USER=my_user_1653975062_1727371851 --max-messages 100 --group-instance-id instance166597187
2022-03-30 16:54:57 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 16:54:57 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-03-30 16:54:57 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 16:54:57 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:675] [kafka.listeners.ListenersST - After Each] - Clean up after test
2022-03-30 16:54:57 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 16:54:57 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testSendMessagesCustomListenerTlsScramSha
2022-03-30 16:54:57 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1653975062-1727371851 in namespace namespace-3
2022-03-30 16:54:57 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1653975062-1727371851
2022-03-30 16:54:57 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1653975062-1727371851 not ready, will try again in 10000 ms (179987ms till timeout)
2022-03-30 16:54:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-a11b9173-kafka-clients not ready, will try again in 10000 ms (449963ms till timeout)
2022-03-30 16:55:07 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-31e27e7e-kafka-clients in namespace namespace-3
2022-03-30 16:55:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-31e27e7e-kafka-clients
2022-03-30 16:55:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-31e27e7e-kafka-clients not ready, will try again in 10000 ms (479988ms till timeout)
2022-03-30 16:55:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1482193652-649135537 in namespace namespace-2
2022-03-30 16:55:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1482193652-649135537
2022-03-30 16:55:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-a11b9173 in namespace namespace-2
2022-03-30 16:55:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-a11b9173
2022-03-30 16:55:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-a11b9173 not ready, will try again in 10000 ms (839994ms till timeout)
2022-03-30 16:55:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-31e27e7e-kafka-clients not ready, will try again in 10000 ms (469977ms till timeout)
2022-03-30 16:55:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 16:55:18 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-2 for test case:testSendMessagesTlsScramSha
2022-03-30 16:55:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-2 removal
2022-03-30 16:55:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (479928ms till timeout)
2022-03-30 16:55:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (478857ms till timeout)
2022-03-30 16:55:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (477772ms till timeout)
2022-03-30 16:55:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (476698ms till timeout)
2022-03-30 16:55:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (475624ms till timeout)
2022-03-30 16:55:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (474547ms till timeout)
2022-03-30 16:55:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (473471ms till timeout)
2022-03-30 16:55:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (472393ms till timeout)
2022-03-30 16:55:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (471320ms till timeout)
2022-03-30 16:55:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-31e27e7e-kafka-clients not ready, will try again in 10000 ms (459968ms till timeout)
2022-03-30 16:55:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (470244ms till timeout)
2022-03-30 16:55:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (469170ms till timeout)
2022-03-30 16:55:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (468101ms till timeout)
2022-03-30 16:55:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (467025ms till timeout)
2022-03-30 16:55:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (465947ms till timeout)
2022-03-30 16:55:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (464872ms till timeout)
2022-03-30 16:55:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (463799ms till timeout)
2022-03-30 16:55:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (462724ms till timeout)
2022-03-30 16:55:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (461647ms till timeout)
2022-03-30 16:55:37 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-31e27e7e-kafka-clients not ready, will try again in 10000 ms (449959ms till timeout)
2022-03-30 16:55:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (460575ms till timeout)
2022-03-30 16:55:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (459502ms till timeout)
2022-03-30 16:55:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (458433ms till timeout)
2022-03-30 16:55:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (457363ms till timeout)
2022-03-30 16:55:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:42 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:42 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (456286ms till timeout)
2022-03-30 16:55:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (455213ms till timeout)
2022-03-30 16:55:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:44 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:44 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (454136ms till timeout)
2022-03-30 16:55:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (453060ms till timeout)
2022-03-30 16:55:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (451983ms till timeout)
2022-03-30 16:55:47 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-629448478-708445494 in namespace namespace-3
2022-03-30 16:55:47 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-629448478-708445494
2022-03-30 16:55:47 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-629448478-708445494 not ready, will try again in 10000 ms (179992ms till timeout)
2022-03-30 16:55:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (450907ms till timeout)
2022-03-30 16:55:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (449822ms till timeout)
2022-03-30 16:55:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (448748ms till timeout)
2022-03-30 16:55:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (447675ms till timeout)
2022-03-30 16:55:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (446596ms till timeout)
2022-03-30 16:55:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (445526ms till timeout)
2022-03-30 16:55:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (444448ms till timeout)
2022-03-30 16:55:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (443370ms till timeout)
2022-03-30 16:55:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (442301ms till timeout)
2022-03-30 16:55:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (441229ms till timeout)
2022-03-30 16:55:57 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-31e27e7e in namespace namespace-3
2022-03-30 16:55:57 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-31e27e7e
2022-03-30 16:55:57 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-31e27e7e not ready, will try again in 10000 ms (839997ms till timeout)
2022-03-30 16:55:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (440157ms till timeout)
2022-03-30 16:55:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:55:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:55:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (439084ms till timeout)
2022-03-30 16:56:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:56:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:56:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:56:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (438003ms till timeout)
2022-03-30 16:56:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:56:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-3 get Namespace namespace-2 -o yaml
2022-03-30 16:56:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 16:56:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 16:56:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-2" not found
2022-03-30 16:56:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 16:56:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[namespace-3], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@afd54539=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@cf7c9d53=[]}
2022-03-30 16:56:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:267] testSendMessagesTlsScramSha - Notifies waiting test cases:[testUpdateUser, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testCruiseControlBasicAPIRequests, testSendMessagesTlsScramSha, testSendMessagesCustomListenerTlsScramSha] to and randomly select one to start execution
2022-03-30 16:56:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:93] [kafka.listeners.ListenersST] - Removing parallel test: testSendMessagesTlsScramSha
2022-03-30 16:56:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:97] [kafka.listeners.ListenersST] - Parallel test count: 1
2022-03-30 16:56:01 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesTlsScramSha-FINISHED
2022-03-30 16:56:01 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 16:56:07 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 16:56:07 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-3 for test case:testSendMessagesCustomListenerTlsScramSha
2022-03-30 16:56:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-3 removal
2022-03-30 16:56:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:56:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (479933ms till timeout)
2022-03-30 16:56:08 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:08 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:08 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:56:08 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (478865ms till timeout)
2022-03-30 16:56:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:09 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:09 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:56:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (477784ms till timeout)
2022-03-30 16:56:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:56:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (476712ms till timeout)
2022-03-30 16:56:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:56:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (475630ms till timeout)
2022-03-30 16:56:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:56:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (474550ms till timeout)
2022-03-30 16:56:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:14 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:14 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:56:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (473474ms till timeout)
2022-03-30 16:56:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:56:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (472395ms till timeout)
2022-03-30 16:56:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:56:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (471320ms till timeout)
2022-03-30 16:56:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:56:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (470248ms till timeout)
2022-03-30 16:56:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:56:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (469172ms till timeout)
2022-03-30 16:56:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:56:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (468098ms till timeout)
2022-03-30 16:56:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:56:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (467023ms till timeout)
2022-03-30 16:56:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:21 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:21 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:56:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (465950ms till timeout)
2022-03-30 16:56:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:22 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:22 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:56:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (464878ms till timeout)
2022-03-30 16:56:23 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:56:23 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (463809ms till timeout)
2022-03-30 16:56:24 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:24 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:24 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:56:24 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (462736ms till timeout)
2022-03-30 16:56:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:56:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (461659ms till timeout)
2022-03-30 16:56:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:26 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:26 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:56:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (460570ms till timeout)
2022-03-30 16:56:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:56:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (459498ms till timeout)
2022-03-30 16:56:29 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:29 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:29 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:56:29 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (458427ms till timeout)
2022-03-30 16:56:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:56:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (457348ms till timeout)
2022-03-30 16:56:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:56:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (456276ms till timeout)
2022-03-30 16:56:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:56:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (455204ms till timeout)
2022-03-30 16:56:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:56:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (454132ms till timeout)
2022-03-30 16:56:34 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:34 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-3 get Namespace namespace-3 -o yaml
2022-03-30 16:56:34 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 16:56:34 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 16:56:34 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-3" not found
2022-03-30 16:56:34 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 16:56:34 [ForkJoinPool-3-worker-1] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@afd54539=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@cf7c9d53=[]}
2022-03-30 16:56:34 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:267] testSendMessagesCustomListenerTlsScramSha - Notifies waiting test cases:[testUpdateUser, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testCruiseControlBasicAPIRequests, testSendMessagesTlsScramSha, testSendMessagesCustomListenerTlsScramSha] to and randomly select one to start execution
2022-03-30 16:56:34 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:93] [kafka.listeners.ListenersST] - Removing parallel test: testSendMessagesCustomListenerTlsScramSha
2022-03-30 16:56:34 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:97] [kafka.listeners.ListenersST] - Parallel test count: 0
2022-03-30 16:56:34 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesCustomListenerTlsScramSha-FINISHED
2022-03-30 16:56:34 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 16:56:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 16:56:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:690] [kafka.listeners.ListenersST - After All] - Clean up after test suite
2022-03-30 16:56:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 16:56:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context ListenersST is everything deleted.
2022-03-30 16:56:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 16:56:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace listeners-st removal
2022-03-30 16:56:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace listeners-st -o yaml
2022-03-30 16:56:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace listeners-st -o yaml
2022-03-30 16:56:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:56:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace listeners-st removal not ready, will try again in 1000 ms (479932ms till timeout)
2022-03-30 16:56:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace listeners-st -o yaml
2022-03-30 16:56:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace listeners-st -o yaml
2022-03-30 16:56:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:56:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace listeners-st removal not ready, will try again in 1000 ms (478861ms till timeout)
2022-03-30 16:56:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace listeners-st -o yaml
2022-03-30 16:56:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace listeners-st -o yaml
2022-03-30 16:56:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:56:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace listeners-st removal not ready, will try again in 1000 ms (477784ms till timeout)
2022-03-30 16:56:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace listeners-st -o yaml
2022-03-30 16:56:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace listeners-st -o yaml
2022-03-30 16:56:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:56:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace listeners-st removal not ready, will try again in 1000 ms (476711ms till timeout)
2022-03-30 16:56:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace listeners-st -o yaml
2022-03-30 16:56:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace listeners-st -o yaml
2022-03-30 16:56:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:56:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace listeners-st removal not ready, will try again in 1000 ms (475639ms till timeout)
2022-03-30 16:56:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace listeners-st -o yaml
2022-03-30 16:56:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-3 get Namespace listeners-st -o yaml
2022-03-30 16:56:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 16:56:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 16:56:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "listeners-st" not found
2022-03-30 16:56:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 16:56:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@3d7a59f3=[], io.strimzi.test.logs.CollectorElement@afd54539=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@cf7c9d53=[]}
2022-03-30 16:56:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:254] ListenersST - Notifies waiting test suites:[UserST, HttpBridgeTlsST, CruiseControlApiST, CruiseControlST, ListenersST] to and randomly select one to start execution
2022-03-30 16:56:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:85] [kafka.listeners.ListenersST] - Removing parallel suite: ListenersST
2022-03-30 16:56:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:89] [kafka.listeners.ListenersST] - Parallel suites count: 0
[[1;34mINFO[m] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 231.549 s - in io.strimzi.systemtest.kafka.listeners.ListenersST
[[1;34mINFO[m] Running io.strimzi.systemtest.security.SecurityST
2022-03-30 16:56:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 16:56:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:667] [security.SecurityST - Before All] - Setup test suite environment
2022-03-30 16:56:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:69] [security.SecurityST] - Adding parallel suite: SecurityST
2022-03-30 16:56:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:73] [security.SecurityST] - Parallel suites count: 1
2022-03-30 16:56:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:184] SecurityST suite now can proceed its execution
2022-03-30 16:56:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-30 16:56:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestSuiteNamespaceManager:129] Test suite `SecurityST` creates these additional namespaces:[security-st]
2022-03-30 16:56:39 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: security-st
2022-03-30 16:56:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace security-st
2022-03-30 16:56:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace security-st -o json
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace security-st -o json
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T16:56:39Z",
        "name": "security-st",
        "resourceVersion": "53491",
        "selfLink": "/api/v1/namespaces/security-st",
        "uid": "8e40d585-5459-4aca-91eb-33b1eb1b668e"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@3d7a59f3=[], io.strimzi.test.logs.CollectorElement@afd54539=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@cf7c9d53=[], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: security-st
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=security-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: security-st
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testAutoRenewAllCaCertsTriggeredByAnno-STARTED
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:659] [security.SecurityST - Before Each] - Setup test case environment
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:77] [security.SecurityST] - Adding parallel test: testAutoRenewAllCaCertsTriggeredByAnno
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:81] [security.SecurityST] - Parallel test count: 1
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:230] testAutoRenewAllCaCertsTriggeredByAnno test now can proceed its execution
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2, testSendSimpleMessageTls=my-cluster-128d9a40, testUpdateUser=my-cluster-75998b92, testSendMessagesTlsScramSha=my-cluster-a11b9173, testReceiveSimpleMessageTls=my-cluster-90021a9f, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e, testCruiseControlBasicAPIRequests=my-cluster-6da231a3, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de}
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testAutoRenewAllCaCertsTriggeredByAnno=my-user-308582398-741623568, testSendSimpleMessageTls=my-user-323988616-1355569855, testUpdateUser=my-user-275427717-1183444028, testSendMessagesTlsScramSha=my-user-1791509549-1350003609, testReceiveSimpleMessageTls=my-user-491397521-1196361913, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-695287754-1004738297, testSendMessagesCustomListenerTlsScramSha=my-user-1653975062-1727371851, testCruiseControlBasicAPIRequests=my-user-918568860-36694764, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-83332781-1273282344}
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1728678022-515015095, testSendSimpleMessageTls=my-topic-1040221322-670398147, testUpdateUser=my-topic-275186280-765877519, testSendMessagesTlsScramSha=my-topic-1482193652-649135537, testReceiveSimpleMessageTls=my-topic-941665209-436577257, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1609923788-1785060017, testSendMessagesCustomListenerTlsScramSha=my-topic-629448478-708445494, testCruiseControlBasicAPIRequests=my-topic-710218582-658397020, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1536476030-1482182833}
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2-kafka-clients, testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients, testSendMessagesTlsScramSha=my-cluster-a11b9173-kafka-clients, testReceiveSimpleMessageTls=my-cluster-90021a9f-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-6da231a3-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de-kafka-clients}
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-4 for test case:testAutoRenewAllCaCertsTriggeredByAnno
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-4
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-4
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace security-st get Namespace namespace-4 -o json
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace security-st get Namespace namespace-4 -o json
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T16:56:40Z",
        "name": "namespace-4",
        "resourceVersion": "53495",
        "selfLink": "/api/v1/namespaces/namespace-4",
        "uid": "9300cfc8-6c96-440d-a465-b1c900871c24"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@3d7a59f3=[], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-4], io.strimzi.test.logs.CollectorElement@afd54539=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@cf7c9d53=[], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-4
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-4, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-4
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:596] Creating a cluster
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-60aa0ca2 in namespace namespace-4
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-60aa0ca2
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-60aa0ca2 will have desired state: Ready
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-60aa0ca2 will have desired state: Ready
2022-03-30 16:56:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1799998ms till timeout)
2022-03-30 16:56:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1798994ms till timeout)
2022-03-30 16:56:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1797991ms till timeout)
2022-03-30 16:56:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1796986ms till timeout)
2022-03-30 16:56:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1795983ms till timeout)
2022-03-30 16:56:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1794979ms till timeout)
2022-03-30 16:56:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1793976ms till timeout)
2022-03-30 16:56:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1792971ms till timeout)
2022-03-30 16:56:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1791967ms till timeout)
2022-03-30 16:56:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1790963ms till timeout)
2022-03-30 16:56:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1789960ms till timeout)
2022-03-30 16:56:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1788956ms till timeout)
2022-03-30 16:56:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1787952ms till timeout)
2022-03-30 16:56:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1786948ms till timeout)
2022-03-30 16:56:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1785945ms till timeout)
2022-03-30 16:56:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1784941ms till timeout)
2022-03-30 16:56:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1783938ms till timeout)
2022-03-30 16:56:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1782934ms till timeout)
2022-03-30 16:56:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1781931ms till timeout)
2022-03-30 16:56:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1780927ms till timeout)
2022-03-30 16:57:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1779923ms till timeout)
2022-03-30 16:57:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1778920ms till timeout)
2022-03-30 16:57:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1777916ms till timeout)
2022-03-30 16:57:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1776913ms till timeout)
2022-03-30 16:57:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1775909ms till timeout)
2022-03-30 16:57:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1774905ms till timeout)
2022-03-30 16:57:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1773902ms till timeout)
2022-03-30 16:57:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1772898ms till timeout)
2022-03-30 16:57:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1771894ms till timeout)
2022-03-30 16:57:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1770891ms till timeout)
2022-03-30 16:57:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1769887ms till timeout)
2022-03-30 16:57:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1768882ms till timeout)
2022-03-30 16:57:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1767878ms till timeout)
2022-03-30 16:57:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1766873ms till timeout)
2022-03-30 16:57:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1765868ms till timeout)
2022-03-30 16:57:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1764864ms till timeout)
2022-03-30 16:57:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1763861ms till timeout)
2022-03-30 16:57:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1762857ms till timeout)
2022-03-30 16:57:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1761854ms till timeout)
2022-03-30 16:57:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1760850ms till timeout)
2022-03-30 16:57:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1759847ms till timeout)
2022-03-30 16:57:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1758843ms till timeout)
2022-03-30 16:57:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1757840ms till timeout)
2022-03-30 16:57:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1756836ms till timeout)
2022-03-30 16:57:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1755832ms till timeout)
2022-03-30 16:57:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1754829ms till timeout)
2022-03-30 16:57:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1753825ms till timeout)
2022-03-30 16:57:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1752822ms till timeout)
2022-03-30 16:57:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1751819ms till timeout)
2022-03-30 16:57:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1750815ms till timeout)
2022-03-30 16:57:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1749802ms till timeout)
2022-03-30 16:57:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1748799ms till timeout)
2022-03-30 16:57:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1747796ms till timeout)
2022-03-30 16:57:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1746792ms till timeout)
2022-03-30 16:57:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1745788ms till timeout)
2022-03-30 16:57:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1744785ms till timeout)
2022-03-30 16:57:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1743781ms till timeout)
2022-03-30 16:57:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1742777ms till timeout)
2022-03-30 16:57:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1741774ms till timeout)
2022-03-30 16:57:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1740770ms till timeout)
2022-03-30 16:57:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1739767ms till timeout)
2022-03-30 16:57:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1738763ms till timeout)
2022-03-30 16:57:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1737759ms till timeout)
2022-03-30 16:57:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1736756ms till timeout)
2022-03-30 16:57:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1735752ms till timeout)
2022-03-30 16:57:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1734749ms till timeout)
2022-03-30 16:57:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1733745ms till timeout)
2022-03-30 16:57:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1732741ms till timeout)
2022-03-30 16:57:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1731738ms till timeout)
2022-03-30 16:57:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1730734ms till timeout)
2022-03-30 16:57:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1729731ms till timeout)
2022-03-30 16:57:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1728727ms till timeout)
2022-03-30 16:57:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1727723ms till timeout)
2022-03-30 16:57:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1726720ms till timeout)
2022-03-30 16:57:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1725716ms till timeout)
2022-03-30 16:57:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1724712ms till timeout)
2022-03-30 16:57:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1723708ms till timeout)
2022-03-30 16:57:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1722705ms till timeout)
2022-03-30 16:57:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1721701ms till timeout)
2022-03-30 16:57:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1720697ms till timeout)
2022-03-30 16:58:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1719694ms till timeout)
2022-03-30 16:58:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1718690ms till timeout)
2022-03-30 16:58:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1717687ms till timeout)
2022-03-30 16:58:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1716683ms till timeout)
2022-03-30 16:58:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1715679ms till timeout)
2022-03-30 16:58:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1714676ms till timeout)
2022-03-30 16:58:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1713672ms till timeout)
2022-03-30 16:58:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1712669ms till timeout)
2022-03-30 16:58:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1711665ms till timeout)
2022-03-30 16:58:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1710661ms till timeout)
2022-03-30 16:58:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1709658ms till timeout)
2022-03-30 16:58:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1708654ms till timeout)
2022-03-30 16:58:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1707651ms till timeout)
2022-03-30 16:58:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1706647ms till timeout)
2022-03-30 16:58:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1705644ms till timeout)
2022-03-30 16:58:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1704641ms till timeout)
2022-03-30 16:58:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1703637ms till timeout)
2022-03-30 16:58:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1702634ms till timeout)
2022-03-30 16:58:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1701630ms till timeout)
2022-03-30 16:58:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1700627ms till timeout)
2022-03-30 16:58:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1699623ms till timeout)
2022-03-30 16:58:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1698619ms till timeout)
2022-03-30 16:58:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1697616ms till timeout)
2022-03-30 16:58:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1696613ms till timeout)
2022-03-30 16:58:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1695609ms till timeout)
2022-03-30 16:58:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1694606ms till timeout)
2022-03-30 16:58:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1693602ms till timeout)
2022-03-30 16:58:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1692599ms till timeout)
2022-03-30 16:58:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1691595ms till timeout)
2022-03-30 16:58:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1690592ms till timeout)
2022-03-30 16:58:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1689588ms till timeout)
2022-03-30 16:58:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1688585ms till timeout)
2022-03-30 16:58:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1687581ms till timeout)
2022-03-30 16:58:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1686578ms till timeout)
2022-03-30 16:58:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1685574ms till timeout)
2022-03-30 16:58:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1684571ms till timeout)
2022-03-30 16:58:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1683567ms till timeout)
2022-03-30 16:58:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1682564ms till timeout)
2022-03-30 16:58:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1681560ms till timeout)
2022-03-30 16:58:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1680557ms till timeout)
2022-03-30 16:58:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (1679553ms till timeout)
2022-03-30 16:58:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-60aa0ca2 is in desired state: Ready
2022-03-30 16:58:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-308582398-741623568 in namespace namespace-4
2022-03-30 16:58:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-03-30 16:58:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-308582398-741623568
2022-03-30 16:58:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-308582398-741623568 will have desired state: Ready
2022-03-30 16:58:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-308582398-741623568 will have desired state: Ready
2022-03-30 16:58:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-308582398-741623568 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 16:58:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-308582398-741623568 is in desired state: Ready
2022-03-30 16:58:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1728678022-515015095 in namespace namespace-4
2022-03-30 16:58:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-03-30 16:58:42 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1728678022-515015095
2022-03-30 16:58:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1728678022-515015095 will have desired state: Ready
2022-03-30 16:58:42 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1728678022-515015095 will have desired state: Ready
2022-03-30 16:58:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1728678022-515015095 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 16:58:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1728678022-515015095 is in desired state: Ready
2022-03-30 16:58:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-60aa0ca2-kafka-clients in namespace namespace-4
2022-03-30 16:58:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-03-30 16:58:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-60aa0ca2-kafka-clients
2022-03-30 16:58:43 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-60aa0ca2-kafka-clients will be ready
2022-03-30 16:58:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-60aa0ca2-kafka-clients will be ready
2022-03-30 16:58:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-kafka-clients will be ready not ready, will try again in 1000 ms (479995ms till timeout)
2022-03-30 16:58:44 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-60aa0ca2-kafka-clients is ready
2022-03-30 16:58:44 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-30 16:58:44 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:269] Checking produced and consumed messages to pod:my-cluster-60aa0ca2-kafka-clients-5697d49f7b-dkw76
2022-03-30 16:58:44 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@40ba7f63, which are set.
2022-03-30 16:58:44 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@2201889e, messages=[], arguments=[--topic, my-topic-1728678022-515015095, --bootstrap-server, my-cluster-60aa0ca2-kafka-bootstrap.namespace-4.svc:9092, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-60aa0ca2-kafka-clients-5697d49f7b-dkw76', podNamespace='namespace-4', bootstrapServer='my-cluster-60aa0ca2-kafka-bootstrap.namespace-4.svc:9092', topicName='my-topic-1728678022-515015095', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@40ba7f63}
2022-03-30 16:58:44 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-60aa0ca2-kafka-bootstrap.namespace-4.svc:9092:my-topic-1728678022-515015095 from pod my-cluster-60aa0ca2-kafka-clients-5697d49f7b-dkw76
2022-03-30 16:58:44 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-60aa0ca2-kafka-clients-5697d49f7b-dkw76 -n namespace-4 -- /opt/kafka/producer.sh --topic my-topic-1728678022-515015095 --bootstrap-server my-cluster-60aa0ca2-kafka-bootstrap.namespace-4.svc:9092 --max-messages 100
2022-03-30 16:58:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-60aa0ca2-kafka-clients-5697d49f7b-dkw76 -n namespace-4 -- /opt/kafka/producer.sh --topic my-topic-1728678022-515015095 --bootstrap-server my-cluster-60aa0ca2-kafka-bootstrap.namespace-4.svc:9092 --max-messages 100
2022-03-30 16:58:47 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-03-30 16:58:47 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-03-30 16:58:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3f34928b, which are set.
2022-03-30 16:58:47 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@2ce6b369, messages=[], arguments=[--group-id, my-consumer-group-1947518796, --topic, my-topic-1728678022-515015095, --bootstrap-server, my-cluster-60aa0ca2-kafka-bootstrap.namespace-4.svc:9092, --max-messages, 100, --group-instance-id, instance1400639998], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-60aa0ca2-kafka-clients-5697d49f7b-dkw76', podNamespace='namespace-4', bootstrapServer='my-cluster-60aa0ca2-kafka-bootstrap.namespace-4.svc:9092', topicName='my-topic-1728678022-515015095', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1947518796', consumerInstanceId='instance1400639998', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3f34928b}
2022-03-30 16:58:47 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-60aa0ca2-kafka-bootstrap.namespace-4.svc:9092#my-topic-1728678022-515015095 from pod my-cluster-60aa0ca2-kafka-clients-5697d49f7b-dkw76
2022-03-30 16:58:47 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-60aa0ca2-kafka-clients-5697d49f7b-dkw76 -n namespace-4 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1947518796 --topic my-topic-1728678022-515015095 --bootstrap-server my-cluster-60aa0ca2-kafka-bootstrap.namespace-4.svc:9092 --max-messages 100 --group-instance-id instance1400639998
2022-03-30 16:58:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-60aa0ca2-kafka-clients-5697d49f7b-dkw76 -n namespace-4 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1947518796 --topic my-topic-1728678022-515015095 --bootstrap-server my-cluster-60aa0ca2-kafka-bootstrap.namespace-4.svc:9092 --max-messages 100 --group-instance-id instance1400639998
2022-03-30 16:58:52 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-30 16:58:52 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-03-30 16:58:52 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:283] Triggering CA cert renewal by adding the annotation
2022-03-30 16:58:52 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:295] Patching secret my-cluster-60aa0ca2-cluster-ca-cert with strimzi.io/force-renew
2022-03-30 16:58:52 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:295] Patching secret my-cluster-60aa0ca2-clients-ca-cert with strimzi.io/force-renew
2022-03-30 16:58:52 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:300] Wait for zk to rolling restart ...
2022-03-30 16:58:52 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-60aa0ca2-zookeeper rolling update
2022-03-30 16:58:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:74] Waiting for rolling update of component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-zookeeper, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})
2022-03-30 16:58:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for component with name my-cluster-60aa0ca2-zookeeper rolling update
2022-03-30 16:58:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-60aa0ca2-zookeeper-0=a4a5fd56-c10c-46c5-bfc5-860e04ba00a8, my-cluster-60aa0ca2-zookeeper-1=65ea52ce-8f80-4b37-a77e-f4824628e702, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:58:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-60aa0ca2-zookeeper-0=a4a5fd56-c10c-46c5-bfc5-860e04ba00a8, my-cluster-60aa0ca2-zookeeper-1=65ea52ce-8f80-4b37-a77e-f4824628e702, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:58:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-60aa0ca2-zookeeper-0=a4a5fd56-c10c-46c5-bfc5-860e04ba00a8, my-cluster-60aa0ca2-zookeeper-1=65ea52ce-8f80-4b37-a77e-f4824628e702, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:58:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-60aa0ca2-zookeeper-2 hasn't rolled
2022-03-30 16:58:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-60aa0ca2-zookeeper rolling update not ready, will try again in 5000 ms (1799996ms till timeout)
2022-03-30 16:58:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-60aa0ca2-zookeeper-0=a4a5fd56-c10c-46c5-bfc5-860e04ba00a8, my-cluster-60aa0ca2-zookeeper-1=65ea52ce-8f80-4b37-a77e-f4824628e702, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:58:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-60aa0ca2-zookeeper-0=a4a5fd56-c10c-46c5-bfc5-860e04ba00a8, my-cluster-60aa0ca2-zookeeper-1=65ea52ce-8f80-4b37-a77e-f4824628e702, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:58:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-60aa0ca2-zookeeper-0=a4a5fd56-c10c-46c5-bfc5-860e04ba00a8, my-cluster-60aa0ca2-zookeeper-1=65ea52ce-8f80-4b37-a77e-f4824628e702, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:58:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-60aa0ca2-zookeeper-2 hasn't rolled
2022-03-30 16:58:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-60aa0ca2-zookeeper rolling update not ready, will try again in 5000 ms (1794989ms till timeout)
2022-03-30 16:59:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-60aa0ca2-zookeeper-0=a4a5fd56-c10c-46c5-bfc5-860e04ba00a8, my-cluster-60aa0ca2-zookeeper-1=65ea52ce-8f80-4b37-a77e-f4824628e702, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-60aa0ca2-zookeeper-0=702e3dca-30a3-49fe-b6e6-2bdb73ad6a11, my-cluster-60aa0ca2-zookeeper-1=65ea52ce-8f80-4b37-a77e-f4824628e702, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-60aa0ca2-zookeeper-0=702e3dca-30a3-49fe-b6e6-2bdb73ad6a11, my-cluster-60aa0ca2-zookeeper-1=65ea52ce-8f80-4b37-a77e-f4824628e702, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-60aa0ca2-zookeeper-2 hasn't rolled
2022-03-30 16:59:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-60aa0ca2-zookeeper rolling update not ready, will try again in 5000 ms (1789984ms till timeout)
2022-03-30 16:59:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-60aa0ca2-zookeeper-0=a4a5fd56-c10c-46c5-bfc5-860e04ba00a8, my-cluster-60aa0ca2-zookeeper-1=65ea52ce-8f80-4b37-a77e-f4824628e702, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-60aa0ca2-zookeeper-0=702e3dca-30a3-49fe-b6e6-2bdb73ad6a11, my-cluster-60aa0ca2-zookeeper-1=65ea52ce-8f80-4b37-a77e-f4824628e702, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-60aa0ca2-zookeeper-0=702e3dca-30a3-49fe-b6e6-2bdb73ad6a11, my-cluster-60aa0ca2-zookeeper-1=65ea52ce-8f80-4b37-a77e-f4824628e702, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-60aa0ca2-zookeeper-2 hasn't rolled
2022-03-30 16:59:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-60aa0ca2-zookeeper rolling update not ready, will try again in 5000 ms (1784978ms till timeout)
2022-03-30 16:59:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-60aa0ca2-zookeeper-0=a4a5fd56-c10c-46c5-bfc5-860e04ba00a8, my-cluster-60aa0ca2-zookeeper-1=65ea52ce-8f80-4b37-a77e-f4824628e702, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-60aa0ca2-zookeeper-0=702e3dca-30a3-49fe-b6e6-2bdb73ad6a11, my-cluster-60aa0ca2-zookeeper-1=65ea52ce-8f80-4b37-a77e-f4824628e702, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-60aa0ca2-zookeeper-0=702e3dca-30a3-49fe-b6e6-2bdb73ad6a11, my-cluster-60aa0ca2-zookeeper-1=65ea52ce-8f80-4b37-a77e-f4824628e702, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-60aa0ca2-zookeeper-2 hasn't rolled
2022-03-30 16:59:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-60aa0ca2-zookeeper rolling update not ready, will try again in 5000 ms (1779973ms till timeout)
2022-03-30 16:59:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-60aa0ca2-zookeeper-0=a4a5fd56-c10c-46c5-bfc5-860e04ba00a8, my-cluster-60aa0ca2-zookeeper-1=65ea52ce-8f80-4b37-a77e-f4824628e702, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-60aa0ca2-zookeeper-0=702e3dca-30a3-49fe-b6e6-2bdb73ad6a11, my-cluster-60aa0ca2-zookeeper-1=65ea52ce-8f80-4b37-a77e-f4824628e702, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-60aa0ca2-zookeeper-0=702e3dca-30a3-49fe-b6e6-2bdb73ad6a11, my-cluster-60aa0ca2-zookeeper-1=65ea52ce-8f80-4b37-a77e-f4824628e702, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-60aa0ca2-zookeeper-2 hasn't rolled
2022-03-30 16:59:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-60aa0ca2-zookeeper rolling update not ready, will try again in 5000 ms (1774964ms till timeout)
2022-03-30 16:59:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-60aa0ca2-zookeeper-0=a4a5fd56-c10c-46c5-bfc5-860e04ba00a8, my-cluster-60aa0ca2-zookeeper-1=65ea52ce-8f80-4b37-a77e-f4824628e702, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-60aa0ca2-zookeeper-0=702e3dca-30a3-49fe-b6e6-2bdb73ad6a11, my-cluster-60aa0ca2-zookeeper-1=65ea52ce-8f80-4b37-a77e-f4824628e702, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-60aa0ca2-zookeeper-0=702e3dca-30a3-49fe-b6e6-2bdb73ad6a11, my-cluster-60aa0ca2-zookeeper-1=65ea52ce-8f80-4b37-a77e-f4824628e702, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-60aa0ca2-zookeeper-2 hasn't rolled
2022-03-30 16:59:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-60aa0ca2-zookeeper rolling update not ready, will try again in 5000 ms (1769959ms till timeout)
2022-03-30 16:59:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-60aa0ca2-zookeeper-0=a4a5fd56-c10c-46c5-bfc5-860e04ba00a8, my-cluster-60aa0ca2-zookeeper-1=65ea52ce-8f80-4b37-a77e-f4824628e702, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-60aa0ca2-zookeeper-0=702e3dca-30a3-49fe-b6e6-2bdb73ad6a11, my-cluster-60aa0ca2-zookeeper-1=65ea52ce-8f80-4b37-a77e-f4824628e702, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-60aa0ca2-zookeeper-0=702e3dca-30a3-49fe-b6e6-2bdb73ad6a11, my-cluster-60aa0ca2-zookeeper-1=65ea52ce-8f80-4b37-a77e-f4824628e702, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-60aa0ca2-zookeeper-2 hasn't rolled
2022-03-30 16:59:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-60aa0ca2-zookeeper rolling update not ready, will try again in 5000 ms (1764953ms till timeout)
2022-03-30 16:59:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-60aa0ca2-zookeeper-0=a4a5fd56-c10c-46c5-bfc5-860e04ba00a8, my-cluster-60aa0ca2-zookeeper-1=65ea52ce-8f80-4b37-a77e-f4824628e702, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-60aa0ca2-zookeeper-0=702e3dca-30a3-49fe-b6e6-2bdb73ad6a11, my-cluster-60aa0ca2-zookeeper-1=cca60113-e990-4f02-abd0-7400110a6e26, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-60aa0ca2-zookeeper-0=702e3dca-30a3-49fe-b6e6-2bdb73ad6a11, my-cluster-60aa0ca2-zookeeper-1=cca60113-e990-4f02-abd0-7400110a6e26, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-60aa0ca2-zookeeper-2 hasn't rolled
2022-03-30 16:59:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-60aa0ca2-zookeeper rolling update not ready, will try again in 5000 ms (1759947ms till timeout)
2022-03-30 16:59:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-60aa0ca2-zookeeper-0=a4a5fd56-c10c-46c5-bfc5-860e04ba00a8, my-cluster-60aa0ca2-zookeeper-1=65ea52ce-8f80-4b37-a77e-f4824628e702, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-60aa0ca2-zookeeper-0=702e3dca-30a3-49fe-b6e6-2bdb73ad6a11, my-cluster-60aa0ca2-zookeeper-1=cca60113-e990-4f02-abd0-7400110a6e26, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-60aa0ca2-zookeeper-0=702e3dca-30a3-49fe-b6e6-2bdb73ad6a11, my-cluster-60aa0ca2-zookeeper-1=cca60113-e990-4f02-abd0-7400110a6e26, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-60aa0ca2-zookeeper-2 hasn't rolled
2022-03-30 16:59:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-60aa0ca2-zookeeper rolling update not ready, will try again in 5000 ms (1754942ms till timeout)
2022-03-30 16:59:42 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-60aa0ca2-zookeeper-0=a4a5fd56-c10c-46c5-bfc5-860e04ba00a8, my-cluster-60aa0ca2-zookeeper-1=65ea52ce-8f80-4b37-a77e-f4824628e702, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:42 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-60aa0ca2-zookeeper-0=702e3dca-30a3-49fe-b6e6-2bdb73ad6a11, my-cluster-60aa0ca2-zookeeper-1=cca60113-e990-4f02-abd0-7400110a6e26, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:42 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-60aa0ca2-zookeeper-0=702e3dca-30a3-49fe-b6e6-2bdb73ad6a11, my-cluster-60aa0ca2-zookeeper-1=cca60113-e990-4f02-abd0-7400110a6e26, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:42 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-60aa0ca2-zookeeper-2 hasn't rolled
2022-03-30 16:59:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-60aa0ca2-zookeeper rolling update not ready, will try again in 5000 ms (1749936ms till timeout)
2022-03-30 16:59:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-60aa0ca2-zookeeper-0=a4a5fd56-c10c-46c5-bfc5-860e04ba00a8, my-cluster-60aa0ca2-zookeeper-1=65ea52ce-8f80-4b37-a77e-f4824628e702, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-60aa0ca2-zookeeper-0=702e3dca-30a3-49fe-b6e6-2bdb73ad6a11, my-cluster-60aa0ca2-zookeeper-1=cca60113-e990-4f02-abd0-7400110a6e26, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-60aa0ca2-zookeeper-0=702e3dca-30a3-49fe-b6e6-2bdb73ad6a11, my-cluster-60aa0ca2-zookeeper-1=cca60113-e990-4f02-abd0-7400110a6e26, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-60aa0ca2-zookeeper-2 hasn't rolled
2022-03-30 16:59:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-60aa0ca2-zookeeper rolling update not ready, will try again in 5000 ms (1744930ms till timeout)
2022-03-30 16:59:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-60aa0ca2-zookeeper-0=a4a5fd56-c10c-46c5-bfc5-860e04ba00a8, my-cluster-60aa0ca2-zookeeper-1=65ea52ce-8f80-4b37-a77e-f4824628e702, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-60aa0ca2-zookeeper-0=702e3dca-30a3-49fe-b6e6-2bdb73ad6a11, my-cluster-60aa0ca2-zookeeper-1=cca60113-e990-4f02-abd0-7400110a6e26, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-60aa0ca2-zookeeper-0=702e3dca-30a3-49fe-b6e6-2bdb73ad6a11, my-cluster-60aa0ca2-zookeeper-1=cca60113-e990-4f02-abd0-7400110a6e26, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-60aa0ca2-zookeeper-2 hasn't rolled
2022-03-30 16:59:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-60aa0ca2-zookeeper rolling update not ready, will try again in 5000 ms (1739924ms till timeout)
2022-03-30 16:59:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-60aa0ca2-zookeeper-0=a4a5fd56-c10c-46c5-bfc5-860e04ba00a8, my-cluster-60aa0ca2-zookeeper-1=65ea52ce-8f80-4b37-a77e-f4824628e702, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-60aa0ca2-zookeeper-0=702e3dca-30a3-49fe-b6e6-2bdb73ad6a11, my-cluster-60aa0ca2-zookeeper-1=cca60113-e990-4f02-abd0-7400110a6e26, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-60aa0ca2-zookeeper-0=702e3dca-30a3-49fe-b6e6-2bdb73ad6a11, my-cluster-60aa0ca2-zookeeper-1=cca60113-e990-4f02-abd0-7400110a6e26, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 16:59:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-60aa0ca2-zookeeper-2 hasn't rolled
2022-03-30 16:59:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-60aa0ca2-zookeeper rolling update not ready, will try again in 5000 ms (1734919ms till timeout)
2022-03-30 17:00:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-60aa0ca2-zookeeper-0=a4a5fd56-c10c-46c5-bfc5-860e04ba00a8, my-cluster-60aa0ca2-zookeeper-1=65ea52ce-8f80-4b37-a77e-f4824628e702, my-cluster-60aa0ca2-zookeeper-2=3e09d535-75f5-499a-be78-e9b3a33c5af5}
2022-03-30 17:00:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-60aa0ca2-zookeeper-0=702e3dca-30a3-49fe-b6e6-2bdb73ad6a11, my-cluster-60aa0ca2-zookeeper-1=cca60113-e990-4f02-abd0-7400110a6e26, my-cluster-60aa0ca2-zookeeper-2=a00c1ecf-4237-4be6-be08-48ca1007c362}
2022-03-30 17:00:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-60aa0ca2-zookeeper-0=702e3dca-30a3-49fe-b6e6-2bdb73ad6a11, my-cluster-60aa0ca2-zookeeper-1=cca60113-e990-4f02-abd0-7400110a6e26, my-cluster-60aa0ca2-zookeeper-2=a00c1ecf-4237-4be6-be08-48ca1007c362}
2022-03-30 17:00:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:55] All pods seem to have rolled
2022-03-30 17:00:03 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-60aa0ca2-zookeeper has been successfully rolled
2022-03-30 17:00:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:87] Component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-zookeeper, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={}) successfully rolled
2022-03-30 17:00:03 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-60aa0ca2-zookeeper to be ready
2022-03-30 17:00:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-zookeeper, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-30 17:00:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-0 not ready: zookeeper)
2022-03-30 17:00:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-1 not ready: zookeeper)
2022-03-30 17:00:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-zookeeper-2)
2022-03-30 17:00:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-zookeeper, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1799996ms till timeout)
2022-03-30 17:00:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-0 not ready: zookeeper)
2022-03-30 17:00:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-1 not ready: zookeeper)
2022-03-30 17:00:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-zookeeper-2)
2022-03-30 17:00:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-zookeeper, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1798991ms till timeout)
2022-03-30 17:00:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-0 not ready: zookeeper)
2022-03-30 17:00:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-1 not ready: zookeeper)
2022-03-30 17:00:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-zookeeper-2)
2022-03-30 17:00:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-zookeeper, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1797986ms till timeout)
2022-03-30 17:00:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-0 not ready: zookeeper)
2022-03-30 17:00:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-1 not ready: zookeeper)
2022-03-30 17:00:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-zookeeper-2)
2022-03-30 17:00:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-zookeeper, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1796980ms till timeout)
2022-03-30 17:00:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-0 not ready: zookeeper)
2022-03-30 17:00:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-1 not ready: zookeeper)
2022-03-30 17:00:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-zookeeper-2)
2022-03-30 17:00:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-zookeeper, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1795975ms till timeout)
2022-03-30 17:00:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-0 not ready: zookeeper)
2022-03-30 17:00:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-1 not ready: zookeeper)
2022-03-30 17:00:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-zookeeper-2)
2022-03-30 17:00:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-zookeeper, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1794970ms till timeout)
2022-03-30 17:00:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-0 not ready: zookeeper)
2022-03-30 17:00:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-1 not ready: zookeeper)
2022-03-30 17:00:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-zookeeper-2)
2022-03-30 17:00:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-zookeeper, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1793964ms till timeout)
2022-03-30 17:00:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-0 not ready: zookeeper)
2022-03-30 17:00:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-1 not ready: zookeeper)
2022-03-30 17:00:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-zookeeper-2)
2022-03-30 17:00:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-zookeeper, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1792959ms till timeout)
2022-03-30 17:00:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-0 not ready: zookeeper)
2022-03-30 17:00:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-1 not ready: zookeeper)
2022-03-30 17:00:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-zookeeper-2)
2022-03-30 17:00:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-zookeeper, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1791954ms till timeout)
2022-03-30 17:00:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-0 not ready: zookeeper)
2022-03-30 17:00:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-1 not ready: zookeeper)
2022-03-30 17:00:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-zookeeper-2)
2022-03-30 17:00:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-zookeeper, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1790948ms till timeout)
2022-03-30 17:00:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-0 not ready: zookeeper)
2022-03-30 17:00:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-1 not ready: zookeeper)
2022-03-30 17:00:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-zookeeper-2)
2022-03-30 17:00:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-zookeeper, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1789943ms till timeout)
2022-03-30 17:00:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-0 not ready: zookeeper)
2022-03-30 17:00:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-1 not ready: zookeeper)
2022-03-30 17:00:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-zookeeper-2)
2022-03-30 17:00:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-zookeeper, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1788938ms till timeout)
2022-03-30 17:00:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-0 not ready: zookeeper)
2022-03-30 17:00:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-1 not ready: zookeeper)
2022-03-30 17:00:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-zookeeper-2)
2022-03-30 17:00:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-zookeeper, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1787933ms till timeout)
2022-03-30 17:00:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-0 not ready: zookeeper)
2022-03-30 17:00:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-1 not ready: zookeeper)
2022-03-30 17:00:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-zookeeper-2)
2022-03-30 17:00:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-zookeeper, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1786928ms till timeout)
2022-03-30 17:00:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-0 not ready: zookeeper)
2022-03-30 17:00:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-1 not ready: zookeeper)
2022-03-30 17:00:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-zookeeper-2)
2022-03-30 17:00:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-zookeeper, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1785922ms till timeout)
2022-03-30 17:00:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-0 not ready: zookeeper)
2022-03-30 17:00:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-1 not ready: zookeeper)
2022-03-30 17:00:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-zookeeper-2)
2022-03-30 17:00:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-zookeeper, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1784917ms till timeout)
2022-03-30 17:00:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-0 not ready: zookeeper)
2022-03-30 17:00:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-1 not ready: zookeeper)
2022-03-30 17:00:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-zookeeper-2)
2022-03-30 17:00:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-zookeeper, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1783912ms till timeout)
2022-03-30 17:00:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-0 not ready: zookeeper)
2022-03-30 17:00:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-1 not ready: zookeeper)
2022-03-30 17:00:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-zookeeper-2)
2022-03-30 17:00:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-zookeeper, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1782907ms till timeout)
2022-03-30 17:00:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-0 not ready: zookeeper)
2022-03-30 17:00:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-1 not ready: zookeeper)
2022-03-30 17:00:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-zookeeper-2)
2022-03-30 17:00:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-zookeeper, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1781901ms till timeout)
2022-03-30 17:00:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-0 not ready: zookeeper)
2022-03-30 17:00:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-1 not ready: zookeeper)
2022-03-30 17:00:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-2 not ready: zookeeper)
2022-03-30 17:00:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-zookeeper-0, my-cluster-60aa0ca2-zookeeper-1, my-cluster-60aa0ca2-zookeeper-2 are ready
2022-03-30 17:00:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-zookeeper, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1780895ms till timeout)
2022-03-30 17:00:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-0 not ready: zookeeper)
2022-03-30 17:00:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-1 not ready: zookeeper)
2022-03-30 17:00:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-2 not ready: zookeeper)
2022-03-30 17:00:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-zookeeper-0, my-cluster-60aa0ca2-zookeeper-1, my-cluster-60aa0ca2-zookeeper-2 are ready
2022-03-30 17:00:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-zookeeper, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1779891ms till timeout)
2022-03-30 17:00:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-0 not ready: zookeeper)
2022-03-30 17:00:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-1 not ready: zookeeper)
2022-03-30 17:00:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-2 not ready: zookeeper)
2022-03-30 17:00:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-zookeeper-0, my-cluster-60aa0ca2-zookeeper-1, my-cluster-60aa0ca2-zookeeper-2 are ready
2022-03-30 17:00:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-zookeeper, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1778884ms till timeout)
2022-03-30 17:00:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-0 not ready: zookeeper)
2022-03-30 17:00:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-1 not ready: zookeeper)
2022-03-30 17:00:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-2 not ready: zookeeper)
2022-03-30 17:00:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-zookeeper-0, my-cluster-60aa0ca2-zookeeper-1, my-cluster-60aa0ca2-zookeeper-2 are ready
2022-03-30 17:00:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-zookeeper, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1777878ms till timeout)
2022-03-30 17:00:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-0 not ready: zookeeper)
2022-03-30 17:00:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-1 not ready: zookeeper)
2022-03-30 17:00:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-2 not ready: zookeeper)
2022-03-30 17:00:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-zookeeper-0, my-cluster-60aa0ca2-zookeeper-1, my-cluster-60aa0ca2-zookeeper-2 are ready
2022-03-30 17:00:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-zookeeper, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1776873ms till timeout)
2022-03-30 17:00:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-0 not ready: zookeeper)
2022-03-30 17:00:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-1 not ready: zookeeper)
2022-03-30 17:00:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-2 not ready: zookeeper)
2022-03-30 17:00:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-zookeeper-0, my-cluster-60aa0ca2-zookeeper-1, my-cluster-60aa0ca2-zookeeper-2 are ready
2022-03-30 17:00:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-zookeeper, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1775868ms till timeout)
2022-03-30 17:00:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-0 not ready: zookeeper)
2022-03-30 17:00:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-1 not ready: zookeeper)
2022-03-30 17:00:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-2 not ready: zookeeper)
2022-03-30 17:00:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-zookeeper-0, my-cluster-60aa0ca2-zookeeper-1, my-cluster-60aa0ca2-zookeeper-2 are ready
2022-03-30 17:00:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-zookeeper, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1774863ms till timeout)
2022-03-30 17:00:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-0 not ready: zookeeper)
2022-03-30 17:00:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-1 not ready: zookeeper)
2022-03-30 17:00:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-2 not ready: zookeeper)
2022-03-30 17:00:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-zookeeper-0, my-cluster-60aa0ca2-zookeeper-1, my-cluster-60aa0ca2-zookeeper-2 are ready
2022-03-30 17:00:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-zookeeper, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1773858ms till timeout)
2022-03-30 17:00:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-0 not ready: zookeeper)
2022-03-30 17:00:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-1 not ready: zookeeper)
2022-03-30 17:00:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-2 not ready: zookeeper)
2022-03-30 17:00:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-zookeeper-0, my-cluster-60aa0ca2-zookeeper-1, my-cluster-60aa0ca2-zookeeper-2 are ready
2022-03-30 17:00:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-zookeeper, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1772852ms till timeout)
2022-03-30 17:00:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-0 not ready: zookeeper)
2022-03-30 17:00:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-1 not ready: zookeeper)
2022-03-30 17:00:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-2 not ready: zookeeper)
2022-03-30 17:00:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-zookeeper-0, my-cluster-60aa0ca2-zookeeper-1, my-cluster-60aa0ca2-zookeeper-2 are ready
2022-03-30 17:00:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-zookeeper, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1771847ms till timeout)
2022-03-30 17:00:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-0 not ready: zookeeper)
2022-03-30 17:00:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-1 not ready: zookeeper)
2022-03-30 17:00:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-zookeeper-2 not ready: zookeeper)
2022-03-30 17:00:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-zookeeper-0, my-cluster-60aa0ca2-zookeeper-1, my-cluster-60aa0ca2-zookeeper-2 are ready
2022-03-30 17:00:32 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:304] Wait for kafka to rolling restart ...
2022-03-30 17:00:32 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-60aa0ca2-kafka rolling update
2022-03-30 17:00:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:74] Waiting for rolling update of component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})
2022-03-30 17:00:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for component with name my-cluster-60aa0ca2-kafka rolling update
2022-03-30 17:00:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=74858ba5-7992-4d52-a1a7-b86ed584fe12, my-cluster-60aa0ca2-kafka-2=1e311bfd-8f8b-4906-b7be-9d4a7580cca6}
2022-03-30 17:00:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=68bbfa47-5a73-41a6-96fd-9e9c64023a38, my-cluster-60aa0ca2-kafka-2=1e311bfd-8f8b-4906-b7be-9d4a7580cca6}
2022-03-30 17:00:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=68bbfa47-5a73-41a6-96fd-9e9c64023a38, my-cluster-60aa0ca2-kafka-2=1e311bfd-8f8b-4906-b7be-9d4a7580cca6}
2022-03-30 17:00:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-60aa0ca2-kafka-2 hasn't rolled
2022-03-30 17:00:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-60aa0ca2-kafka rolling update not ready, will try again in 5000 ms (1799996ms till timeout)
2022-03-30 17:00:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=74858ba5-7992-4d52-a1a7-b86ed584fe12, my-cluster-60aa0ca2-kafka-2=1e311bfd-8f8b-4906-b7be-9d4a7580cca6}
2022-03-30 17:00:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=68bbfa47-5a73-41a6-96fd-9e9c64023a38, my-cluster-60aa0ca2-kafka-2=1e311bfd-8f8b-4906-b7be-9d4a7580cca6}
2022-03-30 17:00:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=68bbfa47-5a73-41a6-96fd-9e9c64023a38, my-cluster-60aa0ca2-kafka-2=1e311bfd-8f8b-4906-b7be-9d4a7580cca6}
2022-03-30 17:00:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-60aa0ca2-kafka-2 hasn't rolled
2022-03-30 17:00:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-60aa0ca2-kafka rolling update not ready, will try again in 5000 ms (1794990ms till timeout)
2022-03-30 17:00:42 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=74858ba5-7992-4d52-a1a7-b86ed584fe12, my-cluster-60aa0ca2-kafka-2=1e311bfd-8f8b-4906-b7be-9d4a7580cca6}
2022-03-30 17:00:42 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=68bbfa47-5a73-41a6-96fd-9e9c64023a38, my-cluster-60aa0ca2-kafka-2=1e311bfd-8f8b-4906-b7be-9d4a7580cca6}
2022-03-30 17:00:42 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=68bbfa47-5a73-41a6-96fd-9e9c64023a38, my-cluster-60aa0ca2-kafka-2=1e311bfd-8f8b-4906-b7be-9d4a7580cca6}
2022-03-30 17:00:42 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-60aa0ca2-kafka-2 hasn't rolled
2022-03-30 17:00:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-60aa0ca2-kafka rolling update not ready, will try again in 5000 ms (1789985ms till timeout)
2022-03-30 17:00:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=74858ba5-7992-4d52-a1a7-b86ed584fe12, my-cluster-60aa0ca2-kafka-2=1e311bfd-8f8b-4906-b7be-9d4a7580cca6}
2022-03-30 17:00:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=68bbfa47-5a73-41a6-96fd-9e9c64023a38, my-cluster-60aa0ca2-kafka-2=1e311bfd-8f8b-4906-b7be-9d4a7580cca6}
2022-03-30 17:00:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=68bbfa47-5a73-41a6-96fd-9e9c64023a38, my-cluster-60aa0ca2-kafka-2=1e311bfd-8f8b-4906-b7be-9d4a7580cca6}
2022-03-30 17:00:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-60aa0ca2-kafka-2 hasn't rolled
2022-03-30 17:00:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-60aa0ca2-kafka rolling update not ready, will try again in 5000 ms (1784979ms till timeout)
2022-03-30 17:00:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=74858ba5-7992-4d52-a1a7-b86ed584fe12, my-cluster-60aa0ca2-kafka-2=1e311bfd-8f8b-4906-b7be-9d4a7580cca6}
2022-03-30 17:00:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=68bbfa47-5a73-41a6-96fd-9e9c64023a38, my-cluster-60aa0ca2-kafka-2=1e311bfd-8f8b-4906-b7be-9d4a7580cca6}
2022-03-30 17:00:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=68bbfa47-5a73-41a6-96fd-9e9c64023a38, my-cluster-60aa0ca2-kafka-2=1e311bfd-8f8b-4906-b7be-9d4a7580cca6}
2022-03-30 17:00:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-60aa0ca2-kafka-2 hasn't rolled
2022-03-30 17:00:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-60aa0ca2-kafka rolling update not ready, will try again in 5000 ms (1779973ms till timeout)
2022-03-30 17:00:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=74858ba5-7992-4d52-a1a7-b86ed584fe12, my-cluster-60aa0ca2-kafka-2=1e311bfd-8f8b-4906-b7be-9d4a7580cca6}
2022-03-30 17:00:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=68bbfa47-5a73-41a6-96fd-9e9c64023a38, my-cluster-60aa0ca2-kafka-2=1e311bfd-8f8b-4906-b7be-9d4a7580cca6}
2022-03-30 17:00:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=68bbfa47-5a73-41a6-96fd-9e9c64023a38, my-cluster-60aa0ca2-kafka-2=1e311bfd-8f8b-4906-b7be-9d4a7580cca6}
2022-03-30 17:00:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-60aa0ca2-kafka-2 hasn't rolled
2022-03-30 17:00:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-60aa0ca2-kafka rolling update not ready, will try again in 5000 ms (1774968ms till timeout)
2022-03-30 17:01:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=74858ba5-7992-4d52-a1a7-b86ed584fe12, my-cluster-60aa0ca2-kafka-2=1e311bfd-8f8b-4906-b7be-9d4a7580cca6}
2022-03-30 17:01:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=68bbfa47-5a73-41a6-96fd-9e9c64023a38, my-cluster-60aa0ca2-kafka-2=9202430e-454f-4342-b539-1ddf3b0ec90f}
2022-03-30 17:01:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=68bbfa47-5a73-41a6-96fd-9e9c64023a38, my-cluster-60aa0ca2-kafka-2=9202430e-454f-4342-b539-1ddf3b0ec90f}
2022-03-30 17:01:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-60aa0ca2-kafka-0 hasn't rolled
2022-03-30 17:01:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-60aa0ca2-kafka rolling update not ready, will try again in 5000 ms (1769962ms till timeout)
2022-03-30 17:01:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=74858ba5-7992-4d52-a1a7-b86ed584fe12, my-cluster-60aa0ca2-kafka-2=1e311bfd-8f8b-4906-b7be-9d4a7580cca6}
2022-03-30 17:01:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=68bbfa47-5a73-41a6-96fd-9e9c64023a38, my-cluster-60aa0ca2-kafka-2=9202430e-454f-4342-b539-1ddf3b0ec90f}
2022-03-30 17:01:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=68bbfa47-5a73-41a6-96fd-9e9c64023a38, my-cluster-60aa0ca2-kafka-2=9202430e-454f-4342-b539-1ddf3b0ec90f}
2022-03-30 17:01:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-60aa0ca2-kafka-0 hasn't rolled
2022-03-30 17:01:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-60aa0ca2-kafka rolling update not ready, will try again in 5000 ms (1764957ms till timeout)
2022-03-30 17:01:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=74858ba5-7992-4d52-a1a7-b86ed584fe12, my-cluster-60aa0ca2-kafka-2=1e311bfd-8f8b-4906-b7be-9d4a7580cca6}
2022-03-30 17:01:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=68bbfa47-5a73-41a6-96fd-9e9c64023a38, my-cluster-60aa0ca2-kafka-2=9202430e-454f-4342-b539-1ddf3b0ec90f}
2022-03-30 17:01:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=68bbfa47-5a73-41a6-96fd-9e9c64023a38, my-cluster-60aa0ca2-kafka-2=9202430e-454f-4342-b539-1ddf3b0ec90f}
2022-03-30 17:01:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-60aa0ca2-kafka-0 hasn't rolled
2022-03-30 17:01:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-60aa0ca2-kafka rolling update not ready, will try again in 5000 ms (1759951ms till timeout)
2022-03-30 17:01:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=74858ba5-7992-4d52-a1a7-b86ed584fe12, my-cluster-60aa0ca2-kafka-2=1e311bfd-8f8b-4906-b7be-9d4a7580cca6}
2022-03-30 17:01:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=68bbfa47-5a73-41a6-96fd-9e9c64023a38, my-cluster-60aa0ca2-kafka-2=9202430e-454f-4342-b539-1ddf3b0ec90f}
2022-03-30 17:01:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=68bbfa47-5a73-41a6-96fd-9e9c64023a38, my-cluster-60aa0ca2-kafka-2=9202430e-454f-4342-b539-1ddf3b0ec90f}
2022-03-30 17:01:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-60aa0ca2-kafka-0 hasn't rolled
2022-03-30 17:01:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-60aa0ca2-kafka rolling update not ready, will try again in 5000 ms (1754946ms till timeout)
2022-03-30 17:01:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=74858ba5-7992-4d52-a1a7-b86ed584fe12, my-cluster-60aa0ca2-kafka-2=1e311bfd-8f8b-4906-b7be-9d4a7580cca6}
2022-03-30 17:01:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=68bbfa47-5a73-41a6-96fd-9e9c64023a38, my-cluster-60aa0ca2-kafka-2=9202430e-454f-4342-b539-1ddf3b0ec90f}
2022-03-30 17:01:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=68bbfa47-5a73-41a6-96fd-9e9c64023a38, my-cluster-60aa0ca2-kafka-2=9202430e-454f-4342-b539-1ddf3b0ec90f}
2022-03-30 17:01:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-60aa0ca2-kafka-0 hasn't rolled
2022-03-30 17:01:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-60aa0ca2-kafka rolling update not ready, will try again in 5000 ms (1749941ms till timeout)
2022-03-30 17:01:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=74858ba5-7992-4d52-a1a7-b86ed584fe12, my-cluster-60aa0ca2-kafka-2=1e311bfd-8f8b-4906-b7be-9d4a7580cca6}
2022-03-30 17:01:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=68bbfa47-5a73-41a6-96fd-9e9c64023a38, my-cluster-60aa0ca2-kafka-2=9202430e-454f-4342-b539-1ddf3b0ec90f}
2022-03-30 17:01:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=68bbfa47-5a73-41a6-96fd-9e9c64023a38, my-cluster-60aa0ca2-kafka-2=9202430e-454f-4342-b539-1ddf3b0ec90f}
2022-03-30 17:01:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-60aa0ca2-kafka-0 hasn't rolled
2022-03-30 17:01:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-60aa0ca2-kafka rolling update not ready, will try again in 5000 ms (1744935ms till timeout)
2022-03-30 17:01:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-60aa0ca2-kafka-0=683782ce-c8c4-44b9-b03b-297f0aeb786f, my-cluster-60aa0ca2-kafka-1=74858ba5-7992-4d52-a1a7-b86ed584fe12, my-cluster-60aa0ca2-kafka-2=1e311bfd-8f8b-4906-b7be-9d4a7580cca6}
2022-03-30 17:01:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-60aa0ca2-kafka-0=7ab40baf-8f63-4fe0-ad68-a2d28afd1fb9, my-cluster-60aa0ca2-kafka-1=68bbfa47-5a73-41a6-96fd-9e9c64023a38, my-cluster-60aa0ca2-kafka-2=9202430e-454f-4342-b539-1ddf3b0ec90f}
2022-03-30 17:01:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-60aa0ca2-kafka-0=7ab40baf-8f63-4fe0-ad68-a2d28afd1fb9, my-cluster-60aa0ca2-kafka-1=68bbfa47-5a73-41a6-96fd-9e9c64023a38, my-cluster-60aa0ca2-kafka-2=9202430e-454f-4342-b539-1ddf3b0ec90f}
2022-03-30 17:01:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:55] All pods seem to have rolled
2022-03-30 17:01:32 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-60aa0ca2-kafka has been successfully rolled
2022-03-30 17:01:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:87] Component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={}) successfully rolled
2022-03-30 17:01:32 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-60aa0ca2-kafka to be ready
2022-03-30 17:01:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-30 17:01:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-kafka-0)
2022-03-30 17:01:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1799996ms till timeout)
2022-03-30 17:01:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-kafka-0)
2022-03-30 17:01:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1798991ms till timeout)
2022-03-30 17:01:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-kafka-0)
2022-03-30 17:01:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1797986ms till timeout)
2022-03-30 17:01:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-kafka-0)
2022-03-30 17:01:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1796981ms till timeout)
2022-03-30 17:01:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-kafka-0)
2022-03-30 17:01:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1795975ms till timeout)
2022-03-30 17:01:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-kafka-0)
2022-03-30 17:01:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1794970ms till timeout)
2022-03-30 17:01:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-kafka-0)
2022-03-30 17:01:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1793965ms till timeout)
2022-03-30 17:01:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-kafka-0)
2022-03-30 17:01:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1792960ms till timeout)
2022-03-30 17:01:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-kafka-0)
2022-03-30 17:01:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1791954ms till timeout)
2022-03-30 17:01:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-kafka-0)
2022-03-30 17:01:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1790949ms till timeout)
2022-03-30 17:01:42 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-kafka-0)
2022-03-30 17:01:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1789944ms till timeout)
2022-03-30 17:01:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-kafka-0)
2022-03-30 17:01:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1788939ms till timeout)
2022-03-30 17:01:44 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-kafka-0)
2022-03-30 17:01:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1787934ms till timeout)
2022-03-30 17:01:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-kafka-0)
2022-03-30 17:01:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1786929ms till timeout)
2022-03-30 17:01:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-kafka-0)
2022-03-30 17:01:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1785925ms till timeout)
2022-03-30 17:01:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-kafka-0)
2022-03-30 17:01:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1784920ms till timeout)
2022-03-30 17:01:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-kafka-0)
2022-03-30 17:01:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1783915ms till timeout)
2022-03-30 17:01:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-kafka-0)
2022-03-30 17:01:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1782909ms till timeout)
2022-03-30 17:01:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-kafka-0)
2022-03-30 17:01:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1781904ms till timeout)
2022-03-30 17:01:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-kafka-0)
2022-03-30 17:01:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1780899ms till timeout)
2022-03-30 17:01:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-kafka-0)
2022-03-30 17:01:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1779894ms till timeout)
2022-03-30 17:01:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-60aa0ca2-kafka-0)
2022-03-30 17:01:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1778889ms till timeout)
2022-03-30 17:01:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-0 not ready: kafka)
2022-03-30 17:01:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-1 not ready: kafka)
2022-03-30 17:01:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-2 not ready: kafka)
2022-03-30 17:01:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-kafka-0, my-cluster-60aa0ca2-kafka-1, my-cluster-60aa0ca2-kafka-2 are ready
2022-03-30 17:01:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1777884ms till timeout)
2022-03-30 17:01:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-0 not ready: kafka)
2022-03-30 17:01:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-1 not ready: kafka)
2022-03-30 17:01:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-2 not ready: kafka)
2022-03-30 17:01:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-kafka-0, my-cluster-60aa0ca2-kafka-1, my-cluster-60aa0ca2-kafka-2 are ready
2022-03-30 17:01:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1776879ms till timeout)
2022-03-30 17:01:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-0 not ready: kafka)
2022-03-30 17:01:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-1 not ready: kafka)
2022-03-30 17:01:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-2 not ready: kafka)
2022-03-30 17:01:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-kafka-0, my-cluster-60aa0ca2-kafka-1, my-cluster-60aa0ca2-kafka-2 are ready
2022-03-30 17:01:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1775874ms till timeout)
2022-03-30 17:01:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-0 not ready: kafka)
2022-03-30 17:01:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-1 not ready: kafka)
2022-03-30 17:01:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-2 not ready: kafka)
2022-03-30 17:01:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-kafka-0, my-cluster-60aa0ca2-kafka-1, my-cluster-60aa0ca2-kafka-2 are ready
2022-03-30 17:01:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1774868ms till timeout)
2022-03-30 17:01:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-0 not ready: kafka)
2022-03-30 17:01:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-1 not ready: kafka)
2022-03-30 17:01:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-2 not ready: kafka)
2022-03-30 17:01:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-kafka-0, my-cluster-60aa0ca2-kafka-1, my-cluster-60aa0ca2-kafka-2 are ready
2022-03-30 17:01:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1773863ms till timeout)
2022-03-30 17:01:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-0 not ready: kafka)
2022-03-30 17:01:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-1 not ready: kafka)
2022-03-30 17:01:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-2 not ready: kafka)
2022-03-30 17:01:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-kafka-0, my-cluster-60aa0ca2-kafka-1, my-cluster-60aa0ca2-kafka-2 are ready
2022-03-30 17:01:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1772857ms till timeout)
2022-03-30 17:02:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-0 not ready: kafka)
2022-03-30 17:02:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-1 not ready: kafka)
2022-03-30 17:02:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-2 not ready: kafka)
2022-03-30 17:02:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-kafka-0, my-cluster-60aa0ca2-kafka-1, my-cluster-60aa0ca2-kafka-2 are ready
2022-03-30 17:02:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1771852ms till timeout)
2022-03-30 17:02:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-0 not ready: kafka)
2022-03-30 17:02:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-1 not ready: kafka)
2022-03-30 17:02:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-2 not ready: kafka)
2022-03-30 17:02:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-kafka-0, my-cluster-60aa0ca2-kafka-1, my-cluster-60aa0ca2-kafka-2 are ready
2022-03-30 17:02:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1770846ms till timeout)
2022-03-30 17:02:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-0 not ready: kafka)
2022-03-30 17:02:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-1 not ready: kafka)
2022-03-30 17:02:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-2 not ready: kafka)
2022-03-30 17:02:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-kafka-0, my-cluster-60aa0ca2-kafka-1, my-cluster-60aa0ca2-kafka-2 are ready
2022-03-30 17:02:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1769840ms till timeout)
2022-03-30 17:02:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-0 not ready: kafka)
2022-03-30 17:02:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-1 not ready: kafka)
2022-03-30 17:02:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-2 not ready: kafka)
2022-03-30 17:02:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-kafka-0, my-cluster-60aa0ca2-kafka-1, my-cluster-60aa0ca2-kafka-2 are ready
2022-03-30 17:02:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-60aa0ca2-kafka, strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1768835ms till timeout)
2022-03-30 17:02:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-0 not ready: kafka)
2022-03-30 17:02:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-1 not ready: kafka)
2022-03-30 17:02:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-2 not ready: kafka)
2022-03-30 17:02:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-kafka-0, my-cluster-60aa0ca2-kafka-1, my-cluster-60aa0ca2-kafka-2 are ready
2022-03-30 17:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:308] Wait for EO to rolling restart ...
2022-03-30 17:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-60aa0ca2-entity-operator rolling update
2022-03-30 17:02:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Deployment my-cluster-60aa0ca2-entity-operator rolling update in namespace:namespace-4
2022-03-30 17:02:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-60aa0ca2-entity-operator-744dcb966c-q4bxr=7ef052e5-1285-4fbf-8c5c-4f5e65fc94e5}
2022-03-30 17:02:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm=6fb3ca81-c97e-47a5-8ba8-a072d04fe2a7}
2022-03-30 17:02:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:119] All pods seem to have rolled
2022-03-30 17:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready
2022-03-30 17:02:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready
2022-03-30 17:02:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (479997ms till timeout)
2022-03-30 17:02:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (478994ms till timeout)
2022-03-30 17:02:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (477990ms till timeout)
2022-03-30 17:02:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (476987ms till timeout)
2022-03-30 17:02:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (475983ms till timeout)
2022-03-30 17:02:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (474980ms till timeout)
2022-03-30 17:02:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (473976ms till timeout)
2022-03-30 17:02:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (472972ms till timeout)
2022-03-30 17:02:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (471968ms till timeout)
2022-03-30 17:02:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (470965ms till timeout)
2022-03-30 17:02:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (469961ms till timeout)
2022-03-30 17:02:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (468958ms till timeout)
2022-03-30 17:02:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (467954ms till timeout)
2022-03-30 17:02:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (466951ms till timeout)
2022-03-30 17:02:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (465947ms till timeout)
2022-03-30 17:02:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (464944ms till timeout)
2022-03-30 17:02:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (463940ms till timeout)
2022-03-30 17:02:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (462936ms till timeout)
2022-03-30 17:02:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (461933ms till timeout)
2022-03-30 17:02:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (460929ms till timeout)
2022-03-30 17:02:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (459926ms till timeout)
2022-03-30 17:02:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (458922ms till timeout)
2022-03-30 17:02:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (457919ms till timeout)
2022-03-30 17:02:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (456915ms till timeout)
2022-03-30 17:02:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (455911ms till timeout)
2022-03-30 17:02:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (454908ms till timeout)
2022-03-30 17:02:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (453904ms till timeout)
2022-03-30 17:02:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (452901ms till timeout)
2022-03-30 17:02:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (451897ms till timeout)
2022-03-30 17:02:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (450894ms till timeout)
2022-03-30 17:02:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (449890ms till timeout)
2022-03-30 17:02:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (448887ms till timeout)
2022-03-30 17:02:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (447883ms till timeout)
2022-03-30 17:02:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (446880ms till timeout)
2022-03-30 17:02:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (445876ms till timeout)
2022-03-30 17:02:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (444872ms till timeout)
2022-03-30 17:02:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (443869ms till timeout)
2022-03-30 17:02:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (442865ms till timeout)
2022-03-30 17:02:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (441861ms till timeout)
2022-03-30 17:02:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (440857ms till timeout)
2022-03-30 17:02:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (439854ms till timeout)
2022-03-30 17:02:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (438850ms till timeout)
2022-03-30 17:02:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (437846ms till timeout)
2022-03-30 17:02:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (436843ms till timeout)
2022-03-30 17:02:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (435839ms till timeout)
2022-03-30 17:02:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (434836ms till timeout)
2022-03-30 17:02:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (433832ms till timeout)
2022-03-30 17:02:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (432829ms till timeout)
2022-03-30 17:02:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (431825ms till timeout)
2022-03-30 17:02:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (430821ms till timeout)
2022-03-30 17:02:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (429818ms till timeout)
2022-03-30 17:02:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (428814ms till timeout)
2022-03-30 17:02:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (427811ms till timeout)
2022-03-30 17:02:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (426807ms till timeout)
2022-03-30 17:02:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (425804ms till timeout)
2022-03-30 17:02:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (424800ms till timeout)
2022-03-30 17:03:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (423797ms till timeout)
2022-03-30 17:03:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (422793ms till timeout)
2022-03-30 17:03:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (421789ms till timeout)
2022-03-30 17:03:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (420786ms till timeout)
2022-03-30 17:03:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (419782ms till timeout)
2022-03-30 17:03:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (418778ms till timeout)
2022-03-30 17:03:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (417775ms till timeout)
2022-03-30 17:03:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (416771ms till timeout)
2022-03-30 17:03:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (415768ms till timeout)
2022-03-30 17:03:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (414764ms till timeout)
2022-03-30 17:03:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (413761ms till timeout)
2022-03-30 17:03:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (412757ms till timeout)
2022-03-30 17:03:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (411754ms till timeout)
2022-03-30 17:03:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (410750ms till timeout)
2022-03-30 17:03:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (409747ms till timeout)
2022-03-30 17:03:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (408743ms till timeout)
2022-03-30 17:03:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (407739ms till timeout)
2022-03-30 17:03:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (406736ms till timeout)
2022-03-30 17:03:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (405732ms till timeout)
2022-03-30 17:03:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (404729ms till timeout)
2022-03-30 17:03:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (403725ms till timeout)
2022-03-30 17:03:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (402721ms till timeout)
2022-03-30 17:03:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (401718ms till timeout)
2022-03-30 17:03:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (400714ms till timeout)
2022-03-30 17:03:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (399710ms till timeout)
2022-03-30 17:03:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (398706ms till timeout)
2022-03-30 17:03:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (397703ms till timeout)
2022-03-30 17:03:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-entity-operator will be ready not ready, will try again in 1000 ms (396700ms till timeout)
2022-03-30 17:03:28 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-60aa0ca2-entity-operator is ready
2022-03-30 17:03:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-60aa0ca2-entity-operator}, additionalProperties={})to be ready
2022-03-30 17:03:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm not ready: tls-sidecar)
2022-03-30 17:03:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm not ready: topic-operator)
2022-03-30 17:03:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm not ready: user-operator)
2022-03-30 17:03:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm are ready
2022-03-30 17:03:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-60aa0ca2-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599995ms till timeout)
2022-03-30 17:03:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm not ready: tls-sidecar)
2022-03-30 17:03:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm not ready: topic-operator)
2022-03-30 17:03:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm not ready: user-operator)
2022-03-30 17:03:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm are ready
2022-03-30 17:03:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-60aa0ca2-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598987ms till timeout)
2022-03-30 17:03:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm not ready: tls-sidecar)
2022-03-30 17:03:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm not ready: topic-operator)
2022-03-30 17:03:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm not ready: user-operator)
2022-03-30 17:03:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm are ready
2022-03-30 17:03:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-60aa0ca2-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597982ms till timeout)
2022-03-30 17:03:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm not ready: tls-sidecar)
2022-03-30 17:03:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm not ready: topic-operator)
2022-03-30 17:03:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm not ready: user-operator)
2022-03-30 17:03:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm are ready
2022-03-30 17:03:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-60aa0ca2-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596977ms till timeout)
2022-03-30 17:03:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm not ready: tls-sidecar)
2022-03-30 17:03:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm not ready: topic-operator)
2022-03-30 17:03:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm not ready: user-operator)
2022-03-30 17:03:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm are ready
2022-03-30 17:03:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-60aa0ca2-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595971ms till timeout)
2022-03-30 17:03:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm not ready: tls-sidecar)
2022-03-30 17:03:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm not ready: topic-operator)
2022-03-30 17:03:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm not ready: user-operator)
2022-03-30 17:03:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm are ready
2022-03-30 17:03:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-60aa0ca2-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594966ms till timeout)
2022-03-30 17:03:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm not ready: tls-sidecar)
2022-03-30 17:03:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm not ready: topic-operator)
2022-03-30 17:03:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm not ready: user-operator)
2022-03-30 17:03:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm are ready
2022-03-30 17:03:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-60aa0ca2-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593961ms till timeout)
2022-03-30 17:03:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm not ready: tls-sidecar)
2022-03-30 17:03:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm not ready: topic-operator)
2022-03-30 17:03:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm not ready: user-operator)
2022-03-30 17:03:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm are ready
2022-03-30 17:03:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-60aa0ca2-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592956ms till timeout)
2022-03-30 17:03:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm not ready: tls-sidecar)
2022-03-30 17:03:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm not ready: topic-operator)
2022-03-30 17:03:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm not ready: user-operator)
2022-03-30 17:03:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm are ready
2022-03-30 17:03:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-60aa0ca2-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591951ms till timeout)
2022-03-30 17:03:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm not ready: tls-sidecar)
2022-03-30 17:03:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm not ready: topic-operator)
2022-03-30 17:03:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm not ready: user-operator)
2022-03-30 17:03:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm are ready
2022-03-30 17:03:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-60aa0ca2-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590946ms till timeout)
2022-03-30 17:03:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm not ready: tls-sidecar)
2022-03-30 17:03:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm not ready: topic-operator)
2022-03-30 17:03:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm not ready: user-operator)
2022-03-30 17:03:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-entity-operator-6c6974b9fd-jc2lm are ready
2022-03-30 17:03:38 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-60aa0ca2-entity-operator rolling update finished
2022-03-30 17:03:38 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:312] Wait for CC and KE to rolling restart ...
2022-03-30 17:03:38 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-60aa0ca2-kafka-exporter rolling update
2022-03-30 17:03:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Deployment my-cluster-60aa0ca2-kafka-exporter rolling update in namespace:namespace-4
2022-03-30 17:03:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-60aa0ca2-kafka-exporter-65d9d467b5-5xgvl=764941a6-fde9-4c52-87a9-ac26fb79c8dc}
2022-03-30 17:03:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-60aa0ca2-kafka-exporter-65d9d467b5-5xgvl=764941a6-fde9-4c52-87a9-ac26fb79c8dc}
2022-03-30 17:03:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-60aa0ca2-kafka-exporter-65d9d467b5-5xgvl=764941a6-fde9-4c52-87a9-ac26fb79c8dc}
2022-03-30 17:03:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Deployment my-cluster-60aa0ca2-kafka-exporter rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (599995ms till timeout)
2022-03-30 17:03:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-60aa0ca2-kafka-exporter-65d9d467b5-5xgvl=764941a6-fde9-4c52-87a9-ac26fb79c8dc}
2022-03-30 17:03:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-60aa0ca2-kafka-exporter-65d9d467b5-5xgvl=764941a6-fde9-4c52-87a9-ac26fb79c8dc}
2022-03-30 17:03:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-60aa0ca2-kafka-exporter-65d9d467b5-5xgvl=764941a6-fde9-4c52-87a9-ac26fb79c8dc}
2022-03-30 17:03:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Deployment my-cluster-60aa0ca2-kafka-exporter rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (594987ms till timeout)
2022-03-30 17:03:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-60aa0ca2-kafka-exporter-65d9d467b5-5xgvl=764941a6-fde9-4c52-87a9-ac26fb79c8dc}
2022-03-30 17:03:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-60aa0ca2-kafka-exporter-65d9d467b5-5xgvl=764941a6-fde9-4c52-87a9-ac26fb79c8dc, my-cluster-60aa0ca2-kafka-exporter-68dd9c684b-xhdt6=d6077403-a7ec-4935-9445-daa4b4fec16e}
2022-03-30 17:03:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-60aa0ca2-kafka-exporter-65d9d467b5-5xgvl=764941a6-fde9-4c52-87a9-ac26fb79c8dc}
2022-03-30 17:03:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Deployment my-cluster-60aa0ca2-kafka-exporter rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (589980ms till timeout)
2022-03-30 17:03:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-60aa0ca2-kafka-exporter-65d9d467b5-5xgvl=764941a6-fde9-4c52-87a9-ac26fb79c8dc}
2022-03-30 17:03:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-60aa0ca2-kafka-exporter-65d9d467b5-5xgvl=764941a6-fde9-4c52-87a9-ac26fb79c8dc, my-cluster-60aa0ca2-kafka-exporter-68dd9c684b-xhdt6=d6077403-a7ec-4935-9445-daa4b4fec16e}
2022-03-30 17:03:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-60aa0ca2-kafka-exporter-65d9d467b5-5xgvl=764941a6-fde9-4c52-87a9-ac26fb79c8dc}
2022-03-30 17:03:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Deployment my-cluster-60aa0ca2-kafka-exporter rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (584972ms till timeout)
2022-03-30 17:03:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-60aa0ca2-kafka-exporter-65d9d467b5-5xgvl=764941a6-fde9-4c52-87a9-ac26fb79c8dc}
2022-03-30 17:03:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-60aa0ca2-kafka-exporter-65d9d467b5-5xgvl=764941a6-fde9-4c52-87a9-ac26fb79c8dc, my-cluster-60aa0ca2-kafka-exporter-68dd9c684b-xhdt6=d6077403-a7ec-4935-9445-daa4b4fec16e}
2022-03-30 17:03:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-60aa0ca2-kafka-exporter-65d9d467b5-5xgvl=764941a6-fde9-4c52-87a9-ac26fb79c8dc}
2022-03-30 17:03:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Deployment my-cluster-60aa0ca2-kafka-exporter rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (579965ms till timeout)
2022-03-30 17:04:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-60aa0ca2-kafka-exporter-65d9d467b5-5xgvl=764941a6-fde9-4c52-87a9-ac26fb79c8dc}
2022-03-30 17:04:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-60aa0ca2-kafka-exporter-65d9d467b5-5xgvl=764941a6-fde9-4c52-87a9-ac26fb79c8dc, my-cluster-60aa0ca2-kafka-exporter-68dd9c684b-xhdt6=d6077403-a7ec-4935-9445-daa4b4fec16e}
2022-03-30 17:04:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-60aa0ca2-kafka-exporter-65d9d467b5-5xgvl=764941a6-fde9-4c52-87a9-ac26fb79c8dc}
2022-03-30 17:04:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Deployment my-cluster-60aa0ca2-kafka-exporter rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (574957ms till timeout)
2022-03-30 17:04:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-60aa0ca2-kafka-exporter-65d9d467b5-5xgvl=764941a6-fde9-4c52-87a9-ac26fb79c8dc}
2022-03-30 17:04:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-60aa0ca2-kafka-exporter-65d9d467b5-5xgvl=764941a6-fde9-4c52-87a9-ac26fb79c8dc, my-cluster-60aa0ca2-kafka-exporter-68dd9c684b-xhdt6=d6077403-a7ec-4935-9445-daa4b4fec16e}
2022-03-30 17:04:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-60aa0ca2-kafka-exporter-65d9d467b5-5xgvl=764941a6-fde9-4c52-87a9-ac26fb79c8dc}
2022-03-30 17:04:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Deployment my-cluster-60aa0ca2-kafka-exporter rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (569950ms till timeout)
2022-03-30 17:04:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-60aa0ca2-kafka-exporter-65d9d467b5-5xgvl=764941a6-fde9-4c52-87a9-ac26fb79c8dc}
2022-03-30 17:04:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-60aa0ca2-kafka-exporter-68dd9c684b-xhdt6=d6077403-a7ec-4935-9445-daa4b4fec16e}
2022-03-30 17:04:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:119] All pods seem to have rolled
2022-03-30 17:04:13 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-60aa0ca2-kafka-exporter will be ready
2022-03-30 17:04:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-60aa0ca2-kafka-exporter will be ready
2022-03-30 17:04:13 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-60aa0ca2-kafka-exporter is ready
2022-03-30 17:04:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-60aa0ca2-kafka-exporter}, additionalProperties={})to be ready
2022-03-30 17:04:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-exporter-68dd9c684b-xhdt6 not ready: my-cluster-60aa0ca2-kafka-exporter)
2022-03-30 17:04:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-kafka-exporter-68dd9c684b-xhdt6 are ready
2022-03-30 17:04:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-60aa0ca2-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-30 17:04:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-exporter-68dd9c684b-xhdt6 not ready: my-cluster-60aa0ca2-kafka-exporter)
2022-03-30 17:04:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-kafka-exporter-68dd9c684b-xhdt6 are ready
2022-03-30 17:04:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-60aa0ca2-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598993ms till timeout)
2022-03-30 17:04:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-exporter-68dd9c684b-xhdt6 not ready: my-cluster-60aa0ca2-kafka-exporter)
2022-03-30 17:04:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-kafka-exporter-68dd9c684b-xhdt6 are ready
2022-03-30 17:04:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-60aa0ca2-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597988ms till timeout)
2022-03-30 17:04:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-exporter-68dd9c684b-xhdt6 not ready: my-cluster-60aa0ca2-kafka-exporter)
2022-03-30 17:04:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-kafka-exporter-68dd9c684b-xhdt6 are ready
2022-03-30 17:04:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-60aa0ca2-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596983ms till timeout)
2022-03-30 17:04:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-exporter-68dd9c684b-xhdt6 not ready: my-cluster-60aa0ca2-kafka-exporter)
2022-03-30 17:04:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-kafka-exporter-68dd9c684b-xhdt6 are ready
2022-03-30 17:04:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-60aa0ca2-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595978ms till timeout)
2022-03-30 17:04:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-exporter-68dd9c684b-xhdt6 not ready: my-cluster-60aa0ca2-kafka-exporter)
2022-03-30 17:04:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-kafka-exporter-68dd9c684b-xhdt6 are ready
2022-03-30 17:04:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-60aa0ca2-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594974ms till timeout)
2022-03-30 17:04:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-exporter-68dd9c684b-xhdt6 not ready: my-cluster-60aa0ca2-kafka-exporter)
2022-03-30 17:04:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-kafka-exporter-68dd9c684b-xhdt6 are ready
2022-03-30 17:04:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-60aa0ca2-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593969ms till timeout)
2022-03-30 17:04:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-exporter-68dd9c684b-xhdt6 not ready: my-cluster-60aa0ca2-kafka-exporter)
2022-03-30 17:04:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-kafka-exporter-68dd9c684b-xhdt6 are ready
2022-03-30 17:04:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-60aa0ca2-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592965ms till timeout)
2022-03-30 17:04:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-exporter-68dd9c684b-xhdt6 not ready: my-cluster-60aa0ca2-kafka-exporter)
2022-03-30 17:04:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-kafka-exporter-68dd9c684b-xhdt6 are ready
2022-03-30 17:04:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-60aa0ca2-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591960ms till timeout)
2022-03-30 17:04:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-exporter-68dd9c684b-xhdt6 not ready: my-cluster-60aa0ca2-kafka-exporter)
2022-03-30 17:04:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-kafka-exporter-68dd9c684b-xhdt6 are ready
2022-03-30 17:04:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-60aa0ca2, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-60aa0ca2-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590955ms till timeout)
2022-03-30 17:04:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-60aa0ca2-kafka-exporter-68dd9c684b-xhdt6 not ready: my-cluster-60aa0ca2-kafka-exporter)
2022-03-30 17:04:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-60aa0ca2-kafka-exporter-68dd9c684b-xhdt6 are ready
2022-03-30 17:04:23 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-60aa0ca2-kafka-exporter rolling update finished
2022-03-30 17:04:23 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:317] Checking the certificates have been replaced
2022-03-30 17:04:23 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:331] Checking consumed messages to pod:my-cluster-60aa0ca2-kafka-clients-5697d49f7b-dkw76
2022-03-30 17:04:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@52040c85, which are set.
2022-03-30 17:04:23 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@6742db7c, messages=[], arguments=[--group-id, my-consumer-group-1704074803, --topic, my-topic-1728678022-515015095, --bootstrap-server, my-cluster-60aa0ca2-kafka-bootstrap.namespace-4.svc:9092, --max-messages, 100, --group-instance-id, instance308637512], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-60aa0ca2-kafka-clients-5697d49f7b-dkw76', podNamespace='namespace-4', bootstrapServer='my-cluster-60aa0ca2-kafka-bootstrap.namespace-4.svc:9092', topicName='my-topic-1728678022-515015095', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1704074803', consumerInstanceId='instance308637512', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@52040c85}
2022-03-30 17:04:23 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-60aa0ca2-kafka-bootstrap.namespace-4.svc:9092#my-topic-1728678022-515015095 from pod my-cluster-60aa0ca2-kafka-clients-5697d49f7b-dkw76
2022-03-30 17:04:23 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-60aa0ca2-kafka-clients-5697d49f7b-dkw76 -n namespace-4 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1704074803 --topic my-topic-1728678022-515015095 --bootstrap-server my-cluster-60aa0ca2-kafka-bootstrap.namespace-4.svc:9092 --max-messages 100 --group-instance-id instance308637512
2022-03-30 17:04:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-60aa0ca2-kafka-clients-5697d49f7b-dkw76 -n namespace-4 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1704074803 --topic my-topic-1728678022-515015095 --bootstrap-server my-cluster-60aa0ca2-kafka-bootstrap.namespace-4.svc:9092 --max-messages 100 --group-instance-id instance308637512
2022-03-30 17:04:29 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-30 17:04:29 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-03-30 17:04:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser bob-my-cluster-60aa0ca2 in namespace namespace-4
2022-03-30 17:04:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-03-30 17:04:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:bob-my-cluster-60aa0ca2
2022-03-30 17:04:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: bob-my-cluster-60aa0ca2 will have desired state: Ready
2022-03-30 17:04:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: bob-my-cluster-60aa0ca2 will have desired state: Ready
2022-03-30 17:04:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaUser: bob-my-cluster-60aa0ca2 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 17:04:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: bob-my-cluster-60aa0ca2 is in desired state: Ready
2022-03-30 17:04:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-60aa0ca2-kafka-clients-tls in namespace namespace-4
2022-03-30 17:04:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-03-30 17:04:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-60aa0ca2-kafka-clients-tls
2022-03-30 17:04:30 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-60aa0ca2-kafka-clients-tls will be ready
2022-03-30 17:04:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-60aa0ca2-kafka-clients-tls will be ready
2022-03-30 17:04:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-kafka-clients-tls will be ready not ready, will try again in 1000 ms (479996ms till timeout)
2022-03-30 17:04:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-60aa0ca2-kafka-clients-tls will be ready not ready, will try again in 1000 ms (478993ms till timeout)
2022-03-30 17:04:32 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-60aa0ca2-kafka-clients-tls is ready
2022-03-30 17:04:32 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:355] Checking consumed messages to pod:my-cluster-60aa0ca2-kafka-clients-tls-ffc445cbf-vwzsm
2022-03-30 17:04:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@758610b4, which are set.
2022-03-30 17:04:32 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@7070e2af, messages=[], arguments=[--group-id, my-consumer-group-956353401, --topic, my-topic-1728678022-515015095, --bootstrap-server, my-cluster-60aa0ca2-kafka-bootstrap.namespace-4.svc:9093, USER=bob_my_cluster_60aa0ca2, --max-messages, 100, --group-instance-id, instance41880476], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-60aa0ca2-kafka-clients-tls-ffc445cbf-vwzsm', podNamespace='namespace-4', bootstrapServer='my-cluster-60aa0ca2-kafka-bootstrap.namespace-4.svc:9093', topicName='my-topic-1728678022-515015095', maxMessages=100, kafkaUsername='bob-my-cluster-60aa0ca2', consumerGroupName='my-consumer-group-956353401', consumerInstanceId='instance41880476', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@758610b4}
2022-03-30 17:04:32 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-60aa0ca2-kafka-bootstrap.namespace-4.svc:9093#my-topic-1728678022-515015095 from pod my-cluster-60aa0ca2-kafka-clients-tls-ffc445cbf-vwzsm
2022-03-30 17:04:32 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-60aa0ca2-kafka-clients-tls-ffc445cbf-vwzsm -n namespace-4 -- /opt/kafka/consumer.sh --group-id my-consumer-group-956353401 --topic my-topic-1728678022-515015095 --bootstrap-server my-cluster-60aa0ca2-kafka-bootstrap.namespace-4.svc:9093 USER=bob_my_cluster_60aa0ca2 --max-messages 100 --group-instance-id instance41880476
2022-03-30 17:04:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-60aa0ca2-kafka-clients-tls-ffc445cbf-vwzsm -n namespace-4 -- /opt/kafka/consumer.sh --group-id my-consumer-group-956353401 --topic my-topic-1728678022-515015095 --bootstrap-server my-cluster-60aa0ca2-kafka-bootstrap.namespace-4.svc:9093 USER=bob_my_cluster_60aa0ca2 --max-messages 100 --group-instance-id instance41880476
2022-03-30 17:04:39 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-30 17:04:39 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-03-30 17:04:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 17:04:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:675] [security.SecurityST - After Each] - Clean up after test
2022-03-30 17:04:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 17:04:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testAutoRenewAllCaCertsTriggeredByAnno
2022-03-30 17:04:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-60aa0ca2-kafka-clients in namespace namespace-4
2022-03-30 17:04:39 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-308582398-741623568 in namespace namespace-4
2022-03-30 17:04:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-60aa0ca2-kafka-clients
2022-03-30 17:04:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-308582398-741623568
2022-03-30 17:04:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-308582398-741623568 not ready, will try again in 10000 ms (179988ms till timeout)
2022-03-30 17:04:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-60aa0ca2-kafka-clients not ready, will try again in 10000 ms (479986ms till timeout)
2022-03-30 17:04:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1728678022-515015095 in namespace namespace-4
2022-03-30 17:04:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1728678022-515015095
2022-03-30 17:04:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-60aa0ca2 in namespace namespace-4
2022-03-30 17:04:49 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-4, for cruise control Kafka cluster my-cluster-60aa0ca2
2022-03-30 17:04:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-60aa0ca2-kafka-clients not ready, will try again in 10000 ms (469933ms till timeout)
2022-03-30 17:04:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-60aa0ca2
2022-03-30 17:04:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-60aa0ca2 not ready, will try again in 10000 ms (839995ms till timeout)
2022-03-30 17:04:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-60aa0ca2-kafka-clients not ready, will try again in 10000 ms (459923ms till timeout)
2022-03-30 17:04:59 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-60aa0ca2-kafka-clients-tls in namespace namespace-4
2022-03-30 17:04:59 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-60aa0ca2-kafka-clients-tls
2022-03-30 17:04:59 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-60aa0ca2-kafka-clients-tls not ready, will try again in 10000 ms (479992ms till timeout)
2022-03-30 17:05:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-60aa0ca2-kafka-clients not ready, will try again in 10000 ms (449914ms till timeout)
2022-03-30 17:05:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-60aa0ca2-kafka-clients-tls not ready, will try again in 10000 ms (469983ms till timeout)
2022-03-30 17:05:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-60aa0ca2-kafka-clients not ready, will try again in 10000 ms (439905ms till timeout)
2022-03-30 17:05:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-60aa0ca2-kafka-clients-tls not ready, will try again in 10000 ms (459975ms till timeout)
2022-03-30 17:05:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-60aa0ca2-kafka-clients not ready, will try again in 10000 ms (429896ms till timeout)
2022-03-30 17:05:29 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-60aa0ca2-kafka-clients-tls not ready, will try again in 10000 ms (449967ms till timeout)
2022-03-30 17:05:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-60aa0ca2-kafka-clients not ready, will try again in 10000 ms (419888ms till timeout)
2022-03-30 17:05:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-60aa0ca2-kafka-clients-tls not ready, will try again in 10000 ms (439959ms till timeout)
2022-03-30 17:05:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser bob-my-cluster-60aa0ca2 in namespace namespace-4
2022-03-30 17:05:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:bob-my-cluster-60aa0ca2
2022-03-30 17:05:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:bob-my-cluster-60aa0ca2 not ready, will try again in 10000 ms (179995ms till timeout)
2022-03-30 17:05:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 17:05:59 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-4 for test case:testAutoRenewAllCaCertsTriggeredByAnno
2022-03-30 17:05:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-4 removal
2022-03-30 17:05:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-4 -o yaml
2022-03-30 17:05:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-4 -o yaml
2022-03-30 17:05:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:05:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (479929ms till timeout)
2022-03-30 17:06:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-4 -o yaml
2022-03-30 17:06:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-4 -o yaml
2022-03-30 17:06:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:06:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (478855ms till timeout)
2022-03-30 17:06:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-4 -o yaml
2022-03-30 17:06:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-4 -o yaml
2022-03-30 17:06:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:06:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (477783ms till timeout)
2022-03-30 17:06:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-4 -o yaml
2022-03-30 17:06:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-4 -o yaml
2022-03-30 17:06:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:06:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (476708ms till timeout)
2022-03-30 17:06:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-4 -o yaml
2022-03-30 17:06:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-4 -o yaml
2022-03-30 17:06:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:06:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (475629ms till timeout)
2022-03-30 17:06:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-4 -o yaml
2022-03-30 17:06:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-4 -o yaml
2022-03-30 17:06:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:06:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (474549ms till timeout)
2022-03-30 17:06:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-4 -o yaml
2022-03-30 17:06:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-4 get Namespace namespace-4 -o yaml
2022-03-30 17:06:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 17:06:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 17:06:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-4" not found
2022-03-30 17:06:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 17:06:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@3d7a59f3=[], io.strimzi.test.logs.CollectorElement@7386938d=[], io.strimzi.test.logs.CollectorElement@afd54539=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@cf7c9d53=[], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-30 17:06:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:267] testAutoRenewAllCaCertsTriggeredByAnno - Notifies waiting test cases:[testUpdateUser, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testCruiseControlBasicAPIRequests, testSendMessagesTlsScramSha, testSendMessagesCustomListenerTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno] to and randomly select one to start execution
2022-03-30 17:06:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:93] [security.SecurityST] - Removing parallel test: testAutoRenewAllCaCertsTriggeredByAnno
2022-03-30 17:06:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:97] [security.SecurityST] - Parallel test count: 0
2022-03-30 17:06:06 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testAutoRenewAllCaCertsTriggeredByAnno-FINISHED
2022-03-30 17:06:06 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 17:06:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 17:06:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:690] [security.SecurityST - After All] - Clean up after test suite
2022-03-30 17:06:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 17:06:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context SecurityST is everything deleted.
2022-03-30 17:06:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 17:06:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace security-st removal
2022-03-30 17:06:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace security-st -o yaml
2022-03-30 17:06:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace security-st -o yaml
2022-03-30 17:06:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:06:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace security-st removal not ready, will try again in 1000 ms (479931ms till timeout)
2022-03-30 17:06:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace security-st -o yaml
2022-03-30 17:06:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace security-st -o yaml
2022-03-30 17:06:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:06:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace security-st removal not ready, will try again in 1000 ms (478859ms till timeout)
2022-03-30 17:06:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace security-st -o yaml
2022-03-30 17:06:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace security-st -o yaml
2022-03-30 17:06:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:06:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace security-st removal not ready, will try again in 1000 ms (477782ms till timeout)
2022-03-30 17:06:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace security-st -o yaml
2022-03-30 17:06:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace security-st -o yaml
2022-03-30 17:06:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:06:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace security-st removal not ready, will try again in 1000 ms (476712ms till timeout)
2022-03-30 17:06:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace security-st -o yaml
2022-03-30 17:06:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace security-st -o yaml
2022-03-30 17:06:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:06:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace security-st removal not ready, will try again in 1000 ms (475641ms till timeout)
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace security-st -o yaml
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-4 get Namespace security-st -o yaml
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "security-st" not found
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@3d7a59f3=[], io.strimzi.test.logs.CollectorElement@7386938d=[], io.strimzi.test.logs.CollectorElement@afd54539=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@cf7c9d53=[], io.strimzi.test.logs.CollectorElement@bfb2174d=[]}
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:254] SecurityST - Notifies waiting test suites:[UserST, HttpBridgeTlsST, CruiseControlApiST, CruiseControlST, ListenersST, SecurityST] to and randomly select one to start execution
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:85] [security.SecurityST] - Removing parallel suite: SecurityST
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:89] [security.SecurityST] - Parallel suites count: 0
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 571.883 s - in io.strimzi.systemtest.security.SecurityST
[[1;34mINFO[m] Running io.strimzi.systemtest.rollingupdate.RollingUpdateST
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:667] [rollingupdate.RollingUpdateST - Before All] - Setup test suite environment
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:69] [rollingupdate.RollingUpdateST] - Adding parallel suite: RollingUpdateST
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:73] [rollingupdate.RollingUpdateST] - Parallel suites count: 1
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:184] RollingUpdateST suite now can proceed its execution
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestSuiteNamespaceManager:129] Test suite `RollingUpdateST` creates these additional namespaces:[rolling-update-st]
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: rolling-update-st
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace rolling-update-st
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace rolling-update-st -o json
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace rolling-update-st -o json
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T17:06:11Z",
        "name": "rolling-update-st",
        "resourceVersion": "55084",
        "selfLink": "/api/v1/namespaces/rolling-update-st",
        "uid": "e1fdff85-c5b2-43cf-9eeb-20653d9c14ea"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@3d7a59f3=[], io.strimzi.test.logs.CollectorElement@dce7372d=[rolling-update-st], io.strimzi.test.logs.CollectorElement@7386938d=[], io.strimzi.test.logs.CollectorElement@afd54539=[], io.strimzi.test.logs.CollectorElement@cf7c9d53=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@bfb2174d=[]}
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: rolling-update-st
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=rolling-update-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: rolling-update-st
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testKafkaAndZookeeperScaleUpScaleDown-STARTED
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:659] [rollingupdate.RollingUpdateST - Before Each] - Setup test case environment
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:77] [rollingupdate.RollingUpdateST] - Adding parallel test: testKafkaAndZookeeperScaleUpScaleDown
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:81] [rollingupdate.RollingUpdateST] - Parallel test count: 1
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:230] testKafkaAndZookeeperScaleUpScaleDown test now can proceed its execution
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2, testSendSimpleMessageTls=my-cluster-128d9a40, testUpdateUser=my-cluster-75998b92, testSendMessagesTlsScramSha=my-cluster-a11b9173, testReceiveSimpleMessageTls=my-cluster-90021a9f, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e, testCruiseControlBasicAPIRequests=my-cluster-6da231a3, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de}
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1975958319-512406578, testAutoRenewAllCaCertsTriggeredByAnno=my-user-308582398-741623568, testSendSimpleMessageTls=my-user-323988616-1355569855, testUpdateUser=my-user-275427717-1183444028, testSendMessagesTlsScramSha=my-user-1791509549-1350003609, testReceiveSimpleMessageTls=my-user-491397521-1196361913, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-695287754-1004738297, testSendMessagesCustomListenerTlsScramSha=my-user-1653975062-1727371851, testCruiseControlBasicAPIRequests=my-user-918568860-36694764, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-83332781-1273282344}
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-1496158521-938246156, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1728678022-515015095, testSendSimpleMessageTls=my-topic-1040221322-670398147, testUpdateUser=my-topic-275186280-765877519, testSendMessagesTlsScramSha=my-topic-1482193652-649135537, testReceiveSimpleMessageTls=my-topic-941665209-436577257, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1609923788-1785060017, testSendMessagesCustomListenerTlsScramSha=my-topic-629448478-708445494, testCruiseControlBasicAPIRequests=my-topic-710218582-658397020, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1536476030-1482182833}
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2-kafka-clients, testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients, testSendMessagesTlsScramSha=my-cluster-a11b9173-kafka-clients, testReceiveSimpleMessageTls=my-cluster-90021a9f-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-6da231a3-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de-kafka-clients}
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-5 for test case:testKafkaAndZookeeperScaleUpScaleDown
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-5
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-5
2022-03-30 17:06:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace rolling-update-st get Namespace namespace-5 -o json
2022-03-30 17:06:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace rolling-update-st get Namespace namespace-5 -o json
2022-03-30 17:06:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:06:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T17:06:11Z",
        "name": "namespace-5",
        "resourceVersion": "55088",
        "selfLink": "/api/v1/namespaces/namespace-5",
        "uid": "2b6e4e70-359d-4aa5-bb86-5f7eb2b5ef2c"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 17:06:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[namespace-5], io.strimzi.test.logs.CollectorElement@3d7a59f3=[], io.strimzi.test.logs.CollectorElement@dce7372d=[rolling-update-st], io.strimzi.test.logs.CollectorElement@7386938d=[], io.strimzi.test.logs.CollectorElement@afd54539=[], io.strimzi.test.logs.CollectorElement@cf7c9d53=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@bfb2174d=[]}
2022-03-30 17:06:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-5
2022-03-30 17:06:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-5, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 17:06:12 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-5
2022-03-30 17:06:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-e6ec2fe1 in namespace namespace-5
2022-03-30 17:06:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-03-30 17:06:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-e6ec2fe1
2022-03-30 17:06:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-e6ec2fe1 will have desired state: Ready
2022-03-30 17:06:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-e6ec2fe1 will have desired state: Ready
2022-03-30 17:06:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (839997ms till timeout)
2022-03-30 17:06:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (838994ms till timeout)
2022-03-30 17:06:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (837991ms till timeout)
2022-03-30 17:06:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (836987ms till timeout)
2022-03-30 17:06:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (835984ms till timeout)
2022-03-30 17:06:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (834981ms till timeout)
2022-03-30 17:06:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (833977ms till timeout)
2022-03-30 17:06:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (832974ms till timeout)
2022-03-30 17:06:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (831970ms till timeout)
2022-03-30 17:06:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (830967ms till timeout)
2022-03-30 17:06:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (829963ms till timeout)
2022-03-30 17:06:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (828959ms till timeout)
2022-03-30 17:06:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (827956ms till timeout)
2022-03-30 17:06:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (826953ms till timeout)
2022-03-30 17:06:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (825949ms till timeout)
2022-03-30 17:06:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (824946ms till timeout)
2022-03-30 17:06:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (823943ms till timeout)
2022-03-30 17:06:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (822940ms till timeout)
2022-03-30 17:06:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (821936ms till timeout)
2022-03-30 17:06:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (820933ms till timeout)
2022-03-30 17:06:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (819929ms till timeout)
2022-03-30 17:06:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (818926ms till timeout)
2022-03-30 17:06:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (817922ms till timeout)
2022-03-30 17:06:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (816919ms till timeout)
2022-03-30 17:06:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (815916ms till timeout)
2022-03-30 17:06:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (814912ms till timeout)
2022-03-30 17:06:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (813909ms till timeout)
2022-03-30 17:06:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (812906ms till timeout)
2022-03-30 17:06:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (811903ms till timeout)
2022-03-30 17:06:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (810899ms till timeout)
2022-03-30 17:06:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (809896ms till timeout)
2022-03-30 17:06:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (808893ms till timeout)
2022-03-30 17:06:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (807887ms till timeout)
2022-03-30 17:06:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (806883ms till timeout)
2022-03-30 17:06:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (805880ms till timeout)
2022-03-30 17:06:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (804876ms till timeout)
2022-03-30 17:06:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (803873ms till timeout)
2022-03-30 17:06:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (802868ms till timeout)
2022-03-30 17:06:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (801865ms till timeout)
2022-03-30 17:06:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (800852ms till timeout)
2022-03-30 17:06:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (799843ms till timeout)
2022-03-30 17:06:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (798840ms till timeout)
2022-03-30 17:06:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (797836ms till timeout)
2022-03-30 17:06:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (796833ms till timeout)
2022-03-30 17:06:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (795829ms till timeout)
2022-03-30 17:06:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (794826ms till timeout)
2022-03-30 17:06:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (793822ms till timeout)
2022-03-30 17:06:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (792819ms till timeout)
2022-03-30 17:07:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (791815ms till timeout)
2022-03-30 17:07:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (790812ms till timeout)
2022-03-30 17:07:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (789808ms till timeout)
2022-03-30 17:07:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (788805ms till timeout)
2022-03-30 17:07:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (787801ms till timeout)
2022-03-30 17:07:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (786798ms till timeout)
2022-03-30 17:07:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (785794ms till timeout)
2022-03-30 17:07:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (784790ms till timeout)
2022-03-30 17:07:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (783787ms till timeout)
2022-03-30 17:07:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (782784ms till timeout)
2022-03-30 17:07:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (781781ms till timeout)
2022-03-30 17:07:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (780777ms till timeout)
2022-03-30 17:07:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (779773ms till timeout)
2022-03-30 17:07:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (778770ms till timeout)
2022-03-30 17:07:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (777766ms till timeout)
2022-03-30 17:07:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (776763ms till timeout)
2022-03-30 17:07:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (775759ms till timeout)
2022-03-30 17:07:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (774756ms till timeout)
2022-03-30 17:07:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (773750ms till timeout)
2022-03-30 17:07:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (772747ms till timeout)
2022-03-30 17:07:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (771743ms till timeout)
2022-03-30 17:07:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (770740ms till timeout)
2022-03-30 17:07:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (769736ms till timeout)
2022-03-30 17:07:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (768733ms till timeout)
2022-03-30 17:07:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (767730ms till timeout)
2022-03-30 17:07:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (766726ms till timeout)
2022-03-30 17:07:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (765723ms till timeout)
2022-03-30 17:07:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (764719ms till timeout)
2022-03-30 17:07:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (763716ms till timeout)
2022-03-30 17:07:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (762712ms till timeout)
2022-03-30 17:07:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (761708ms till timeout)
2022-03-30 17:07:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (760705ms till timeout)
2022-03-30 17:07:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e6ec2fe1 will have desired state: Ready not ready, will try again in 1000 ms (759702ms till timeout)
2022-03-30 17:07:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-e6ec2fe1 is in desired state: Ready
2022-03-30 17:07:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1975958319-512406578 in namespace namespace-5
2022-03-30 17:07:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-03-30 17:07:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1975958319-512406578
2022-03-30 17:07:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1975958319-512406578 will have desired state: Ready
2022-03-30 17:07:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-1975958319-512406578 will have desired state: Ready
2022-03-30 17:07:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-1975958319-512406578 will have desired state: Ready not ready, will try again in 1000 ms (179999ms till timeout)
2022-03-30 17:07:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1975958319-512406578 is in desired state: Ready
2022-03-30 17:07:34 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:489] Verifying docker image names
2022-03-30 17:07:34 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:172] strimzi-cluster-operator
2022-03-30 17:07:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get pod -l strimzi.io/name=my-cluster-e6ec2fe1-entity-operator -o jsonpath={range .items[*]}{.metadata.name} 
2022-03-30 17:07:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get pod -l strimzi.io/name=my-cluster-e6ec2fe1-entity-operator -o jsonpath={range .items[*]}{.metadata.name} 
2022-03-30 17:07:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:07:34 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:525] Docker images verified
2022-03-30 17:07:34 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateST:292] Running kafkaScaleUpScaleDown my-cluster-e6ec2fe1
2022-03-30 17:07:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1496158521-938246156 in namespace namespace-5
2022-03-30 17:07:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-03-30 17:07:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1496158521-938246156
2022-03-30 17:07:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1496158521-938246156 will have desired state: Ready
2022-03-30 17:07:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1496158521-938246156 will have desired state: Ready
2022-03-30 17:07:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 17:07:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1496158521-938246156 is in desired state: Ready
2022-03-30 17:07:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-e6ec2fe1-kafka-clients in namespace namespace-5
2022-03-30 17:07:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-03-30 17:07:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for pod with prefixmy-cluster-e6ec2fe1-kafka-clients is present.
2022-03-30 17:07:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] pod with prefixmy-cluster-e6ec2fe1-kafka-clients is present. not ready, will try again in 10000 ms (299996ms till timeout)
2022-03-30 17:07:45 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-30 17:07:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Producer and consumer will successfully send and receive messages.
2022-03-30 17:07:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@14e5ed63, which are set.
2022-03-30 17:07:45 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@76819bbf, messages=[], arguments=[--topic, my-topic-1496158521-938246156, --bootstrap-server, my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093, USER=my_user_1975958319_512406578, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-e6ec2fe1-kafka-clients-7b78f4f64c-fxdc9', podNamespace='namespace-5', bootstrapServer='my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093', topicName='my-topic-1496158521-938246156', maxMessages=100, kafkaUsername='my-user-1975958319-512406578', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@14e5ed63}
2022-03-30 17:07:45 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093:my-topic-1496158521-938246156 from pod my-cluster-e6ec2fe1-kafka-clients-7b78f4f64c-fxdc9
2022-03-30 17:07:45 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e6ec2fe1-kafka-clients-7b78f4f64c-fxdc9 -n namespace-5 -- /opt/kafka/producer.sh --topic my-topic-1496158521-938246156 --bootstrap-server my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093 USER=my_user_1975958319_512406578 --max-messages 100
2022-03-30 17:07:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-e6ec2fe1-kafka-clients-7b78f4f64c-fxdc9 -n namespace-5 -- /opt/kafka/producer.sh --topic my-topic-1496158521-938246156 --bootstrap-server my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093 USER=my_user_1975958319_512406578 --max-messages 100
2022-03-30 17:07:49 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-30 17:07:49 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-03-30 17:07:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5ca05a2c, which are set.
2022-03-30 17:07:49 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@2517b7c0, messages=[], arguments=[--group-id, my-consumer-group-1633469378, --topic, my-topic-1496158521-938246156, --bootstrap-server, my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093, USER=my_user_1975958319_512406578, --max-messages, 100, --group-instance-id, instance641244058], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-e6ec2fe1-kafka-clients-7b78f4f64c-fxdc9', podNamespace='namespace-5', bootstrapServer='my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093', topicName='my-topic-1496158521-938246156', maxMessages=100, kafkaUsername='my-user-1975958319-512406578', consumerGroupName='my-consumer-group-1633469378', consumerInstanceId='instance641244058', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5ca05a2c}
2022-03-30 17:07:49 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093:my-topic-1496158521-938246156 from pod my-cluster-e6ec2fe1-kafka-clients-7b78f4f64c-fxdc9
2022-03-30 17:07:49 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e6ec2fe1-kafka-clients-7b78f4f64c-fxdc9 -n namespace-5 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1633469378 --topic my-topic-1496158521-938246156 --bootstrap-server my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093 USER=my_user_1975958319_512406578 --max-messages 100 --group-instance-id instance641244058
2022-03-30 17:07:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-e6ec2fe1-kafka-clients-7b78f4f64c-fxdc9 -n namespace-5 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1633469378 --topic my-topic-1496158521-938246156 --bootstrap-server my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093 USER=my_user_1975958319_512406578 --max-messages 100 --group-instance-id instance641244058
2022-03-30 17:07:56 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 17:07:56 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-03-30 17:07:56 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateST:317] Scale up Kafka to 7
2022-03-30 17:07:56 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-e6ec2fe1-kafka rolling update
2022-03-30 17:07:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:74] Waiting for rolling update of component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})
2022-03-30 17:07:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for component with name my-cluster-e6ec2fe1-kafka rolling update
2022-03-30 17:07:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=0ccfebfc-fc93-40e1-b543-4c928eae4734, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:07:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=0ccfebfc-fc93-40e1-b543-4c928eae4734, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:07:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=0ccfebfc-fc93-40e1-b543-4c928eae4734, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:07:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-1 hasn't rolled
2022-03-30 17:07:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (1799996ms till timeout)
2022-03-30 17:08:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=0ccfebfc-fc93-40e1-b543-4c928eae4734, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:08:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=0ccfebfc-fc93-40e1-b543-4c928eae4734, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:08:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=0ccfebfc-fc93-40e1-b543-4c928eae4734, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:08:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-1 hasn't rolled
2022-03-30 17:08:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (1794991ms till timeout)
2022-03-30 17:08:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=0ccfebfc-fc93-40e1-b543-4c928eae4734, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:08:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=0ccfebfc-fc93-40e1-b543-4c928eae4734, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:08:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=0ccfebfc-fc93-40e1-b543-4c928eae4734, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:08:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-1 hasn't rolled
2022-03-30 17:08:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (1789985ms till timeout)
2022-03-30 17:08:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=0ccfebfc-fc93-40e1-b543-4c928eae4734, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:08:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=0ccfebfc-fc93-40e1-b543-4c928eae4734, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:08:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=0ccfebfc-fc93-40e1-b543-4c928eae4734, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:08:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-1 hasn't rolled
2022-03-30 17:08:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (1784980ms till timeout)
2022-03-30 17:08:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=0ccfebfc-fc93-40e1-b543-4c928eae4734, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:08:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:08:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:08:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-2 hasn't rolled
2022-03-30 17:08:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (1779974ms till timeout)
2022-03-30 17:08:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=0ccfebfc-fc93-40e1-b543-4c928eae4734, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:08:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:08:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:08:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-2 hasn't rolled
2022-03-30 17:08:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (1774969ms till timeout)
2022-03-30 17:08:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=0ccfebfc-fc93-40e1-b543-4c928eae4734, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:08:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:08:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:08:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-2 hasn't rolled
2022-03-30 17:08:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (1769963ms till timeout)
2022-03-30 17:08:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=0ccfebfc-fc93-40e1-b543-4c928eae4734, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:08:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:08:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:08:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-2 hasn't rolled
2022-03-30 17:08:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (1764958ms till timeout)
2022-03-30 17:08:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=0ccfebfc-fc93-40e1-b543-4c928eae4734, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:08:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:08:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:08:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-2 hasn't rolled
2022-03-30 17:08:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (1759953ms till timeout)
2022-03-30 17:08:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=0ccfebfc-fc93-40e1-b543-4c928eae4734, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:08:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:08:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:08:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-2 hasn't rolled
2022-03-30 17:08:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (1754948ms till timeout)
2022-03-30 17:08:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=0ccfebfc-fc93-40e1-b543-4c928eae4734, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:08:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:08:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:08:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-2 hasn't rolled
2022-03-30 17:08:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (1749942ms till timeout)
2022-03-30 17:08:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=0ccfebfc-fc93-40e1-b543-4c928eae4734, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:08:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a}
2022-03-30 17:08:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a}
2022-03-30 17:08:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-0 hasn't rolled
2022-03-30 17:08:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (1744935ms till timeout)
2022-03-30 17:08:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=0ccfebfc-fc93-40e1-b543-4c928eae4734, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:08:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a}
2022-03-30 17:08:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a}
2022-03-30 17:08:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-0 hasn't rolled
2022-03-30 17:08:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (1739930ms till timeout)
2022-03-30 17:09:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=0ccfebfc-fc93-40e1-b543-4c928eae4734, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:09:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a}
2022-03-30 17:09:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a}
2022-03-30 17:09:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-0 hasn't rolled
2022-03-30 17:09:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (1734924ms till timeout)
2022-03-30 17:09:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=0ccfebfc-fc93-40e1-b543-4c928eae4734, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:09:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a}
2022-03-30 17:09:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a}
2022-03-30 17:09:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-0 hasn't rolled
2022-03-30 17:09:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (1729919ms till timeout)
2022-03-30 17:09:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=0ccfebfc-fc93-40e1-b543-4c928eae4734, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:09:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a}
2022-03-30 17:09:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a}
2022-03-30 17:09:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-0 hasn't rolled
2022-03-30 17:09:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (1724914ms till timeout)
2022-03-30 17:09:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=0ccfebfc-fc93-40e1-b543-4c928eae4734, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:09:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a}
2022-03-30 17:09:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a}
2022-03-30 17:09:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-0 hasn't rolled
2022-03-30 17:09:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (1719908ms till timeout)
2022-03-30 17:09:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e3fe8be8-0e84-4ddb-8ae2-81c1cd063fb3, my-cluster-e6ec2fe1-kafka-1=0ccfebfc-fc93-40e1-b543-4c928eae4734, my-cluster-e6ec2fe1-kafka-2=63a04cf8-3036-4f61-9d2a-60f8bdab99b0}
2022-03-30 17:09:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a}
2022-03-30 17:09:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a}
2022-03-30 17:09:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:55] All pods seem to have rolled
2022-03-30 17:09:21 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-e6ec2fe1-kafka has been successfully rolled
2022-03-30 17:09:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:87] Component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) successfully rolled
2022-03-30 17:09:21 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 7 Pod(s) of my-cluster-e6ec2fe1-kafka to be ready
2022-03-30 17:09:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-30 17:09:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:09:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4199997ms till timeout)
2022-03-30 17:09:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:09:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4198992ms till timeout)
2022-03-30 17:09:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:09:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4197987ms till timeout)
2022-03-30 17:09:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:09:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4196982ms till timeout)
2022-03-30 17:09:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:09:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4195977ms till timeout)
2022-03-30 17:09:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:09:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4194972ms till timeout)
2022-03-30 17:09:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:09:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4193967ms till timeout)
2022-03-30 17:09:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:09:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4192962ms till timeout)
2022-03-30 17:09:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:09:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4191958ms till timeout)
2022-03-30 17:09:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:09:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4190953ms till timeout)
2022-03-30 17:09:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:09:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4189949ms till timeout)
2022-03-30 17:09:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:09:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4188944ms till timeout)
2022-03-30 17:09:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:09:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4187940ms till timeout)
2022-03-30 17:09:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:09:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4186935ms till timeout)
2022-03-30 17:09:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:09:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4185930ms till timeout)
2022-03-30 17:09:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:09:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4184926ms till timeout)
2022-03-30 17:09:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:09:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4183921ms till timeout)
2022-03-30 17:09:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:09:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4182917ms till timeout)
2022-03-30 17:09:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:09:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4181912ms till timeout)
2022-03-30 17:09:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:09:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4180908ms till timeout)
2022-03-30 17:09:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:09:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4179903ms till timeout)
2022-03-30 17:09:42 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:09:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4178899ms till timeout)
2022-03-30 17:09:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:09:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4177892ms till timeout)
2022-03-30 17:09:44 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:09:44 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:09:44 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:09:44 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-3)
2022-03-30 17:09:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4176886ms till timeout)
2022-03-30 17:09:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:09:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:09:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:09:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-3)
2022-03-30 17:09:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4175876ms till timeout)
2022-03-30 17:09:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:09:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:09:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:09:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-3)
2022-03-30 17:09:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4174870ms till timeout)
2022-03-30 17:09:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:09:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:09:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:09:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-3)
2022-03-30 17:09:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4173864ms till timeout)
2022-03-30 17:09:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:09:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:09:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:09:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-3)
2022-03-30 17:09:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4172856ms till timeout)
2022-03-30 17:09:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:09:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:09:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:09:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-3)
2022-03-30 17:09:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4171849ms till timeout)
2022-03-30 17:09:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:09:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:09:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:09:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-3)
2022-03-30 17:09:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4170841ms till timeout)
2022-03-30 17:09:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:09:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:09:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:09:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-3)
2022-03-30 17:09:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4169833ms till timeout)
2022-03-30 17:09:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:09:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:09:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:09:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-3)
2022-03-30 17:09:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4168824ms till timeout)
2022-03-30 17:09:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:09:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:09:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:09:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-3)
2022-03-30 17:09:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4167818ms till timeout)
2022-03-30 17:09:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:09:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:09:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:09:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-3)
2022-03-30 17:09:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4166813ms till timeout)
2022-03-30 17:09:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:09:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:09:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:09:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-3)
2022-03-30 17:09:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4165807ms till timeout)
2022-03-30 17:09:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:09:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:09:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:09:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-3)
2022-03-30 17:09:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4164801ms till timeout)
2022-03-30 17:09:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:09:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:09:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:09:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-3)
2022-03-30 17:09:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4163795ms till timeout)
2022-03-30 17:09:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:09:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:09:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:09:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-3)
2022-03-30 17:09:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4162789ms till timeout)
2022-03-30 17:09:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:09:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:09:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:09:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-3)
2022-03-30 17:09:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4161783ms till timeout)
2022-03-30 17:10:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:10:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:10:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:10:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-3)
2022-03-30 17:10:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4160777ms till timeout)
2022-03-30 17:10:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:10:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:10:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:10:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-3 not ready: kafka)
2022-03-30 17:10:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-4)
2022-03-30 17:10:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4159771ms till timeout)
2022-03-30 17:10:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:10:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:10:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:10:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-3 not ready: kafka)
2022-03-30 17:10:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-4)
2022-03-30 17:10:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4158765ms till timeout)
2022-03-30 17:10:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:10:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:10:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:10:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-3 not ready: kafka)
2022-03-30 17:10:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-4)
2022-03-30 17:10:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4157758ms till timeout)
2022-03-30 17:10:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:10:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:10:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:10:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-3 not ready: kafka)
2022-03-30 17:10:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-4)
2022-03-30 17:10:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4156753ms till timeout)
2022-03-30 17:10:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:10:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:10:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:10:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-3 not ready: kafka)
2022-03-30 17:10:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-4)
2022-03-30 17:10:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4155746ms till timeout)
2022-03-30 17:10:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:10:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:10:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:10:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-3 not ready: kafka)
2022-03-30 17:10:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-4)
2022-03-30 17:10:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4154740ms till timeout)
2022-03-30 17:10:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:10:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:10:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:10:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-3 not ready: kafka)
2022-03-30 17:10:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-4)
2022-03-30 17:10:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4153734ms till timeout)
2022-03-30 17:10:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:10:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:10:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:10:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-3 not ready: kafka)
2022-03-30 17:10:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-4 not ready: kafka)
2022-03-30 17:10:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-5)
2022-03-30 17:10:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4152727ms till timeout)
2022-03-30 17:10:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:10:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:10:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:10:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-3 not ready: kafka)
2022-03-30 17:10:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-4 not ready: kafka)
2022-03-30 17:10:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-5)
2022-03-30 17:10:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4151720ms till timeout)
2022-03-30 17:10:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:10:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:10:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:10:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-3 not ready: kafka)
2022-03-30 17:10:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-4 not ready: kafka)
2022-03-30 17:10:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-5 not ready: kafka)
2022-03-30 17:10:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-6 not ready: kafka)
2022-03-30 17:10:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e6ec2fe1-kafka-0, my-cluster-e6ec2fe1-kafka-1, my-cluster-e6ec2fe1-kafka-2, my-cluster-e6ec2fe1-kafka-3, my-cluster-e6ec2fe1-kafka-4, my-cluster-e6ec2fe1-kafka-5, my-cluster-e6ec2fe1-kafka-6 are ready
2022-03-30 17:10:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4150714ms till timeout)
2022-03-30 17:10:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:10:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:10:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:10:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-3 not ready: kafka)
2022-03-30 17:10:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-4 not ready: kafka)
2022-03-30 17:10:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-5 not ready: kafka)
2022-03-30 17:10:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-6 not ready: kafka)
2022-03-30 17:10:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e6ec2fe1-kafka-0, my-cluster-e6ec2fe1-kafka-1, my-cluster-e6ec2fe1-kafka-2, my-cluster-e6ec2fe1-kafka-3, my-cluster-e6ec2fe1-kafka-4, my-cluster-e6ec2fe1-kafka-5, my-cluster-e6ec2fe1-kafka-6 are ready
2022-03-30 17:10:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4149708ms till timeout)
2022-03-30 17:10:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:10:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:10:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:10:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-3 not ready: kafka)
2022-03-30 17:10:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-4 not ready: kafka)
2022-03-30 17:10:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-5 not ready: kafka)
2022-03-30 17:10:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-6 not ready: kafka)
2022-03-30 17:10:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e6ec2fe1-kafka-0, my-cluster-e6ec2fe1-kafka-1, my-cluster-e6ec2fe1-kafka-2, my-cluster-e6ec2fe1-kafka-3, my-cluster-e6ec2fe1-kafka-4, my-cluster-e6ec2fe1-kafka-5, my-cluster-e6ec2fe1-kafka-6 are ready
2022-03-30 17:10:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4148702ms till timeout)
2022-03-30 17:10:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:10:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:10:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:10:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-3 not ready: kafka)
2022-03-30 17:10:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-4 not ready: kafka)
2022-03-30 17:10:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-5 not ready: kafka)
2022-03-30 17:10:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-6 not ready: kafka)
2022-03-30 17:10:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e6ec2fe1-kafka-0, my-cluster-e6ec2fe1-kafka-1, my-cluster-e6ec2fe1-kafka-2, my-cluster-e6ec2fe1-kafka-3, my-cluster-e6ec2fe1-kafka-4, my-cluster-e6ec2fe1-kafka-5, my-cluster-e6ec2fe1-kafka-6 are ready
2022-03-30 17:10:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4147696ms till timeout)
2022-03-30 17:10:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:10:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:10:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:10:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-3 not ready: kafka)
2022-03-30 17:10:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-4 not ready: kafka)
2022-03-30 17:10:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-5 not ready: kafka)
2022-03-30 17:10:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-6 not ready: kafka)
2022-03-30 17:10:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e6ec2fe1-kafka-0, my-cluster-e6ec2fe1-kafka-1, my-cluster-e6ec2fe1-kafka-2, my-cluster-e6ec2fe1-kafka-3, my-cluster-e6ec2fe1-kafka-4, my-cluster-e6ec2fe1-kafka-5, my-cluster-e6ec2fe1-kafka-6 are ready
2022-03-30 17:10:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4146690ms till timeout)
2022-03-30 17:10:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:10:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:10:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:10:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-3 not ready: kafka)
2022-03-30 17:10:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-4 not ready: kafka)
2022-03-30 17:10:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-5 not ready: kafka)
2022-03-30 17:10:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-6 not ready: kafka)
2022-03-30 17:10:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e6ec2fe1-kafka-0, my-cluster-e6ec2fe1-kafka-1, my-cluster-e6ec2fe1-kafka-2, my-cluster-e6ec2fe1-kafka-3, my-cluster-e6ec2fe1-kafka-4, my-cluster-e6ec2fe1-kafka-5, my-cluster-e6ec2fe1-kafka-6 are ready
2022-03-30 17:10:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4145684ms till timeout)
2022-03-30 17:10:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:10:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:10:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:10:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-3 not ready: kafka)
2022-03-30 17:10:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-4 not ready: kafka)
2022-03-30 17:10:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-5 not ready: kafka)
2022-03-30 17:10:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-6 not ready: kafka)
2022-03-30 17:10:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e6ec2fe1-kafka-0, my-cluster-e6ec2fe1-kafka-1, my-cluster-e6ec2fe1-kafka-2, my-cluster-e6ec2fe1-kafka-3, my-cluster-e6ec2fe1-kafka-4, my-cluster-e6ec2fe1-kafka-5, my-cluster-e6ec2fe1-kafka-6 are ready
2022-03-30 17:10:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4144678ms till timeout)
2022-03-30 17:10:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:10:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:10:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:10:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-3 not ready: kafka)
2022-03-30 17:10:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-4 not ready: kafka)
2022-03-30 17:10:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-5 not ready: kafka)
2022-03-30 17:10:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-6 not ready: kafka)
2022-03-30 17:10:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e6ec2fe1-kafka-0, my-cluster-e6ec2fe1-kafka-1, my-cluster-e6ec2fe1-kafka-2, my-cluster-e6ec2fe1-kafka-3, my-cluster-e6ec2fe1-kafka-4, my-cluster-e6ec2fe1-kafka-5, my-cluster-e6ec2fe1-kafka-6 are ready
2022-03-30 17:10:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4143671ms till timeout)
2022-03-30 17:10:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:10:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:10:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:10:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-3 not ready: kafka)
2022-03-30 17:10:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-4 not ready: kafka)
2022-03-30 17:10:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-5 not ready: kafka)
2022-03-30 17:10:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-6 not ready: kafka)
2022-03-30 17:10:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e6ec2fe1-kafka-0, my-cluster-e6ec2fe1-kafka-1, my-cluster-e6ec2fe1-kafka-2, my-cluster-e6ec2fe1-kafka-3, my-cluster-e6ec2fe1-kafka-4, my-cluster-e6ec2fe1-kafka-5, my-cluster-e6ec2fe1-kafka-6 are ready
2022-03-30 17:10:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4142665ms till timeout)
2022-03-30 17:10:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:10:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:10:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:10:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-3 not ready: kafka)
2022-03-30 17:10:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-4 not ready: kafka)
2022-03-30 17:10:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-5 not ready: kafka)
2022-03-30 17:10:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-6 not ready: kafka)
2022-03-30 17:10:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e6ec2fe1-kafka-0, my-cluster-e6ec2fe1-kafka-1, my-cluster-e6ec2fe1-kafka-2, my-cluster-e6ec2fe1-kafka-3, my-cluster-e6ec2fe1-kafka-4, my-cluster-e6ec2fe1-kafka-5, my-cluster-e6ec2fe1-kafka-6 are ready
2022-03-30 17:10:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4141659ms till timeout)
2022-03-30 17:10:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:10:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:10:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:10:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-3 not ready: kafka)
2022-03-30 17:10:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-4 not ready: kafka)
2022-03-30 17:10:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-5 not ready: kafka)
2022-03-30 17:10:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-6 not ready: kafka)
2022-03-30 17:10:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e6ec2fe1-kafka-0, my-cluster-e6ec2fe1-kafka-1, my-cluster-e6ec2fe1-kafka-2, my-cluster-e6ec2fe1-kafka-3, my-cluster-e6ec2fe1-kafka-4, my-cluster-e6ec2fe1-kafka-5, my-cluster-e6ec2fe1-kafka-6 are ready
2022-03-30 17:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-e6ec2fe1 will have desired state: Ready
2022-03-30 17:10:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-e6ec2fe1 will have desired state: Ready
2022-03-30 17:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-e6ec2fe1 is in desired state: Ready
2022-03-30 17:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-e6ec2fe1 is ready
2022-03-30 17:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateST:327] Kafka scale up to 7 finished
2022-03-30 17:10:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Consumer will successfully receive messages.
2022-03-30 17:10:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@53882b01, which are set.
2022-03-30 17:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@4c9d9ab5, messages=[], arguments=[--group-id, my-consumer-group-336204912, --topic, my-topic-1496158521-938246156, --bootstrap-server, my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093, USER=my_user_1975958319_512406578, --max-messages, 100, --group-instance-id, instance1108089708], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-e6ec2fe1-kafka-clients-7b78f4f64c-fxdc9', podNamespace='namespace-5', bootstrapServer='my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093', topicName='my-topic-1496158521-938246156', maxMessages=100, kafkaUsername='my-user-1975958319-512406578', consumerGroupName='my-consumer-group-336204912', consumerInstanceId='instance1108089708', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@53882b01}
2022-03-30 17:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093:my-topic-1496158521-938246156 from pod my-cluster-e6ec2fe1-kafka-clients-7b78f4f64c-fxdc9
2022-03-30 17:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e6ec2fe1-kafka-clients-7b78f4f64c-fxdc9 -n namespace-5 -- /opt/kafka/consumer.sh --group-id my-consumer-group-336204912 --topic my-topic-1496158521-938246156 --bootstrap-server my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093 USER=my_user_1975958319_512406578 --max-messages 100 --group-instance-id instance1108089708
2022-03-30 17:10:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-e6ec2fe1-kafka-clients-7b78f4f64c-fxdc9 -n namespace-5 -- /opt/kafka/consumer.sh --group-id my-consumer-group-336204912 --topic my-topic-1496158521-938246156 --bootstrap-server my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093 USER=my_user_1975958319_512406578 --max-messages 100 --group-instance-id instance1108089708
2022-03-30 17:10:27 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 17:10:27 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-03-30 17:10:27 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateST:339] Scale up Zookeeper to 5
2022-03-30 17:10:27 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 5 Pod(s) of my-cluster-e6ec2fe1-zookeeper to be ready
2022-03-30 17:10:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-30 17:10:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:10:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2999996ms till timeout)
2022-03-30 17:10:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:10:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2998991ms till timeout)
2022-03-30 17:10:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:10:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2997986ms till timeout)
2022-03-30 17:10:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:10:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2996980ms till timeout)
2022-03-30 17:10:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:10:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2995975ms till timeout)
2022-03-30 17:10:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:10:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2994970ms till timeout)
2022-03-30 17:10:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:10:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2993965ms till timeout)
2022-03-30 17:10:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:10:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2992960ms till timeout)
2022-03-30 17:10:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:10:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2991955ms till timeout)
2022-03-30 17:10:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:10:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2990949ms till timeout)
2022-03-30 17:10:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:10:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2989944ms till timeout)
2022-03-30 17:10:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:10:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2988939ms till timeout)
2022-03-30 17:10:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:10:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2987934ms till timeout)
2022-03-30 17:10:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:10:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2986929ms till timeout)
2022-03-30 17:10:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:10:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2985925ms till timeout)
2022-03-30 17:10:42 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:10:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2984920ms till timeout)
2022-03-30 17:10:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:10:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2983915ms till timeout)
2022-03-30 17:10:44 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:10:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2982909ms till timeout)
2022-03-30 17:10:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:10:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2981903ms till timeout)
2022-03-30 17:10:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:10:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2980898ms till timeout)
2022-03-30 17:10:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:10:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2979893ms till timeout)
2022-03-30 17:10:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:10:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2978889ms till timeout)
2022-03-30 17:10:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:10:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2977883ms till timeout)
2022-03-30 17:10:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:10:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2976879ms till timeout)
2022-03-30 17:10:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:10:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2975874ms till timeout)
2022-03-30 17:10:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:10:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2974869ms till timeout)
2022-03-30 17:10:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:10:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2973864ms till timeout)
2022-03-30 17:10:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:10:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2972859ms till timeout)
2022-03-30 17:10:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:10:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2971853ms till timeout)
2022-03-30 17:10:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 17:10:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2970848ms till timeout)
2022-03-30 17:10:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:10:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:10:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:10:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:10:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-zookeeper-4)
2022-03-30 17:10:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2969842ms till timeout)
2022-03-30 17:10:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:10:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:10:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:10:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:10:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-zookeeper-4)
2022-03-30 17:10:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2968837ms till timeout)
2022-03-30 17:10:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:10:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:10:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:10:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:10:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-zookeeper-4)
2022-03-30 17:10:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2967831ms till timeout)
2022-03-30 17:11:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-zookeeper-4)
2022-03-30 17:11:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2966825ms till timeout)
2022-03-30 17:11:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-zookeeper-4)
2022-03-30 17:11:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2965819ms till timeout)
2022-03-30 17:11:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-zookeeper-4)
2022-03-30 17:11:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2964814ms till timeout)
2022-03-30 17:11:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-zookeeper-4)
2022-03-30 17:11:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2963809ms till timeout)
2022-03-30 17:11:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-zookeeper-4)
2022-03-30 17:11:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2962803ms till timeout)
2022-03-30 17:11:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-zookeeper-4)
2022-03-30 17:11:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2961798ms till timeout)
2022-03-30 17:11:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-zookeeper-4)
2022-03-30 17:11:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2960792ms till timeout)
2022-03-30 17:11:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-zookeeper-4)
2022-03-30 17:11:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2959786ms till timeout)
2022-03-30 17:11:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-zookeeper-4)
2022-03-30 17:11:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2958781ms till timeout)
2022-03-30 17:11:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-zookeeper-4)
2022-03-30 17:11:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2957775ms till timeout)
2022-03-30 17:11:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-zookeeper-4)
2022-03-30 17:11:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2956770ms till timeout)
2022-03-30 17:11:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-zookeeper-4)
2022-03-30 17:11:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2955764ms till timeout)
2022-03-30 17:11:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-zookeeper-4)
2022-03-30 17:11:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2954756ms till timeout)
2022-03-30 17:11:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-zookeeper-4)
2022-03-30 17:11:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2953751ms till timeout)
2022-03-30 17:11:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-zookeeper-4)
2022-03-30 17:11:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2952745ms till timeout)
2022-03-30 17:11:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-zookeeper-4)
2022-03-30 17:11:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2951740ms till timeout)
2022-03-30 17:11:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-zookeeper-4)
2022-03-30 17:11:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2950734ms till timeout)
2022-03-30 17:11:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-zookeeper-4)
2022-03-30 17:11:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2949728ms till timeout)
2022-03-30 17:11:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-zookeeper-4)
2022-03-30 17:11:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2948723ms till timeout)
2022-03-30 17:11:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-zookeeper-4)
2022-03-30 17:11:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2947717ms till timeout)
2022-03-30 17:11:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-zookeeper-4)
2022-03-30 17:11:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2946711ms till timeout)
2022-03-30 17:11:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-zookeeper-4)
2022-03-30 17:11:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2945706ms till timeout)
2022-03-30 17:11:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-zookeeper-4)
2022-03-30 17:11:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2944700ms till timeout)
2022-03-30 17:11:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-zookeeper-4)
2022-03-30 17:11:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2943694ms till timeout)
2022-03-30 17:11:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-zookeeper-4)
2022-03-30 17:11:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2942689ms till timeout)
2022-03-30 17:11:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-zookeeper-4)
2022-03-30 17:11:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2941683ms till timeout)
2022-03-30 17:11:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-4 not ready: zookeeper)
2022-03-30 17:11:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e6ec2fe1-zookeeper-0, my-cluster-e6ec2fe1-zookeeper-1, my-cluster-e6ec2fe1-zookeeper-2, my-cluster-e6ec2fe1-zookeeper-3, my-cluster-e6ec2fe1-zookeeper-4 are ready
2022-03-30 17:11:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2940678ms till timeout)
2022-03-30 17:11:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-4 not ready: zookeeper)
2022-03-30 17:11:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e6ec2fe1-zookeeper-0, my-cluster-e6ec2fe1-zookeeper-1, my-cluster-e6ec2fe1-zookeeper-2, my-cluster-e6ec2fe1-zookeeper-3, my-cluster-e6ec2fe1-zookeeper-4 are ready
2022-03-30 17:11:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2939672ms till timeout)
2022-03-30 17:11:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-4 not ready: zookeeper)
2022-03-30 17:11:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e6ec2fe1-zookeeper-0, my-cluster-e6ec2fe1-zookeeper-1, my-cluster-e6ec2fe1-zookeeper-2, my-cluster-e6ec2fe1-zookeeper-3, my-cluster-e6ec2fe1-zookeeper-4 are ready
2022-03-30 17:11:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2938667ms till timeout)
2022-03-30 17:11:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-4 not ready: zookeeper)
2022-03-30 17:11:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e6ec2fe1-zookeeper-0, my-cluster-e6ec2fe1-zookeeper-1, my-cluster-e6ec2fe1-zookeeper-2, my-cluster-e6ec2fe1-zookeeper-3, my-cluster-e6ec2fe1-zookeeper-4 are ready
2022-03-30 17:11:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2937661ms till timeout)
2022-03-30 17:11:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-4 not ready: zookeeper)
2022-03-30 17:11:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e6ec2fe1-zookeeper-0, my-cluster-e6ec2fe1-zookeeper-1, my-cluster-e6ec2fe1-zookeeper-2, my-cluster-e6ec2fe1-zookeeper-3, my-cluster-e6ec2fe1-zookeeper-4 are ready
2022-03-30 17:11:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2936656ms till timeout)
2022-03-30 17:11:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-4 not ready: zookeeper)
2022-03-30 17:11:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e6ec2fe1-zookeeper-0, my-cluster-e6ec2fe1-zookeeper-1, my-cluster-e6ec2fe1-zookeeper-2, my-cluster-e6ec2fe1-zookeeper-3, my-cluster-e6ec2fe1-zookeeper-4 are ready
2022-03-30 17:11:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2935651ms till timeout)
2022-03-30 17:11:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-4 not ready: zookeeper)
2022-03-30 17:11:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e6ec2fe1-zookeeper-0, my-cluster-e6ec2fe1-zookeeper-1, my-cluster-e6ec2fe1-zookeeper-2, my-cluster-e6ec2fe1-zookeeper-3, my-cluster-e6ec2fe1-zookeeper-4 are ready
2022-03-30 17:11:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2934646ms till timeout)
2022-03-30 17:11:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-4 not ready: zookeeper)
2022-03-30 17:11:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e6ec2fe1-zookeeper-0, my-cluster-e6ec2fe1-zookeeper-1, my-cluster-e6ec2fe1-zookeeper-2, my-cluster-e6ec2fe1-zookeeper-3, my-cluster-e6ec2fe1-zookeeper-4 are ready
2022-03-30 17:11:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2933640ms till timeout)
2022-03-30 17:11:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-4 not ready: zookeeper)
2022-03-30 17:11:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e6ec2fe1-zookeeper-0, my-cluster-e6ec2fe1-zookeeper-1, my-cluster-e6ec2fe1-zookeeper-2, my-cluster-e6ec2fe1-zookeeper-3, my-cluster-e6ec2fe1-zookeeper-4 are ready
2022-03-30 17:11:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2932629ms till timeout)
2022-03-30 17:11:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-4 not ready: zookeeper)
2022-03-30 17:11:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e6ec2fe1-zookeeper-0, my-cluster-e6ec2fe1-zookeeper-1, my-cluster-e6ec2fe1-zookeeper-2, my-cluster-e6ec2fe1-zookeeper-3, my-cluster-e6ec2fe1-zookeeper-4 are ready
2022-03-30 17:11:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-zookeeper, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2931623ms till timeout)
2022-03-30 17:11:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-0 not ready: zookeeper)
2022-03-30 17:11:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-1 not ready: zookeeper)
2022-03-30 17:11:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-2 not ready: zookeeper)
2022-03-30 17:11:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-3 not ready: zookeeper)
2022-03-30 17:11:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-zookeeper-4 not ready: zookeeper)
2022-03-30 17:11:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e6ec2fe1-zookeeper-0, my-cluster-e6ec2fe1-zookeeper-1, my-cluster-e6ec2fe1-zookeeper-2, my-cluster-e6ec2fe1-zookeeper-3, my-cluster-e6ec2fe1-zookeeper-4 are ready
2022-03-30 17:11:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-e6ec2fe1 will have desired state: Ready
2022-03-30 17:11:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-e6ec2fe1 will have desired state: Ready
2022-03-30 17:11:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-e6ec2fe1 is in desired state: Ready
2022-03-30 17:11:36 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-e6ec2fe1 is ready
2022-03-30 17:11:36 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateST:342] Kafka scale up to 5 finished
2022-03-30 17:11:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Consumer will successfully receive messages.
2022-03-30 17:11:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@377ba79e, which are set.
2022-03-30 17:11:36 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@26082d3b, messages=[], arguments=[--group-id, my-consumer-group-373728589, --topic, my-topic-1496158521-938246156, --bootstrap-server, my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093, USER=my_user_1975958319_512406578, --max-messages, 100, --group-instance-id, instance863744261], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-e6ec2fe1-kafka-clients-7b78f4f64c-fxdc9', podNamespace='namespace-5', bootstrapServer='my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093', topicName='my-topic-1496158521-938246156', maxMessages=100, kafkaUsername='my-user-1975958319-512406578', consumerGroupName='my-consumer-group-373728589', consumerInstanceId='instance863744261', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@377ba79e}
2022-03-30 17:11:36 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093:my-topic-1496158521-938246156 from pod my-cluster-e6ec2fe1-kafka-clients-7b78f4f64c-fxdc9
2022-03-30 17:11:36 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e6ec2fe1-kafka-clients-7b78f4f64c-fxdc9 -n namespace-5 -- /opt/kafka/consumer.sh --group-id my-consumer-group-373728589 --topic my-topic-1496158521-938246156 --bootstrap-server my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093 USER=my_user_1975958319_512406578 --max-messages 100 --group-instance-id instance863744261
2022-03-30 17:11:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-e6ec2fe1-kafka-clients-7b78f4f64c-fxdc9 -n namespace-5 -- /opt/kafka/consumer.sh --group-id my-consumer-group-373728589 --topic my-topic-1496158521-938246156 --bootstrap-server my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093 USER=my_user_1975958319_512406578 --max-messages 100 --group-instance-id instance863744261
2022-03-30 17:11:43 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 17:11:43 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-03-30 17:11:43 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateST:351] Scale down Kafka to 3
2022-03-30 17:11:43 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-e6ec2fe1-kafka rolling update
2022-03-30 17:11:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:74] Waiting for rolling update of component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})
2022-03-30 17:11:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for component with name my-cluster-e6ec2fe1-kafka rolling update
2022-03-30 17:11:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f, my-cluster-e6ec2fe1-kafka-5=2dc9f955-e987-470d-aef1-e722402a3f8b, my-cluster-e6ec2fe1-kafka-6=8bfa6974-acf8-44e8-9b76-036ce4773849}
2022-03-30 17:11:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f, my-cluster-e6ec2fe1-kafka-5=2dc9f955-e987-470d-aef1-e722402a3f8b, my-cluster-e6ec2fe1-kafka-6=8bfa6974-acf8-44e8-9b76-036ce4773849}
2022-03-30 17:11:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f, my-cluster-e6ec2fe1-kafka-5=2dc9f955-e987-470d-aef1-e722402a3f8b, my-cluster-e6ec2fe1-kafka-6=8bfa6974-acf8-44e8-9b76-036ce4773849}
2022-03-30 17:11:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-1 hasn't rolled
2022-03-30 17:11:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (4199995ms till timeout)
2022-03-30 17:11:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f, my-cluster-e6ec2fe1-kafka-5=2dc9f955-e987-470d-aef1-e722402a3f8b, my-cluster-e6ec2fe1-kafka-6=8bfa6974-acf8-44e8-9b76-036ce4773849}
2022-03-30 17:11:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f, my-cluster-e6ec2fe1-kafka-5=2dc9f955-e987-470d-aef1-e722402a3f8b, my-cluster-e6ec2fe1-kafka-6=8bfa6974-acf8-44e8-9b76-036ce4773849}
2022-03-30 17:11:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f, my-cluster-e6ec2fe1-kafka-5=2dc9f955-e987-470d-aef1-e722402a3f8b, my-cluster-e6ec2fe1-kafka-6=8bfa6974-acf8-44e8-9b76-036ce4773849}
2022-03-30 17:11:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-1 hasn't rolled
2022-03-30 17:11:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (4194989ms till timeout)
2022-03-30 17:11:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f, my-cluster-e6ec2fe1-kafka-5=2dc9f955-e987-470d-aef1-e722402a3f8b, my-cluster-e6ec2fe1-kafka-6=8bfa6974-acf8-44e8-9b76-036ce4773849}
2022-03-30 17:11:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f, my-cluster-e6ec2fe1-kafka-5=2dc9f955-e987-470d-aef1-e722402a3f8b, my-cluster-e6ec2fe1-kafka-6=8bfa6974-acf8-44e8-9b76-036ce4773849}
2022-03-30 17:11:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f, my-cluster-e6ec2fe1-kafka-5=2dc9f955-e987-470d-aef1-e722402a3f8b, my-cluster-e6ec2fe1-kafka-6=8bfa6974-acf8-44e8-9b76-036ce4773849}
2022-03-30 17:11:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-1 hasn't rolled
2022-03-30 17:11:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (4189983ms till timeout)
2022-03-30 17:11:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f, my-cluster-e6ec2fe1-kafka-5=2dc9f955-e987-470d-aef1-e722402a3f8b, my-cluster-e6ec2fe1-kafka-6=8bfa6974-acf8-44e8-9b76-036ce4773849}
2022-03-30 17:11:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f, my-cluster-e6ec2fe1-kafka-5=2dc9f955-e987-470d-aef1-e722402a3f8b, my-cluster-e6ec2fe1-kafka-6=8bfa6974-acf8-44e8-9b76-036ce4773849}
2022-03-30 17:11:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f, my-cluster-e6ec2fe1-kafka-5=2dc9f955-e987-470d-aef1-e722402a3f8b, my-cluster-e6ec2fe1-kafka-6=8bfa6974-acf8-44e8-9b76-036ce4773849}
2022-03-30 17:11:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-1 hasn't rolled
2022-03-30 17:11:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (4184976ms till timeout)
2022-03-30 17:12:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f, my-cluster-e6ec2fe1-kafka-5=2dc9f955-e987-470d-aef1-e722402a3f8b, my-cluster-e6ec2fe1-kafka-6=8bfa6974-acf8-44e8-9b76-036ce4773849}
2022-03-30 17:12:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f, my-cluster-e6ec2fe1-kafka-5=2dc9f955-e987-470d-aef1-e722402a3f8b}
2022-03-30 17:12:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f, my-cluster-e6ec2fe1-kafka-5=2dc9f955-e987-470d-aef1-e722402a3f8b}
2022-03-30 17:12:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-1 hasn't rolled
2022-03-30 17:12:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (4179970ms till timeout)
2022-03-30 17:12:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f, my-cluster-e6ec2fe1-kafka-5=2dc9f955-e987-470d-aef1-e722402a3f8b, my-cluster-e6ec2fe1-kafka-6=8bfa6974-acf8-44e8-9b76-036ce4773849}
2022-03-30 17:12:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f}
2022-03-30 17:12:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f}
2022-03-30 17:12:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-1 hasn't rolled
2022-03-30 17:12:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (4174965ms till timeout)
2022-03-30 17:12:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f, my-cluster-e6ec2fe1-kafka-5=2dc9f955-e987-470d-aef1-e722402a3f8b, my-cluster-e6ec2fe1-kafka-6=8bfa6974-acf8-44e8-9b76-036ce4773849}
2022-03-30 17:12:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f}
2022-03-30 17:12:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f}
2022-03-30 17:12:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-1 hasn't rolled
2022-03-30 17:12:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (4169959ms till timeout)
2022-03-30 17:12:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f, my-cluster-e6ec2fe1-kafka-5=2dc9f955-e987-470d-aef1-e722402a3f8b, my-cluster-e6ec2fe1-kafka-6=8bfa6974-acf8-44e8-9b76-036ce4773849}
2022-03-30 17:12:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f}
2022-03-30 17:12:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f}
2022-03-30 17:12:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-1 hasn't rolled
2022-03-30 17:12:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (4164953ms till timeout)
2022-03-30 17:12:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f, my-cluster-e6ec2fe1-kafka-5=2dc9f955-e987-470d-aef1-e722402a3f8b, my-cluster-e6ec2fe1-kafka-6=8bfa6974-acf8-44e8-9b76-036ce4773849}
2022-03-30 17:12:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55}
2022-03-30 17:12:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55}
2022-03-30 17:12:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-1 hasn't rolled
2022-03-30 17:12:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (4159948ms till timeout)
2022-03-30 17:12:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f, my-cluster-e6ec2fe1-kafka-5=2dc9f955-e987-470d-aef1-e722402a3f8b, my-cluster-e6ec2fe1-kafka-6=8bfa6974-acf8-44e8-9b76-036ce4773849}
2022-03-30 17:12:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a}
2022-03-30 17:12:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a}
2022-03-30 17:12:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-1 hasn't rolled
2022-03-30 17:12:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (4154943ms till timeout)
2022-03-30 17:12:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f, my-cluster-e6ec2fe1-kafka-5=2dc9f955-e987-470d-aef1-e722402a3f8b, my-cluster-e6ec2fe1-kafka-6=8bfa6974-acf8-44e8-9b76-036ce4773849}
2022-03-30 17:12:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=62069e39-8e53-44ac-85d0-d050d1d332bf, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a}
2022-03-30 17:12:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=62069e39-8e53-44ac-85d0-d050d1d332bf, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a}
2022-03-30 17:12:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-1 hasn't rolled
2022-03-30 17:12:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (4149938ms till timeout)
2022-03-30 17:12:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f, my-cluster-e6ec2fe1-kafka-5=2dc9f955-e987-470d-aef1-e722402a3f8b, my-cluster-e6ec2fe1-kafka-6=8bfa6974-acf8-44e8-9b76-036ce4773849}
2022-03-30 17:12:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=62069e39-8e53-44ac-85d0-d050d1d332bf, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a}
2022-03-30 17:12:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=62069e39-8e53-44ac-85d0-d050d1d332bf, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a}
2022-03-30 17:12:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-1 hasn't rolled
2022-03-30 17:12:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (4144933ms till timeout)
2022-03-30 17:12:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f, my-cluster-e6ec2fe1-kafka-5=2dc9f955-e987-470d-aef1-e722402a3f8b, my-cluster-e6ec2fe1-kafka-6=8bfa6974-acf8-44e8-9b76-036ce4773849}
2022-03-30 17:12:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=62069e39-8e53-44ac-85d0-d050d1d332bf, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a}
2022-03-30 17:12:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=62069e39-8e53-44ac-85d0-d050d1d332bf, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a}
2022-03-30 17:12:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-1 hasn't rolled
2022-03-30 17:12:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (4139928ms till timeout)
2022-03-30 17:12:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f, my-cluster-e6ec2fe1-kafka-5=2dc9f955-e987-470d-aef1-e722402a3f8b, my-cluster-e6ec2fe1-kafka-6=8bfa6974-acf8-44e8-9b76-036ce4773849}
2022-03-30 17:12:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=62069e39-8e53-44ac-85d0-d050d1d332bf, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a}
2022-03-30 17:12:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=62069e39-8e53-44ac-85d0-d050d1d332bf, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a}
2022-03-30 17:12:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-1 hasn't rolled
2022-03-30 17:12:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (4134923ms till timeout)
2022-03-30 17:12:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f, my-cluster-e6ec2fe1-kafka-5=2dc9f955-e987-470d-aef1-e722402a3f8b, my-cluster-e6ec2fe1-kafka-6=8bfa6974-acf8-44e8-9b76-036ce4773849}
2022-03-30 17:12:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=62069e39-8e53-44ac-85d0-d050d1d332bf, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a}
2022-03-30 17:12:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=62069e39-8e53-44ac-85d0-d050d1d332bf, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a}
2022-03-30 17:12:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-1 hasn't rolled
2022-03-30 17:12:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (4129912ms till timeout)
2022-03-30 17:12:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f, my-cluster-e6ec2fe1-kafka-5=2dc9f955-e987-470d-aef1-e722402a3f8b, my-cluster-e6ec2fe1-kafka-6=8bfa6974-acf8-44e8-9b76-036ce4773849}
2022-03-30 17:12:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=62069e39-8e53-44ac-85d0-d050d1d332bf, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=89ce26cd-802c-406e-9213-2d37aae6317e}
2022-03-30 17:12:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=62069e39-8e53-44ac-85d0-d050d1d332bf, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=89ce26cd-802c-406e-9213-2d37aae6317e}
2022-03-30 17:12:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-1 hasn't rolled
2022-03-30 17:12:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (4124907ms till timeout)
2022-03-30 17:13:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f, my-cluster-e6ec2fe1-kafka-5=2dc9f955-e987-470d-aef1-e722402a3f8b, my-cluster-e6ec2fe1-kafka-6=8bfa6974-acf8-44e8-9b76-036ce4773849}
2022-03-30 17:13:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=62069e39-8e53-44ac-85d0-d050d1d332bf, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=89ce26cd-802c-406e-9213-2d37aae6317e}
2022-03-30 17:13:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=62069e39-8e53-44ac-85d0-d050d1d332bf, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=89ce26cd-802c-406e-9213-2d37aae6317e}
2022-03-30 17:13:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-1 hasn't rolled
2022-03-30 17:13:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (4119902ms till timeout)
2022-03-30 17:13:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f, my-cluster-e6ec2fe1-kafka-5=2dc9f955-e987-470d-aef1-e722402a3f8b, my-cluster-e6ec2fe1-kafka-6=8bfa6974-acf8-44e8-9b76-036ce4773849}
2022-03-30 17:13:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=62069e39-8e53-44ac-85d0-d050d1d332bf, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=89ce26cd-802c-406e-9213-2d37aae6317e}
2022-03-30 17:13:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=62069e39-8e53-44ac-85d0-d050d1d332bf, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=89ce26cd-802c-406e-9213-2d37aae6317e}
2022-03-30 17:13:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-1 hasn't rolled
2022-03-30 17:13:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (4114896ms till timeout)
2022-03-30 17:13:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f, my-cluster-e6ec2fe1-kafka-5=2dc9f955-e987-470d-aef1-e722402a3f8b, my-cluster-e6ec2fe1-kafka-6=8bfa6974-acf8-44e8-9b76-036ce4773849}
2022-03-30 17:13:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=62069e39-8e53-44ac-85d0-d050d1d332bf, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=89ce26cd-802c-406e-9213-2d37aae6317e}
2022-03-30 17:13:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=62069e39-8e53-44ac-85d0-d050d1d332bf, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=89ce26cd-802c-406e-9213-2d37aae6317e}
2022-03-30 17:13:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-1 hasn't rolled
2022-03-30 17:13:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (4109891ms till timeout)
2022-03-30 17:13:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f, my-cluster-e6ec2fe1-kafka-5=2dc9f955-e987-470d-aef1-e722402a3f8b, my-cluster-e6ec2fe1-kafka-6=8bfa6974-acf8-44e8-9b76-036ce4773849}
2022-03-30 17:13:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=62069e39-8e53-44ac-85d0-d050d1d332bf, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=89ce26cd-802c-406e-9213-2d37aae6317e}
2022-03-30 17:13:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=62069e39-8e53-44ac-85d0-d050d1d332bf, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=89ce26cd-802c-406e-9213-2d37aae6317e}
2022-03-30 17:13:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-1 hasn't rolled
2022-03-30 17:13:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (4104886ms till timeout)
2022-03-30 17:13:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f, my-cluster-e6ec2fe1-kafka-5=2dc9f955-e987-470d-aef1-e722402a3f8b, my-cluster-e6ec2fe1-kafka-6=8bfa6974-acf8-44e8-9b76-036ce4773849}
2022-03-30 17:13:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=62069e39-8e53-44ac-85d0-d050d1d332bf, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=89ce26cd-802c-406e-9213-2d37aae6317e}
2022-03-30 17:13:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=62069e39-8e53-44ac-85d0-d050d1d332bf, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=89ce26cd-802c-406e-9213-2d37aae6317e}
2022-03-30 17:13:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-1 hasn't rolled
2022-03-30 17:13:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (4099880ms till timeout)
2022-03-30 17:13:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f, my-cluster-e6ec2fe1-kafka-5=2dc9f955-e987-470d-aef1-e722402a3f8b, my-cluster-e6ec2fe1-kafka-6=8bfa6974-acf8-44e8-9b76-036ce4773849}
2022-03-30 17:13:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=62069e39-8e53-44ac-85d0-d050d1d332bf, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=89ce26cd-802c-406e-9213-2d37aae6317e}
2022-03-30 17:13:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=62069e39-8e53-44ac-85d0-d050d1d332bf, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=89ce26cd-802c-406e-9213-2d37aae6317e}
2022-03-30 17:13:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-e6ec2fe1-kafka-1 hasn't rolled
2022-03-30 17:13:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] component with name my-cluster-e6ec2fe1-kafka rolling update not ready, will try again in 5000 ms (4094875ms till timeout)
2022-03-30 17:13:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-e6ec2fe1-kafka-0=e5779516-8dc0-4206-bfdf-bc362a764f42, my-cluster-e6ec2fe1-kafka-1=4abf44f7-0272-42bb-9a06-72b2833146eb, my-cluster-e6ec2fe1-kafka-2=99ff95c7-12f2-48a7-96a1-36e418b22a4a, my-cluster-e6ec2fe1-kafka-3=991040b5-ea6c-4713-8380-7aa1817e5b55, my-cluster-e6ec2fe1-kafka-4=64fd8976-c4ba-4a28-bbf2-369e05d1d99f, my-cluster-e6ec2fe1-kafka-5=2dc9f955-e987-470d-aef1-e722402a3f8b, my-cluster-e6ec2fe1-kafka-6=8bfa6974-acf8-44e8-9b76-036ce4773849}
2022-03-30 17:13:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-e6ec2fe1-kafka-0=62069e39-8e53-44ac-85d0-d050d1d332bf, my-cluster-e6ec2fe1-kafka-1=56519c16-6e81-4430-9bb6-58003862dc42, my-cluster-e6ec2fe1-kafka-2=89ce26cd-802c-406e-9213-2d37aae6317e}
2022-03-30 17:13:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-e6ec2fe1-kafka-0=62069e39-8e53-44ac-85d0-d050d1d332bf, my-cluster-e6ec2fe1-kafka-1=56519c16-6e81-4430-9bb6-58003862dc42, my-cluster-e6ec2fe1-kafka-2=89ce26cd-802c-406e-9213-2d37aae6317e}
2022-03-30 17:13:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:55] All pods seem to have rolled
2022-03-30 17:13:33 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-e6ec2fe1-kafka has been successfully rolled
2022-03-30 17:13:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [RollingUpdateUtils:87] Component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={}) successfully rolled
2022-03-30 17:13:33 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-e6ec2fe1-kafka to be ready
2022-03-30 17:13:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-30 17:13:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:13:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-1)
2022-03-30 17:13:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1799996ms till timeout)
2022-03-30 17:13:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:13:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-1)
2022-03-30 17:13:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1798992ms till timeout)
2022-03-30 17:13:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:13:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-1)
2022-03-30 17:13:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1797987ms till timeout)
2022-03-30 17:13:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:13:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-1)
2022-03-30 17:13:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1796982ms till timeout)
2022-03-30 17:13:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:13:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-1)
2022-03-30 17:13:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1795977ms till timeout)
2022-03-30 17:13:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:13:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-1)
2022-03-30 17:13:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1794969ms till timeout)
2022-03-30 17:13:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:13:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-1)
2022-03-30 17:13:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1793964ms till timeout)
2022-03-30 17:13:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:13:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-1)
2022-03-30 17:13:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1792959ms till timeout)
2022-03-30 17:13:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:13:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-1)
2022-03-30 17:13:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1791954ms till timeout)
2022-03-30 17:13:42 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:13:42 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-1)
2022-03-30 17:13:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1790949ms till timeout)
2022-03-30 17:13:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:13:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-1)
2022-03-30 17:13:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1789944ms till timeout)
2022-03-30 17:13:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:13:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-1)
2022-03-30 17:13:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1788939ms till timeout)
2022-03-30 17:13:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:13:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-1)
2022-03-30 17:13:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1787934ms till timeout)
2022-03-30 17:13:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:13:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-1)
2022-03-30 17:13:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1786929ms till timeout)
2022-03-30 17:13:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:13:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-1)
2022-03-30 17:13:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1785924ms till timeout)
2022-03-30 17:13:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:13:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-1)
2022-03-30 17:13:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1784919ms till timeout)
2022-03-30 17:13:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:13:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-1)
2022-03-30 17:13:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1783914ms till timeout)
2022-03-30 17:13:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:13:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-1)
2022-03-30 17:13:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1782910ms till timeout)
2022-03-30 17:13:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:13:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-1)
2022-03-30 17:13:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1781905ms till timeout)
2022-03-30 17:13:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:13:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-1)
2022-03-30 17:13:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1780900ms till timeout)
2022-03-30 17:13:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:13:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-e6ec2fe1-kafka-1)
2022-03-30 17:13:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1779895ms till timeout)
2022-03-30 17:13:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:13:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:13:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:13:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e6ec2fe1-kafka-0, my-cluster-e6ec2fe1-kafka-1, my-cluster-e6ec2fe1-kafka-2 are ready
2022-03-30 17:13:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1778890ms till timeout)
2022-03-30 17:13:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:13:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:13:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:13:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e6ec2fe1-kafka-0, my-cluster-e6ec2fe1-kafka-1, my-cluster-e6ec2fe1-kafka-2 are ready
2022-03-30 17:13:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1777885ms till timeout)
2022-03-30 17:13:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:13:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:13:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:13:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e6ec2fe1-kafka-0, my-cluster-e6ec2fe1-kafka-1, my-cluster-e6ec2fe1-kafka-2 are ready
2022-03-30 17:13:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1776881ms till timeout)
2022-03-30 17:13:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:13:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:13:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:13:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e6ec2fe1-kafka-0, my-cluster-e6ec2fe1-kafka-1, my-cluster-e6ec2fe1-kafka-2 are ready
2022-03-30 17:13:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1775877ms till timeout)
2022-03-30 17:13:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:13:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:13:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:13:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e6ec2fe1-kafka-0, my-cluster-e6ec2fe1-kafka-1, my-cluster-e6ec2fe1-kafka-2 are ready
2022-03-30 17:13:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1774872ms till timeout)
2022-03-30 17:14:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:14:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:14:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:14:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e6ec2fe1-kafka-0, my-cluster-e6ec2fe1-kafka-1, my-cluster-e6ec2fe1-kafka-2 are ready
2022-03-30 17:14:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1773867ms till timeout)
2022-03-30 17:14:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:14:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:14:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:14:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e6ec2fe1-kafka-0, my-cluster-e6ec2fe1-kafka-1, my-cluster-e6ec2fe1-kafka-2 are ready
2022-03-30 17:14:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1772862ms till timeout)
2022-03-30 17:14:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:14:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:14:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:14:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e6ec2fe1-kafka-0, my-cluster-e6ec2fe1-kafka-1, my-cluster-e6ec2fe1-kafka-2 are ready
2022-03-30 17:14:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1771857ms till timeout)
2022-03-30 17:14:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:14:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:14:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:14:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e6ec2fe1-kafka-0, my-cluster-e6ec2fe1-kafka-1, my-cluster-e6ec2fe1-kafka-2 are ready
2022-03-30 17:14:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1770852ms till timeout)
2022-03-30 17:14:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:14:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:14:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:14:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e6ec2fe1-kafka-0, my-cluster-e6ec2fe1-kafka-1, my-cluster-e6ec2fe1-kafka-2 are ready
2022-03-30 17:14:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-e6ec2fe1-kafka, strimzi.io/cluster=my-cluster-e6ec2fe1, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1769847ms till timeout)
2022-03-30 17:14:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-0 not ready: kafka)
2022-03-30 17:14:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-1 not ready: kafka)
2022-03-30 17:14:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e6ec2fe1-kafka-2 not ready: kafka)
2022-03-30 17:14:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e6ec2fe1-kafka-0, my-cluster-e6ec2fe1-kafka-1, my-cluster-e6ec2fe1-kafka-2 are ready
2022-03-30 17:14:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-e6ec2fe1 will have desired state: Ready
2022-03-30 17:14:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-e6ec2fe1 will have desired state: Ready
2022-03-30 17:14:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-e6ec2fe1 is in desired state: Ready
2022-03-30 17:14:05 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-e6ec2fe1 is ready
2022-03-30 17:14:05 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateST:356] Kafka scale down to 3 finished
2022-03-30 17:14:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Consumer will successfully receive messages.
2022-03-30 17:14:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5dde1211, which are set.
2022-03-30 17:14:05 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@1eff24e7, messages=[], arguments=[--group-id, my-consumer-group-58338882, --topic, my-topic-1496158521-938246156, --bootstrap-server, my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093, USER=my_user_1975958319_512406578, --max-messages, 100, --group-instance-id, instance1958072185], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-e6ec2fe1-kafka-clients-7b78f4f64c-fxdc9', podNamespace='namespace-5', bootstrapServer='my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093', topicName='my-topic-1496158521-938246156', maxMessages=100, kafkaUsername='my-user-1975958319-512406578', consumerGroupName='my-consumer-group-58338882', consumerInstanceId='instance1958072185', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5dde1211}
2022-03-30 17:14:05 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093:my-topic-1496158521-938246156 from pod my-cluster-e6ec2fe1-kafka-clients-7b78f4f64c-fxdc9
2022-03-30 17:14:05 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e6ec2fe1-kafka-clients-7b78f4f64c-fxdc9 -n namespace-5 -- /opt/kafka/consumer.sh --group-id my-consumer-group-58338882 --topic my-topic-1496158521-938246156 --bootstrap-server my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093 USER=my_user_1975958319_512406578 --max-messages 100 --group-instance-id instance1958072185
2022-03-30 17:14:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-e6ec2fe1-kafka-clients-7b78f4f64c-fxdc9 -n namespace-5 -- /opt/kafka/consumer.sh --group-id my-consumer-group-58338882 --topic my-topic-1496158521-938246156 --bootstrap-server my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093 USER=my_user_1975958319_512406578 --max-messages 100 --group-instance-id instance1958072185
2022-03-30 17:14:12 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 17:14:12 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-03-30 17:14:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1496158521-938246156-new in namespace namespace-5
2022-03-30 17:14:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-03-30 17:14:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1496158521-938246156-new
2022-03-30 17:14:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready
2022-03-30 17:14:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready
2022-03-30 17:14:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 17:14:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (178995ms till timeout)
2022-03-30 17:14:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (177992ms till timeout)
2022-03-30 17:14:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (176989ms till timeout)
2022-03-30 17:14:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (175986ms till timeout)
2022-03-30 17:14:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (174982ms till timeout)
2022-03-30 17:14:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (173979ms till timeout)
2022-03-30 17:14:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (172976ms till timeout)
2022-03-30 17:14:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (171973ms till timeout)
2022-03-30 17:14:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (170970ms till timeout)
2022-03-30 17:14:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (169967ms till timeout)
2022-03-30 17:14:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (168965ms till timeout)
2022-03-30 17:14:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (167961ms till timeout)
2022-03-30 17:14:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (166959ms till timeout)
2022-03-30 17:14:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (165955ms till timeout)
2022-03-30 17:14:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (164952ms till timeout)
2022-03-30 17:14:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (163950ms till timeout)
2022-03-30 17:14:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (162947ms till timeout)
2022-03-30 17:14:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (161944ms till timeout)
2022-03-30 17:14:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (160941ms till timeout)
2022-03-30 17:14:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (159938ms till timeout)
2022-03-30 17:14:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (158935ms till timeout)
2022-03-30 17:14:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (157932ms till timeout)
2022-03-30 17:14:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (156929ms till timeout)
2022-03-30 17:14:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (155926ms till timeout)
2022-03-30 17:14:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (154922ms till timeout)
2022-03-30 17:14:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (153919ms till timeout)
2022-03-30 17:14:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (152915ms till timeout)
2022-03-30 17:14:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (151912ms till timeout)
2022-03-30 17:14:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (150909ms till timeout)
2022-03-30 17:14:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (149906ms till timeout)
2022-03-30 17:14:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (148903ms till timeout)
2022-03-30 17:14:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (147900ms till timeout)
2022-03-30 17:14:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (146897ms till timeout)
2022-03-30 17:14:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (145894ms till timeout)
2022-03-30 17:14:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (144890ms till timeout)
2022-03-30 17:14:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (143887ms till timeout)
2022-03-30 17:14:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (142884ms till timeout)
2022-03-30 17:14:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (141881ms till timeout)
2022-03-30 17:14:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (140879ms till timeout)
2022-03-30 17:14:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (139875ms till timeout)
2022-03-30 17:14:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (138873ms till timeout)
2022-03-30 17:14:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (137869ms till timeout)
2022-03-30 17:14:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (136867ms till timeout)
2022-03-30 17:14:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (135863ms till timeout)
2022-03-30 17:14:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (134861ms till timeout)
2022-03-30 17:14:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (133857ms till timeout)
2022-03-30 17:14:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (132855ms till timeout)
2022-03-30 17:15:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (131852ms till timeout)
2022-03-30 17:15:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (130849ms till timeout)
2022-03-30 17:15:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (129845ms till timeout)
2022-03-30 17:15:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (128842ms till timeout)
2022-03-30 17:15:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (127839ms till timeout)
2022-03-30 17:15:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (126836ms till timeout)
2022-03-30 17:15:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (125833ms till timeout)
2022-03-30 17:15:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (124830ms till timeout)
2022-03-30 17:15:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (123827ms till timeout)
2022-03-30 17:15:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (122824ms till timeout)
2022-03-30 17:15:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (121821ms till timeout)
2022-03-30 17:15:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (120818ms till timeout)
2022-03-30 17:15:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (119815ms till timeout)
2022-03-30 17:15:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (118812ms till timeout)
2022-03-30 17:15:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (117809ms till timeout)
2022-03-30 17:15:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (116806ms till timeout)
2022-03-30 17:15:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (115803ms till timeout)
2022-03-30 17:15:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (114800ms till timeout)
2022-03-30 17:15:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (113797ms till timeout)
2022-03-30 17:15:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (112794ms till timeout)
2022-03-30 17:15:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (111791ms till timeout)
2022-03-30 17:15:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (110788ms till timeout)
2022-03-30 17:15:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (109784ms till timeout)
2022-03-30 17:15:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (108781ms till timeout)
2022-03-30 17:15:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (107779ms till timeout)
2022-03-30 17:15:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (106776ms till timeout)
2022-03-30 17:15:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (105772ms till timeout)
2022-03-30 17:15:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (104769ms till timeout)
2022-03-30 17:15:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (103767ms till timeout)
2022-03-30 17:15:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (102764ms till timeout)
2022-03-30 17:15:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (101761ms till timeout)
2022-03-30 17:15:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (100758ms till timeout)
2022-03-30 17:15:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1496158521-938246156-new will have desired state: Ready not ready, will try again in 1000 ms (99754ms till timeout)
2022-03-30 17:15:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1496158521-938246156-new is in desired state: Ready
2022-03-30 17:15:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Producer and consumer will successfully send and receive messages.
2022-03-30 17:15:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@42c5fb55, which are set.
2022-03-30 17:15:33 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@4fd1734f, messages=[], arguments=[--topic, my-topic-1496158521-938246156-new, --bootstrap-server, my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093, USER=my_user_1975958319_512406578, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-e6ec2fe1-kafka-clients-7b78f4f64c-fxdc9', podNamespace='namespace-5', bootstrapServer='my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093', topicName='my-topic-1496158521-938246156-new', maxMessages=100, kafkaUsername='my-user-1975958319-512406578', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@42c5fb55}
2022-03-30 17:15:33 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093:my-topic-1496158521-938246156-new from pod my-cluster-e6ec2fe1-kafka-clients-7b78f4f64c-fxdc9
2022-03-30 17:15:33 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e6ec2fe1-kafka-clients-7b78f4f64c-fxdc9 -n namespace-5 -- /opt/kafka/producer.sh --topic my-topic-1496158521-938246156-new --bootstrap-server my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093 USER=my_user_1975958319_512406578 --max-messages 100
2022-03-30 17:15:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-e6ec2fe1-kafka-clients-7b78f4f64c-fxdc9 -n namespace-5 -- /opt/kafka/producer.sh --topic my-topic-1496158521-938246156-new --bootstrap-server my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093 USER=my_user_1975958319_512406578 --max-messages 100
2022-03-30 17:15:37 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-30 17:15:37 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-03-30 17:15:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2acdcd7a, which are set.
2022-03-30 17:15:37 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@473f9425, messages=[], arguments=[--group-id, my-consumer-group-357999753, --topic, my-topic-1496158521-938246156-new, --bootstrap-server, my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093, USER=my_user_1975958319_512406578, --max-messages, 100, --group-instance-id, instance1960607817], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-e6ec2fe1-kafka-clients-7b78f4f64c-fxdc9', podNamespace='namespace-5', bootstrapServer='my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093', topicName='my-topic-1496158521-938246156-new', maxMessages=100, kafkaUsername='my-user-1975958319-512406578', consumerGroupName='my-consumer-group-357999753', consumerInstanceId='instance1960607817', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2acdcd7a}
2022-03-30 17:15:37 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093:my-topic-1496158521-938246156-new from pod my-cluster-e6ec2fe1-kafka-clients-7b78f4f64c-fxdc9
2022-03-30 17:15:37 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e6ec2fe1-kafka-clients-7b78f4f64c-fxdc9 -n namespace-5 -- /opt/kafka/consumer.sh --group-id my-consumer-group-357999753 --topic my-topic-1496158521-938246156-new --bootstrap-server my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093 USER=my_user_1975958319_512406578 --max-messages 100 --group-instance-id instance1960607817
2022-03-30 17:15:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-e6ec2fe1-kafka-clients-7b78f4f64c-fxdc9 -n namespace-5 -- /opt/kafka/consumer.sh --group-id my-consumer-group-357999753 --topic my-topic-1496158521-938246156-new --bootstrap-server my-cluster-e6ec2fe1-kafka-bootstrap.namespace-5.svc:9093 USER=my_user_1975958319_512406578 --max-messages 100 --group-instance-id instance1960607817
2022-03-30 17:15:44 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 17:15:44 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-03-30 17:15:44 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 17:15:44 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:675] [rollingupdate.RollingUpdateST - After Each] - Clean up after test
2022-03-30 17:15:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 17:15:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaAndZookeeperScaleUpScaleDown
2022-03-30 17:15:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1496158521-938246156 in namespace namespace-5
2022-03-30 17:15:44 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1975958319-512406578 in namespace namespace-5
2022-03-30 17:15:44 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1975958319-512406578
2022-03-30 17:15:44 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1496158521-938246156
2022-03-30 17:15:44 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1975958319-512406578 not ready, will try again in 10000 ms (179991ms till timeout)
2022-03-30 17:15:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1496158521-938246156 not ready, will try again in 10000 ms (179991ms till timeout)
2022-03-30 17:15:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-e6ec2fe1 in namespace namespace-5
2022-03-30 17:15:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1496158521-938246156-new in namespace namespace-5
2022-03-30 17:15:54 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-e6ec2fe1
2022-03-30 17:15:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1496158521-938246156-new
2022-03-30 17:15:54 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-e6ec2fe1 not ready, will try again in 10000 ms (839994ms till timeout)
2022-03-30 17:15:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1496158521-938246156-new not ready, will try again in 10000 ms (179989ms till timeout)
2022-03-30 17:16:04 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-e6ec2fe1-kafka-clients in namespace namespace-5
2022-03-30 17:16:04 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-e6ec2fe1-kafka-clients
2022-03-30 17:16:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-e6ec2fe1-kafka-clients not ready, will try again in 10000 ms (479946ms till timeout)
2022-03-30 17:16:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-e6ec2fe1-kafka-clients not ready, will try again in 10000 ms (469938ms till timeout)
2022-03-30 17:16:24 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-e6ec2fe1-kafka-clients not ready, will try again in 10000 ms (459929ms till timeout)
2022-03-30 17:16:34 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-e6ec2fe1-kafka-clients not ready, will try again in 10000 ms (449920ms till timeout)
2022-03-30 17:16:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 17:16:44 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-5 for test case:testKafkaAndZookeeperScaleUpScaleDown
2022-03-30 17:16:44 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-5 removal
2022-03-30 17:16:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace namespace-5 -o yaml
2022-03-30 17:16:44 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace namespace-5 -o yaml
2022-03-30 17:16:44 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:16:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (479929ms till timeout)
2022-03-30 17:16:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace namespace-5 -o yaml
2022-03-30 17:16:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace namespace-5 -o yaml
2022-03-30 17:16:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:16:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (478855ms till timeout)
2022-03-30 17:16:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace namespace-5 -o yaml
2022-03-30 17:16:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace namespace-5 -o yaml
2022-03-30 17:16:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:16:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (477781ms till timeout)
2022-03-30 17:16:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace namespace-5 -o yaml
2022-03-30 17:16:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace namespace-5 -o yaml
2022-03-30 17:16:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:16:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (476702ms till timeout)
2022-03-30 17:16:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace namespace-5 -o yaml
2022-03-30 17:16:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace namespace-5 -o yaml
2022-03-30 17:16:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:16:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (475627ms till timeout)
2022-03-30 17:16:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace namespace-5 -o yaml
2022-03-30 17:16:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace namespace-5 -o yaml
2022-03-30 17:16:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:16:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (474549ms till timeout)
2022-03-30 17:16:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace namespace-5 -o yaml
2022-03-30 17:16:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-5 get Namespace namespace-5 -o yaml
2022-03-30 17:16:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 17:16:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 17:16:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-5" not found
2022-03-30 17:16:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 17:16:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[], io.strimzi.test.logs.CollectorElement@3d7a59f3=[], io.strimzi.test.logs.CollectorElement@dce7372d=[rolling-update-st], io.strimzi.test.logs.CollectorElement@7386938d=[], io.strimzi.test.logs.CollectorElement@afd54539=[], io.strimzi.test.logs.CollectorElement@cf7c9d53=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@bfb2174d=[]}
2022-03-30 17:16:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:267] testKafkaAndZookeeperScaleUpScaleDown - Notifies waiting test cases:[testUpdateUser, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testCruiseControlBasicAPIRequests, testSendMessagesTlsScramSha, testSendMessagesCustomListenerTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown] to and randomly select one to start execution
2022-03-30 17:16:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:93] [rollingupdate.RollingUpdateST] - Removing parallel test: testKafkaAndZookeeperScaleUpScaleDown
2022-03-30 17:16:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:97] [rollingupdate.RollingUpdateST] - Parallel test count: 0
2022-03-30 17:16:50 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testKafkaAndZookeeperScaleUpScaleDown-FINISHED
2022-03-30 17:16:50 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 17:16:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 17:16:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:690] [rollingupdate.RollingUpdateST - After All] - Clean up after test suite
2022-03-30 17:16:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 17:16:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context RollingUpdateST is everything deleted.
2022-03-30 17:16:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 17:16:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace rolling-update-st removal
2022-03-30 17:16:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace rolling-update-st -o yaml
2022-03-30 17:16:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace rolling-update-st -o yaml
2022-03-30 17:16:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:16:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace rolling-update-st removal not ready, will try again in 1000 ms (479934ms till timeout)
2022-03-30 17:16:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace rolling-update-st -o yaml
2022-03-30 17:16:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace rolling-update-st -o yaml
2022-03-30 17:16:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:16:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace rolling-update-st removal not ready, will try again in 1000 ms (478858ms till timeout)
2022-03-30 17:16:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace rolling-update-st -o yaml
2022-03-30 17:16:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace rolling-update-st -o yaml
2022-03-30 17:16:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:16:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace rolling-update-st removal not ready, will try again in 1000 ms (477790ms till timeout)
2022-03-30 17:16:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace rolling-update-st -o yaml
2022-03-30 17:16:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace rolling-update-st -o yaml
2022-03-30 17:16:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:16:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace rolling-update-st removal not ready, will try again in 1000 ms (476722ms till timeout)
2022-03-30 17:16:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace rolling-update-st -o yaml
2022-03-30 17:16:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-5 get Namespace rolling-update-st -o yaml
2022-03-30 17:16:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:16:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace rolling-update-st removal not ready, will try again in 1000 ms (475646ms till timeout)
2022-03-30 17:16:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-5 get Namespace rolling-update-st -o yaml
2022-03-30 17:16:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-5 get Namespace rolling-update-st -o yaml
2022-03-30 17:16:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 17:16:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 17:16:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "rolling-update-st" not found
2022-03-30 17:16:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 17:16:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[], io.strimzi.test.logs.CollectorElement@3d7a59f3=[], io.strimzi.test.logs.CollectorElement@dce7372d=[], io.strimzi.test.logs.CollectorElement@7386938d=[], io.strimzi.test.logs.CollectorElement@afd54539=[], io.strimzi.test.logs.CollectorElement@cf7c9d53=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@bfb2174d=[]}
2022-03-30 17:16:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:254] RollingUpdateST - Notifies waiting test suites:[UserST, HttpBridgeTlsST, CruiseControlApiST, CruiseControlST, ListenersST, SecurityST, RollingUpdateST] to and randomly select one to start execution
2022-03-30 17:16:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:85] [rollingupdate.RollingUpdateST] - Removing parallel suite: RollingUpdateST
2022-03-30 17:16:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:89] [rollingupdate.RollingUpdateST] - Parallel suites count: 0
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 644.266 s - in io.strimzi.systemtest.rollingupdate.RollingUpdateST
[[1;34mINFO[m] Running io.strimzi.systemtest.specific.SpecificIsolatedST
2022-03-30 17:16:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 17:16:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:667] [specific.SpecificIsolatedST - Before All] - Setup test suite environment
2022-03-30 17:16:56 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 17:16:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 0
2022-03-30 17:17:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 1
2022-03-30 17:17:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 0
2022-03-30 17:17:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 2
2022-03-30 17:17:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 0
2022-03-30 17:17:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 3
2022-03-30 17:17:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 0
2022-03-30 17:17:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 4
2022-03-30 17:17:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 0
2022-03-30 17:17:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 5
2022-03-30 17:17:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:136] Suite specific.SpecificIsolatedST has locked the @IsolatedSuite and other @IsolatedSuites must wait until lock is released.
2022-03-30 17:17:21 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-30 17:17:21 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-30 17:17:21 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-30 17:17:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 17:17:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-30 17:17:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 17:17:21 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 17:17:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 17:17:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:17:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 17:17:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 17:17:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 17:17:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 17:17:21 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 17:17:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 17:17:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io not ready, will try again in 10000 ms (179971ms till timeout)
2022-03-30 17:17:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179828ms till timeout)
2022-03-30 17:17:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 17:17:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 17:17:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io not ready, will try again in 10000 ms (179993ms till timeout)
2022-03-30 17:17:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 17:17:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 17:17:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179921ms till timeout)
2022-03-30 17:17:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 17:17:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 17:17:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179946ms till timeout)
2022-03-30 17:17:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 17:17:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 17:17:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 17:17:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 17:17:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 17:17:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 17:17:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 17:17:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 17:17:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 17:17:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 17:17:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 17:17:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 17:17:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 17:17:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 17:17:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:17:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 17:17:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 17:17:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 17:17:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 17:17:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 17:17:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 17:17:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 17:17:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 17:17:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 17:17:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 17:17:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 17:17:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:17:41 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 17:17:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator not ready, will try again in 10000 ms (479975ms till timeout)
2022-03-30 17:17:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 17:17:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 17:17:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 17:17:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 17:17:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:17:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 17:17:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 17:17:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-30 17:17:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 17:17:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 17:17:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v56979
2022-03-30 17:17:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v56979
2022-03-30 17:17:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=56979&allowWatchBookmarks=true&watch=true...
2022-03-30 17:17:51 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 17:17:56 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 56993
2022-03-30 17:17:56 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 56994
2022-03-30 17:17:56 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v56993 in namespace default
2022-03-30 17:17:56 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@7199a026
2022-03-30 17:17:56 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 17:17:56 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@c556a7b
2022-03-30 17:17:56 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5e99e8d5
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-30 17:17:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@cbb87a8, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5e99e8d5, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-30 17:17:56 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-30 17:17:56 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@c556a7b
2022-03-30 17:17:56 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@c556a7b
2022-03-30 17:17:56 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-30 17:17:56 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 17:17:56 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 17:17:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-30 17:17:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T17:17:56Z",
        "name": "infra-namespace",
        "resourceVersion": "56995",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "28b98991-634f-4330-9890-6612d4d059ce"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 17:17:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479991ms till timeout)
2022-03-30 17:17:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478987ms till timeout)
2022-03-30 17:17:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477984ms till timeout)
2022-03-30 17:18:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476980ms till timeout)
2022-03-30 17:18:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475977ms till timeout)
2022-03-30 17:18:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474974ms till timeout)
2022-03-30 17:18:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473971ms till timeout)
2022-03-30 17:18:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472968ms till timeout)
2022-03-30 17:18:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471965ms till timeout)
2022-03-30 17:18:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470961ms till timeout)
2022-03-30 17:18:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469958ms till timeout)
2022-03-30 17:18:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468955ms till timeout)
2022-03-30 17:18:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467952ms till timeout)
2022-03-30 17:18:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466948ms till timeout)
2022-03-30 17:18:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465945ms till timeout)
2022-03-30 17:18:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464942ms till timeout)
2022-03-30 17:18:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463939ms till timeout)
2022-03-30 17:18:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462935ms till timeout)
2022-03-30 17:18:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461932ms till timeout)
2022-03-30 17:18:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (460929ms till timeout)
2022-03-30 17:18:17 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-30 17:18:17 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-30 17:18:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-30 17:18:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-pxwxj not ready: strimzi-cluster-operator)
2022-03-30 17:18:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-pxwxj are ready
2022-03-30 17:18:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 17:18:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-pxwxj not ready: strimzi-cluster-operator)
2022-03-30 17:18:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-pxwxj are ready
2022-03-30 17:18:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-30 17:18:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-pxwxj not ready: strimzi-cluster-operator)
2022-03-30 17:18:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-pxwxj are ready
2022-03-30 17:18:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597990ms till timeout)
2022-03-30 17:18:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-pxwxj not ready: strimzi-cluster-operator)
2022-03-30 17:18:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-pxwxj are ready
2022-03-30 17:18:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596986ms till timeout)
2022-03-30 17:18:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-pxwxj not ready: strimzi-cluster-operator)
2022-03-30 17:18:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-pxwxj are ready
2022-03-30 17:18:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595983ms till timeout)
2022-03-30 17:18:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-pxwxj not ready: strimzi-cluster-operator)
2022-03-30 17:18:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-pxwxj are ready
2022-03-30 17:18:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594979ms till timeout)
2022-03-30 17:18:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-pxwxj not ready: strimzi-cluster-operator)
2022-03-30 17:18:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-pxwxj are ready
2022-03-30 17:18:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593976ms till timeout)
2022-03-30 17:18:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-pxwxj not ready: strimzi-cluster-operator)
2022-03-30 17:18:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-pxwxj are ready
2022-03-30 17:18:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592970ms till timeout)
2022-03-30 17:18:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-pxwxj not ready: strimzi-cluster-operator)
2022-03-30 17:18:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-pxwxj are ready
2022-03-30 17:18:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591967ms till timeout)
2022-03-30 17:18:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-pxwxj not ready: strimzi-cluster-operator)
2022-03-30 17:18:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-pxwxj are ready
2022-03-30 17:18:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590963ms till timeout)
2022-03-30 17:18:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-pxwxj not ready: strimzi-cluster-operator)
2022-03-30 17:18:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-pxwxj are ready
2022-03-30 17:18:27 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-30 17:18:27 [ForkJoinPool-3-worker-3] [32mINFO [m [SpecificIsolatedST:503] 0.21.4
2022-03-30 17:18:27 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 17:18:27 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.specific.SpecificIsolatedST.testRackAwareConnectCorrectDeployment-STARTED
2022-03-30 17:18:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 17:18:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:659] [specific.SpecificIsolatedST - Before Each] - Setup test case environment
2022-03-30 17:18:27 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 17:18:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2, testSendSimpleMessageTls=my-cluster-128d9a40, testUpdateUser=my-cluster-75998b92, testSendMessagesTlsScramSha=my-cluster-a11b9173, testReceiveSimpleMessageTls=my-cluster-90021a9f, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e, testCruiseControlBasicAPIRequests=my-cluster-6da231a3, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc}
2022-03-30 17:18:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1975958319-512406578, testAutoRenewAllCaCertsTriggeredByAnno=my-user-308582398-741623568, testSendSimpleMessageTls=my-user-323988616-1355569855, testUpdateUser=my-user-275427717-1183444028, testSendMessagesTlsScramSha=my-user-1791509549-1350003609, testReceiveSimpleMessageTls=my-user-491397521-1196361913, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-695287754-1004738297, testSendMessagesCustomListenerTlsScramSha=my-user-1653975062-1727371851, testCruiseControlBasicAPIRequests=my-user-918568860-36694764, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-83332781-1273282344, testRackAwareConnectCorrectDeployment=my-user-1839832768-1329691231}
2022-03-30 17:18:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-1496158521-938246156, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1728678022-515015095, testSendSimpleMessageTls=my-topic-1040221322-670398147, testUpdateUser=my-topic-275186280-765877519, testSendMessagesTlsScramSha=my-topic-1482193652-649135537, testReceiveSimpleMessageTls=my-topic-941665209-436577257, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1609923788-1785060017, testSendMessagesCustomListenerTlsScramSha=my-topic-629448478-708445494, testCruiseControlBasicAPIRequests=my-topic-710218582-658397020, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1536476030-1482182833, testRackAwareConnectCorrectDeployment=my-topic-1526242410-482838543}
2022-03-30 17:18:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2-kafka-clients, testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients, testSendMessagesTlsScramSha=my-cluster-a11b9173-kafka-clients, testReceiveSimpleMessageTls=my-cluster-90021a9f-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-6da231a3-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc-kafka-clients}
2022-03-30 17:18:27 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-30 17:18:27 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-30 17:18:27 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-30 17:18:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 17:18:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-30 17:18:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 17:18:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 17:18:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 17:18:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io not ready, will try again in 10000 ms (179968ms till timeout)
2022-03-30 17:18:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 17:18:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179868ms till timeout)
2022-03-30 17:18:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:18:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 17:18:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 17:18:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 17:18:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 17:18:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 17:18:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 17:18:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 17:18:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io not ready, will try again in 10000 ms (179993ms till timeout)
2022-03-30 17:18:38 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 17:18:38 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 17:18:38 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179963ms till timeout)
2022-03-30 17:18:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 17:18:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 17:18:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179906ms till timeout)
2022-03-30 17:18:48 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 17:18:48 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 17:18:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io not ready, will try again in 10000 ms (179992ms till timeout)
2022-03-30 17:18:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 17:18:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 17:18:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 17:18:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 17:18:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 17:18:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 17:18:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 17:18:58 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 17:18:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:18:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 17:18:58 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io not ready, will try again in 10000 ms (179971ms till timeout)
2022-03-30 17:18:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator not ready, will try again in 10000 ms (479947ms till timeout)
2022-03-30 17:19:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 17:19:08 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 17:19:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 17:19:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator not ready, will try again in 10000 ms (469939ms till timeout)
2022-03-30 17:19:08 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 17:19:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 17:19:08 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 17:19:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 17:19:08 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 17:19:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 17:19:08 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 17:19:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:19:08 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 17:19:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 17:19:08 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 17:19:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 17:19:08 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 17:19:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 17:19:08 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 17:19:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 17:19:08 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 17:19:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:19:08 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 17:19:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 17:19:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-30 17:19:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 17:19:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 17:19:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v57197
2022-03-30 17:19:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v57197
2022-03-30 17:19:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=57197&allowWatchBookmarks=true&watch=true...
2022-03-30 17:19:18 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 17:19:18 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 57198
2022-03-30 17:19:23 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 57213
2022-03-30 17:19:23 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 57214
2022-03-30 17:19:23 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v57213 in namespace default
2022-03-30 17:19:23 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@12212975
2022-03-30 17:19:23 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 17:19:23 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@231d568f
2022-03-30 17:19:23 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@231d568f
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:223] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@cbb87a8, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5e99e8d5, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=30000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-30 17:19:23 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@231d568f
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-30 17:19:23 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 17:19:23 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T17:19:23Z",
        "name": "infra-namespace",
        "resourceVersion": "57215",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "c1c65f71-e190-48f4-9b5e-c826da23a752"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:19:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 17:19:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 17:19:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-30 17:19:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:19:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 17:19:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 17:19:24 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-30 17:19:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 17:19:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 17:19:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 17:19:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 17:19:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 17:19:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 17:19:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 17:19:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 17:19:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 17:19:24 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 17:19:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:19:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 17:19:24 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 17:19:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 17:19:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 17:19:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 17:19:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 17:19:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 17:19:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 17:19:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 17:19:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 17:19:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:19:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 17:19:24 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 17:19:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 17:19:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479996ms till timeout)
2022-03-30 17:19:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478992ms till timeout)
2022-03-30 17:19:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477989ms till timeout)
2022-03-30 17:19:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476986ms till timeout)
2022-03-30 17:19:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475982ms till timeout)
2022-03-30 17:19:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474979ms till timeout)
2022-03-30 17:19:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473975ms till timeout)
2022-03-30 17:19:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472972ms till timeout)
2022-03-30 17:19:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471969ms till timeout)
2022-03-30 17:19:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470966ms till timeout)
2022-03-30 17:19:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469963ms till timeout)
2022-03-30 17:19:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468959ms till timeout)
2022-03-30 17:19:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467956ms till timeout)
2022-03-30 17:19:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466953ms till timeout)
2022-03-30 17:19:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465949ms till timeout)
2022-03-30 17:19:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464946ms till timeout)
2022-03-30 17:19:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463943ms till timeout)
2022-03-30 17:19:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462939ms till timeout)
2022-03-30 17:19:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461936ms till timeout)
2022-03-30 17:19:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (460933ms till timeout)
2022-03-30 17:19:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (459930ms till timeout)
2022-03-30 17:19:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (458926ms till timeout)
2022-03-30 17:19:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (457923ms till timeout)
2022-03-30 17:19:47 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-30 17:19:47 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-30 17:19:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-30 17:19:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-kh86l not ready: strimzi-cluster-operator)
2022-03-30 17:19:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-kh86l are ready
2022-03-30 17:19:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599957ms till timeout)
2022-03-30 17:19:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-kh86l not ready: strimzi-cluster-operator)
2022-03-30 17:19:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-kh86l are ready
2022-03-30 17:19:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598954ms till timeout)
2022-03-30 17:19:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-kh86l not ready: strimzi-cluster-operator)
2022-03-30 17:19:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-kh86l are ready
2022-03-30 17:19:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597950ms till timeout)
2022-03-30 17:19:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-kh86l not ready: strimzi-cluster-operator)
2022-03-30 17:19:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-kh86l are ready
2022-03-30 17:19:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596946ms till timeout)
2022-03-30 17:19:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-kh86l not ready: strimzi-cluster-operator)
2022-03-30 17:19:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-kh86l are ready
2022-03-30 17:19:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595942ms till timeout)
2022-03-30 17:19:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-kh86l not ready: strimzi-cluster-operator)
2022-03-30 17:19:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-kh86l are ready
2022-03-30 17:19:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594939ms till timeout)
2022-03-30 17:19:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-kh86l not ready: strimzi-cluster-operator)
2022-03-30 17:19:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-kh86l are ready
2022-03-30 17:19:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593935ms till timeout)
2022-03-30 17:19:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-kh86l not ready: strimzi-cluster-operator)
2022-03-30 17:19:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-kh86l are ready
2022-03-30 17:19:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592931ms till timeout)
2022-03-30 17:19:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-kh86l not ready: strimzi-cluster-operator)
2022-03-30 17:19:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-kh86l are ready
2022-03-30 17:19:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591928ms till timeout)
2022-03-30 17:19:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-kh86l not ready: strimzi-cluster-operator)
2022-03-30 17:19:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-kh86l are ready
2022-03-30 17:19:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590924ms till timeout)
2022-03-30 17:19:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-kh86l not ready: strimzi-cluster-operator)
2022-03-30 17:19:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-kh86l are ready
2022-03-30 17:19:57 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-30 17:19:57 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment strimzi-cluster-operator rolling update
2022-03-30 17:19:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Deployment strimzi-cluster-operator rolling update in namespace:infra-namespace
2022-03-30 17:19:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {strimzi-cluster-operator-78689684d4-pxwxj=baf0bad4-96b6-48eb-9de9-b183ae21e8ab}
2022-03-30 17:19:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {strimzi-cluster-operator-77554ffdfb-kh86l=afd75f11-eb38-41f0-8653-238837cd135e}
2022-03-30 17:19:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:119] All pods seem to have rolled
2022-03-30 17:19:57 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 17:19:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 17:19:57 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-30 17:19:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-30 17:19:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-kh86l not ready: strimzi-cluster-operator)
2022-03-30 17:19:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-kh86l are ready
2022-03-30 17:19:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-30 17:19:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-kh86l not ready: strimzi-cluster-operator)
2022-03-30 17:19:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-kh86l are ready
2022-03-30 17:19:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-30 17:19:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-kh86l not ready: strimzi-cluster-operator)
2022-03-30 17:19:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-kh86l are ready
2022-03-30 17:19:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597990ms till timeout)
2022-03-30 17:20:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-kh86l not ready: strimzi-cluster-operator)
2022-03-30 17:20:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-kh86l are ready
2022-03-30 17:20:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596987ms till timeout)
2022-03-30 17:20:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-kh86l not ready: strimzi-cluster-operator)
2022-03-30 17:20:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-kh86l are ready
2022-03-30 17:20:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595983ms till timeout)
2022-03-30 17:20:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-kh86l not ready: strimzi-cluster-operator)
2022-03-30 17:20:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-kh86l are ready
2022-03-30 17:20:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594979ms till timeout)
2022-03-30 17:20:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-kh86l not ready: strimzi-cluster-operator)
2022-03-30 17:20:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-kh86l are ready
2022-03-30 17:20:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593975ms till timeout)
2022-03-30 17:20:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-kh86l not ready: strimzi-cluster-operator)
2022-03-30 17:20:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-kh86l are ready
2022-03-30 17:20:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592972ms till timeout)
2022-03-30 17:20:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-kh86l not ready: strimzi-cluster-operator)
2022-03-30 17:20:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-kh86l are ready
2022-03-30 17:20:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591968ms till timeout)
2022-03-30 17:20:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-kh86l not ready: strimzi-cluster-operator)
2022-03-30 17:20:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-kh86l are ready
2022-03-30 17:20:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590964ms till timeout)
2022-03-30 17:20:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-kh86l not ready: strimzi-cluster-operator)
2022-03-30 17:20:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-kh86l are ready
2022-03-30 17:20:07 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment strimzi-cluster-operator rolling update finished
2022-03-30 17:20:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-b2005abc in namespace infra-namespace
2022-03-30 17:20:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-b2005abc
2022-03-30 17:20:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-b2005abc will have desired state: Ready
2022-03-30 17:20:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-b2005abc will have desired state: Ready
2022-03-30 17:20:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (839998ms till timeout)
2022-03-30 17:20:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (838995ms till timeout)
2022-03-30 17:20:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (837991ms till timeout)
2022-03-30 17:20:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (836988ms till timeout)
2022-03-30 17:20:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (835984ms till timeout)
2022-03-30 17:20:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (834981ms till timeout)
2022-03-30 17:20:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (833978ms till timeout)
2022-03-30 17:20:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (832974ms till timeout)
2022-03-30 17:20:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (831970ms till timeout)
2022-03-30 17:20:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (830967ms till timeout)
2022-03-30 17:20:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (829963ms till timeout)
2022-03-30 17:20:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (828959ms till timeout)
2022-03-30 17:20:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (827955ms till timeout)
2022-03-30 17:20:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (826952ms till timeout)
2022-03-30 17:20:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (825948ms till timeout)
2022-03-30 17:20:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (824945ms till timeout)
2022-03-30 17:20:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (823942ms till timeout)
2022-03-30 17:20:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (822939ms till timeout)
2022-03-30 17:20:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (821936ms till timeout)
2022-03-30 17:20:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (820933ms till timeout)
2022-03-30 17:20:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (819930ms till timeout)
2022-03-30 17:20:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (818926ms till timeout)
2022-03-30 17:20:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (817923ms till timeout)
2022-03-30 17:20:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (816920ms till timeout)
2022-03-30 17:20:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (815916ms till timeout)
2022-03-30 17:20:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (814913ms till timeout)
2022-03-30 17:20:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (813910ms till timeout)
2022-03-30 17:20:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (812905ms till timeout)
2022-03-30 17:20:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (811902ms till timeout)
2022-03-30 17:20:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (810898ms till timeout)
2022-03-30 17:20:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (809895ms till timeout)
2022-03-30 17:20:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (808892ms till timeout)
2022-03-30 17:20:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (807886ms till timeout)
2022-03-30 17:20:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (806883ms till timeout)
2022-03-30 17:20:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (805879ms till timeout)
2022-03-30 17:20:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (804875ms till timeout)
2022-03-30 17:20:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (803872ms till timeout)
2022-03-30 17:20:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (802868ms till timeout)
2022-03-30 17:20:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (801865ms till timeout)
2022-03-30 17:20:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (800861ms till timeout)
2022-03-30 17:20:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (799856ms till timeout)
2022-03-30 17:20:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (798853ms till timeout)
2022-03-30 17:20:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (797849ms till timeout)
2022-03-30 17:20:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (796844ms till timeout)
2022-03-30 17:20:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (795841ms till timeout)
2022-03-30 17:20:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (794837ms till timeout)
2022-03-30 17:20:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (793834ms till timeout)
2022-03-30 17:20:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (792831ms till timeout)
2022-03-30 17:20:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (791828ms till timeout)
2022-03-30 17:20:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (790824ms till timeout)
2022-03-30 17:20:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (789821ms till timeout)
2022-03-30 17:20:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (788817ms till timeout)
2022-03-30 17:20:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (787814ms till timeout)
2022-03-30 17:21:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (786810ms till timeout)
2022-03-30 17:21:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (785807ms till timeout)
2022-03-30 17:21:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (784804ms till timeout)
2022-03-30 17:21:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (783801ms till timeout)
2022-03-30 17:21:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (782798ms till timeout)
2022-03-30 17:21:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (781794ms till timeout)
2022-03-30 17:21:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (780791ms till timeout)
2022-03-30 17:21:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (779788ms till timeout)
2022-03-30 17:21:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (778785ms till timeout)
2022-03-30 17:21:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (777782ms till timeout)
2022-03-30 17:21:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (776777ms till timeout)
2022-03-30 17:21:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (775773ms till timeout)
2022-03-30 17:21:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (774770ms till timeout)
2022-03-30 17:21:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (773767ms till timeout)
2022-03-30 17:21:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (772763ms till timeout)
2022-03-30 17:21:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (771760ms till timeout)
2022-03-30 17:21:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (770755ms till timeout)
2022-03-30 17:21:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (769752ms till timeout)
2022-03-30 17:21:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (768748ms till timeout)
2022-03-30 17:21:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (767745ms till timeout)
2022-03-30 17:21:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (766742ms till timeout)
2022-03-30 17:21:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (765738ms till timeout)
2022-03-30 17:21:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (764735ms till timeout)
2022-03-30 17:21:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (763732ms till timeout)
2022-03-30 17:21:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (762728ms till timeout)
2022-03-30 17:21:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (761724ms till timeout)
2022-03-30 17:21:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (760721ms till timeout)
2022-03-30 17:21:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (759706ms till timeout)
2022-03-30 17:21:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (758703ms till timeout)
2022-03-30 17:21:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (757700ms till timeout)
2022-03-30 17:21:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (756696ms till timeout)
2022-03-30 17:21:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (755693ms till timeout)
2022-03-30 17:21:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-b2005abc is in desired state: Ready
2022-03-30 17:21:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic rw-my-topic-1699747579-993901798 in namespace infra-namespace
2022-03-30 17:21:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:rw-my-topic-1699747579-993901798
2022-03-30 17:21:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: rw-my-topic-1699747579-993901798 will have desired state: Ready
2022-03-30 17:21:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: rw-my-topic-1699747579-993901798 will have desired state: Ready
2022-03-30 17:21:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: rw-my-topic-1699747579-993901798 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 17:21:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: rw-my-topic-1699747579-993901798 is in desired state: Ready
2022-03-30 17:21:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-b2005abc-kafka-clients in namespace infra-namespace
2022-03-30 17:21:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-b2005abc-kafka-clients
2022-03-30 17:21:33 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-b2005abc-kafka-clients will be ready
2022-03-30 17:21:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-b2005abc-kafka-clients will be ready
2022-03-30 17:21:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-b2005abc-kafka-clients will be ready not ready, will try again in 1000 ms (479997ms till timeout)
2022-03-30 17:21:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-b2005abc-kafka-clients will be ready not ready, will try again in 1000 ms (478995ms till timeout)
2022-03-30 17:21:35 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-b2005abc-kafka-clients is ready
2022-03-30 17:21:35 [ForkJoinPool-3-worker-3] [32mINFO [m [SpecificIsolatedST:308] Deploy KafkaConnect with correct rack-aware topology key: rack-key
2022-03-30 17:21:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-b2005abc-scraper in namespace infra-namespace
2022-03-30 17:21:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-b2005abc-scraper
2022-03-30 17:21:35 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-b2005abc-scraper will be ready
2022-03-30 17:21:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-b2005abc-scraper will be ready
2022-03-30 17:21:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-b2005abc-scraper will be ready not ready, will try again in 1000 ms (479998ms till timeout)
2022-03-30 17:21:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-b2005abc-scraper will be ready not ready, will try again in 1000 ms (478996ms till timeout)
2022-03-30 17:21:37 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-b2005abc-scraper is ready
2022-03-30 17:21:37 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-b2005abc-scraper to be ready
2022-03-30 17:21:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-b2005abc-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready
2022-03-30 17:21:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b2005abc-scraper-5c7c554dcc-nswt5 not ready: my-cluster-b2005abc-scraper)
2022-03-30 17:21:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-b2005abc-scraper-5c7c554dcc-nswt5 are ready
2022-03-30 17:21:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-b2005abc-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-30 17:21:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b2005abc-scraper-5c7c554dcc-nswt5 not ready: my-cluster-b2005abc-scraper)
2022-03-30 17:21:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-b2005abc-scraper-5c7c554dcc-nswt5 are ready
2022-03-30 17:21:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-b2005abc-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598992ms till timeout)
2022-03-30 17:21:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b2005abc-scraper-5c7c554dcc-nswt5 not ready: my-cluster-b2005abc-scraper)
2022-03-30 17:21:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-b2005abc-scraper-5c7c554dcc-nswt5 are ready
2022-03-30 17:21:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-b2005abc-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597988ms till timeout)
2022-03-30 17:21:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b2005abc-scraper-5c7c554dcc-nswt5 not ready: my-cluster-b2005abc-scraper)
2022-03-30 17:21:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-b2005abc-scraper-5c7c554dcc-nswt5 are ready
2022-03-30 17:21:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-b2005abc-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596983ms till timeout)
2022-03-30 17:21:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b2005abc-scraper-5c7c554dcc-nswt5 not ready: my-cluster-b2005abc-scraper)
2022-03-30 17:21:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-b2005abc-scraper-5c7c554dcc-nswt5 are ready
2022-03-30 17:21:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-b2005abc-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595979ms till timeout)
2022-03-30 17:21:42 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b2005abc-scraper-5c7c554dcc-nswt5 not ready: my-cluster-b2005abc-scraper)
2022-03-30 17:21:42 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-b2005abc-scraper-5c7c554dcc-nswt5 are ready
2022-03-30 17:21:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-b2005abc-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594974ms till timeout)
2022-03-30 17:21:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b2005abc-scraper-5c7c554dcc-nswt5 not ready: my-cluster-b2005abc-scraper)
2022-03-30 17:21:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-b2005abc-scraper-5c7c554dcc-nswt5 are ready
2022-03-30 17:21:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-b2005abc-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593970ms till timeout)
2022-03-30 17:21:44 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b2005abc-scraper-5c7c554dcc-nswt5 not ready: my-cluster-b2005abc-scraper)
2022-03-30 17:21:44 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-b2005abc-scraper-5c7c554dcc-nswt5 are ready
2022-03-30 17:21:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-b2005abc-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592966ms till timeout)
2022-03-30 17:21:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b2005abc-scraper-5c7c554dcc-nswt5 not ready: my-cluster-b2005abc-scraper)
2022-03-30 17:21:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-b2005abc-scraper-5c7c554dcc-nswt5 are ready
2022-03-30 17:21:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-b2005abc-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591961ms till timeout)
2022-03-30 17:21:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b2005abc-scraper-5c7c554dcc-nswt5 not ready: my-cluster-b2005abc-scraper)
2022-03-30 17:21:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-b2005abc-scraper-5c7c554dcc-nswt5 are ready
2022-03-30 17:21:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-b2005abc-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590956ms till timeout)
2022-03-30 17:21:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b2005abc-scraper-5c7c554dcc-nswt5 not ready: my-cluster-b2005abc-scraper)
2022-03-30 17:21:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-b2005abc-scraper-5c7c554dcc-nswt5 are ready
2022-03-30 17:21:47 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-b2005abc-scraper is ready
2022-03-30 17:21:47 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-b2005abc-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-03-30 17:21:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyResource:227] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=my-cluster-b2005abc-allow, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8083, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9404, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8080, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9999, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-b2005abc, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-b2005abc-connect}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 17:21:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-b2005abc-allow in namespace infra-namespace
2022-03-30 17:21:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:my-cluster-b2005abc-allow
2022-03-30 17:21:47 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-03-30 17:21:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-b2005abc in namespace infra-namespace
2022-03-30 17:21:47 [ForkJoinPool-3-worker-3] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkaconnects' with unstable version 'v1beta2'
2022-03-30 17:21:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaConnect:my-cluster-b2005abc
2022-03-30 17:21:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-b2005abc will have desired state: Ready
2022-03-30 17:21:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaConnect: my-cluster-b2005abc will have desired state: Ready
2022-03-30 17:21:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 17:21:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (598995ms till timeout)
2022-03-30 17:21:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (597991ms till timeout)
2022-03-30 17:21:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (596988ms till timeout)
2022-03-30 17:21:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (595985ms till timeout)
2022-03-30 17:21:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (594982ms till timeout)
2022-03-30 17:21:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (593978ms till timeout)
2022-03-30 17:21:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (592974ms till timeout)
2022-03-30 17:21:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (591971ms till timeout)
2022-03-30 17:21:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (590967ms till timeout)
2022-03-30 17:21:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (589962ms till timeout)
2022-03-30 17:21:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (588958ms till timeout)
2022-03-30 17:21:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (587955ms till timeout)
2022-03-30 17:22:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (586951ms till timeout)
2022-03-30 17:22:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (585948ms till timeout)
2022-03-30 17:22:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (584944ms till timeout)
2022-03-30 17:22:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (583941ms till timeout)
2022-03-30 17:22:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (582937ms till timeout)
2022-03-30 17:22:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (581934ms till timeout)
2022-03-30 17:22:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (580930ms till timeout)
2022-03-30 17:22:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (579927ms till timeout)
2022-03-30 17:22:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (578923ms till timeout)
2022-03-30 17:22:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (577920ms till timeout)
2022-03-30 17:22:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (576916ms till timeout)
2022-03-30 17:22:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (575912ms till timeout)
2022-03-30 17:22:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (574909ms till timeout)
2022-03-30 17:22:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (573905ms till timeout)
2022-03-30 17:22:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (572902ms till timeout)
2022-03-30 17:22:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (571899ms till timeout)
2022-03-30 17:22:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (570895ms till timeout)
2022-03-30 17:22:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (569892ms till timeout)
2022-03-30 17:22:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (568888ms till timeout)
2022-03-30 17:22:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (567885ms till timeout)
2022-03-30 17:22:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (566882ms till timeout)
2022-03-30 17:22:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (565878ms till timeout)
2022-03-30 17:22:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (564875ms till timeout)
2022-03-30 17:22:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (563871ms till timeout)
2022-03-30 17:22:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (562868ms till timeout)
2022-03-30 17:22:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (561865ms till timeout)
2022-03-30 17:22:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (560861ms till timeout)
2022-03-30 17:22:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (559858ms till timeout)
2022-03-30 17:22:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (558854ms till timeout)
2022-03-30 17:22:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (557851ms till timeout)
2022-03-30 17:22:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (556847ms till timeout)
2022-03-30 17:22:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (555844ms till timeout)
2022-03-30 17:22:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (554840ms till timeout)
2022-03-30 17:22:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (553837ms till timeout)
2022-03-30 17:22:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (552833ms till timeout)
2022-03-30 17:22:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (551829ms till timeout)
2022-03-30 17:22:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (550826ms till timeout)
2022-03-30 17:22:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (549822ms till timeout)
2022-03-30 17:22:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (548819ms till timeout)
2022-03-30 17:22:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (547816ms till timeout)
2022-03-30 17:22:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (546812ms till timeout)
2022-03-30 17:22:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (545809ms till timeout)
2022-03-30 17:22:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (544806ms till timeout)
2022-03-30 17:22:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (543803ms till timeout)
2022-03-30 17:22:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (542799ms till timeout)
2022-03-30 17:22:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (541796ms till timeout)
2022-03-30 17:22:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (540792ms till timeout)
2022-03-30 17:22:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (539789ms till timeout)
2022-03-30 17:22:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (538786ms till timeout)
2022-03-30 17:22:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (537782ms till timeout)
2022-03-30 17:22:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (536779ms till timeout)
2022-03-30 17:22:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (535776ms till timeout)
2022-03-30 17:22:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (534773ms till timeout)
2022-03-30 17:22:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (533769ms till timeout)
2022-03-30 17:22:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (532766ms till timeout)
2022-03-30 17:22:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (531762ms till timeout)
2022-03-30 17:22:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (530758ms till timeout)
2022-03-30 17:22:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (529755ms till timeout)
2022-03-30 17:22:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (528752ms till timeout)
2022-03-30 17:23:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-b2005abc will have desired state: Ready not ready, will try again in 1000 ms (527748ms till timeout)
2022-03-30 17:23:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-b2005abc is in desired state: Ready
2022-03-30 17:23:01 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-b2005abc-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-03-30 17:23:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyResource:227] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=my-cluster-b2005abc-allow, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8083, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9404, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8080, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9999, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-b2005abc, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-b2005abc-connect}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 17:23:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-b2005abc-allow in namespace infra-namespace
2022-03-30 17:23:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:my-cluster-b2005abc-allow
2022-03-30 17:23:01 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-03-30 17:23:01 [ForkJoinPool-3-worker-3] [32mINFO [m [SpecificIsolatedST:333] PodName: my-cluster-b2005abc-connect-547649b948-5xvzp
NodeAffinity: NodeAffinity(preferredDuringSchedulingIgnoredDuringExecution=[], requiredDuringSchedulingIgnoredDuringExecution=NodeSelector(nodeSelectorTerms=[NodeSelectorTerm(matchExpressions=[NodeSelectorRequirement(key=rack-key, operator=Exists, values=[], additionalProperties={})], matchFields=[], additionalProperties={})], additionalProperties={}), additionalProperties={})
2022-03-30 17:23:01 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:154] Send and receive messages through KafkaConnect
2022-03-30 17:23:01 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:63] Waiting until KafkaConnect API is available
2022-03-30 17:23:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Waiting until KafkaConnect API is available
2022-03-30 17:23:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace exec my-cluster-b2005abc-connect-547649b948-5xvzp -- /bin/bash -c curl -I http://localhost:8083/connectors
2022-03-30 17:23:01 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-b2005abc-connect-547649b948-5xvzp -- /bin/bash -c curl -I http://localhost:8083/connectors
2022-03-30 17:23:01 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-03-30 17:23:01 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:66] KafkaConnect API is available
2022-03-30 17:23:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace exec my-cluster-b2005abc-kafka-clients-7c7d97c4dd-nbv5s -- /bin/bash -c curl -X POST -H "Content-Type: application/json" --data '{ "name": "sink-test", "config": { "connector.class": "FileStreamSink", "tasks.max": "1", "topics": "rw-my-topic-1699747579-993901798", "file": "/tmp/test-file-sink.txt" } }' http://my-cluster-b2005abc-connect-api.infra-namespace.svc:8083/connectors
2022-03-30 17:23:01 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-b2005abc-kafka-clients-7c7d97c4dd-nbv5s -- /bin/bash -c curl -X POST -H "Content-Type: application/json" --data '{ "name": "sink-test", "config": { "connector.class": "FileStreamSink", "tasks.max": "1", "topics": "rw-my-topic-1699747579-993901798", "file": "/tmp/test-file-sink.txt" } }' http://my-cluster-b2005abc-connect-api.infra-namespace.svc:8083/connectors
2022-03-30 17:23:01 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-03-30 17:23:01 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-30 17:23:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1f6eae44, which are set.
2022-03-30 17:23:01 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@66d9fec6, messages=[], arguments=[--topic, rw-my-topic-1699747579-993901798, --bootstrap-server, my-cluster-b2005abc-kafka-bootstrap.infra-namespace.svc:9092, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-b2005abc-kafka-clients-7c7d97c4dd-nbv5s', podNamespace='infra-namespace', bootstrapServer='my-cluster-b2005abc-kafka-bootstrap.infra-namespace.svc:9092', topicName='rw-my-topic-1699747579-993901798', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1f6eae44}
2022-03-30 17:23:01 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-b2005abc-kafka-bootstrap.infra-namespace.svc:9092:rw-my-topic-1699747579-993901798 from pod my-cluster-b2005abc-kafka-clients-7c7d97c4dd-nbv5s
2022-03-30 17:23:01 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-b2005abc-kafka-clients-7c7d97c4dd-nbv5s -n infra-namespace -- /opt/kafka/producer.sh --topic rw-my-topic-1699747579-993901798 --bootstrap-server my-cluster-b2005abc-kafka-bootstrap.infra-namespace.svc:9092 --max-messages 100
2022-03-30 17:23:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-b2005abc-kafka-clients-7c7d97c4dd-nbv5s -n infra-namespace -- /opt/kafka/producer.sh --topic rw-my-topic-1699747579-993901798 --bootstrap-server my-cluster-b2005abc-kafka-bootstrap.infra-namespace.svc:9092 --max-messages 100
2022-03-30 17:23:04 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-03-30 17:23:04 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-03-30 17:23:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3d4b6853, which are set.
2022-03-30 17:23:04 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@3fc244d3, messages=[], arguments=[--group-id, my-consumer-group-936544850, --topic, rw-my-topic-1699747579-993901798, --bootstrap-server, my-cluster-b2005abc-kafka-bootstrap.infra-namespace.svc:9092, --max-messages, 100, --group-instance-id, instance1019427628], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-b2005abc-kafka-clients-7c7d97c4dd-nbv5s', podNamespace='infra-namespace', bootstrapServer='my-cluster-b2005abc-kafka-bootstrap.infra-namespace.svc:9092', topicName='rw-my-topic-1699747579-993901798', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-936544850', consumerInstanceId='instance1019427628', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3d4b6853}
2022-03-30 17:23:04 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-b2005abc-kafka-bootstrap.infra-namespace.svc:9092#rw-my-topic-1699747579-993901798 from pod my-cluster-b2005abc-kafka-clients-7c7d97c4dd-nbv5s
2022-03-30 17:23:04 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-b2005abc-kafka-clients-7c7d97c4dd-nbv5s -n infra-namespace -- /opt/kafka/consumer.sh --group-id my-consumer-group-936544850 --topic rw-my-topic-1699747579-993901798 --bootstrap-server my-cluster-b2005abc-kafka-bootstrap.infra-namespace.svc:9092 --max-messages 100 --group-instance-id instance1019427628
2022-03-30 17:23:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-b2005abc-kafka-clients-7c7d97c4dd-nbv5s -n infra-namespace -- /opt/kafka/consumer.sh --group-id my-consumer-group-936544850 --topic rw-my-topic-1699747579-993901798 --bootstrap-server my-cluster-b2005abc-kafka-bootstrap.infra-namespace.svc:9092 --max-messages 100 --group-instance-id instance1019427628
2022-03-30 17:23:09 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-30 17:23:09 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-03-30 17:23:09 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-b2005abc-connect-547649b948-5xvzp
2022-03-30 17:23:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for messages in file sink
2022-03-30 17:23:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace exec my-cluster-b2005abc-connect-547649b948-5xvzp -- /bin/bash -c cat /tmp/test-file-sink.txt
2022-03-30 17:23:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:421] Command: kubectl --namespace infra-namespace exec my-cluster-b2005abc-connect-547649b948-5xvzp -- /bin/bash -c cat /tmp/test-file-sink.txt
2022-03-30 17:23:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:421] Return code: 0
2022-03-30 17:23:09 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:77] Expected messages are in file sink on my-cluster-b2005abc-connect-547649b948-5xvzp
2022-03-30 17:23:09 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-30 17:23:09 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-30 17:23:09 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-30 17:23:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 17:23:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-30 17:23:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 17:23:09 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 17:23:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 17:23:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io not ready, will try again in 10000 ms (179992ms till timeout)
2022-03-30 17:23:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 17:23:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179811ms till timeout)
2022-03-30 17:23:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:23:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 17:23:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 17:23:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 17:23:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 17:23:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 17:23:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io not ready, will try again in 10000 ms (179933ms till timeout)
2022-03-30 17:23:20 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 17:23:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 17:23:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179942ms till timeout)
2022-03-30 17:23:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 17:23:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 17:23:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 17:23:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 17:23:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179948ms till timeout)
2022-03-30 17:23:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 17:23:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 17:23:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 17:23:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 17:23:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 17:23:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 17:23:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 17:23:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 17:23:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io not ready, will try again in 10000 ms (179919ms till timeout)
2022-03-30 17:23:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 17:23:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 17:23:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 17:23:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 17:23:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 17:23:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 17:23:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:23:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 17:23:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator not ready, will try again in 10000 ms (479951ms till timeout)
2022-03-30 17:23:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 17:23:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 17:23:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 17:23:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 17:23:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 17:23:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 17:23:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:23:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 17:23:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 17:23:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 17:23:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 17:23:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 17:23:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 17:23:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 17:23:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 17:23:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 17:23:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:23:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 17:23:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator not ready, will try again in 10000 ms (469942ms till timeout)
2022-03-30 17:24:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 17:24:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-30 17:24:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 17:24:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 17:24:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v57956
2022-03-30 17:24:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v57956
2022-03-30 17:24:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=57956&allowWatchBookmarks=true&watch=true...
2022-03-30 17:24:00 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 17:24:00 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 57958
2022-03-30 17:24:05 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 58072
2022-03-30 17:24:42 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 58106
2022-03-30 17:24:42 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v58072 in namespace default
2022-03-30 17:24:42 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5e99e8d5
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-30 17:24:42 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@5736d8c9
2022-03-30 17:24:42 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@cbb87a8, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5e99e8d5, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-30 17:24:42 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 17:24:42 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-30 17:24:42 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@530956f7
2022-03-30 17:24:42 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@530956f7
2022-03-30 17:24:42 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@530956f7
2022-03-30 17:24:42 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 17:24:42 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 17:24:42 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-30 17:24:42 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-30 17:24:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T17:24:42Z",
        "name": "infra-namespace",
        "resourceVersion": "58107",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "1ca9976f-e7dc-4454-8023-54db7f15edea"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 17:24:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479997ms till timeout)
2022-03-30 17:24:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478994ms till timeout)
2022-03-30 17:24:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477990ms till timeout)
2022-03-30 17:24:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476987ms till timeout)
2022-03-30 17:24:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475984ms till timeout)
2022-03-30 17:24:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474980ms till timeout)
2022-03-30 17:24:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473977ms till timeout)
2022-03-30 17:24:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472974ms till timeout)
2022-03-30 17:24:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471970ms till timeout)
2022-03-30 17:24:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470967ms till timeout)
2022-03-30 17:24:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469964ms till timeout)
2022-03-30 17:24:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468961ms till timeout)
2022-03-30 17:24:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467958ms till timeout)
2022-03-30 17:24:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466954ms till timeout)
2022-03-30 17:24:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465951ms till timeout)
2022-03-30 17:24:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464947ms till timeout)
2022-03-30 17:24:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463944ms till timeout)
2022-03-30 17:25:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462941ms till timeout)
2022-03-30 17:25:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461937ms till timeout)
2022-03-30 17:25:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (460934ms till timeout)
2022-03-30 17:25:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (459931ms till timeout)
2022-03-30 17:25:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (458928ms till timeout)
2022-03-30 17:25:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (457924ms till timeout)
2022-03-30 17:25:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (456921ms till timeout)
2022-03-30 17:25:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (455918ms till timeout)
2022-03-30 17:25:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (454914ms till timeout)
2022-03-30 17:25:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (453911ms till timeout)
2022-03-30 17:25:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (452908ms till timeout)
2022-03-30 17:25:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (451904ms till timeout)
2022-03-30 17:25:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (450901ms till timeout)
2022-03-30 17:25:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (449898ms till timeout)
2022-03-30 17:25:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (448894ms till timeout)
2022-03-30 17:25:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (447891ms till timeout)
2022-03-30 17:25:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (446887ms till timeout)
2022-03-30 17:25:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (445884ms till timeout)
2022-03-30 17:25:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (444880ms till timeout)
2022-03-30 17:25:19 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-30 17:25:19 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-30 17:25:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-30 17:25:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-n64vr not ready: strimzi-cluster-operator)
2022-03-30 17:25:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-n64vr are ready
2022-03-30 17:25:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 17:25:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-n64vr not ready: strimzi-cluster-operator)
2022-03-30 17:25:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-n64vr are ready
2022-03-30 17:25:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-30 17:25:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-n64vr not ready: strimzi-cluster-operator)
2022-03-30 17:25:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-n64vr are ready
2022-03-30 17:25:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597990ms till timeout)
2022-03-30 17:25:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-n64vr not ready: strimzi-cluster-operator)
2022-03-30 17:25:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-n64vr are ready
2022-03-30 17:25:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596986ms till timeout)
2022-03-30 17:25:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-n64vr not ready: strimzi-cluster-operator)
2022-03-30 17:25:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-n64vr are ready
2022-03-30 17:25:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595982ms till timeout)
2022-03-30 17:25:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-n64vr not ready: strimzi-cluster-operator)
2022-03-30 17:25:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-n64vr are ready
2022-03-30 17:25:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594978ms till timeout)
2022-03-30 17:25:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-n64vr not ready: strimzi-cluster-operator)
2022-03-30 17:25:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-n64vr are ready
2022-03-30 17:25:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593974ms till timeout)
2022-03-30 17:25:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-n64vr not ready: strimzi-cluster-operator)
2022-03-30 17:25:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-n64vr are ready
2022-03-30 17:25:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592970ms till timeout)
2022-03-30 17:25:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-n64vr not ready: strimzi-cluster-operator)
2022-03-30 17:25:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-n64vr are ready
2022-03-30 17:25:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591966ms till timeout)
2022-03-30 17:25:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-n64vr not ready: strimzi-cluster-operator)
2022-03-30 17:25:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-n64vr are ready
2022-03-30 17:25:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590962ms till timeout)
2022-03-30 17:25:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-n64vr not ready: strimzi-cluster-operator)
2022-03-30 17:25:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-n64vr are ready
2022-03-30 17:25:29 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-30 17:25:29 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment strimzi-cluster-operator rolling update
2022-03-30 17:25:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Deployment strimzi-cluster-operator rolling update in namespace:infra-namespace
2022-03-30 17:25:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {strimzi-cluster-operator-77554ffdfb-kh86l=afd75f11-eb38-41f0-8653-238837cd135e}
2022-03-30 17:25:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {strimzi-cluster-operator-78689684d4-n64vr=d55273a6-3b2c-402d-88ff-d46cf3e9055c}
2022-03-30 17:25:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:119] All pods seem to have rolled
2022-03-30 17:25:29 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 17:25:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 17:25:29 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-30 17:25:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-30 17:25:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-n64vr not ready: strimzi-cluster-operator)
2022-03-30 17:25:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-n64vr are ready
2022-03-30 17:25:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 17:25:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-n64vr not ready: strimzi-cluster-operator)
2022-03-30 17:25:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-n64vr are ready
2022-03-30 17:25:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-30 17:25:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-n64vr not ready: strimzi-cluster-operator)
2022-03-30 17:25:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-n64vr are ready
2022-03-30 17:25:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597991ms till timeout)
2022-03-30 17:25:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-n64vr not ready: strimzi-cluster-operator)
2022-03-30 17:25:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-n64vr are ready
2022-03-30 17:25:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596987ms till timeout)
2022-03-30 17:25:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-n64vr not ready: strimzi-cluster-operator)
2022-03-30 17:25:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-n64vr are ready
2022-03-30 17:25:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595983ms till timeout)
2022-03-30 17:25:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-n64vr not ready: strimzi-cluster-operator)
2022-03-30 17:25:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-n64vr are ready
2022-03-30 17:25:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594979ms till timeout)
2022-03-30 17:25:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-n64vr not ready: strimzi-cluster-operator)
2022-03-30 17:25:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-n64vr are ready
2022-03-30 17:25:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593975ms till timeout)
2022-03-30 17:25:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-n64vr not ready: strimzi-cluster-operator)
2022-03-30 17:25:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-n64vr are ready
2022-03-30 17:25:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592971ms till timeout)
2022-03-30 17:25:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-n64vr not ready: strimzi-cluster-operator)
2022-03-30 17:25:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-n64vr are ready
2022-03-30 17:25:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591967ms till timeout)
2022-03-30 17:25:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-n64vr not ready: strimzi-cluster-operator)
2022-03-30 17:25:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-n64vr are ready
2022-03-30 17:25:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590964ms till timeout)
2022-03-30 17:25:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-n64vr not ready: strimzi-cluster-operator)
2022-03-30 17:25:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-n64vr are ready
2022-03-30 17:25:39 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment strimzi-cluster-operator rolling update finished
2022-03-30 17:25:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 17:25:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:675] [specific.SpecificIsolatedST - After Each] - Clean up after test
2022-03-30 17:25:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 17:25:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testRackAwareConnectCorrectDeployment
2022-03-30 17:25:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-b2005abc-allow in namespace infra-namespace
2022-03-30 17:25:39 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic rw-my-topic-1699747579-993901798 in namespace infra-namespace
2022-03-30 17:25:39 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:rw-my-topic-1699747579-993901798
2022-03-30 17:25:39 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-b2005abc-kafka-clients in namespace infra-namespace
2022-03-30 17:25:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:my-cluster-b2005abc-allow
2022-03-30 17:25:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-b2005abc-scraper in namespace infra-namespace
2022-03-30 17:25:39 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-b2005abc-kafka-clients
2022-03-30 17:25:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-b2005abc-scraper
2022-03-30 17:25:39 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-b2005abc in namespace infra-namespace
2022-03-30 17:25:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-b2005abc-allow in namespace infra-namespace
2022-03-30 17:25:39 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-b2005abc
2022-03-30 17:25:39 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-b2005abc in namespace infra-namespace
2022-03-30 17:25:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:my-cluster-b2005abc-allow
2022-03-30 17:25:39 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaConnect:my-cluster-b2005abc
2022-03-30 17:25:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 17:25:39 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.specific.SpecificIsolatedST.testRackAwareConnectCorrectDeployment-FINISHED
2022-03-30 17:25:39 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 17:25:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 17:25:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:690] [specific.SpecificIsolatedST - After All] - Clean up after test suite
2022-03-30 17:25:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 17:25:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context SpecificIsolatedST is everything deleted.
2022-03-30 17:25:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 523.834 s - in io.strimzi.systemtest.specific.SpecificIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.watcher.AllNamespaceIsolatedST
2022-03-30 17:25:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 17:25:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:667] [watcher.AllNamespaceIsolatedST - Before All] - Setup test suite environment
2022-03-30 17:25:39 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 17:25:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 0
2022-03-30 17:25:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 1
2022-03-30 17:25:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 0
2022-03-30 17:25:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 2
2022-03-30 17:25:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 0
2022-03-30 17:25:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 3
2022-03-30 17:25:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 0
2022-03-30 17:25:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 4
2022-03-30 17:25:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 0
2022-03-30 17:26:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 5
2022-03-30 17:26:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:136] Suite watcher.AllNamespaceIsolatedST has locked the @IsolatedSuite and other @IsolatedSuites must wait until lock is released.
2022-03-30 17:26:04 [ForkJoinPool-3-worker-3] [32mINFO [m [AllNamespaceIsolatedST:190] Creating resources before the test class
2022-03-30 17:26:04 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-30 17:26:04 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-30 17:26:04 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-30 17:26:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 17:26:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-30 17:26:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 17:26:04 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 17:26:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 17:26:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io not ready, will try again in 10000 ms (179987ms till timeout)
2022-03-30 17:26:05 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 17:26:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179808ms till timeout)
2022-03-30 17:26:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:26:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 17:26:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 17:26:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 17:26:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 17:26:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 17:26:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 17:26:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 17:26:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io not ready, will try again in 10000 ms (179993ms till timeout)
2022-03-30 17:26:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 17:26:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 17:26:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 17:26:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 17:26:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 17:26:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 17:26:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 17:26:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 17:26:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 17:26:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 17:26:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 17:26:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 17:26:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 17:26:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 17:26:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 17:26:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 17:26:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:26:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 17:26:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 17:26:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 17:26:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 17:26:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 17:26:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 17:26:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 17:26:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 17:26:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 17:26:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 17:26:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 17:26:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:26:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 17:26:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator not ready, will try again in 10000 ms (479982ms till timeout)
2022-03-30 17:26:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 17:26:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 17:26:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179906ms till timeout)
2022-03-30 17:26:25 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 17:26:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 17:26:25 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 17:26:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 17:26:25 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:26:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 17:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 17:26:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-30 17:26:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 17:26:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 17:26:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v58329
2022-03-30 17:26:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v58329
2022-03-30 17:26:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=58329&allowWatchBookmarks=true&watch=true...
2022-03-30 17:26:35 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 17:26:35 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 58331
2022-03-30 17:26:40 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 58345
2022-03-30 17:26:40 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 58346
2022-03-30 17:26:40 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v58345 in namespace default
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5e99e8d5
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace, second-namespace-test, third-namespace-test]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-30 17:26:40 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@73db1be6
2022-03-30 17:26:40 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@cbb87a8, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5e99e8d5, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace, second-namespace-test, third-namespace-test], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-30 17:26:40 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@77e48145
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-30 17:26:40 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@77e48145
2022-03-30 17:26:40 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@77e48145
2022-03-30 17:26:40 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 17:26:40 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T17:26:40Z",
        "name": "infra-namespace",
        "resourceVersion": "58347",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "ac8b307e-d2a3-4264-a545-6fcde16fb7e2"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: second-namespace-test
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace second-namespace-test
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace second-namespace-test -o json
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace second-namespace-test -o json
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T17:26:40Z",
        "name": "second-namespace-test",
        "resourceVersion": "58352",
        "selfLink": "/api/v1/namespaces/second-namespace-test",
        "uid": "34229919-bf04-4c68-9e4b-47cb3d44874c"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace, second-namespace-test]}
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: third-namespace-test
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace third-namespace-test
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace third-namespace-test -o json
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace third-namespace-test -o json
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T17:26:40Z",
        "name": "third-namespace-test",
        "resourceVersion": "58356",
        "selfLink": "/api/v1/namespaces/third-namespace-test",
        "uid": "4ca15c42-196b-4307-b073-af1ed9c81025"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace, second-namespace-test, third-namespace-test]}
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:26:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=second-namespace-test, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: second-namespace-test
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=third-namespace-test, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: third-namespace-test
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace second-namespace-test
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace second-namespace-test
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace third-namespace-test
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace third-namespace-test
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 17:26:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479996ms till timeout)
2022-03-30 17:26:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478993ms till timeout)
2022-03-30 17:26:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477990ms till timeout)
2022-03-30 17:26:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476987ms till timeout)
2022-03-30 17:26:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475984ms till timeout)
2022-03-30 17:26:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474981ms till timeout)
2022-03-30 17:26:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473977ms till timeout)
2022-03-30 17:26:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472974ms till timeout)
2022-03-30 17:26:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471970ms till timeout)
2022-03-30 17:26:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470967ms till timeout)
2022-03-30 17:26:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469964ms till timeout)
2022-03-30 17:26:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468960ms till timeout)
2022-03-30 17:26:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467957ms till timeout)
2022-03-30 17:26:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466954ms till timeout)
2022-03-30 17:26:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465951ms till timeout)
2022-03-30 17:26:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464948ms till timeout)
2022-03-30 17:26:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463945ms till timeout)
2022-03-30 17:26:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462942ms till timeout)
2022-03-30 17:26:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461939ms till timeout)
2022-03-30 17:27:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (460936ms till timeout)
2022-03-30 17:27:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (459932ms till timeout)
2022-03-30 17:27:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (458929ms till timeout)
2022-03-30 17:27:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (457926ms till timeout)
2022-03-30 17:27:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (456923ms till timeout)
2022-03-30 17:27:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (455920ms till timeout)
2022-03-30 17:27:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (454917ms till timeout)
2022-03-30 17:27:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (453914ms till timeout)
2022-03-30 17:27:08 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-30 17:27:08 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-30 17:27:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-30 17:27:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-2rd78 not ready: strimzi-cluster-operator)
2022-03-30 17:27:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-2rd78 are ready
2022-03-30 17:27:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-30 17:27:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-2rd78 not ready: strimzi-cluster-operator)
2022-03-30 17:27:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-2rd78 are ready
2022-03-30 17:27:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-30 17:27:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-2rd78 not ready: strimzi-cluster-operator)
2022-03-30 17:27:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-2rd78 are ready
2022-03-30 17:27:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597990ms till timeout)
2022-03-30 17:27:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-2rd78 not ready: strimzi-cluster-operator)
2022-03-30 17:27:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-2rd78 are ready
2022-03-30 17:27:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596986ms till timeout)
2022-03-30 17:27:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-2rd78 not ready: strimzi-cluster-operator)
2022-03-30 17:27:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-2rd78 are ready
2022-03-30 17:27:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595982ms till timeout)
2022-03-30 17:27:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-2rd78 not ready: strimzi-cluster-operator)
2022-03-30 17:27:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-2rd78 are ready
2022-03-30 17:27:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594978ms till timeout)
2022-03-30 17:27:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-2rd78 not ready: strimzi-cluster-operator)
2022-03-30 17:27:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-2rd78 are ready
2022-03-30 17:27:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593975ms till timeout)
2022-03-30 17:27:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-2rd78 not ready: strimzi-cluster-operator)
2022-03-30 17:27:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-2rd78 are ready
2022-03-30 17:27:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592971ms till timeout)
2022-03-30 17:27:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-2rd78 not ready: strimzi-cluster-operator)
2022-03-30 17:27:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-2rd78 are ready
2022-03-30 17:27:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591967ms till timeout)
2022-03-30 17:27:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-2rd78 not ready: strimzi-cluster-operator)
2022-03-30 17:27:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-2rd78 are ready
2022-03-30 17:27:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590964ms till timeout)
2022-03-30 17:27:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-2rd78 not ready: strimzi-cluster-operator)
2022-03-30 17:27:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-2rd78 are ready
2022-03-30 17:27:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-30 17:27:18 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: third-namespace-test
2022-03-30 17:27:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster in namespace third-namespace-test
2022-03-30 17:27:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster
2022-03-30 17:27:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster will have desired state: Ready
2022-03-30 17:27:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster will have desired state: Ready
2022-03-30 17:27:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (839998ms till timeout)
2022-03-30 17:27:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (838995ms till timeout)
2022-03-30 17:27:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (837991ms till timeout)
2022-03-30 17:27:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (836988ms till timeout)
2022-03-30 17:27:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (835985ms till timeout)
2022-03-30 17:27:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (834982ms till timeout)
2022-03-30 17:27:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (833978ms till timeout)
2022-03-30 17:27:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (832975ms till timeout)
2022-03-30 17:27:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (831972ms till timeout)
2022-03-30 17:27:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (830969ms till timeout)
2022-03-30 17:27:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (829965ms till timeout)
2022-03-30 17:27:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (828961ms till timeout)
2022-03-30 17:27:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (827957ms till timeout)
2022-03-30 17:27:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (826954ms till timeout)
2022-03-30 17:27:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (825951ms till timeout)
2022-03-30 17:27:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (824948ms till timeout)
2022-03-30 17:27:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (823944ms till timeout)
2022-03-30 17:27:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (822941ms till timeout)
2022-03-30 17:27:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (821937ms till timeout)
2022-03-30 17:27:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (820934ms till timeout)
2022-03-30 17:27:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (819930ms till timeout)
2022-03-30 17:27:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (818927ms till timeout)
2022-03-30 17:27:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (817923ms till timeout)
2022-03-30 17:27:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (816920ms till timeout)
2022-03-30 17:27:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (815917ms till timeout)
2022-03-30 17:27:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (814914ms till timeout)
2022-03-30 17:27:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (813910ms till timeout)
2022-03-30 17:27:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (812907ms till timeout)
2022-03-30 17:27:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (811903ms till timeout)
2022-03-30 17:27:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (810900ms till timeout)
2022-03-30 17:27:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (809897ms till timeout)
2022-03-30 17:27:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (808894ms till timeout)
2022-03-30 17:27:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (807890ms till timeout)
2022-03-30 17:27:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (806887ms till timeout)
2022-03-30 17:27:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (805883ms till timeout)
2022-03-30 17:27:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (804880ms till timeout)
2022-03-30 17:27:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (803877ms till timeout)
2022-03-30 17:27:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (802873ms till timeout)
2022-03-30 17:27:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (801870ms till timeout)
2022-03-30 17:27:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (800867ms till timeout)
2022-03-30 17:27:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (799863ms till timeout)
2022-03-30 17:27:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (798860ms till timeout)
2022-03-30 17:28:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (797857ms till timeout)
2022-03-30 17:28:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (796853ms till timeout)
2022-03-30 17:28:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (795850ms till timeout)
2022-03-30 17:28:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (794847ms till timeout)
2022-03-30 17:28:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (793844ms till timeout)
2022-03-30 17:28:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (792840ms till timeout)
2022-03-30 17:28:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (791837ms till timeout)
2022-03-30 17:28:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (790834ms till timeout)
2022-03-30 17:28:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (789830ms till timeout)
2022-03-30 17:28:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (788827ms till timeout)
2022-03-30 17:28:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (787823ms till timeout)
2022-03-30 17:28:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (786820ms till timeout)
2022-03-30 17:28:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (785816ms till timeout)
2022-03-30 17:28:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (784813ms till timeout)
2022-03-30 17:28:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (783810ms till timeout)
2022-03-30 17:28:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (782807ms till timeout)
2022-03-30 17:28:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (781804ms till timeout)
2022-03-30 17:28:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (780801ms till timeout)
2022-03-30 17:28:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (779797ms till timeout)
2022-03-30 17:28:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (778794ms till timeout)
2022-03-30 17:28:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (777791ms till timeout)
2022-03-30 17:28:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (776788ms till timeout)
2022-03-30 17:28:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (775785ms till timeout)
2022-03-30 17:28:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (774782ms till timeout)
2022-03-30 17:28:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (773778ms till timeout)
2022-03-30 17:28:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (772775ms till timeout)
2022-03-30 17:28:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (771772ms till timeout)
2022-03-30 17:28:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (770769ms till timeout)
2022-03-30 17:28:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (769766ms till timeout)
2022-03-30 17:28:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (768763ms till timeout)
2022-03-30 17:28:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (767760ms till timeout)
2022-03-30 17:28:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster is in desired state: Ready
2022-03-30 17:28:31 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-namespace-test
2022-03-30 17:28:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-second in namespace second-namespace-test
2022-03-30 17:28:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-second
2022-03-30 17:28:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-second will have desired state: Ready
2022-03-30 17:28:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-second will have desired state: Ready
2022-03-30 17:28:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (839998ms till timeout)
2022-03-30 17:28:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (838995ms till timeout)
2022-03-30 17:28:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (837992ms till timeout)
2022-03-30 17:28:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (836989ms till timeout)
2022-03-30 17:28:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (835986ms till timeout)
2022-03-30 17:28:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (834982ms till timeout)
2022-03-30 17:28:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (833979ms till timeout)
2022-03-30 17:28:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (832976ms till timeout)
2022-03-30 17:28:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (831973ms till timeout)
2022-03-30 17:28:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (830969ms till timeout)
2022-03-30 17:28:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (829966ms till timeout)
2022-03-30 17:28:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (828963ms till timeout)
2022-03-30 17:28:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (827960ms till timeout)
2022-03-30 17:28:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (826956ms till timeout)
2022-03-30 17:28:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (825953ms till timeout)
2022-03-30 17:28:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (824950ms till timeout)
2022-03-30 17:28:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (823948ms till timeout)
2022-03-30 17:28:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (822944ms till timeout)
2022-03-30 17:28:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (821941ms till timeout)
2022-03-30 17:28:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (820938ms till timeout)
2022-03-30 17:28:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (819935ms till timeout)
2022-03-30 17:28:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (818932ms till timeout)
2022-03-30 17:28:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (817929ms till timeout)
2022-03-30 17:28:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (816926ms till timeout)
2022-03-30 17:28:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (815922ms till timeout)
2022-03-30 17:28:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (814919ms till timeout)
2022-03-30 17:28:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (813916ms till timeout)
2022-03-30 17:28:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (812913ms till timeout)
2022-03-30 17:28:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (811910ms till timeout)
2022-03-30 17:29:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (810907ms till timeout)
2022-03-30 17:29:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (809902ms till timeout)
2022-03-30 17:29:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (808899ms till timeout)
2022-03-30 17:29:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (807894ms till timeout)
2022-03-30 17:29:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (806891ms till timeout)
2022-03-30 17:29:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (805887ms till timeout)
2022-03-30 17:29:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (804883ms till timeout)
2022-03-30 17:29:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (803879ms till timeout)
2022-03-30 17:29:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (802876ms till timeout)
2022-03-30 17:29:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (801873ms till timeout)
2022-03-30 17:29:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (800870ms till timeout)
2022-03-30 17:29:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (799867ms till timeout)
2022-03-30 17:29:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (798863ms till timeout)
2022-03-30 17:29:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (797860ms till timeout)
2022-03-30 17:29:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (796857ms till timeout)
2022-03-30 17:29:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (795855ms till timeout)
2022-03-30 17:29:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (794851ms till timeout)
2022-03-30 17:29:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (793848ms till timeout)
2022-03-30 17:29:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (792845ms till timeout)
2022-03-30 17:29:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (791842ms till timeout)
2022-03-30 17:29:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (790839ms till timeout)
2022-03-30 17:29:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (789836ms till timeout)
2022-03-30 17:29:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (788833ms till timeout)
2022-03-30 17:29:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (787830ms till timeout)
2022-03-30 17:29:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (786827ms till timeout)
2022-03-30 17:29:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (785823ms till timeout)
2022-03-30 17:29:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (784820ms till timeout)
2022-03-30 17:29:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (783817ms till timeout)
2022-03-30 17:29:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (782812ms till timeout)
2022-03-30 17:29:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (781809ms till timeout)
2022-03-30 17:29:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (780805ms till timeout)
2022-03-30 17:29:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (779802ms till timeout)
2022-03-30 17:29:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (778799ms till timeout)
2022-03-30 17:29:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (777796ms till timeout)
2022-03-30 17:29:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (776793ms till timeout)
2022-03-30 17:29:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (775790ms till timeout)
2022-03-30 17:29:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (774786ms till timeout)
2022-03-30 17:29:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (773783ms till timeout)
2022-03-30 17:29:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (772780ms till timeout)
2022-03-30 17:29:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (771776ms till timeout)
2022-03-30 17:29:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (770773ms till timeout)
2022-03-30 17:29:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (769770ms till timeout)
2022-03-30 17:29:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (768766ms till timeout)
2022-03-30 17:29:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (767763ms till timeout)
2022-03-30 17:29:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-second is in desired state: Ready
2022-03-30 17:29:45 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 17:29:45 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 17:29:45 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.watcher.AllNamespaceIsolatedST.testKafkaInDifferentNsThanClusterOperator-STARTED
2022-03-30 17:29:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 17:29:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:659] [watcher.AllNamespaceIsolatedST - Before Each] - Setup test case environment
2022-03-30 17:29:45 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 17:29:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2, testSendSimpleMessageTls=my-cluster-128d9a40, testUpdateUser=my-cluster-75998b92, testSendMessagesTlsScramSha=my-cluster-a11b9173, testReceiveSimpleMessageTls=my-cluster-90021a9f, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e, testCruiseControlBasicAPIRequests=my-cluster-6da231a3, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc}
2022-03-30 17:29:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1975958319-512406578, testAutoRenewAllCaCertsTriggeredByAnno=my-user-308582398-741623568, testSendSimpleMessageTls=my-user-323988616-1355569855, testUpdateUser=my-user-275427717-1183444028, testSendMessagesTlsScramSha=my-user-1791509549-1350003609, testReceiveSimpleMessageTls=my-user-491397521-1196361913, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-695287754-1004738297, testKafkaInDifferentNsThanClusterOperator=my-user-908767181-1543724508, testSendMessagesCustomListenerTlsScramSha=my-user-1653975062-1727371851, testCruiseControlBasicAPIRequests=my-user-918568860-36694764, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-83332781-1273282344, testRackAwareConnectCorrectDeployment=my-user-1839832768-1329691231}
2022-03-30 17:29:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-1496158521-938246156, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1728678022-515015095, testSendSimpleMessageTls=my-topic-1040221322-670398147, testUpdateUser=my-topic-275186280-765877519, testSendMessagesTlsScramSha=my-topic-1482193652-649135537, testReceiveSimpleMessageTls=my-topic-941665209-436577257, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1609923788-1785060017, testKafkaInDifferentNsThanClusterOperator=my-topic-2051437288-2087424913, testSendMessagesCustomListenerTlsScramSha=my-topic-629448478-708445494, testCruiseControlBasicAPIRequests=my-topic-710218582-658397020, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1536476030-1482182833, testRackAwareConnectCorrectDeployment=my-topic-1526242410-482838543}
2022-03-30 17:29:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2-kafka-clients, testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients, testSendMessagesTlsScramSha=my-cluster-a11b9173-kafka-clients, testReceiveSimpleMessageTls=my-cluster-90021a9f-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-6da231a3-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc-kafka-clients}
2022-03-30 17:29:45 [ForkJoinPool-3-worker-3] [32mINFO [m [AllNamespaceIsolatedST:82] Deploying Kafka cluster in different namespace than CO when CO watches all namespaces
2022-03-30 17:29:45 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-namespace-test
2022-03-30 17:29:45 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractNamespaceST:46] Check if Kafka Cluster my-cluster-second in namespace second-namespace-test
2022-03-30 17:29:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Kafka Cluster status is not in desired state: Ready
2022-03-30 17:29:45 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractNamespaceST:51] Kafka condition status: True
2022-03-30 17:29:45 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractNamespaceST:52] Kafka condition type: Ready
2022-03-30 17:29:45 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 17:29:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 17:29:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:675] [watcher.AllNamespaceIsolatedST - After Each] - Clean up after test
2022-03-30 17:29:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 17:29:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context testKafkaInDifferentNsThanClusterOperator is everything deleted.
2022-03-30 17:29:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 17:29:45 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.watcher.AllNamespaceIsolatedST.testKafkaInDifferentNsThanClusterOperator-FINISHED
2022-03-30 17:29:45 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 17:29:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 17:29:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:690] [watcher.AllNamespaceIsolatedST - After All] - Clean up after test suite
2022-03-30 17:29:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 17:29:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for AllNamespaceIsolatedST
2022-03-30 17:29:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-second in namespace second-namespace-test
2022-03-30 17:29:45 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster in namespace third-namespace-test
2022-03-30 17:29:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-second
2022-03-30 17:29:45 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster
2022-03-30 17:29:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-second not ready, will try again in 10000 ms (839996ms till timeout)
2022-03-30 17:29:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster not ready, will try again in 10000 ms (839996ms till timeout)
2022-03-30 17:29:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 255.179 s - in io.strimzi.systemtest.watcher.AllNamespaceIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.connect.ConnectIsolatedST
2022-03-30 17:29:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 17:29:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:667] [connect.ConnectIsolatedST - Before All] - Setup test suite environment
2022-03-30 17:29:55 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 17:29:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 0
2022-03-30 17:30:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 1
2022-03-30 17:30:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 0
2022-03-30 17:30:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 2
2022-03-30 17:30:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 0
2022-03-30 17:30:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 3
2022-03-30 17:30:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 0
2022-03-30 17:30:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 4
2022-03-30 17:30:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 0
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 5
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:136] Suite connect.ConnectIsolatedST has locked the @IsolatedSuite and other @IsolatedSuites must wait until lock is released.
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 17:30:20 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 17:30:20 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 17:30:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io not ready, will try again in 10000 ms (179990ms till timeout)
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace second-namespace-test
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace third-namespace-test
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace third-namespace-test
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 17:30:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator not ready, will try again in 10000 ms (479920ms till timeout)
2022-03-30 17:30:30 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 17:30:30 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 17:30:30 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 17:30:30 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 17:30:30 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io not ready, will try again in 10000 ms (179990ms till timeout)
2022-03-30 17:30:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator not ready, will try again in 10000 ms (469912ms till timeout)
2022-03-30 17:30:40 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 17:30:40 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 17:30:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io not ready, will try again in 10000 ms (179973ms till timeout)
2022-03-30 17:30:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace second-namespace-test
2022-03-30 17:30:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 17:30:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 17:30:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 17:30:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 17:30:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 17:30:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 17:30:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 17:30:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 17:30:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 17:30:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 17:30:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 17:30:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 17:30:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 17:30:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179865ms till timeout)
2022-03-30 17:30:50 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 17:30:50 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 17:30:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179914ms till timeout)
2022-03-30 17:30:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 17:30:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 17:30:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 17:30:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 17:30:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 17:30:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 17:30:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io not ready, will try again in 10000 ms (179952ms till timeout)
2022-03-30 17:31:00 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 17:31:00 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 17:31:00 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:31:00 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 17:31:00 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:31:00 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 17:31:00 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 17:31:00 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 17:31:00 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 17:31:00 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 17:31:00 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 17:31:00 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 17:31:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 17:31:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: second-namespace-test
2022-03-30 17:31:00 [ForkJoinPool-3-worker-5] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-30 17:31:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 17:31:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 17:31:00 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 17:31:00 [ForkJoinPool-3-worker-5] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 17:31:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v59327
2022-03-30 17:31:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v59327
2022-03-30 17:31:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dsecond-namespace-test&resourceVersion=59327&allowWatchBookmarks=true&watch=true...
2022-03-30 17:31:00 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v59327
2022-03-30 17:31:00 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v59327
2022-03-30 17:31:00 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=59327&allowWatchBookmarks=true&watch=true...
2022-03-30 17:31:00 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 17:31:00 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 17:31:00 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 59328
2022-03-30 17:31:00 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 59329
2022-03-30 17:31:05 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 59379
2022-03-30 17:31:05 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 59383
2022-03-30 17:31:05 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v59379 in namespace default
2022-03-30 17:31:05 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@4826bba3
2022-03-30 17:31:05 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 17:31:05 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@4c19267f
2022-03-30 17:31:05 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@4c19267f
2022-03-30 17:31:05 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@4c19267f
2022-03-30 17:31:05 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 17:31:05 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 17:31:06 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 59415
2022-03-30 17:31:06 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 59416
2022-03-30 17:31:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: third-namespace-test
2022-03-30 17:31:06 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v59415 in namespace default
2022-03-30 17:31:06 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@691c7594
2022-03-30 17:31:06 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 17:31:06 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@282ab4a0
2022-03-30 17:31:06 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@282ab4a0
2022-03-30 17:31:06 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@282ab4a0
2022-03-30 17:31:06 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 17:31:06 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 17:31:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 17:31:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 17:31:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v59417
2022-03-30 17:31:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v59417
2022-03-30 17:31:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dthird-namespace-test&resourceVersion=59417&allowWatchBookmarks=true&watch=true...
2022-03-30 17:31:06 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 17:31:06 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 59418
2022-03-30 17:31:11 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 59456
2022-03-30 17:31:11 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 59457
2022-03-30 17:31:11 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v59456 in namespace default
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5e99e8d5
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-30 17:31:11 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@669beada
2022-03-30 17:31:11 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@cbb87a8, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5e99e8d5, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-30 17:31:11 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@235daece
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-30 17:31:11 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@235daece
2022-03-30 17:31:11 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@235daece
2022-03-30 17:31:11 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 17:31:11 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T17:31:11Z",
        "name": "infra-namespace",
        "resourceVersion": "59458",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "9d743b73-1e04-4029-9ed4-2ace57aefbf2"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 17:31:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479993ms till timeout)
2022-03-30 17:31:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478989ms till timeout)
2022-03-30 17:31:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477986ms till timeout)
2022-03-30 17:31:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476983ms till timeout)
2022-03-30 17:31:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475979ms till timeout)
2022-03-30 17:31:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474976ms till timeout)
2022-03-30 17:31:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473973ms till timeout)
2022-03-30 17:31:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472970ms till timeout)
2022-03-30 17:31:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471967ms till timeout)
2022-03-30 17:31:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470964ms till timeout)
2022-03-30 17:31:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469960ms till timeout)
2022-03-30 17:31:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468957ms till timeout)
2022-03-30 17:31:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467955ms till timeout)
2022-03-30 17:31:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466951ms till timeout)
2022-03-30 17:31:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465948ms till timeout)
2022-03-30 17:31:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464945ms till timeout)
2022-03-30 17:31:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463942ms till timeout)
2022-03-30 17:31:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462939ms till timeout)
2022-03-30 17:31:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461936ms till timeout)
2022-03-30 17:31:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (460933ms till timeout)
2022-03-30 17:31:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (459930ms till timeout)
2022-03-30 17:31:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (458927ms till timeout)
2022-03-30 17:31:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (457924ms till timeout)
2022-03-30 17:31:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (456921ms till timeout)
2022-03-30 17:31:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (455917ms till timeout)
2022-03-30 17:31:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (454914ms till timeout)
2022-03-30 17:31:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (453911ms till timeout)
2022-03-30 17:31:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (452908ms till timeout)
2022-03-30 17:31:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (451904ms till timeout)
2022-03-30 17:31:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (450901ms till timeout)
2022-03-30 17:31:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (449898ms till timeout)
2022-03-30 17:31:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (448895ms till timeout)
2022-03-30 17:31:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (447892ms till timeout)
2022-03-30 17:31:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (446889ms till timeout)
2022-03-30 17:31:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (445886ms till timeout)
2022-03-30 17:31:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (444883ms till timeout)
2022-03-30 17:31:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (443880ms till timeout)
2022-03-30 17:31:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (442877ms till timeout)
2022-03-30 17:31:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (441874ms till timeout)
2022-03-30 17:31:51 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-30 17:31:51 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-30 17:31:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-30 17:31:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-46sw2 not ready: strimzi-cluster-operator)
2022-03-30 17:31:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-46sw2 are ready
2022-03-30 17:31:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 17:31:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-46sw2 not ready: strimzi-cluster-operator)
2022-03-30 17:31:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-46sw2 are ready
2022-03-30 17:31:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-30 17:31:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-46sw2 not ready: strimzi-cluster-operator)
2022-03-30 17:31:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-46sw2 are ready
2022-03-30 17:31:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597991ms till timeout)
2022-03-30 17:31:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-46sw2 not ready: strimzi-cluster-operator)
2022-03-30 17:31:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-46sw2 are ready
2022-03-30 17:31:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596987ms till timeout)
2022-03-30 17:31:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-46sw2 not ready: strimzi-cluster-operator)
2022-03-30 17:31:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-46sw2 are ready
2022-03-30 17:31:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595983ms till timeout)
2022-03-30 17:31:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-46sw2 not ready: strimzi-cluster-operator)
2022-03-30 17:31:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-46sw2 are ready
2022-03-30 17:31:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594980ms till timeout)
2022-03-30 17:31:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-46sw2 not ready: strimzi-cluster-operator)
2022-03-30 17:31:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-46sw2 are ready
2022-03-30 17:31:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593976ms till timeout)
2022-03-30 17:31:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-46sw2 not ready: strimzi-cluster-operator)
2022-03-30 17:31:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-46sw2 are ready
2022-03-30 17:31:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592972ms till timeout)
2022-03-30 17:31:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-46sw2 not ready: strimzi-cluster-operator)
2022-03-30 17:31:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-46sw2 are ready
2022-03-30 17:31:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591969ms till timeout)
2022-03-30 17:32:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-46sw2 not ready: strimzi-cluster-operator)
2022-03-30 17:32:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-46sw2 are ready
2022-03-30 17:32:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590965ms till timeout)
2022-03-30 17:32:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-46sw2 not ready: strimzi-cluster-operator)
2022-03-30 17:32:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-46sw2 are ready
2022-03-30 17:32:01 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-30 17:32:01 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 17:32:01 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testMultiNodeKafkaConnectWithConnectorCreation-STARTED
2022-03-30 17:32:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 17:32:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:659] [connect.ConnectIsolatedST - Before Each] - Setup test case environment
2022-03-30 17:32:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:77] [connect.ConnectIsolatedST] - Adding parallel test: testMultiNodeKafkaConnectWithConnectorCreation
2022-03-30 17:32:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:81] [connect.ConnectIsolatedST] - Parallel test count: 1
2022-03-30 17:32:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:230] testMultiNodeKafkaConnectWithConnectorCreation test now can proceed its execution
2022-03-30 17:32:01 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 17:32:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1, testSendSimpleMessageTls=my-cluster-128d9a40, testUpdateUser=my-cluster-75998b92, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e, testCruiseControlBasicAPIRequests=my-cluster-6da231a3, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2, testSendMessagesTlsScramSha=my-cluster-a11b9173, testReceiveSimpleMessageTls=my-cluster-90021a9f, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc}
2022-03-30 17:32:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1975958319-512406578, testSendSimpleMessageTls=my-user-323988616-1355569855, testUpdateUser=my-user-275427717-1183444028, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-695287754-1004738297, testKafkaInDifferentNsThanClusterOperator=my-user-908767181-1543724508, testSendMessagesCustomListenerTlsScramSha=my-user-1653975062-1727371851, testCruiseControlBasicAPIRequests=my-user-918568860-36694764, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1337805046-1917759354, testAutoRenewAllCaCertsTriggeredByAnno=my-user-308582398-741623568, testSendMessagesTlsScramSha=my-user-1791509549-1350003609, testReceiveSimpleMessageTls=my-user-491397521-1196361913, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-83332781-1273282344, testRackAwareConnectCorrectDeployment=my-user-1839832768-1329691231}
2022-03-30 17:32:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-1496158521-938246156, testSendSimpleMessageTls=my-topic-1040221322-670398147, testUpdateUser=my-topic-275186280-765877519, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1609923788-1785060017, testKafkaInDifferentNsThanClusterOperator=my-topic-2051437288-2087424913, testSendMessagesCustomListenerTlsScramSha=my-topic-629448478-708445494, testCruiseControlBasicAPIRequests=my-topic-710218582-658397020, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-604305255-1585536778, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1728678022-515015095, testSendMessagesTlsScramSha=my-topic-1482193652-649135537, testReceiveSimpleMessageTls=my-topic-941665209-436577257, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1536476030-1482182833, testRackAwareConnectCorrectDeployment=my-topic-1526242410-482838543}
2022-03-30 17:32:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1-kafka-clients, testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-6da231a3-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2-kafka-clients, testSendMessagesTlsScramSha=my-cluster-a11b9173-kafka-clients, testReceiveSimpleMessageTls=my-cluster-90021a9f-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc-kafka-clients}
2022-03-30 17:32:01 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-6 for test case:testMultiNodeKafkaConnectWithConnectorCreation
2022-03-30 17:32:01 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-6
2022-03-30 17:32:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-6
2022-03-30 17:32:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-6 -o json
2022-03-30 17:32:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-6 -o json
2022-03-30 17:32:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:32:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T17:32:01Z",
        "name": "namespace-6",
        "resourceVersion": "59569",
        "selfLink": "/api/v1/namespaces/namespace-6",
        "uid": "29ac04d0-d9ed-4881-af3c-54e2ccffa459"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 17:32:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@c27758f5=[namespace-6]}
2022-03-30 17:32:01 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-6
2022-03-30 17:32:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-6, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 17:32:01 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-6
2022-03-30 17:32:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-e556bd90 in namespace namespace-6
2022-03-30 17:32:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-03-30 17:32:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-e556bd90
2022-03-30 17:32:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-e556bd90 will have desired state: Ready
2022-03-30 17:32:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-e556bd90 will have desired state: Ready
2022-03-30 17:32:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (839998ms till timeout)
2022-03-30 17:32:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (838995ms till timeout)
2022-03-30 17:32:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (837992ms till timeout)
2022-03-30 17:32:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (836989ms till timeout)
2022-03-30 17:32:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (835986ms till timeout)
2022-03-30 17:32:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (834983ms till timeout)
2022-03-30 17:32:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (833980ms till timeout)
2022-03-30 17:32:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (832977ms till timeout)
2022-03-30 17:32:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (831974ms till timeout)
2022-03-30 17:32:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (830970ms till timeout)
2022-03-30 17:32:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (829967ms till timeout)
2022-03-30 17:32:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (828960ms till timeout)
2022-03-30 17:32:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (827954ms till timeout)
2022-03-30 17:32:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (826951ms till timeout)
2022-03-30 17:32:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (825948ms till timeout)
2022-03-30 17:32:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (824944ms till timeout)
2022-03-30 17:32:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (823941ms till timeout)
2022-03-30 17:32:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (822938ms till timeout)
2022-03-30 17:32:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (821935ms till timeout)
2022-03-30 17:32:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (820932ms till timeout)
2022-03-30 17:32:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (819928ms till timeout)
2022-03-30 17:32:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (818926ms till timeout)
2022-03-30 17:32:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (817922ms till timeout)
2022-03-30 17:32:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (816919ms till timeout)
2022-03-30 17:32:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (815916ms till timeout)
2022-03-30 17:32:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (814913ms till timeout)
2022-03-30 17:32:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (813910ms till timeout)
2022-03-30 17:32:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (812907ms till timeout)
2022-03-30 17:32:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (811904ms till timeout)
2022-03-30 17:32:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (810900ms till timeout)
2022-03-30 17:32:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (809897ms till timeout)
2022-03-30 17:32:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (808894ms till timeout)
2022-03-30 17:32:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (807890ms till timeout)
2022-03-30 17:32:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (806887ms till timeout)
2022-03-30 17:32:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (805884ms till timeout)
2022-03-30 17:32:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (804881ms till timeout)
2022-03-30 17:32:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (803878ms till timeout)
2022-03-30 17:32:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (802875ms till timeout)
2022-03-30 17:32:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (801872ms till timeout)
2022-03-30 17:32:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (800867ms till timeout)
2022-03-30 17:32:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (799864ms till timeout)
2022-03-30 17:32:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (798860ms till timeout)
2022-03-30 17:32:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (797856ms till timeout)
2022-03-30 17:32:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (796853ms till timeout)
2022-03-30 17:32:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (795845ms till timeout)
2022-03-30 17:32:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (794841ms till timeout)
2022-03-30 17:32:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (793838ms till timeout)
2022-03-30 17:32:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (792835ms till timeout)
2022-03-30 17:32:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (791832ms till timeout)
2022-03-30 17:32:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (790829ms till timeout)
2022-03-30 17:32:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (789825ms till timeout)
2022-03-30 17:32:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (788822ms till timeout)
2022-03-30 17:32:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (787819ms till timeout)
2022-03-30 17:32:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (786816ms till timeout)
2022-03-30 17:32:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (785812ms till timeout)
2022-03-30 17:32:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (784809ms till timeout)
2022-03-30 17:32:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (783806ms till timeout)
2022-03-30 17:32:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (782803ms till timeout)
2022-03-30 17:32:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (781800ms till timeout)
2022-03-30 17:33:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (780797ms till timeout)
2022-03-30 17:33:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (779793ms till timeout)
2022-03-30 17:33:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (778790ms till timeout)
2022-03-30 17:33:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (777787ms till timeout)
2022-03-30 17:33:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (776784ms till timeout)
2022-03-30 17:33:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (775781ms till timeout)
2022-03-30 17:33:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (774777ms till timeout)
2022-03-30 17:33:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (773775ms till timeout)
2022-03-30 17:33:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (772771ms till timeout)
2022-03-30 17:33:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (771768ms till timeout)
2022-03-30 17:33:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (770765ms till timeout)
2022-03-30 17:33:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (769761ms till timeout)
2022-03-30 17:33:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (768758ms till timeout)
2022-03-30 17:33:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (767754ms till timeout)
2022-03-30 17:33:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (766751ms till timeout)
2022-03-30 17:33:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (765748ms till timeout)
2022-03-30 17:33:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (764745ms till timeout)
2022-03-30 17:33:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (763742ms till timeout)
2022-03-30 17:33:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (762739ms till timeout)
2022-03-30 17:33:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (761736ms till timeout)
2022-03-30 17:33:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (760732ms till timeout)
2022-03-30 17:33:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (759729ms till timeout)
2022-03-30 17:33:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (758726ms till timeout)
2022-03-30 17:33:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (757723ms till timeout)
2022-03-30 17:33:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (756720ms till timeout)
2022-03-30 17:33:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (755717ms till timeout)
2022-03-30 17:33:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (754714ms till timeout)
2022-03-30 17:33:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-e556bd90 is in desired state: Ready
2022-03-30 17:33:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-e556bd90-scraper in namespace namespace-6
2022-03-30 17:33:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-03-30 17:33:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-e556bd90-scraper
2022-03-30 17:33:27 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-e556bd90-scraper will be ready
2022-03-30 17:33:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-e556bd90-scraper will be ready
2022-03-30 17:33:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-e556bd90-scraper will be ready not ready, will try again in 1000 ms (479996ms till timeout)
2022-03-30 17:33:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-e556bd90-scraper will be ready not ready, will try again in 1000 ms (478993ms till timeout)
2022-03-30 17:33:29 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-e556bd90-scraper is ready
2022-03-30 17:33:29 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-e556bd90-scraper to be ready
2022-03-30 17:33:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-e556bd90-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready
2022-03-30 17:33:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e556bd90-scraper-5f4d9c7dfb-x9kb4 not ready: my-cluster-e556bd90-scraper)
2022-03-30 17:33:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e556bd90-scraper-5f4d9c7dfb-x9kb4 are ready
2022-03-30 17:33:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-e556bd90-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-30 17:33:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e556bd90-scraper-5f4d9c7dfb-x9kb4 not ready: my-cluster-e556bd90-scraper)
2022-03-30 17:33:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e556bd90-scraper-5f4d9c7dfb-x9kb4 are ready
2022-03-30 17:33:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-e556bd90-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598993ms till timeout)
2022-03-30 17:33:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e556bd90-scraper-5f4d9c7dfb-x9kb4 not ready: my-cluster-e556bd90-scraper)
2022-03-30 17:33:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e556bd90-scraper-5f4d9c7dfb-x9kb4 are ready
2022-03-30 17:33:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-e556bd90-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597989ms till timeout)
2022-03-30 17:33:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e556bd90-scraper-5f4d9c7dfb-x9kb4 not ready: my-cluster-e556bd90-scraper)
2022-03-30 17:33:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e556bd90-scraper-5f4d9c7dfb-x9kb4 are ready
2022-03-30 17:33:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-e556bd90-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596985ms till timeout)
2022-03-30 17:33:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e556bd90-scraper-5f4d9c7dfb-x9kb4 not ready: my-cluster-e556bd90-scraper)
2022-03-30 17:33:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e556bd90-scraper-5f4d9c7dfb-x9kb4 are ready
2022-03-30 17:33:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-e556bd90-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595981ms till timeout)
2022-03-30 17:33:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e556bd90-scraper-5f4d9c7dfb-x9kb4 not ready: my-cluster-e556bd90-scraper)
2022-03-30 17:33:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e556bd90-scraper-5f4d9c7dfb-x9kb4 are ready
2022-03-30 17:33:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-e556bd90-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594977ms till timeout)
2022-03-30 17:33:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e556bd90-scraper-5f4d9c7dfb-x9kb4 not ready: my-cluster-e556bd90-scraper)
2022-03-30 17:33:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e556bd90-scraper-5f4d9c7dfb-x9kb4 are ready
2022-03-30 17:33:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-e556bd90-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593973ms till timeout)
2022-03-30 17:33:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e556bd90-scraper-5f4d9c7dfb-x9kb4 not ready: my-cluster-e556bd90-scraper)
2022-03-30 17:33:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e556bd90-scraper-5f4d9c7dfb-x9kb4 are ready
2022-03-30 17:33:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-e556bd90-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592969ms till timeout)
2022-03-30 17:33:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e556bd90-scraper-5f4d9c7dfb-x9kb4 not ready: my-cluster-e556bd90-scraper)
2022-03-30 17:33:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e556bd90-scraper-5f4d9c7dfb-x9kb4 are ready
2022-03-30 17:33:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-e556bd90-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591965ms till timeout)
2022-03-30 17:33:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e556bd90-scraper-5f4d9c7dfb-x9kb4 not ready: my-cluster-e556bd90-scraper)
2022-03-30 17:33:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e556bd90-scraper-5f4d9c7dfb-x9kb4 are ready
2022-03-30 17:33:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-e556bd90-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590961ms till timeout)
2022-03-30 17:33:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-e556bd90-scraper-5f4d9c7dfb-x9kb4 not ready: my-cluster-e556bd90-scraper)
2022-03-30 17:33:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods my-cluster-e556bd90-scraper-5f4d9c7dfb-x9kb4 are ready
2022-03-30 17:33:39 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-e556bd90-scraper is ready
2022-03-30 17:33:39 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-e556bd90-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-03-30 17:33:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyResource:227] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=my-cluster-e556bd90-allow, namespace=namespace-6, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8083, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9404, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8080, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9999, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-e556bd90, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-e556bd90-connect}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 17:33:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-e556bd90-allow in namespace namespace-6
2022-03-30 17:33:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-03-30 17:33:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:my-cluster-e556bd90-allow
2022-03-30 17:33:39 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-03-30 17:33:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-e556bd90 in namespace namespace-6
2022-03-30 17:33:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-03-30 17:33:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaConnect:my-cluster-e556bd90
2022-03-30 17:33:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-e556bd90 will have desired state: Ready
2022-03-30 17:33:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaConnect: my-cluster-e556bd90 will have desired state: Ready
2022-03-30 17:33:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 17:33:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (598995ms till timeout)
2022-03-30 17:33:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (597992ms till timeout)
2022-03-30 17:33:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (596988ms till timeout)
2022-03-30 17:33:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (595985ms till timeout)
2022-03-30 17:33:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (594981ms till timeout)
2022-03-30 17:33:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (593978ms till timeout)
2022-03-30 17:33:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (592975ms till timeout)
2022-03-30 17:33:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (591971ms till timeout)
2022-03-30 17:33:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (590945ms till timeout)
2022-03-30 17:33:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (589932ms till timeout)
2022-03-30 17:33:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (588928ms till timeout)
2022-03-30 17:33:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (587925ms till timeout)
2022-03-30 17:33:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (586922ms till timeout)
2022-03-30 17:33:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (585919ms till timeout)
2022-03-30 17:33:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (584916ms till timeout)
2022-03-30 17:33:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (583913ms till timeout)
2022-03-30 17:33:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (582910ms till timeout)
2022-03-30 17:33:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (581906ms till timeout)
2022-03-30 17:33:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (580903ms till timeout)
2022-03-30 17:33:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (579900ms till timeout)
2022-03-30 17:34:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (578897ms till timeout)
2022-03-30 17:34:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (577894ms till timeout)
2022-03-30 17:34:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (576891ms till timeout)
2022-03-30 17:34:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (575887ms till timeout)
2022-03-30 17:34:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (574884ms till timeout)
2022-03-30 17:34:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (573881ms till timeout)
2022-03-30 17:34:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (572878ms till timeout)
2022-03-30 17:34:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (571874ms till timeout)
2022-03-30 17:34:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (570871ms till timeout)
2022-03-30 17:34:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (569868ms till timeout)
2022-03-30 17:34:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (568864ms till timeout)
2022-03-30 17:34:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (567861ms till timeout)
2022-03-30 17:34:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (566858ms till timeout)
2022-03-30 17:34:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (565855ms till timeout)
2022-03-30 17:34:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (564851ms till timeout)
2022-03-30 17:34:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (563848ms till timeout)
2022-03-30 17:34:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (562845ms till timeout)
2022-03-30 17:34:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (561842ms till timeout)
2022-03-30 17:34:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (560839ms till timeout)
2022-03-30 17:34:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (559835ms till timeout)
2022-03-30 17:34:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (558832ms till timeout)
2022-03-30 17:34:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (557829ms till timeout)
2022-03-30 17:34:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (556826ms till timeout)
2022-03-30 17:34:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (555823ms till timeout)
2022-03-30 17:34:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (554820ms till timeout)
2022-03-30 17:34:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (553816ms till timeout)
2022-03-30 17:34:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (552813ms till timeout)
2022-03-30 17:34:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (551810ms till timeout)
2022-03-30 17:34:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (550807ms till timeout)
2022-03-30 17:34:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (549804ms till timeout)
2022-03-30 17:34:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (548800ms till timeout)
2022-03-30 17:34:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (547797ms till timeout)
2022-03-30 17:34:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (546794ms till timeout)
2022-03-30 17:34:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (545791ms till timeout)
2022-03-30 17:34:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (544788ms till timeout)
2022-03-30 17:34:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (543785ms till timeout)
2022-03-30 17:34:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (542782ms till timeout)
2022-03-30 17:34:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (541779ms till timeout)
2022-03-30 17:34:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (540776ms till timeout)
2022-03-30 17:34:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (539773ms till timeout)
2022-03-30 17:34:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (538769ms till timeout)
2022-03-30 17:34:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (537766ms till timeout)
2022-03-30 17:34:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (536763ms till timeout)
2022-03-30 17:34:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (535760ms till timeout)
2022-03-30 17:34:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (534757ms till timeout)
2022-03-30 17:34:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (533754ms till timeout)
2022-03-30 17:34:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (532751ms till timeout)
2022-03-30 17:34:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (531747ms till timeout)
2022-03-30 17:34:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (530744ms till timeout)
2022-03-30 17:34:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (529741ms till timeout)
2022-03-30 17:34:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-e556bd90 is in desired state: Ready
2022-03-30 17:34:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector my-cluster-e556bd90 in namespace namespace-6
2022-03-30 17:34:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-03-30 17:34:50 [ForkJoinPool-3-worker-3] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkaconnectors' with unstable version 'v1beta2'
2022-03-30 17:34:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaConnector:my-cluster-e556bd90
2022-03-30 17:34:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-e556bd90 will have desired state: Ready
2022-03-30 17:34:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaConnector: my-cluster-e556bd90 will have desired state: Ready
2022-03-30 17:34:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnector: my-cluster-e556bd90 will have desired state: Ready not ready, will try again in 1000 ms (239998ms till timeout)
2022-03-30 17:34:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-e556bd90 is in desired state: Ready
2022-03-30 17:34:51 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-30 17:34:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 exec my-cluster-e556bd90-connect-6958b86d76-428fl -- curl -X GET http://localhost:8083/connectors/my-cluster-e556bd90/status
2022-03-30 17:34:52 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-6 exec my-cluster-e556bd90-connect-6958b86d76-428fl -- curl -X GET http://localhost:8083/connectors/my-cluster-e556bd90/status
2022-03-30 17:34:52 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-03-30 17:34:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-e556bd90-hello-world-producer in namespace namespace-6
2022-03-30 17:34:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-03-30 17:34:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-e556bd90-hello-world-consumer in namespace namespace-6
2022-03-30 17:34:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-03-30 17:34:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:my-cluster-e556bd90-hello-world-producer
2022-03-30 17:34:52 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-e556bd90-hello-world-producer will be in active state
2022-03-30 17:34:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for job active
2022-03-30 17:34:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] job active not ready, will try again in 1000 ms (179992ms till timeout)
2022-03-30 17:34:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:my-cluster-e556bd90-hello-world-consumer
2022-03-30 17:34:53 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-e556bd90-hello-world-consumer will be in active state
2022-03-30 17:34:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for job active
2022-03-30 17:34:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:61] Waiting till producer my-cluster-e556bd90-hello-world-producer and consumer my-cluster-e556bd90-hello-world-consumer finish
2022-03-30 17:34:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for clients finished
2022-03-30 17:34:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (219998ms till timeout)
2022-03-30 17:34:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (218995ms till timeout)
2022-03-30 17:34:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (217992ms till timeout)
2022-03-30 17:34:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (216988ms till timeout)
2022-03-30 17:34:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (215983ms till timeout)
2022-03-30 17:34:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [JobUtils:37] Waiting for ReplicaSet of Deployment my-cluster-e556bd90-hello-world-producer deletion
2022-03-30 17:34:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for ReplicaSet my-cluster-e556bd90-hello-world-producer to be deleted
2022-03-30 17:34:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] ReplicaSet my-cluster-e556bd90-hello-world-producer to be deleted not ready, will try again in 5000 ms (179996ms till timeout)
2022-03-30 17:35:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [JobUtils:40] Job my-cluster-e556bd90-hello-world-producer was deleted
2022-03-30 17:35:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [JobUtils:37] Waiting for ReplicaSet of Deployment my-cluster-e556bd90-hello-world-consumer deletion
2022-03-30 17:35:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for ReplicaSet my-cluster-e556bd90-hello-world-consumer to be deleted
2022-03-30 17:35:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] ReplicaSet my-cluster-e556bd90-hello-world-consumer to be deleted not ready, will try again in 5000 ms (179997ms till timeout)
2022-03-30 17:35:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [JobUtils:40] Job my-cluster-e556bd90-hello-world-consumer was deleted
2022-03-30 17:35:08 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-e556bd90-connect-6958b86d76-jvj94
2022-03-30 17:35:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for messages in file sink
2022-03-30 17:35:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 exec my-cluster-e556bd90-connect-6958b86d76-jvj94 -- /bin/bash -c cat /tmp/test-file-sink.txt
2022-03-30 17:35:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:421] Command: kubectl --namespace namespace-6 exec my-cluster-e556bd90-connect-6958b86d76-jvj94 -- /bin/bash -c cat /tmp/test-file-sink.txt
2022-03-30 17:35:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:421] Return code: 0
2022-03-30 17:35:08 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:77] Expected messages are in file sink on my-cluster-e556bd90-connect-6958b86d76-jvj94
2022-03-30 17:35:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 17:35:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:675] [connect.ConnectIsolatedST - After Each] - Clean up after test
2022-03-30 17:35:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 17:35:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testMultiNodeKafkaConnectWithConnectorCreation
2022-03-30 17:35:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector my-cluster-e556bd90 in namespace namespace-6
2022-03-30 17:35:08 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-e556bd90-scraper in namespace namespace-6
2022-03-30 17:35:08 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-e556bd90-scraper
2022-03-30 17:35:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaConnector:my-cluster-e556bd90
2022-03-30 17:35:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-e556bd90-scraper not ready, will try again in 10000 ms (479992ms till timeout)
2022-03-30 17:35:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaConnector:my-cluster-e556bd90 not ready, will try again in 10000 ms (239993ms till timeout)
2022-03-30 17:35:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-e556bd90 in namespace namespace-6
2022-03-30 17:35:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-e556bd90-scraper not ready, will try again in 10000 ms (469987ms till timeout)
2022-03-30 17:35:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaConnect:my-cluster-e556bd90
2022-03-30 17:35:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaConnect:my-cluster-e556bd90 not ready, will try again in 10000 ms (599992ms till timeout)
2022-03-30 17:35:28 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-e556bd90-scraper not ready, will try again in 10000 ms (459981ms till timeout)
2022-03-30 17:35:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-e556bd90-hello-world-consumer in namespace namespace-6
2022-03-30 17:35:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:my-cluster-e556bd90-hello-world-consumer
2022-03-30 17:35:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-e556bd90-hello-world-producer in namespace namespace-6
2022-03-30 17:35:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:my-cluster-e556bd90-hello-world-producer
2022-03-30 17:35:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-e556bd90 in namespace namespace-6
2022-03-30 17:35:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-e556bd90
2022-03-30 17:35:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-e556bd90 not ready, will try again in 10000 ms (839997ms till timeout)
2022-03-30 17:35:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-e556bd90-scraper not ready, will try again in 10000 ms (449975ms till timeout)
2022-03-30 17:35:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-e556bd90-allow in namespace namespace-6
2022-03-30 17:35:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:my-cluster-e556bd90-allow
2022-03-30 17:35:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 17:35:48 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-6 for test case:testMultiNodeKafkaConnectWithConnectorCreation
2022-03-30 17:35:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-6 removal
2022-03-30 17:35:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:35:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:35:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:35:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (479926ms till timeout)
2022-03-30 17:35:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:35:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:35:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:35:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (478855ms till timeout)
2022-03-30 17:35:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:35:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:35:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:35:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (477779ms till timeout)
2022-03-30 17:35:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:35:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:35:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:35:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (476706ms till timeout)
2022-03-30 17:35:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:35:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:35:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:35:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (475637ms till timeout)
2022-03-30 17:35:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:35:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:35:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:35:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (474567ms till timeout)
2022-03-30 17:35:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:35:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:35:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:35:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (473496ms till timeout)
2022-03-30 17:35:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:35:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:35:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:35:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (472429ms till timeout)
2022-03-30 17:35:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:35:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:35:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:35:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (471355ms till timeout)
2022-03-30 17:35:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:35:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:35:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:35:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (470278ms till timeout)
2022-03-30 17:35:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:35:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:35:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:35:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (469204ms till timeout)
2022-03-30 17:36:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:36:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:36:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:36:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (468136ms till timeout)
2022-03-30 17:36:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:36:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:36:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:36:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (467063ms till timeout)
2022-03-30 17:36:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:36:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:36:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:36:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (465987ms till timeout)
2022-03-30 17:36:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:36:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:36:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:36:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (464911ms till timeout)
2022-03-30 17:36:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:36:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:36:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:36:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (463840ms till timeout)
2022-03-30 17:36:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:36:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:36:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:36:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (462767ms till timeout)
2022-03-30 17:36:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:36:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:36:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:36:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (461697ms till timeout)
2022-03-30 17:36:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:36:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:36:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:36:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (460627ms till timeout)
2022-03-30 17:36:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:36:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:36:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:36:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (459555ms till timeout)
2022-03-30 17:36:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:36:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:36:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:36:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (458484ms till timeout)
2022-03-30 17:36:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:36:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:36:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:36:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (457412ms till timeout)
2022-03-30 17:36:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:36:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:36:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:36:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (456333ms till timeout)
2022-03-30 17:36:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:36:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:36:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:36:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (455253ms till timeout)
2022-03-30 17:36:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:36:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:36:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:36:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (454182ms till timeout)
2022-03-30 17:36:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:36:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 17:36:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 17:36:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 17:36:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-6" not found
2022-03-30 17:36:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 17:36:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@c27758f5=[]}
2022-03-30 17:36:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:267] testMultiNodeKafkaConnectWithConnectorCreation - Notifies waiting test cases:[testUpdateUser, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testCruiseControlBasicAPIRequests, testSendMessagesTlsScramSha, testSendMessagesCustomListenerTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testMultiNodeKafkaConnectWithConnectorCreation] to and randomly select one to start execution
2022-03-30 17:36:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:93] [connect.ConnectIsolatedST] - Removing parallel test: testMultiNodeKafkaConnectWithConnectorCreation
2022-03-30 17:36:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:97] [connect.ConnectIsolatedST] - Parallel test count: 0
2022-03-30 17:36:15 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testMultiNodeKafkaConnectWithConnectorCreation-FINISHED
2022-03-30 17:36:15 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 17:36:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 17:36:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:690] [connect.ConnectIsolatedST - After All] - Clean up after test suite
2022-03-30 17:36:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 17:36:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context ConnectIsolatedST is everything deleted.
2022-03-30 17:36:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 380.115 s - in io.strimzi.systemtest.connect.ConnectIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST
2022-03-30 17:36:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 17:36:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:667] [mirrormaker.MirrorMaker2IsolatedST - Before All] - Setup test suite environment
2022-03-30 17:36:15 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 17:36:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 0
2022-03-30 17:36:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 1
2022-03-30 17:36:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 0
2022-03-30 17:36:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 2
2022-03-30 17:36:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 0
2022-03-30 17:36:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 3
2022-03-30 17:36:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 0
2022-03-30 17:36:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 4
2022-03-30 17:36:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 0
2022-03-30 17:36:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 5
2022-03-30 17:36:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:136] Suite mirrormaker.MirrorMaker2IsolatedST has locked the @IsolatedSuite and other @IsolatedSuites must wait until lock is released.
2022-03-30 17:36:40 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-30 17:36:40 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-30 17:36:40 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-30 17:36:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 17:36:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-30 17:36:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 17:36:40 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 17:36:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 17:36:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io not ready, will try again in 10000 ms (179992ms till timeout)
2022-03-30 17:36:40 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 17:36:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179844ms till timeout)
2022-03-30 17:36:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:36:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 17:36:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 17:36:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 17:36:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 17:36:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 17:36:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 17:36:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 17:36:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 17:36:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 17:36:50 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 17:36:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179948ms till timeout)
2022-03-30 17:36:50 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 17:36:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179948ms till timeout)
2022-03-30 17:37:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 17:37:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 17:37:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 17:37:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 17:37:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 17:37:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 17:37:00 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 17:37:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:37:00 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 17:37:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 17:37:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io not ready, will try again in 10000 ms (179976ms till timeout)
2022-03-30 17:37:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator not ready, will try again in 10000 ms (479973ms till timeout)
2022-03-30 17:37:10 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 17:37:10 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 17:37:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator not ready, will try again in 10000 ms (469963ms till timeout)
2022-03-30 17:37:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io not ready, will try again in 10000 ms (179990ms till timeout)
2022-03-30 17:37:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 17:37:20 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 17:37:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 17:37:20 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 17:37:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 17:37:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io not ready, will try again in 10000 ms (179957ms till timeout)
2022-03-30 17:37:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 17:37:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:37:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 17:37:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 17:37:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 17:37:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 17:37:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 17:37:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 17:37:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 17:37:20 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:37:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 17:37:20 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 17:37:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 17:37:20 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 17:37:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 17:37:30 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 17:37:30 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 17:37:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 17:37:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-30 17:37:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 17:37:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 17:37:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v60476
2022-03-30 17:37:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v60476
2022-03-30 17:37:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=60476&allowWatchBookmarks=true&watch=true...
2022-03-30 17:37:30 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 17:37:30 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 60477
2022-03-30 17:37:35 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 60492
2022-03-30 17:37:35 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 60493
2022-03-30 17:37:35 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v60492 in namespace default
2022-03-30 17:37:35 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5e99e8d5
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=30000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-30 17:37:35 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@42ef98aa
2022-03-30 17:37:35 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 17:37:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@cbb87a8, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5e99e8d5, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=30000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-30 17:37:35 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@46d0042f
2022-03-30 17:37:35 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-30 17:37:35 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@46d0042f
2022-03-30 17:37:35 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@46d0042f
2022-03-30 17:37:35 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 17:37:35 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 17:37:35 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-30 17:37:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-30 17:37:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T17:37:35Z",
        "name": "infra-namespace",
        "resourceVersion": "60494",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "da3d065f-c5d6-45c5-83e8-231439c963f4"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 17:37:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479996ms till timeout)
2022-03-30 17:37:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478993ms till timeout)
2022-03-30 17:37:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477990ms till timeout)
2022-03-30 17:37:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476987ms till timeout)
2022-03-30 17:37:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475983ms till timeout)
2022-03-30 17:37:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474980ms till timeout)
2022-03-30 17:37:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473977ms till timeout)
2022-03-30 17:37:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472974ms till timeout)
2022-03-30 17:37:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471971ms till timeout)
2022-03-30 17:37:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470968ms till timeout)
2022-03-30 17:37:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469965ms till timeout)
2022-03-30 17:37:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468962ms till timeout)
2022-03-30 17:37:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467959ms till timeout)
2022-03-30 17:37:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466956ms till timeout)
2022-03-30 17:37:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465953ms till timeout)
2022-03-30 17:37:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464950ms till timeout)
2022-03-30 17:37:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463947ms till timeout)
2022-03-30 17:37:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462944ms till timeout)
2022-03-30 17:37:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461941ms till timeout)
2022-03-30 17:37:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (460938ms till timeout)
2022-03-30 17:37:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (459934ms till timeout)
2022-03-30 17:37:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (458931ms till timeout)
2022-03-30 17:37:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (457928ms till timeout)
2022-03-30 17:37:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (456925ms till timeout)
2022-03-30 17:38:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (455922ms till timeout)
2022-03-30 17:38:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (454919ms till timeout)
2022-03-30 17:38:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (453916ms till timeout)
2022-03-30 17:38:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (452913ms till timeout)
2022-03-30 17:38:04 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-30 17:38:04 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-30 17:38:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-30 17:38:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-cj5rm not ready: strimzi-cluster-operator)
2022-03-30 17:38:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-cj5rm are ready
2022-03-30 17:38:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-30 17:38:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-cj5rm not ready: strimzi-cluster-operator)
2022-03-30 17:38:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-cj5rm are ready
2022-03-30 17:38:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-30 17:38:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-cj5rm not ready: strimzi-cluster-operator)
2022-03-30 17:38:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-cj5rm are ready
2022-03-30 17:38:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597990ms till timeout)
2022-03-30 17:38:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-cj5rm not ready: strimzi-cluster-operator)
2022-03-30 17:38:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-cj5rm are ready
2022-03-30 17:38:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596986ms till timeout)
2022-03-30 17:38:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-cj5rm not ready: strimzi-cluster-operator)
2022-03-30 17:38:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-cj5rm are ready
2022-03-30 17:38:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595983ms till timeout)
2022-03-30 17:38:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-cj5rm not ready: strimzi-cluster-operator)
2022-03-30 17:38:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-cj5rm are ready
2022-03-30 17:38:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594980ms till timeout)
2022-03-30 17:38:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-cj5rm not ready: strimzi-cluster-operator)
2022-03-30 17:38:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-cj5rm are ready
2022-03-30 17:38:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593976ms till timeout)
2022-03-30 17:38:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-cj5rm not ready: strimzi-cluster-operator)
2022-03-30 17:38:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-cj5rm are ready
2022-03-30 17:38:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592972ms till timeout)
2022-03-30 17:38:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-cj5rm not ready: strimzi-cluster-operator)
2022-03-30 17:38:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-cj5rm are ready
2022-03-30 17:38:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591969ms till timeout)
2022-03-30 17:38:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-cj5rm not ready: strimzi-cluster-operator)
2022-03-30 17:38:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-cj5rm are ready
2022-03-30 17:38:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590966ms till timeout)
2022-03-30 17:38:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-cj5rm not ready: strimzi-cluster-operator)
2022-03-30 17:38:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-cj5rm are ready
2022-03-30 17:38:14 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-30 17:38:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 17:38:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testMirrorMaker2TlsAndTlsClientAuth-STARTED
2022-03-30 17:38:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 17:38:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:659] [mirrormaker.MirrorMaker2IsolatedST - Before Each] - Setup test case environment
2022-03-30 17:38:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:77] [mirrormaker.MirrorMaker2IsolatedST] - Adding parallel test: testMirrorMaker2TlsAndTlsClientAuth
2022-03-30 17:38:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:81] [mirrormaker.MirrorMaker2IsolatedST] - Parallel test count: 1
2022-03-30 17:38:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:230] testMirrorMaker2TlsAndTlsClientAuth test now can proceed its execution
2022-03-30 17:38:14 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 17:38:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1, testSendSimpleMessageTls=my-cluster-128d9a40, testUpdateUser=my-cluster-75998b92, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e, testCruiseControlBasicAPIRequests=my-cluster-6da231a3, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2, testSendMessagesTlsScramSha=my-cluster-a11b9173, testReceiveSimpleMessageTls=my-cluster-90021a9f, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc}
2022-03-30 17:38:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1975958319-512406578, testSendSimpleMessageTls=my-user-323988616-1355569855, testUpdateUser=my-user-275427717-1183444028, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-695287754-1004738297, testKafkaInDifferentNsThanClusterOperator=my-user-908767181-1543724508, testSendMessagesCustomListenerTlsScramSha=my-user-1653975062-1727371851, testCruiseControlBasicAPIRequests=my-user-918568860-36694764, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1337805046-1917759354, testAutoRenewAllCaCertsTriggeredByAnno=my-user-308582398-741623568, testSendMessagesTlsScramSha=my-user-1791509549-1350003609, testReceiveSimpleMessageTls=my-user-491397521-1196361913, testMirrorMaker2TlsAndTlsClientAuth=my-user-711655589-495294801, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-83332781-1273282344, testRackAwareConnectCorrectDeployment=my-user-1839832768-1329691231}
2022-03-30 17:38:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-1496158521-938246156, testSendSimpleMessageTls=my-topic-1040221322-670398147, testUpdateUser=my-topic-275186280-765877519, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1609923788-1785060017, testKafkaInDifferentNsThanClusterOperator=my-topic-2051437288-2087424913, testSendMessagesCustomListenerTlsScramSha=my-topic-629448478-708445494, testCruiseControlBasicAPIRequests=my-topic-710218582-658397020, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-604305255-1585536778, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1728678022-515015095, testSendMessagesTlsScramSha=my-topic-1482193652-649135537, testReceiveSimpleMessageTls=my-topic-941665209-436577257, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1152366002-738531989, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1536476030-1482182833, testRackAwareConnectCorrectDeployment=my-topic-1526242410-482838543}
2022-03-30 17:38:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1-kafka-clients, testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-6da231a3-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2-kafka-clients, testSendMessagesTlsScramSha=my-cluster-a11b9173-kafka-clients, testReceiveSimpleMessageTls=my-cluster-90021a9f-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc-kafka-clients}
2022-03-30 17:38:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-7 for test case:testMirrorMaker2TlsAndTlsClientAuth
2022-03-30 17:38:14 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-7
2022-03-30 17:38:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-7
2022-03-30 17:38:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-7 -o json
2022-03-30 17:38:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-7 -o json
2022-03-30 17:38:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:38:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T17:38:14Z",
        "name": "namespace-7",
        "resourceVersion": "60596",
        "selfLink": "/api/v1/namespaces/namespace-7",
        "uid": "b45e8630-80cf-486f-82f4-fbf253175666"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 17:38:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@529573c9=[namespace-7]}
2022-03-30 17:38:14 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-7
2022-03-30 17:38:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-7, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 17:38:14 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-7
2022-03-30 17:38:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-499ae2e7-source in namespace namespace-7
2022-03-30 17:38:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-30 17:38:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-499ae2e7-source
2022-03-30 17:38:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-499ae2e7-source will have desired state: Ready
2022-03-30 17:38:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-499ae2e7-source will have desired state: Ready
2022-03-30 17:38:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (839998ms till timeout)
2022-03-30 17:38:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (838994ms till timeout)
2022-03-30 17:38:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (837991ms till timeout)
2022-03-30 17:38:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (836988ms till timeout)
2022-03-30 17:38:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (835985ms till timeout)
2022-03-30 17:38:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (834982ms till timeout)
2022-03-30 17:38:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (833979ms till timeout)
2022-03-30 17:38:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (832976ms till timeout)
2022-03-30 17:38:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (831972ms till timeout)
2022-03-30 17:38:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (830969ms till timeout)
2022-03-30 17:38:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (829966ms till timeout)
2022-03-30 17:38:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (828963ms till timeout)
2022-03-30 17:38:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (827960ms till timeout)
2022-03-30 17:38:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (826956ms till timeout)
2022-03-30 17:38:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (825953ms till timeout)
2022-03-30 17:38:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (824950ms till timeout)
2022-03-30 17:38:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (823947ms till timeout)
2022-03-30 17:38:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (822943ms till timeout)
2022-03-30 17:38:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (821940ms till timeout)
2022-03-30 17:38:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (820937ms till timeout)
2022-03-30 17:38:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (819934ms till timeout)
2022-03-30 17:38:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (818930ms till timeout)
2022-03-30 17:38:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (817927ms till timeout)
2022-03-30 17:38:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (816924ms till timeout)
2022-03-30 17:38:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (815921ms till timeout)
2022-03-30 17:38:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (814918ms till timeout)
2022-03-30 17:38:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (813914ms till timeout)
2022-03-30 17:38:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (812911ms till timeout)
2022-03-30 17:38:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (811908ms till timeout)
2022-03-30 17:38:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (810904ms till timeout)
2022-03-30 17:38:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (809901ms till timeout)
2022-03-30 17:38:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (808898ms till timeout)
2022-03-30 17:38:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (807894ms till timeout)
2022-03-30 17:38:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (806891ms till timeout)
2022-03-30 17:38:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (805888ms till timeout)
2022-03-30 17:38:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (804885ms till timeout)
2022-03-30 17:38:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (803881ms till timeout)
2022-03-30 17:38:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (802878ms till timeout)
2022-03-30 17:38:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (801875ms till timeout)
2022-03-30 17:38:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (800872ms till timeout)
2022-03-30 17:38:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (799869ms till timeout)
2022-03-30 17:38:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (798865ms till timeout)
2022-03-30 17:38:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (797862ms till timeout)
2022-03-30 17:38:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (796858ms till timeout)
2022-03-30 17:38:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (795855ms till timeout)
2022-03-30 17:38:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (794852ms till timeout)
2022-03-30 17:39:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (793849ms till timeout)
2022-03-30 17:39:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (792845ms till timeout)
2022-03-30 17:39:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (791842ms till timeout)
2022-03-30 17:39:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (790839ms till timeout)
2022-03-30 17:39:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (789836ms till timeout)
2022-03-30 17:39:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (788832ms till timeout)
2022-03-30 17:39:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (787829ms till timeout)
2022-03-30 17:39:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (786826ms till timeout)
2022-03-30 17:39:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (785823ms till timeout)
2022-03-30 17:39:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (784820ms till timeout)
2022-03-30 17:39:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (783816ms till timeout)
2022-03-30 17:39:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (782813ms till timeout)
2022-03-30 17:39:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (781810ms till timeout)
2022-03-30 17:39:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (780807ms till timeout)
2022-03-30 17:39:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (779804ms till timeout)
2022-03-30 17:39:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (778801ms till timeout)
2022-03-30 17:39:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (777797ms till timeout)
2022-03-30 17:39:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (776794ms till timeout)
2022-03-30 17:39:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (775791ms till timeout)
2022-03-30 17:39:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (774788ms till timeout)
2022-03-30 17:39:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (773784ms till timeout)
2022-03-30 17:39:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (772781ms till timeout)
2022-03-30 17:39:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (771778ms till timeout)
2022-03-30 17:39:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (770775ms till timeout)
2022-03-30 17:39:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (769772ms till timeout)
2022-03-30 17:39:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-source will have desired state: Ready not ready, will try again in 1000 ms (768769ms till timeout)
2022-03-30 17:39:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-499ae2e7-source is in desired state: Ready
2022-03-30 17:39:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-499ae2e7-target in namespace namespace-7
2022-03-30 17:39:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-30 17:39:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-499ae2e7-target
2022-03-30 17:39:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-499ae2e7-target will have desired state: Ready
2022-03-30 17:39:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-499ae2e7-target will have desired state: Ready
2022-03-30 17:39:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (839997ms till timeout)
2022-03-30 17:39:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (838994ms till timeout)
2022-03-30 17:39:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (837990ms till timeout)
2022-03-30 17:39:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (836987ms till timeout)
2022-03-30 17:39:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (835984ms till timeout)
2022-03-30 17:39:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (834981ms till timeout)
2022-03-30 17:39:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (833977ms till timeout)
2022-03-30 17:39:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (832974ms till timeout)
2022-03-30 17:39:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (831971ms till timeout)
2022-03-30 17:39:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (830968ms till timeout)
2022-03-30 17:39:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (829965ms till timeout)
2022-03-30 17:39:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (828961ms till timeout)
2022-03-30 17:39:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (827958ms till timeout)
2022-03-30 17:39:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (826955ms till timeout)
2022-03-30 17:39:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (825952ms till timeout)
2022-03-30 17:39:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (824949ms till timeout)
2022-03-30 17:39:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (823946ms till timeout)
2022-03-30 17:39:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (822943ms till timeout)
2022-03-30 17:39:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (821940ms till timeout)
2022-03-30 17:39:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (820937ms till timeout)
2022-03-30 17:39:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (819933ms till timeout)
2022-03-30 17:39:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (818930ms till timeout)
2022-03-30 17:39:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (817927ms till timeout)
2022-03-30 17:39:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (816924ms till timeout)
2022-03-30 17:39:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (815920ms till timeout)
2022-03-30 17:39:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (814917ms till timeout)
2022-03-30 17:39:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (813914ms till timeout)
2022-03-30 17:39:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (812911ms till timeout)
2022-03-30 17:39:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (811908ms till timeout)
2022-03-30 17:39:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (810904ms till timeout)
2022-03-30 17:39:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (809901ms till timeout)
2022-03-30 17:39:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (808899ms till timeout)
2022-03-30 17:39:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (807896ms till timeout)
2022-03-30 17:40:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (806892ms till timeout)
2022-03-30 17:40:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (805889ms till timeout)
2022-03-30 17:40:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (804886ms till timeout)
2022-03-30 17:40:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (803883ms till timeout)
2022-03-30 17:40:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (802880ms till timeout)
2022-03-30 17:40:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (801877ms till timeout)
2022-03-30 17:40:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (800874ms till timeout)
2022-03-30 17:40:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (799871ms till timeout)
2022-03-30 17:40:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (798868ms till timeout)
2022-03-30 17:40:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (797865ms till timeout)
2022-03-30 17:40:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (796861ms till timeout)
2022-03-30 17:40:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (795858ms till timeout)
2022-03-30 17:40:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (794855ms till timeout)
2022-03-30 17:40:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (793852ms till timeout)
2022-03-30 17:40:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (792849ms till timeout)
2022-03-30 17:40:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (791846ms till timeout)
2022-03-30 17:40:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (790843ms till timeout)
2022-03-30 17:40:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (789840ms till timeout)
2022-03-30 17:40:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (788836ms till timeout)
2022-03-30 17:40:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (787833ms till timeout)
2022-03-30 17:40:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (786830ms till timeout)
2022-03-30 17:40:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (785828ms till timeout)
2022-03-30 17:40:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (784825ms till timeout)
2022-03-30 17:40:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (783821ms till timeout)
2022-03-30 17:40:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (782818ms till timeout)
2022-03-30 17:40:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (781815ms till timeout)
2022-03-30 17:40:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (780812ms till timeout)
2022-03-30 17:40:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (779808ms till timeout)
2022-03-30 17:40:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (778805ms till timeout)
2022-03-30 17:40:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (777802ms till timeout)
2022-03-30 17:40:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-499ae2e7-target will have desired state: Ready not ready, will try again in 1000 ms (776798ms till timeout)
2022-03-30 17:40:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-499ae2e7-target is in desired state: Ready
2022-03-30 17:40:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic mirrormaker2-topic-example-222529595 in namespace namespace-7
2022-03-30 17:40:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-30 17:40:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:mirrormaker2-topic-example-222529595
2022-03-30 17:40:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: mirrormaker2-topic-example-222529595 will have desired state: Ready
2022-03-30 17:40:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: mirrormaker2-topic-example-222529595 will have desired state: Ready
2022-03-30 17:40:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: mirrormaker2-topic-example-222529595 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 17:40:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: mirrormaker2-topic-example-222529595 is in desired state: Ready
2022-03-30 17:40:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-499ae2e7-my-user-source in namespace namespace-7
2022-03-30 17:40:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-30 17:40:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-cluster-499ae2e7-my-user-source
2022-03-30 17:40:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-499ae2e7-my-user-source will have desired state: Ready
2022-03-30 17:40:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-cluster-499ae2e7-my-user-source will have desired state: Ready
2022-03-30 17:40:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaUser: my-cluster-499ae2e7-my-user-source will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 17:40:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-499ae2e7-my-user-source is in desired state: Ready
2022-03-30 17:40:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-499ae2e7-my-user-target in namespace namespace-7
2022-03-30 17:40:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-30 17:40:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-cluster-499ae2e7-my-user-target
2022-03-30 17:40:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-499ae2e7-my-user-target will have desired state: Ready
2022-03-30 17:40:33 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-cluster-499ae2e7-my-user-target will have desired state: Ready
2022-03-30 17:40:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaUser: my-cluster-499ae2e7-my-user-target will have desired state: Ready not ready, will try again in 1000 ms (179997ms till timeout)
2022-03-30 17:40:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-499ae2e7-my-user-target is in desired state: Ready
2022-03-30 17:40:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-499ae2e7-kafka-clients in namespace namespace-7
2022-03-30 17:40:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-30 17:40:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for pod with prefixmy-cluster-499ae2e7-kafka-clients is present.
2022-03-30 17:40:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] pod with prefixmy-cluster-499ae2e7-kafka-clients is present. not ready, will try again in 10000 ms (299997ms till timeout)
2022-03-30 17:40:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1152366002-738531989-test-1 in namespace namespace-7
2022-03-30 17:40:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-30 17:40:44 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1152366002-738531989-test-1
2022-03-30 17:40:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1152366002-738531989-test-1 will have desired state: Ready
2022-03-30 17:40:44 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1152366002-738531989-test-1 will have desired state: Ready
2022-03-30 17:40:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1152366002-738531989-test-1 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 17:40:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1152366002-738531989-test-1 is in desired state: Ready
2022-03-30 17:40:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1152366002-738531989-test-2 in namespace namespace-7
2022-03-30 17:40:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-30 17:40:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1152366002-738531989-test-2
2022-03-30 17:40:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1152366002-738531989-test-2 will have desired state: Ready
2022-03-30 17:40:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1152366002-738531989-test-2 will have desired state: Ready
2022-03-30 17:40:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1152366002-738531989-test-2 will have desired state: Ready not ready, will try again in 1000 ms (179994ms till timeout)
2022-03-30 17:40:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1152366002-738531989-test-2 is in desired state: Ready
2022-03-30 17:40:46 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-30 17:40:46 [ForkJoinPool-3-worker-3] [32mINFO [m [MirrorMaker2IsolatedST:328] Setting topic to my-topic-1152366002-738531989-test-2, cluster to my-cluster-499ae2e7-target and changing user to my-cluster-499ae2e7-my-user-target
2022-03-30 17:40:46 [ForkJoinPool-3-worker-3] [32mINFO [m [MirrorMaker2IsolatedST:337] Sending messages to - topic my-topic-1152366002-738531989-test-2, cluster my-cluster-499ae2e7-target and message count of 200
2022-03-30 17:40:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@322cb549, which are set.
2022-03-30 17:40:46 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@4ba3c138, messages=[], arguments=[--topic, my-topic-1152366002-738531989-test-2, --bootstrap-server, my-cluster-499ae2e7-target-kafka-bootstrap.namespace-7.svc:9093, USER=my_cluster_499ae2e7_my_user_target, --max-messages, 200], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-499ae2e7-kafka-clients-5586f8944b-n7spr', podNamespace='namespace-7', bootstrapServer='my-cluster-499ae2e7-target-kafka-bootstrap.namespace-7.svc:9093', topicName='my-topic-1152366002-738531989-test-2', maxMessages=200, kafkaUsername='my-cluster-499ae2e7-my-user-target', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@322cb549}
2022-03-30 17:40:46 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-499ae2e7-target-kafka-bootstrap.namespace-7.svc:9093:my-topic-1152366002-738531989-test-2 from pod my-cluster-499ae2e7-kafka-clients-5586f8944b-n7spr
2022-03-30 17:40:46 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-499ae2e7-kafka-clients-5586f8944b-n7spr -n namespace-7 -- /opt/kafka/producer.sh --topic my-topic-1152366002-738531989-test-2 --bootstrap-server my-cluster-499ae2e7-target-kafka-bootstrap.namespace-7.svc:9093 USER=my_cluster_499ae2e7_my_user_target --max-messages 200
2022-03-30 17:40:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-499ae2e7-kafka-clients-5586f8944b-n7spr -n namespace-7 -- /opt/kafka/producer.sh --topic my-topic-1152366002-738531989-test-2 --bootstrap-server my-cluster-499ae2e7-target-kafka-bootstrap.namespace-7.svc:9093 USER=my_cluster_499ae2e7_my_user_target --max-messages 200
2022-03-30 17:40:49 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-30 17:40:49 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-03-30 17:40:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3475475a, which are set.
2022-03-30 17:40:49 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@63a42a60, messages=[], arguments=[--group-id, my-consumer-group-545665868, --topic, my-topic-1152366002-738531989-test-2, --bootstrap-server, my-cluster-499ae2e7-target-kafka-bootstrap.namespace-7.svc:9093, USER=my_cluster_499ae2e7_my_user_target, --max-messages, 200, --group-instance-id, instance2065060116], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-499ae2e7-kafka-clients-5586f8944b-n7spr', podNamespace='namespace-7', bootstrapServer='my-cluster-499ae2e7-target-kafka-bootstrap.namespace-7.svc:9093', topicName='my-topic-1152366002-738531989-test-2', maxMessages=200, kafkaUsername='my-cluster-499ae2e7-my-user-target', consumerGroupName='my-consumer-group-545665868', consumerInstanceId='instance2065060116', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3475475a}
2022-03-30 17:40:49 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-499ae2e7-target-kafka-bootstrap.namespace-7.svc:9093:my-topic-1152366002-738531989-test-2 from pod my-cluster-499ae2e7-kafka-clients-5586f8944b-n7spr
2022-03-30 17:40:49 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-499ae2e7-kafka-clients-5586f8944b-n7spr -n namespace-7 -- /opt/kafka/consumer.sh --group-id my-consumer-group-545665868 --topic my-topic-1152366002-738531989-test-2 --bootstrap-server my-cluster-499ae2e7-target-kafka-bootstrap.namespace-7.svc:9093 USER=my_cluster_499ae2e7_my_user_target --max-messages 200 --group-instance-id instance2065060116
2022-03-30 17:40:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-499ae2e7-kafka-clients-5586f8944b-n7spr -n namespace-7 -- /opt/kafka/consumer.sh --group-id my-consumer-group-545665868 --topic my-topic-1152366002-738531989-test-2 --bootstrap-server my-cluster-499ae2e7-target-kafka-bootstrap.namespace-7.svc:9093 USER=my_cluster_499ae2e7_my_user_target --max-messages 200 --group-instance-id instance2065060116
2022-03-30 17:40:56 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 17:40:56 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-03-30 17:40:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-499ae2e7 in namespace namespace-7
2022-03-30 17:40:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-30 17:40:56 [ForkJoinPool-3-worker-3] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkamirrormaker2s' with unstable version 'v1beta2'
2022-03-30 17:40:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaMirrorMaker2:my-cluster-499ae2e7
2022-03-30 17:40:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready
2022-03-30 17:40:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready
2022-03-30 17:40:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 17:40:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-30 17:40:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (597990ms till timeout)
2022-03-30 17:40:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (596986ms till timeout)
2022-03-30 17:41:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (595983ms till timeout)
2022-03-30 17:41:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (594979ms till timeout)
2022-03-30 17:41:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (593976ms till timeout)
2022-03-30 17:41:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (592972ms till timeout)
2022-03-30 17:41:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (591969ms till timeout)
2022-03-30 17:41:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (590965ms till timeout)
2022-03-30 17:41:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (589959ms till timeout)
2022-03-30 17:41:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (588955ms till timeout)
2022-03-30 17:41:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (587952ms till timeout)
2022-03-30 17:41:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (586948ms till timeout)
2022-03-30 17:41:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (585945ms till timeout)
2022-03-30 17:41:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (584942ms till timeout)
2022-03-30 17:41:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (583939ms till timeout)
2022-03-30 17:41:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (582935ms till timeout)
2022-03-30 17:41:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (581932ms till timeout)
2022-03-30 17:41:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (580928ms till timeout)
2022-03-30 17:41:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (579925ms till timeout)
2022-03-30 17:41:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (578921ms till timeout)
2022-03-30 17:41:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (577918ms till timeout)
2022-03-30 17:41:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (576915ms till timeout)
2022-03-30 17:41:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (575911ms till timeout)
2022-03-30 17:41:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (574908ms till timeout)
2022-03-30 17:41:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (573904ms till timeout)
2022-03-30 17:41:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (572901ms till timeout)
2022-03-30 17:41:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (571897ms till timeout)
2022-03-30 17:41:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (570894ms till timeout)
2022-03-30 17:41:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (569890ms till timeout)
2022-03-30 17:41:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (568887ms till timeout)
2022-03-30 17:41:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (567884ms till timeout)
2022-03-30 17:41:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (566880ms till timeout)
2022-03-30 17:41:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (565877ms till timeout)
2022-03-30 17:41:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (564873ms till timeout)
2022-03-30 17:41:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (563870ms till timeout)
2022-03-30 17:41:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (562866ms till timeout)
2022-03-30 17:41:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (561863ms till timeout)
2022-03-30 17:41:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (560859ms till timeout)
2022-03-30 17:41:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (559856ms till timeout)
2022-03-30 17:41:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (558852ms till timeout)
2022-03-30 17:41:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (557849ms till timeout)
2022-03-30 17:41:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (556846ms till timeout)
2022-03-30 17:41:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (555842ms till timeout)
2022-03-30 17:41:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (554839ms till timeout)
2022-03-30 17:41:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (553836ms till timeout)
2022-03-30 17:41:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (552832ms till timeout)
2022-03-30 17:41:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (551827ms till timeout)
2022-03-30 17:41:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (550824ms till timeout)
2022-03-30 17:41:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (549820ms till timeout)
2022-03-30 17:41:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (548817ms till timeout)
2022-03-30 17:41:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (547814ms till timeout)
2022-03-30 17:41:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (546811ms till timeout)
2022-03-30 17:41:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (545807ms till timeout)
2022-03-30 17:41:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (544804ms till timeout)
2022-03-30 17:41:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (543800ms till timeout)
2022-03-30 17:41:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (542797ms till timeout)
2022-03-30 17:41:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (541793ms till timeout)
2022-03-30 17:41:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (540790ms till timeout)
2022-03-30 17:41:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (539786ms till timeout)
2022-03-30 17:41:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (538783ms till timeout)
2022-03-30 17:41:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (537780ms till timeout)
2022-03-30 17:42:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (536776ms till timeout)
2022-03-30 17:42:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (535773ms till timeout)
2022-03-30 17:42:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (534769ms till timeout)
2022-03-30 17:42:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (533766ms till timeout)
2022-03-30 17:42:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (532763ms till timeout)
2022-03-30 17:42:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (531759ms till timeout)
2022-03-30 17:42:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (530756ms till timeout)
2022-03-30 17:42:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (529753ms till timeout)
2022-03-30 17:42:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (528749ms till timeout)
2022-03-30 17:42:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (527746ms till timeout)
2022-03-30 17:42:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (526743ms till timeout)
2022-03-30 17:42:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (525739ms till timeout)
2022-03-30 17:42:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (524736ms till timeout)
2022-03-30 17:42:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (523733ms till timeout)
2022-03-30 17:42:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-499ae2e7 will have desired state: Ready not ready, will try again in 1000 ms (522728ms till timeout)
2022-03-30 17:42:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-499ae2e7 is in desired state: Ready
2022-03-30 17:42:15 [ForkJoinPool-3-worker-3] [32mINFO [m [MirrorMaker2IsolatedST:397] Setting topic to mirrormaker2-topic-example-222529595, cluster to my-cluster-499ae2e7-source and changing user to my-cluster-499ae2e7-my-user-source
2022-03-30 17:42:15 [ForkJoinPool-3-worker-3] [32mINFO [m [MirrorMaker2IsolatedST:407] Sending messages to - topic mirrormaker2-topic-example-222529595, cluster my-cluster-499ae2e7-source and message count of 200
2022-03-30 17:42:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@74d52cd5, which are set.
2022-03-30 17:42:15 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@428673ef, messages=[], arguments=[--topic, mirrormaker2-topic-example-222529595, --bootstrap-server, my-cluster-499ae2e7-source-kafka-bootstrap.namespace-7.svc:9093, USER=my_cluster_499ae2e7_my_user_source, --max-messages, 200], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-499ae2e7-kafka-clients-5586f8944b-n7spr', podNamespace='namespace-7', bootstrapServer='my-cluster-499ae2e7-source-kafka-bootstrap.namespace-7.svc:9093', topicName='mirrormaker2-topic-example-222529595', maxMessages=200, kafkaUsername='my-cluster-499ae2e7-my-user-source', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@74d52cd5}
2022-03-30 17:42:15 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-499ae2e7-source-kafka-bootstrap.namespace-7.svc:9093:mirrormaker2-topic-example-222529595 from pod my-cluster-499ae2e7-kafka-clients-5586f8944b-n7spr
2022-03-30 17:42:15 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-499ae2e7-kafka-clients-5586f8944b-n7spr -n namespace-7 -- /opt/kafka/producer.sh --topic mirrormaker2-topic-example-222529595 --bootstrap-server my-cluster-499ae2e7-source-kafka-bootstrap.namespace-7.svc:9093 USER=my_cluster_499ae2e7_my_user_source --max-messages 200
2022-03-30 17:42:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-499ae2e7-kafka-clients-5586f8944b-n7spr -n namespace-7 -- /opt/kafka/producer.sh --topic mirrormaker2-topic-example-222529595 --bootstrap-server my-cluster-499ae2e7-source-kafka-bootstrap.namespace-7.svc:9093 USER=my_cluster_499ae2e7_my_user_source --max-messages 200
2022-03-30 17:42:18 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-30 17:42:18 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-03-30 17:42:18 [ForkJoinPool-3-worker-3] [32mINFO [m [MirrorMaker2IsolatedST:411] Receiving messages from - topic mirrormaker2-topic-example-222529595, cluster my-cluster-499ae2e7-source and message count of 200
2022-03-30 17:42:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6d5aff88, which are set.
2022-03-30 17:42:18 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@3034d572, messages=[], arguments=[--group-id, my-consumer-group-545665868, --topic, mirrormaker2-topic-example-222529595, --bootstrap-server, my-cluster-499ae2e7-source-kafka-bootstrap.namespace-7.svc:9093, USER=my_cluster_499ae2e7_my_user_source, --max-messages, 200, --group-instance-id, instance1808578718], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-499ae2e7-kafka-clients-5586f8944b-n7spr', podNamespace='namespace-7', bootstrapServer='my-cluster-499ae2e7-source-kafka-bootstrap.namespace-7.svc:9093', topicName='mirrormaker2-topic-example-222529595', maxMessages=200, kafkaUsername='my-cluster-499ae2e7-my-user-source', consumerGroupName='my-consumer-group-545665868', consumerInstanceId='instance1808578718', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6d5aff88}
2022-03-30 17:42:18 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-499ae2e7-source-kafka-bootstrap.namespace-7.svc:9093:mirrormaker2-topic-example-222529595 from pod my-cluster-499ae2e7-kafka-clients-5586f8944b-n7spr
2022-03-30 17:42:18 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-499ae2e7-kafka-clients-5586f8944b-n7spr -n namespace-7 -- /opt/kafka/consumer.sh --group-id my-consumer-group-545665868 --topic mirrormaker2-topic-example-222529595 --bootstrap-server my-cluster-499ae2e7-source-kafka-bootstrap.namespace-7.svc:9093 USER=my_cluster_499ae2e7_my_user_source --max-messages 200 --group-instance-id instance1808578718
2022-03-30 17:42:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-499ae2e7-kafka-clients-5586f8944b-n7spr -n namespace-7 -- /opt/kafka/consumer.sh --group-id my-consumer-group-545665868 --topic mirrormaker2-topic-example-222529595 --bootstrap-server my-cluster-499ae2e7-source-kafka-bootstrap.namespace-7.svc:9093 USER=my_cluster_499ae2e7_my_user_source --max-messages 200 --group-instance-id instance1808578718
2022-03-30 17:42:25 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 17:42:25 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-03-30 17:42:25 [ForkJoinPool-3-worker-3] [32mINFO [m [MirrorMaker2IsolatedST:418] Now setting topic to my-cluster-499ae2e7-source.mirrormaker2-topic-example-222529595, cluster to my-cluster-499ae2e7-target and user to my-cluster-499ae2e7-my-user-target - the messages should be mirrored
2022-03-30 17:42:25 [ForkJoinPool-3-worker-3] [32mINFO [m [MirrorMaker2IsolatedST:427] Consumer in target cluster and topic should receive 200 messages
2022-03-30 17:42:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@415cc190, which are set.
2022-03-30 17:42:25 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@5c0b6261, messages=[], arguments=[--group-id, my-consumer-group-545665868, --topic, my-cluster-499ae2e7-source.mirrormaker2-topic-example-222529595, --bootstrap-server, my-cluster-499ae2e7-target-kafka-bootstrap.namespace-7.svc:9093, USER=my_cluster_499ae2e7_my_user_target, --max-messages, 200, --group-instance-id, instance699475386], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-499ae2e7-kafka-clients-5586f8944b-n7spr', podNamespace='namespace-7', bootstrapServer='my-cluster-499ae2e7-target-kafka-bootstrap.namespace-7.svc:9093', topicName='my-cluster-499ae2e7-source.mirrormaker2-topic-example-222529595', maxMessages=200, kafkaUsername='my-cluster-499ae2e7-my-user-target', consumerGroupName='my-consumer-group-545665868', consumerInstanceId='instance699475386', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@415cc190}
2022-03-30 17:42:25 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-499ae2e7-target-kafka-bootstrap.namespace-7.svc:9093:my-cluster-499ae2e7-source.mirrormaker2-topic-example-222529595 from pod my-cluster-499ae2e7-kafka-clients-5586f8944b-n7spr
2022-03-30 17:42:25 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-499ae2e7-kafka-clients-5586f8944b-n7spr -n namespace-7 -- /opt/kafka/consumer.sh --group-id my-consumer-group-545665868 --topic my-cluster-499ae2e7-source.mirrormaker2-topic-example-222529595 --bootstrap-server my-cluster-499ae2e7-target-kafka-bootstrap.namespace-7.svc:9093 USER=my_cluster_499ae2e7_my_user_target --max-messages 200 --group-instance-id instance699475386
2022-03-30 17:42:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-499ae2e7-kafka-clients-5586f8944b-n7spr -n namespace-7 -- /opt/kafka/consumer.sh --group-id my-consumer-group-545665868 --topic my-cluster-499ae2e7-source.mirrormaker2-topic-example-222529595 --bootstrap-server my-cluster-499ae2e7-target-kafka-bootstrap.namespace-7.svc:9093 USER=my_cluster_499ae2e7_my_user_target --max-messages 200 --group-instance-id instance699475386
2022-03-30 17:42:32 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 17:42:32 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-03-30 17:42:32 [ForkJoinPool-3-worker-3] [32mINFO [m [MirrorMaker2IsolatedST:432] Messages successfully mirrored
2022-03-30 17:42:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 17:42:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:675] [mirrormaker.MirrorMaker2IsolatedST - After Each] - Clean up after test
2022-03-30 17:42:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 17:42:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testMirrorMaker2TlsAndTlsClientAuth
2022-03-30 17:42:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-499ae2e7-kafka-clients in namespace namespace-7
2022-03-30 17:42:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic mirrormaker2-topic-example-222529595 in namespace namespace-7
2022-03-30 17:42:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-499ae2e7-kafka-clients
2022-03-30 17:42:32 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:mirrormaker2-topic-example-222529595
2022-03-30 17:42:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:mirrormaker2-topic-example-222529595 not ready, will try again in 10000 ms (179991ms till timeout)
2022-03-30 17:42:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-499ae2e7-kafka-clients not ready, will try again in 10000 ms (479986ms till timeout)
2022-03-30 17:42:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:mirrormaker2-topic-example-222529595 not ready, will try again in 10000 ms (169984ms till timeout)
2022-03-30 17:42:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-499ae2e7-kafka-clients not ready, will try again in 10000 ms (469976ms till timeout)
2022-03-30 17:42:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:mirrormaker2-topic-example-222529595 not ready, will try again in 10000 ms (159978ms till timeout)
2022-03-30 17:42:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-499ae2e7-kafka-clients not ready, will try again in 10000 ms (459966ms till timeout)
2022-03-30 17:43:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:mirrormaker2-topic-example-222529595 not ready, will try again in 10000 ms (149971ms till timeout)
2022-03-30 17:43:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-499ae2e7-kafka-clients not ready, will try again in 10000 ms (449956ms till timeout)
2022-03-30 17:43:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:mirrormaker2-topic-example-222529595 not ready, will try again in 10000 ms (139965ms till timeout)
2022-03-30 17:43:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-499ae2e7-my-user-target in namespace namespace-7
2022-03-30 17:43:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-cluster-499ae2e7-my-user-target
2022-03-30 17:43:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-cluster-499ae2e7-my-user-target not ready, will try again in 10000 ms (179992ms till timeout)
2022-03-30 17:43:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:mirrormaker2-topic-example-222529595 not ready, will try again in 10000 ms (129958ms till timeout)
2022-03-30 17:43:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1152366002-738531989-test-2 in namespace namespace-7
2022-03-30 17:43:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1152366002-738531989-test-2
2022-03-30 17:43:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-499ae2e7 in namespace namespace-7
2022-03-30 17:43:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaMirrorMaker2:my-cluster-499ae2e7
2022-03-30 17:43:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaMirrorMaker2:my-cluster-499ae2e7 not ready, will try again in 10000 ms (599989ms till timeout)
2022-03-30 17:43:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:mirrormaker2-topic-example-222529595 not ready, will try again in 10000 ms (119952ms till timeout)
2022-03-30 17:43:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1152366002-738531989-test-1 in namespace namespace-7
2022-03-30 17:43:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1152366002-738531989-test-1
2022-03-30 17:43:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1152366002-738531989-test-1 not ready, will try again in 10000 ms (179994ms till timeout)
2022-03-30 17:43:42 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-499ae2e7-my-user-source in namespace namespace-7
2022-03-30 17:43:42 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-cluster-499ae2e7-my-user-source
2022-03-30 17:43:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-cluster-499ae2e7-my-user-source not ready, will try again in 10000 ms (179995ms till timeout)
2022-03-30 17:43:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-499ae2e7-target in namespace namespace-7
2022-03-30 17:43:42 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-499ae2e7-target
2022-03-30 17:43:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-499ae2e7-target not ready, will try again in 10000 ms (839956ms till timeout)
2022-03-30 17:43:52 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-499ae2e7-source in namespace namespace-7
2022-03-30 17:43:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-499ae2e7-source
2022-03-30 17:43:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-499ae2e7-source not ready, will try again in 10000 ms (839957ms till timeout)
2022-03-30 17:44:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 17:44:02 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-7 for test case:testMirrorMaker2TlsAndTlsClientAuth
2022-03-30 17:44:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-7 removal
2022-03-30 17:44:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:44:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (479927ms till timeout)
2022-03-30 17:44:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:44:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (478857ms till timeout)
2022-03-30 17:44:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:44:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (477786ms till timeout)
2022-03-30 17:44:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:44:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (476710ms till timeout)
2022-03-30 17:44:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:44:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (475640ms till timeout)
2022-03-30 17:44:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:44:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (474558ms till timeout)
2022-03-30 17:44:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:44:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (473486ms till timeout)
2022-03-30 17:44:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:44:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (472414ms till timeout)
2022-03-30 17:44:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:44:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (471338ms till timeout)
2022-03-30 17:44:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:44:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (470265ms till timeout)
2022-03-30 17:44:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:44:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (469193ms till timeout)
2022-03-30 17:44:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:44:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (468117ms till timeout)
2022-03-30 17:44:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:44:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (467046ms till timeout)
2022-03-30 17:44:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:44:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (465975ms till timeout)
2022-03-30 17:44:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:44:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (464903ms till timeout)
2022-03-30 17:44:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:44:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (463825ms till timeout)
2022-03-30 17:44:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:44:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (462752ms till timeout)
2022-03-30 17:44:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:44:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (461677ms till timeout)
2022-03-30 17:44:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:44:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (460602ms till timeout)
2022-03-30 17:44:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:44:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (459527ms till timeout)
2022-03-30 17:44:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:44:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (458456ms till timeout)
2022-03-30 17:44:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:44:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (457379ms till timeout)
2022-03-30 17:44:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:44:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (456300ms till timeout)
2022-03-30 17:44:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:44:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (455234ms till timeout)
2022-03-30 17:44:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:44:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (454160ms till timeout)
2022-03-30 17:44:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 17:44:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 17:44:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 17:44:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-7" not found
2022-03-30 17:44:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 17:44:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@529573c9=[]}
2022-03-30 17:44:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:267] testMirrorMaker2TlsAndTlsClientAuth - Notifies waiting test cases:[testUpdateUser, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testCruiseControlBasicAPIRequests, testSendMessagesTlsScramSha, testSendMessagesCustomListenerTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMaker2TlsAndTlsClientAuth] to and randomly select one to start execution
2022-03-30 17:44:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:93] [mirrormaker.MirrorMaker2IsolatedST] - Removing parallel test: testMirrorMaker2TlsAndTlsClientAuth
2022-03-30 17:44:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:97] [mirrormaker.MirrorMaker2IsolatedST] - Parallel test count: 0
2022-03-30 17:44:29 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testMirrorMaker2TlsAndTlsClientAuth-FINISHED
2022-03-30 17:44:29 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 17:44:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 17:44:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:690] [mirrormaker.MirrorMaker2IsolatedST - After All] - Clean up after test suite
2022-03-30 17:44:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 17:44:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context MirrorMaker2IsolatedST is everything deleted.
2022-03-30 17:44:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 494.176 s - in io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST
2022-03-30 17:44:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 17:44:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:667] [mirrormaker.MirrorMakerIsolatedST - Before All] - Setup test suite environment
2022-03-30 17:44:29 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 17:44:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 0
2022-03-30 17:44:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 1
2022-03-30 17:44:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 0
2022-03-30 17:44:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 2
2022-03-30 17:44:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 0
2022-03-30 17:44:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 3
2022-03-30 17:44:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 0
2022-03-30 17:44:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 4
2022-03-30 17:44:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 0
2022-03-30 17:44:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 5
2022-03-30 17:44:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:136] Suite mirrormaker.MirrorMakerIsolatedST has locked the @IsolatedSuite and other @IsolatedSuites must wait until lock is released.
2022-03-30 17:44:54 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-30 17:44:54 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-30 17:44:54 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-30 17:44:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 17:44:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-30 17:44:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 17:44:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 17:44:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 17:44:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:44:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 17:44:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 17:44:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 17:44:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 17:44:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 17:44:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 17:44:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io not ready, will try again in 10000 ms (179924ms till timeout)
2022-03-30 17:44:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179830ms till timeout)
2022-03-30 17:45:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 17:45:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 17:45:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 17:45:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 17:45:04 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 17:45:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179947ms till timeout)
2022-03-30 17:45:04 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 17:45:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179903ms till timeout)
2022-03-30 17:45:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 17:45:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 17:45:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 17:45:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 17:45:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 17:45:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 17:45:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:45:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 17:45:14 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 17:45:14 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 17:45:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator not ready, will try again in 10000 ms (479945ms till timeout)
2022-03-30 17:45:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io not ready, will try again in 10000 ms (179986ms till timeout)
2022-03-30 17:45:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 17:45:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 17:45:24 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 17:45:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 17:45:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 17:45:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 17:45:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:45:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io not ready, will try again in 10000 ms (179987ms till timeout)
2022-03-30 17:45:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 17:45:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 17:45:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 17:45:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 17:45:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 17:45:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 17:45:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 17:45:24 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:45:24 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 17:45:24 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 17:45:24 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 17:45:24 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 17:45:24 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 17:45:34 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 17:45:34 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 17:45:34 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 17:45:34 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 17:45:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 17:45:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-30 17:45:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 17:45:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 17:45:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v61719
2022-03-30 17:45:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v61719
2022-03-30 17:45:34 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=61719&allowWatchBookmarks=true&watch=true...
2022-03-30 17:45:35 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 17:45:35 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 61720
2022-03-30 17:45:40 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 61734
2022-03-30 17:45:40 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 61735
2022-03-30 17:45:40 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v61734 in namespace default
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5e99e8d5
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=120000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-30 17:45:40 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@bae1421
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@cbb87a8, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5e99e8d5, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=120000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-30 17:45:40 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-30 17:45:40 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@f91818c
2022-03-30 17:45:40 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@f91818c
2022-03-30 17:45:40 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@f91818c
2022-03-30 17:45:40 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 17:45:40 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T17:45:40Z",
        "name": "infra-namespace",
        "resourceVersion": "61736",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "10eed0c8-feb7-4497-9b84-1e420f564392"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 17:45:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479998ms till timeout)
2022-03-30 17:45:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478994ms till timeout)
2022-03-30 17:45:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477991ms till timeout)
2022-03-30 17:45:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476988ms till timeout)
2022-03-30 17:45:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475985ms till timeout)
2022-03-30 17:45:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474982ms till timeout)
2022-03-30 17:45:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473979ms till timeout)
2022-03-30 17:45:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472976ms till timeout)
2022-03-30 17:45:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471973ms till timeout)
2022-03-30 17:45:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470970ms till timeout)
2022-03-30 17:45:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469967ms till timeout)
2022-03-30 17:45:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468964ms till timeout)
2022-03-30 17:45:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467960ms till timeout)
2022-03-30 17:45:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466957ms till timeout)
2022-03-30 17:45:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465954ms till timeout)
2022-03-30 17:45:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464951ms till timeout)
2022-03-30 17:45:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463948ms till timeout)
2022-03-30 17:45:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462945ms till timeout)
2022-03-30 17:45:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461942ms till timeout)
2022-03-30 17:45:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (460939ms till timeout)
2022-03-30 17:46:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (459936ms till timeout)
2022-03-30 17:46:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (458933ms till timeout)
2022-03-30 17:46:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (457930ms till timeout)
2022-03-30 17:46:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (456926ms till timeout)
2022-03-30 17:46:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (455923ms till timeout)
2022-03-30 17:46:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (454920ms till timeout)
2022-03-30 17:46:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (453917ms till timeout)
2022-03-30 17:46:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (452914ms till timeout)
2022-03-30 17:46:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (451911ms till timeout)
2022-03-30 17:46:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (450908ms till timeout)
2022-03-30 17:46:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (449905ms till timeout)
2022-03-30 17:46:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (448902ms till timeout)
2022-03-30 17:46:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (447899ms till timeout)
2022-03-30 17:46:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (446895ms till timeout)
2022-03-30 17:46:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (445892ms till timeout)
2022-03-30 17:46:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (444889ms till timeout)
2022-03-30 17:46:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (443886ms till timeout)
2022-03-30 17:46:17 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-30 17:46:17 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-30 17:46:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-30 17:46:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-fx54k not ready: strimzi-cluster-operator)
2022-03-30 17:46:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-fx54k are ready
2022-03-30 17:46:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-30 17:46:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-fx54k not ready: strimzi-cluster-operator)
2022-03-30 17:46:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-fx54k are ready
2022-03-30 17:46:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-30 17:46:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-fx54k not ready: strimzi-cluster-operator)
2022-03-30 17:46:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-fx54k are ready
2022-03-30 17:46:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597990ms till timeout)
2022-03-30 17:46:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-fx54k not ready: strimzi-cluster-operator)
2022-03-30 17:46:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-fx54k are ready
2022-03-30 17:46:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596987ms till timeout)
2022-03-30 17:46:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-fx54k not ready: strimzi-cluster-operator)
2022-03-30 17:46:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-fx54k are ready
2022-03-30 17:46:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595983ms till timeout)
2022-03-30 17:46:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-fx54k not ready: strimzi-cluster-operator)
2022-03-30 17:46:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-fx54k are ready
2022-03-30 17:46:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594980ms till timeout)
2022-03-30 17:46:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-fx54k not ready: strimzi-cluster-operator)
2022-03-30 17:46:23 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-fx54k are ready
2022-03-30 17:46:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593976ms till timeout)
2022-03-30 17:46:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-fx54k not ready: strimzi-cluster-operator)
2022-03-30 17:46:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-fx54k are ready
2022-03-30 17:46:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592973ms till timeout)
2022-03-30 17:46:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-fx54k not ready: strimzi-cluster-operator)
2022-03-30 17:46:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-fx54k are ready
2022-03-30 17:46:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591970ms till timeout)
2022-03-30 17:46:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-fx54k not ready: strimzi-cluster-operator)
2022-03-30 17:46:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-fx54k are ready
2022-03-30 17:46:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590966ms till timeout)
2022-03-30 17:46:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-fx54k not ready: strimzi-cluster-operator)
2022-03-30 17:46:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-fx54k are ready
2022-03-30 17:46:27 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-30 17:46:27 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 17:46:27 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST.testMirrorMakerTlsAuthenticated-STARTED
2022-03-30 17:46:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 17:46:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:659] [mirrormaker.MirrorMakerIsolatedST - Before Each] - Setup test case environment
2022-03-30 17:46:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:77] [mirrormaker.MirrorMakerIsolatedST] - Adding parallel test: testMirrorMakerTlsAuthenticated
2022-03-30 17:46:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:81] [mirrormaker.MirrorMakerIsolatedST] - Parallel test count: 1
2022-03-30 17:46:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:230] testMirrorMakerTlsAuthenticated test now can proceed its execution
2022-03-30 17:46:27 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 17:46:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1, testSendSimpleMessageTls=my-cluster-128d9a40, testUpdateUser=my-cluster-75998b92, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e, testCruiseControlBasicAPIRequests=my-cluster-6da231a3, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2, testSendMessagesTlsScramSha=my-cluster-a11b9173, testReceiveSimpleMessageTls=my-cluster-90021a9f, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e}
2022-03-30 17:46:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1975958319-512406578, testSendSimpleMessageTls=my-user-323988616-1355569855, testUpdateUser=my-user-275427717-1183444028, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-695287754-1004738297, testKafkaInDifferentNsThanClusterOperator=my-user-908767181-1543724508, testSendMessagesCustomListenerTlsScramSha=my-user-1653975062-1727371851, testCruiseControlBasicAPIRequests=my-user-918568860-36694764, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1337805046-1917759354, testAutoRenewAllCaCertsTriggeredByAnno=my-user-308582398-741623568, testSendMessagesTlsScramSha=my-user-1791509549-1350003609, testReceiveSimpleMessageTls=my-user-491397521-1196361913, testMirrorMaker2TlsAndTlsClientAuth=my-user-711655589-495294801, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-83332781-1273282344, testRackAwareConnectCorrectDeployment=my-user-1839832768-1329691231, testMirrorMakerTlsAuthenticated=my-user-1552986594-1641210083}
2022-03-30 17:46:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-1496158521-938246156, testSendSimpleMessageTls=my-topic-1040221322-670398147, testUpdateUser=my-topic-275186280-765877519, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1609923788-1785060017, testKafkaInDifferentNsThanClusterOperator=my-topic-2051437288-2087424913, testSendMessagesCustomListenerTlsScramSha=my-topic-629448478-708445494, testCruiseControlBasicAPIRequests=my-topic-710218582-658397020, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-604305255-1585536778, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1728678022-515015095, testSendMessagesTlsScramSha=my-topic-1482193652-649135537, testReceiveSimpleMessageTls=my-topic-941665209-436577257, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1152366002-738531989, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1536476030-1482182833, testRackAwareConnectCorrectDeployment=my-topic-1526242410-482838543, testMirrorMakerTlsAuthenticated=my-topic-1936321612-505632869}
2022-03-30 17:46:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1-kafka-clients, testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-6da231a3-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2-kafka-clients, testSendMessagesTlsScramSha=my-cluster-a11b9173-kafka-clients, testReceiveSimpleMessageTls=my-cluster-90021a9f-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e-kafka-clients}
2022-03-30 17:46:27 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-8 for test case:testMirrorMakerTlsAuthenticated
2022-03-30 17:46:27 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-8
2022-03-30 17:46:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-8
2022-03-30 17:46:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-8 -o json
2022-03-30 17:46:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-8 -o json
2022-03-30 17:46:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:46:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T17:46:27Z",
        "name": "namespace-8",
        "resourceVersion": "61846",
        "selfLink": "/api/v1/namespaces/namespace-8",
        "uid": "21d66dbb-d751-424c-86a9-34f4829ef42f"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 17:46:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@6a42f58d=[namespace-8]}
2022-03-30 17:46:27 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-8
2022-03-30 17:46:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-8, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 17:46:27 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-8
2022-03-30 17:46:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-4ad8959e-source in namespace namespace-8
2022-03-30 17:46:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-30 17:46:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-4ad8959e-source
2022-03-30 17:46:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-4ad8959e-source will have desired state: Ready
2022-03-30 17:46:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-4ad8959e-source will have desired state: Ready
2022-03-30 17:46:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (839998ms till timeout)
2022-03-30 17:46:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (838995ms till timeout)
2022-03-30 17:46:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (837992ms till timeout)
2022-03-30 17:46:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (836989ms till timeout)
2022-03-30 17:46:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (835985ms till timeout)
2022-03-30 17:46:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (834982ms till timeout)
2022-03-30 17:46:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (833979ms till timeout)
2022-03-30 17:46:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (832976ms till timeout)
2022-03-30 17:46:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (831972ms till timeout)
2022-03-30 17:46:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (830969ms till timeout)
2022-03-30 17:46:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (829966ms till timeout)
2022-03-30 17:46:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (828963ms till timeout)
2022-03-30 17:46:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (827960ms till timeout)
2022-03-30 17:46:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (826957ms till timeout)
2022-03-30 17:46:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (825954ms till timeout)
2022-03-30 17:46:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (824951ms till timeout)
2022-03-30 17:46:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (823948ms till timeout)
2022-03-30 17:46:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (822945ms till timeout)
2022-03-30 17:46:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (821942ms till timeout)
2022-03-30 17:46:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (820939ms till timeout)
2022-03-30 17:46:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (819936ms till timeout)
2022-03-30 17:46:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (818933ms till timeout)
2022-03-30 17:46:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (817930ms till timeout)
2022-03-30 17:46:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (816928ms till timeout)
2022-03-30 17:46:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (815925ms till timeout)
2022-03-30 17:46:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (814922ms till timeout)
2022-03-30 17:46:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (813919ms till timeout)
2022-03-30 17:46:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (812916ms till timeout)
2022-03-30 17:46:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (811913ms till timeout)
2022-03-30 17:46:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (810910ms till timeout)
2022-03-30 17:46:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (809907ms till timeout)
2022-03-30 17:46:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (808903ms till timeout)
2022-03-30 17:47:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (807900ms till timeout)
2022-03-30 17:47:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (806897ms till timeout)
2022-03-30 17:47:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (805894ms till timeout)
2022-03-30 17:47:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (804890ms till timeout)
2022-03-30 17:47:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (803887ms till timeout)
2022-03-30 17:47:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (802884ms till timeout)
2022-03-30 17:47:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (801881ms till timeout)
2022-03-30 17:47:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (800878ms till timeout)
2022-03-30 17:47:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (799875ms till timeout)
2022-03-30 17:47:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (798872ms till timeout)
2022-03-30 17:47:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (797869ms till timeout)
2022-03-30 17:47:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (796866ms till timeout)
2022-03-30 17:47:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (795863ms till timeout)
2022-03-30 17:47:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (794859ms till timeout)
2022-03-30 17:47:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (793856ms till timeout)
2022-03-30 17:47:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (792853ms till timeout)
2022-03-30 17:47:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (791850ms till timeout)
2022-03-30 17:47:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (790846ms till timeout)
2022-03-30 17:47:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (789844ms till timeout)
2022-03-30 17:47:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (788840ms till timeout)
2022-03-30 17:47:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (787837ms till timeout)
2022-03-30 17:47:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (786834ms till timeout)
2022-03-30 17:47:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (785831ms till timeout)
2022-03-30 17:47:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (784828ms till timeout)
2022-03-30 17:47:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (783824ms till timeout)
2022-03-30 17:47:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (782821ms till timeout)
2022-03-30 17:47:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (781818ms till timeout)
2022-03-30 17:47:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (780815ms till timeout)
2022-03-30 17:47:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (779812ms till timeout)
2022-03-30 17:47:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (778809ms till timeout)
2022-03-30 17:47:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (777805ms till timeout)
2022-03-30 17:47:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (776802ms till timeout)
2022-03-30 17:47:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (775799ms till timeout)
2022-03-30 17:47:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (774796ms till timeout)
2022-03-30 17:47:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (773792ms till timeout)
2022-03-30 17:47:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (772789ms till timeout)
2022-03-30 17:47:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (771786ms till timeout)
2022-03-30 17:47:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (770783ms till timeout)
2022-03-30 17:47:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (769780ms till timeout)
2022-03-30 17:47:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (768776ms till timeout)
2022-03-30 17:47:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (767773ms till timeout)
2022-03-30 17:47:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (766770ms till timeout)
2022-03-30 17:47:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-source will have desired state: Ready not ready, will try again in 1000 ms (765767ms till timeout)
2022-03-30 17:47:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-4ad8959e-source is in desired state: Ready
2022-03-30 17:47:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-4ad8959e-target in namespace namespace-8
2022-03-30 17:47:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-30 17:47:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-4ad8959e-target
2022-03-30 17:47:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-4ad8959e-target will have desired state: Ready
2022-03-30 17:47:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-4ad8959e-target will have desired state: Ready
2022-03-30 17:47:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (839998ms till timeout)
2022-03-30 17:47:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (838995ms till timeout)
2022-03-30 17:47:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (837992ms till timeout)
2022-03-30 17:47:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (836989ms till timeout)
2022-03-30 17:47:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (835986ms till timeout)
2022-03-30 17:47:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (834983ms till timeout)
2022-03-30 17:47:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (833980ms till timeout)
2022-03-30 17:47:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (832977ms till timeout)
2022-03-30 17:47:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (831974ms till timeout)
2022-03-30 17:47:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (830971ms till timeout)
2022-03-30 17:47:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (829968ms till timeout)
2022-03-30 17:47:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (828965ms till timeout)
2022-03-30 17:47:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (827962ms till timeout)
2022-03-30 17:47:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (826959ms till timeout)
2022-03-30 17:47:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (825956ms till timeout)
2022-03-30 17:47:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (824953ms till timeout)
2022-03-30 17:47:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (823949ms till timeout)
2022-03-30 17:48:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (822946ms till timeout)
2022-03-30 17:48:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (821943ms till timeout)
2022-03-30 17:48:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (820939ms till timeout)
2022-03-30 17:48:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (819936ms till timeout)
2022-03-30 17:48:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (818933ms till timeout)
2022-03-30 17:48:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (817930ms till timeout)
2022-03-30 17:48:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (816927ms till timeout)
2022-03-30 17:48:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (815924ms till timeout)
2022-03-30 17:48:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (814921ms till timeout)
2022-03-30 17:48:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (813918ms till timeout)
2022-03-30 17:48:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (812914ms till timeout)
2022-03-30 17:48:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (811911ms till timeout)
2022-03-30 17:48:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (810908ms till timeout)
2022-03-30 17:48:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (809905ms till timeout)
2022-03-30 17:48:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (808902ms till timeout)
2022-03-30 17:48:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (807898ms till timeout)
2022-03-30 17:48:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (806895ms till timeout)
2022-03-30 17:48:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (805892ms till timeout)
2022-03-30 17:48:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (804890ms till timeout)
2022-03-30 17:48:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (803887ms till timeout)
2022-03-30 17:48:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (802883ms till timeout)
2022-03-30 17:48:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (801880ms till timeout)
2022-03-30 17:48:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (800877ms till timeout)
2022-03-30 17:48:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (799873ms till timeout)
2022-03-30 17:48:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (798871ms till timeout)
2022-03-30 17:48:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (797867ms till timeout)
2022-03-30 17:48:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (796864ms till timeout)
2022-03-30 17:48:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (795861ms till timeout)
2022-03-30 17:48:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (794858ms till timeout)
2022-03-30 17:48:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (793854ms till timeout)
2022-03-30 17:48:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (792851ms till timeout)
2022-03-30 17:48:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (791848ms till timeout)
2022-03-30 17:48:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (790845ms till timeout)
2022-03-30 17:48:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (789842ms till timeout)
2022-03-30 17:48:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (788839ms till timeout)
2022-03-30 17:48:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (787835ms till timeout)
2022-03-30 17:48:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (786832ms till timeout)
2022-03-30 17:48:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (785829ms till timeout)
2022-03-30 17:48:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (784826ms till timeout)
2022-03-30 17:48:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (783819ms till timeout)
2022-03-30 17:48:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (782816ms till timeout)
2022-03-30 17:48:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (781813ms till timeout)
2022-03-30 17:48:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (780809ms till timeout)
2022-03-30 17:48:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (779806ms till timeout)
2022-03-30 17:48:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (778803ms till timeout)
2022-03-30 17:48:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (777800ms till timeout)
2022-03-30 17:48:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (776797ms till timeout)
2022-03-30 17:48:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (775794ms till timeout)
2022-03-30 17:48:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (774791ms till timeout)
2022-03-30 17:48:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (773788ms till timeout)
2022-03-30 17:48:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-4ad8959e-target will have desired state: Ready not ready, will try again in 1000 ms (772785ms till timeout)
2022-03-30 17:48:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-4ad8959e-target is in desired state: Ready
2022-03-30 17:48:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-182655179-1869267583-source-1189016320 in namespace namespace-8
2022-03-30 17:48:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-30 17:48:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-182655179-1869267583-source-1189016320
2022-03-30 17:48:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-182655179-1869267583-source-1189016320 will have desired state: Ready
2022-03-30 17:48:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-182655179-1869267583-source-1189016320 will have desired state: Ready
2022-03-30 17:48:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-182655179-1869267583-source-1189016320 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 17:48:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-182655179-1869267583-source-1189016320 is in desired state: Ready
2022-03-30 17:48:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-4ad8959e-my-user-source in namespace namespace-8
2022-03-30 17:48:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-30 17:48:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-cluster-4ad8959e-my-user-source
2022-03-30 17:48:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-4ad8959e-my-user-source will have desired state: Ready
2022-03-30 17:48:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-cluster-4ad8959e-my-user-source will have desired state: Ready
2022-03-30 17:48:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaUser: my-cluster-4ad8959e-my-user-source will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 17:48:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-4ad8959e-my-user-source is in desired state: Ready
2022-03-30 17:48:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-4ad8959e-my-user-target in namespace namespace-8
2022-03-30 17:48:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-30 17:48:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-cluster-4ad8959e-my-user-target
2022-03-30 17:48:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-4ad8959e-my-user-target will have desired state: Ready
2022-03-30 17:48:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-cluster-4ad8959e-my-user-target will have desired state: Ready
2022-03-30 17:48:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaUser: my-cluster-4ad8959e-my-user-target will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 17:48:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-4ad8959e-my-user-target is in desired state: Ready
2022-03-30 17:48:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-4ad8959e-kafka-clients in namespace namespace-8
2022-03-30 17:48:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-30 17:48:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-4ad8959e-kafka-clients
2022-03-30 17:48:54 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-4ad8959e-kafka-clients will be ready
2022-03-30 17:48:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-4ad8959e-kafka-clients will be ready
2022-03-30 17:48:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-4ad8959e-kafka-clients will be ready not ready, will try again in 1000 ms (479995ms till timeout)
2022-03-30 17:48:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-4ad8959e-kafka-clients will be ready not ready, will try again in 1000 ms (478992ms till timeout)
2022-03-30 17:48:56 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-4ad8959e-kafka-clients is ready
2022-03-30 17:48:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for pod with prefixmy-cluster-4ad8959e-kafka-clients is present.
2022-03-30 17:48:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1936321612-505632869-test-1 in namespace namespace-8
2022-03-30 17:48:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-30 17:48:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1936321612-505632869-test-1
2022-03-30 17:48:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1936321612-505632869-test-1 will have desired state: Ready
2022-03-30 17:48:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1936321612-505632869-test-1 will have desired state: Ready
2022-03-30 17:48:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1936321612-505632869-test-1 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 17:48:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1936321612-505632869-test-1 is in desired state: Ready
2022-03-30 17:48:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1936321612-505632869-test-2 in namespace namespace-8
2022-03-30 17:48:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-30 17:48:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1936321612-505632869-test-2
2022-03-30 17:48:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1936321612-505632869-test-2 will have desired state: Ready
2022-03-30 17:48:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1936321612-505632869-test-2 will have desired state: Ready
2022-03-30 17:48:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1936321612-505632869-test-2 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 17:48:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1936321612-505632869-test-2 is in desired state: Ready
2022-03-30 17:48:58 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-30 17:48:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Producer and consumer will successfully send and receive messages.
2022-03-30 17:48:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4d31972e, which are set.
2022-03-30 17:48:58 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@5c94cc6c, messages=[], arguments=[--topic, my-topic-1936321612-505632869-test-1, --bootstrap-server, my-cluster-4ad8959e-source-kafka-bootstrap.namespace-8.svc:9093, USER=my_cluster_4ad8959e_my_user_source, --max-messages, 200], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-4ad8959e-kafka-clients-54bd7858b6-fl2nd', podNamespace='namespace-8', bootstrapServer='my-cluster-4ad8959e-source-kafka-bootstrap.namespace-8.svc:9093', topicName='my-topic-1936321612-505632869-test-1', maxMessages=200, kafkaUsername='my-cluster-4ad8959e-my-user-source', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4d31972e}
2022-03-30 17:48:58 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-4ad8959e-source-kafka-bootstrap.namespace-8.svc:9093:my-topic-1936321612-505632869-test-1 from pod my-cluster-4ad8959e-kafka-clients-54bd7858b6-fl2nd
2022-03-30 17:48:58 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-4ad8959e-kafka-clients-54bd7858b6-fl2nd -n namespace-8 -- /opt/kafka/producer.sh --topic my-topic-1936321612-505632869-test-1 --bootstrap-server my-cluster-4ad8959e-source-kafka-bootstrap.namespace-8.svc:9093 USER=my_cluster_4ad8959e_my_user_source --max-messages 200
2022-03-30 17:48:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-4ad8959e-kafka-clients-54bd7858b6-fl2nd -n namespace-8 -- /opt/kafka/producer.sh --topic my-topic-1936321612-505632869-test-1 --bootstrap-server my-cluster-4ad8959e-source-kafka-bootstrap.namespace-8.svc:9093 USER=my_cluster_4ad8959e_my_user_source --max-messages 200
2022-03-30 17:49:02 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-30 17:49:02 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-03-30 17:49:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@236d20f4, which are set.
2022-03-30 17:49:02 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@62244b50, messages=[], arguments=[--group-id, my-consumer-group-1697138345, --topic, my-topic-1936321612-505632869-test-1, --bootstrap-server, my-cluster-4ad8959e-source-kafka-bootstrap.namespace-8.svc:9093, USER=my_cluster_4ad8959e_my_user_source, --max-messages, 200, --group-instance-id, instance2145524832], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-4ad8959e-kafka-clients-54bd7858b6-fl2nd', podNamespace='namespace-8', bootstrapServer='my-cluster-4ad8959e-source-kafka-bootstrap.namespace-8.svc:9093', topicName='my-topic-1936321612-505632869-test-1', maxMessages=200, kafkaUsername='my-cluster-4ad8959e-my-user-source', consumerGroupName='my-consumer-group-1697138345', consumerInstanceId='instance2145524832', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@236d20f4}
2022-03-30 17:49:02 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-4ad8959e-source-kafka-bootstrap.namespace-8.svc:9093:my-topic-1936321612-505632869-test-1 from pod my-cluster-4ad8959e-kafka-clients-54bd7858b6-fl2nd
2022-03-30 17:49:02 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-4ad8959e-kafka-clients-54bd7858b6-fl2nd -n namespace-8 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1697138345 --topic my-topic-1936321612-505632869-test-1 --bootstrap-server my-cluster-4ad8959e-source-kafka-bootstrap.namespace-8.svc:9093 USER=my_cluster_4ad8959e_my_user_source --max-messages 200 --group-instance-id instance2145524832
2022-03-30 17:49:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-4ad8959e-kafka-clients-54bd7858b6-fl2nd -n namespace-8 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1697138345 --topic my-topic-1936321612-505632869-test-1 --bootstrap-server my-cluster-4ad8959e-source-kafka-bootstrap.namespace-8.svc:9093 USER=my_cluster_4ad8959e_my_user_source --max-messages 200 --group-instance-id instance2145524832
2022-03-30 17:49:09 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 17:49:09 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-03-30 17:49:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Producer and consumer will successfully send and receive messages.
2022-03-30 17:49:09 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@18a90cd4, which are set.
2022-03-30 17:49:09 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@679f1551, messages=[], arguments=[--topic, my-topic-1936321612-505632869-test-2, --bootstrap-server, my-cluster-4ad8959e-target-kafka-bootstrap.namespace-8.svc:9093, USER=my_cluster_4ad8959e_my_user_target, --max-messages, 200], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-4ad8959e-kafka-clients-54bd7858b6-fl2nd', podNamespace='namespace-8', bootstrapServer='my-cluster-4ad8959e-target-kafka-bootstrap.namespace-8.svc:9093', topicName='my-topic-1936321612-505632869-test-2', maxMessages=200, kafkaUsername='my-cluster-4ad8959e-my-user-target', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@18a90cd4}
2022-03-30 17:49:09 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-4ad8959e-target-kafka-bootstrap.namespace-8.svc:9093:my-topic-1936321612-505632869-test-2 from pod my-cluster-4ad8959e-kafka-clients-54bd7858b6-fl2nd
2022-03-30 17:49:09 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-4ad8959e-kafka-clients-54bd7858b6-fl2nd -n namespace-8 -- /opt/kafka/producer.sh --topic my-topic-1936321612-505632869-test-2 --bootstrap-server my-cluster-4ad8959e-target-kafka-bootstrap.namespace-8.svc:9093 USER=my_cluster_4ad8959e_my_user_target --max-messages 200
2022-03-30 17:49:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-4ad8959e-kafka-clients-54bd7858b6-fl2nd -n namespace-8 -- /opt/kafka/producer.sh --topic my-topic-1936321612-505632869-test-2 --bootstrap-server my-cluster-4ad8959e-target-kafka-bootstrap.namespace-8.svc:9093 USER=my_cluster_4ad8959e_my_user_target --max-messages 200
2022-03-30 17:49:12 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-30 17:49:12 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-03-30 17:49:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2dbff3d6, which are set.
2022-03-30 17:49:12 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@6d0143fa, messages=[], arguments=[--group-id, my-consumer-group-1164493055, --topic, my-topic-1936321612-505632869-test-2, --bootstrap-server, my-cluster-4ad8959e-target-kafka-bootstrap.namespace-8.svc:9093, USER=my_cluster_4ad8959e_my_user_target, --max-messages, 200, --group-instance-id, instance1833946200], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-4ad8959e-kafka-clients-54bd7858b6-fl2nd', podNamespace='namespace-8', bootstrapServer='my-cluster-4ad8959e-target-kafka-bootstrap.namespace-8.svc:9093', topicName='my-topic-1936321612-505632869-test-2', maxMessages=200, kafkaUsername='my-cluster-4ad8959e-my-user-target', consumerGroupName='my-consumer-group-1164493055', consumerInstanceId='instance1833946200', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2dbff3d6}
2022-03-30 17:49:12 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-4ad8959e-target-kafka-bootstrap.namespace-8.svc:9093:my-topic-1936321612-505632869-test-2 from pod my-cluster-4ad8959e-kafka-clients-54bd7858b6-fl2nd
2022-03-30 17:49:12 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-4ad8959e-kafka-clients-54bd7858b6-fl2nd -n namespace-8 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1164493055 --topic my-topic-1936321612-505632869-test-2 --bootstrap-server my-cluster-4ad8959e-target-kafka-bootstrap.namespace-8.svc:9093 USER=my_cluster_4ad8959e_my_user_target --max-messages 200 --group-instance-id instance1833946200
2022-03-30 17:49:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-4ad8959e-kafka-clients-54bd7858b6-fl2nd -n namespace-8 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1164493055 --topic my-topic-1936321612-505632869-test-2 --bootstrap-server my-cluster-4ad8959e-target-kafka-bootstrap.namespace-8.svc:9093 USER=my_cluster_4ad8959e_my_user_target --max-messages 200 --group-instance-id instance1833946200
2022-03-30 17:49:19 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 17:49:19 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-03-30 17:49:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker my-cluster-4ad8959e in namespace namespace-8
2022-03-30 17:49:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-30 17:49:19 [ForkJoinPool-3-worker-3] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkamirrormakers' with unstable version 'v1beta2'
2022-03-30 17:49:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaMirrorMaker:my-cluster-4ad8959e
2022-03-30 17:49:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready
2022-03-30 17:49:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready
2022-03-30 17:49:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (479998ms till timeout)
2022-03-30 17:49:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (478995ms till timeout)
2022-03-30 17:49:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (477991ms till timeout)
2022-03-30 17:49:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (476988ms till timeout)
2022-03-30 17:49:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (475984ms till timeout)
2022-03-30 17:49:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (474980ms till timeout)
2022-03-30 17:49:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (473977ms till timeout)
2022-03-30 17:49:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (472974ms till timeout)
2022-03-30 17:49:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (471970ms till timeout)
2022-03-30 17:49:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (470967ms till timeout)
2022-03-30 17:49:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (469964ms till timeout)
2022-03-30 17:49:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (468958ms till timeout)
2022-03-30 17:49:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (467954ms till timeout)
2022-03-30 17:49:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (466951ms till timeout)
2022-03-30 17:49:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (465948ms till timeout)
2022-03-30 17:49:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (464944ms till timeout)
2022-03-30 17:49:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (463941ms till timeout)
2022-03-30 17:49:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (462937ms till timeout)
2022-03-30 17:49:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (461934ms till timeout)
2022-03-30 17:49:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (460931ms till timeout)
2022-03-30 17:49:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (459928ms till timeout)
2022-03-30 17:49:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (458924ms till timeout)
2022-03-30 17:49:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (457921ms till timeout)
2022-03-30 17:49:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (456918ms till timeout)
2022-03-30 17:49:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (455914ms till timeout)
2022-03-30 17:49:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (454911ms till timeout)
2022-03-30 17:49:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (453907ms till timeout)
2022-03-30 17:49:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (452904ms till timeout)
2022-03-30 17:49:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (451901ms till timeout)
2022-03-30 17:49:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (450898ms till timeout)
2022-03-30 17:49:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (449895ms till timeout)
2022-03-30 17:49:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (448891ms till timeout)
2022-03-30 17:49:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (447888ms till timeout)
2022-03-30 17:49:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (446885ms till timeout)
2022-03-30 17:49:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (445881ms till timeout)
2022-03-30 17:49:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (444878ms till timeout)
2022-03-30 17:49:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (443875ms till timeout)
2022-03-30 17:49:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (442871ms till timeout)
2022-03-30 17:49:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (441868ms till timeout)
2022-03-30 17:49:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (440865ms till timeout)
2022-03-30 17:49:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (439862ms till timeout)
2022-03-30 17:50:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (438858ms till timeout)
2022-03-30 17:50:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (437855ms till timeout)
2022-03-30 17:50:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (436851ms till timeout)
2022-03-30 17:50:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (435848ms till timeout)
2022-03-30 17:50:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (434845ms till timeout)
2022-03-30 17:50:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (433842ms till timeout)
2022-03-30 17:50:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (432839ms till timeout)
2022-03-30 17:50:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (431835ms till timeout)
2022-03-30 17:50:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (430832ms till timeout)
2022-03-30 17:50:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (429829ms till timeout)
2022-03-30 17:50:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (428825ms till timeout)
2022-03-30 17:50:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (427822ms till timeout)
2022-03-30 17:50:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (426819ms till timeout)
2022-03-30 17:50:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (425815ms till timeout)
2022-03-30 17:50:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (424812ms till timeout)
2022-03-30 17:50:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (423809ms till timeout)
2022-03-30 17:50:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (422805ms till timeout)
2022-03-30 17:50:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (421802ms till timeout)
2022-03-30 17:50:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (420799ms till timeout)
2022-03-30 17:50:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (419795ms till timeout)
2022-03-30 17:50:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (418792ms till timeout)
2022-03-30 17:50:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (417789ms till timeout)
2022-03-30 17:50:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (416786ms till timeout)
2022-03-30 17:50:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (415782ms till timeout)
2022-03-30 17:50:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (414779ms till timeout)
2022-03-30 17:50:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (413775ms till timeout)
2022-03-30 17:50:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-4ad8959e will have desired state: Ready not ready, will try again in 1000 ms (412772ms till timeout)
2022-03-30 17:50:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-4ad8959e is in desired state: Ready
2022-03-30 17:50:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Producer and consumer will successfully send and receive messages.
2022-03-30 17:50:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3806cb17, which are set.
2022-03-30 17:50:27 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@6cc9a9d3, messages=[], arguments=[--topic, my-topic-182655179-1869267583-source-1189016320, --bootstrap-server, my-cluster-4ad8959e-source-kafka-bootstrap.namespace-8.svc:9093, USER=my_cluster_4ad8959e_my_user_source, --max-messages, 200], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-4ad8959e-kafka-clients-54bd7858b6-fl2nd', podNamespace='namespace-8', bootstrapServer='my-cluster-4ad8959e-source-kafka-bootstrap.namespace-8.svc:9093', topicName='my-topic-182655179-1869267583-source-1189016320', maxMessages=200, kafkaUsername='my-cluster-4ad8959e-my-user-source', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3806cb17}
2022-03-30 17:50:27 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-4ad8959e-source-kafka-bootstrap.namespace-8.svc:9093:my-topic-182655179-1869267583-source-1189016320 from pod my-cluster-4ad8959e-kafka-clients-54bd7858b6-fl2nd
2022-03-30 17:50:27 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-4ad8959e-kafka-clients-54bd7858b6-fl2nd -n namespace-8 -- /opt/kafka/producer.sh --topic my-topic-182655179-1869267583-source-1189016320 --bootstrap-server my-cluster-4ad8959e-source-kafka-bootstrap.namespace-8.svc:9093 USER=my_cluster_4ad8959e_my_user_source --max-messages 200
2022-03-30 17:50:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-4ad8959e-kafka-clients-54bd7858b6-fl2nd -n namespace-8 -- /opt/kafka/producer.sh --topic my-topic-182655179-1869267583-source-1189016320 --bootstrap-server my-cluster-4ad8959e-source-kafka-bootstrap.namespace-8.svc:9093 USER=my_cluster_4ad8959e_my_user_source --max-messages 200
2022-03-30 17:50:31 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-30 17:50:31 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-03-30 17:50:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@156fa2ad, which are set.
2022-03-30 17:50:31 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@1b3f8d64, messages=[], arguments=[--group-id, my-consumer-group-1522495002, --topic, my-topic-182655179-1869267583-source-1189016320, --bootstrap-server, my-cluster-4ad8959e-source-kafka-bootstrap.namespace-8.svc:9093, USER=my_cluster_4ad8959e_my_user_source, --max-messages, 200, --group-instance-id, instance674893483], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-4ad8959e-kafka-clients-54bd7858b6-fl2nd', podNamespace='namespace-8', bootstrapServer='my-cluster-4ad8959e-source-kafka-bootstrap.namespace-8.svc:9093', topicName='my-topic-182655179-1869267583-source-1189016320', maxMessages=200, kafkaUsername='my-cluster-4ad8959e-my-user-source', consumerGroupName='my-consumer-group-1522495002', consumerInstanceId='instance674893483', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@156fa2ad}
2022-03-30 17:50:31 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-4ad8959e-source-kafka-bootstrap.namespace-8.svc:9093:my-topic-182655179-1869267583-source-1189016320 from pod my-cluster-4ad8959e-kafka-clients-54bd7858b6-fl2nd
2022-03-30 17:50:31 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-4ad8959e-kafka-clients-54bd7858b6-fl2nd -n namespace-8 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1522495002 --topic my-topic-182655179-1869267583-source-1189016320 --bootstrap-server my-cluster-4ad8959e-source-kafka-bootstrap.namespace-8.svc:9093 USER=my_cluster_4ad8959e_my_user_source --max-messages 200 --group-instance-id instance674893483
2022-03-30 17:50:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-4ad8959e-kafka-clients-54bd7858b6-fl2nd -n namespace-8 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1522495002 --topic my-topic-182655179-1869267583-source-1189016320 --bootstrap-server my-cluster-4ad8959e-source-kafka-bootstrap.namespace-8.svc:9093 USER=my_cluster_4ad8959e_my_user_source --max-messages 200 --group-instance-id instance674893483
2022-03-30 17:50:38 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 17:50:38 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-03-30 17:50:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Consumer will successfully receive messages.
2022-03-30 17:50:38 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3a1938e1, which are set.
2022-03-30 17:50:38 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@23c6eafb, messages=[], arguments=[--group-id, my-consumer-group-595703764, --topic, my-topic-182655179-1869267583-source-1189016320, --bootstrap-server, my-cluster-4ad8959e-target-kafka-bootstrap.namespace-8.svc:9093, USER=my_cluster_4ad8959e_my_user_target, --max-messages, 200, --group-instance-id, instance193354156], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-4ad8959e-kafka-clients-54bd7858b6-fl2nd', podNamespace='namespace-8', bootstrapServer='my-cluster-4ad8959e-target-kafka-bootstrap.namespace-8.svc:9093', topicName='my-topic-182655179-1869267583-source-1189016320', maxMessages=200, kafkaUsername='my-cluster-4ad8959e-my-user-target', consumerGroupName='my-consumer-group-595703764', consumerInstanceId='instance193354156', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3a1938e1}
2022-03-30 17:50:38 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-4ad8959e-target-kafka-bootstrap.namespace-8.svc:9093:my-topic-182655179-1869267583-source-1189016320 from pod my-cluster-4ad8959e-kafka-clients-54bd7858b6-fl2nd
2022-03-30 17:50:38 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-4ad8959e-kafka-clients-54bd7858b6-fl2nd -n namespace-8 -- /opt/kafka/consumer.sh --group-id my-consumer-group-595703764 --topic my-topic-182655179-1869267583-source-1189016320 --bootstrap-server my-cluster-4ad8959e-target-kafka-bootstrap.namespace-8.svc:9093 USER=my_cluster_4ad8959e_my_user_target --max-messages 200 --group-instance-id instance193354156
2022-03-30 17:50:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-4ad8959e-kafka-clients-54bd7858b6-fl2nd -n namespace-8 -- /opt/kafka/consumer.sh --group-id my-consumer-group-595703764 --topic my-topic-182655179-1869267583-source-1189016320 --bootstrap-server my-cluster-4ad8959e-target-kafka-bootstrap.namespace-8.svc:9093 USER=my_cluster_4ad8959e_my_user_target --max-messages 200 --group-instance-id instance193354156
2022-03-30 17:50:45 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 17:50:45 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-03-30 17:50:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 17:50:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:675] [mirrormaker.MirrorMakerIsolatedST - After Each] - Clean up after test
2022-03-30 17:50:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 17:50:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testMirrorMakerTlsAuthenticated
2022-03-30 17:50:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-4ad8959e-kafka-clients in namespace namespace-8
2022-03-30 17:50:45 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-182655179-1869267583-source-1189016320 in namespace namespace-8
2022-03-30 17:50:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-4ad8959e-kafka-clients
2022-03-30 17:50:45 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-182655179-1869267583-source-1189016320
2022-03-30 17:50:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-182655179-1869267583-source-1189016320 not ready, will try again in 10000 ms (179990ms till timeout)
2022-03-30 17:50:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-4ad8959e-kafka-clients not ready, will try again in 10000 ms (479984ms till timeout)
2022-03-30 17:50:55 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-4ad8959e-my-user-source in namespace namespace-8
2022-03-30 17:50:55 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-cluster-4ad8959e-my-user-source
2022-03-30 17:50:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-4ad8959e-kafka-clients not ready, will try again in 10000 ms (469974ms till timeout)
2022-03-30 17:50:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-cluster-4ad8959e-my-user-source not ready, will try again in 10000 ms (179994ms till timeout)
2022-03-30 17:51:05 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-4ad8959e-target in namespace namespace-8
2022-03-30 17:51:05 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-4ad8959e-target
2022-03-30 17:51:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-4ad8959e-kafka-clients not ready, will try again in 10000 ms (459961ms till timeout)
2022-03-30 17:51:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-4ad8959e-target not ready, will try again in 10000 ms (839989ms till timeout)
2022-03-30 17:51:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-4ad8959e-source in namespace namespace-8
2022-03-30 17:51:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-4ad8959e-kafka-clients not ready, will try again in 10000 ms (449951ms till timeout)
2022-03-30 17:51:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-4ad8959e-source
2022-03-30 17:51:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-4ad8959e-source not ready, will try again in 10000 ms (839957ms till timeout)
2022-03-30 17:51:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-4ad8959e-my-user-target in namespace namespace-8
2022-03-30 17:51:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-cluster-4ad8959e-my-user-target
2022-03-30 17:51:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-cluster-4ad8959e-my-user-target not ready, will try again in 10000 ms (179994ms till timeout)
2022-03-30 17:51:25 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1936321612-505632869-test-2 in namespace namespace-8
2022-03-30 17:51:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1936321612-505632869-test-2
2022-03-30 17:51:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1936321612-505632869-test-2 not ready, will try again in 10000 ms (179994ms till timeout)
2022-03-30 17:51:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1936321612-505632869-test-1 in namespace namespace-8
2022-03-30 17:51:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1936321612-505632869-test-1
2022-03-30 17:51:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1936321612-505632869-test-1 not ready, will try again in 10000 ms (179994ms till timeout)
2022-03-30 17:51:35 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker my-cluster-4ad8959e in namespace namespace-8
2022-03-30 17:51:35 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaMirrorMaker:my-cluster-4ad8959e
2022-03-30 17:51:35 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaMirrorMaker:my-cluster-4ad8959e not ready, will try again in 10000 ms (479992ms till timeout)
2022-03-30 17:51:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 17:51:45 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-8 for test case:testMirrorMakerTlsAuthenticated
2022-03-30 17:51:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-8 removal
2022-03-30 17:51:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:51:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:51:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:51:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (479928ms till timeout)
2022-03-30 17:51:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:51:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:51:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:51:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (478852ms till timeout)
2022-03-30 17:51:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:51:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:51:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:51:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (477771ms till timeout)
2022-03-30 17:51:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:51:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:51:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:51:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (476696ms till timeout)
2022-03-30 17:51:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:51:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:51:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:51:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (475629ms till timeout)
2022-03-30 17:51:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:51:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:51:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:51:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (474547ms till timeout)
2022-03-30 17:51:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:51:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:51:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:51:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (473472ms till timeout)
2022-03-30 17:51:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:51:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:51:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:51:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (472398ms till timeout)
2022-03-30 17:51:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:51:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:51:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:51:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (471320ms till timeout)
2022-03-30 17:51:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:51:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:51:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:51:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (470248ms till timeout)
2022-03-30 17:51:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:51:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:51:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:51:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (469175ms till timeout)
2022-03-30 17:51:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:51:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:51:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:51:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (468103ms till timeout)
2022-03-30 17:51:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:51:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:51:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:51:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (467027ms till timeout)
2022-03-30 17:51:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:51:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:51:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:51:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (465950ms till timeout)
2022-03-30 17:52:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:52:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:52:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:52:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (464873ms till timeout)
2022-03-30 17:52:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:52:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:52:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:52:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (463797ms till timeout)
2022-03-30 17:52:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:52:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:52:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:52:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (462725ms till timeout)
2022-03-30 17:52:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:52:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:52:03 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:52:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (461655ms till timeout)
2022-03-30 17:52:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:52:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:52:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:52:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (460578ms till timeout)
2022-03-30 17:52:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:52:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:52:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:52:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (459506ms till timeout)
2022-03-30 17:52:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:52:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:52:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:52:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (458428ms till timeout)
2022-03-30 17:52:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:52:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:52:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:52:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (457353ms till timeout)
2022-03-30 17:52:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:52:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:52:08 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:52:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (456286ms till timeout)
2022-03-30 17:52:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:52:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:52:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:52:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (455210ms till timeout)
2022-03-30 17:52:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:52:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:52:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:52:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (454133ms till timeout)
2022-03-30 17:52:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:52:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 17:52:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 17:52:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 17:52:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-8" not found
2022-03-30 17:52:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 17:52:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@6a42f58d=[]}
2022-03-30 17:52:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:267] testMirrorMakerTlsAuthenticated - Notifies waiting test cases:[testUpdateUser, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testCruiseControlBasicAPIRequests, testSendMessagesTlsScramSha, testSendMessagesCustomListenerTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMaker2TlsAndTlsClientAuth, testMirrorMakerTlsAuthenticated] to and randomly select one to start execution
2022-03-30 17:52:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:93] [mirrormaker.MirrorMakerIsolatedST] - Removing parallel test: testMirrorMakerTlsAuthenticated
2022-03-30 17:52:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:97] [mirrormaker.MirrorMakerIsolatedST] - Parallel test count: 0
2022-03-30 17:52:12 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST.testMirrorMakerTlsAuthenticated-FINISHED
2022-03-30 17:52:12 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 17:52:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 17:52:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:690] [mirrormaker.MirrorMakerIsolatedST - After All] - Clean up after test suite
2022-03-30 17:52:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 17:52:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context MirrorMakerIsolatedST is everything deleted.
2022-03-30 17:52:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 462.727 s - in io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.metrics.MetricsIsolatedST
2022-03-30 17:52:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 17:52:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:667] [metrics.MetricsIsolatedST - Before All] - Setup test suite environment
2022-03-30 17:52:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 17:52:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 0
2022-03-30 17:52:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 1
2022-03-30 17:52:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 0
2022-03-30 17:52:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 2
2022-03-30 17:52:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 0
2022-03-30 17:52:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 3
2022-03-30 17:52:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 0
2022-03-30 17:52:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 4
2022-03-30 17:52:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 0
2022-03-30 17:52:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 5
2022-03-30 17:52:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:136] Suite metrics.MetricsIsolatedST has locked the @IsolatedSuite and other @IsolatedSuites must wait until lock is released.
2022-03-30 17:52:37 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-30 17:52:37 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-30 17:52:37 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-30 17:52:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 17:52:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-30 17:52:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 17:52:37 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 17:52:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 17:52:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:52:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 17:52:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 17:52:37 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 17:52:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 17:52:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 17:52:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 17:52:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io not ready, will try again in 10000 ms (179969ms till timeout)
2022-03-30 17:52:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179832ms till timeout)
2022-03-30 17:52:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 17:52:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 17:52:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 17:52:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 17:52:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179948ms till timeout)
2022-03-30 17:52:47 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 17:52:47 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 17:52:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179948ms till timeout)
2022-03-30 17:52:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 17:52:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 17:52:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 17:52:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 17:52:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 17:52:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 17:52:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:52:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 17:52:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 17:52:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator not ready, will try again in 10000 ms (479953ms till timeout)
2022-03-30 17:52:57 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 17:52:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 17:52:57 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 17:52:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 17:52:57 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 17:52:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io not ready, will try again in 10000 ms (179990ms till timeout)
2022-03-30 17:53:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 17:53:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 17:53:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 17:53:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 17:53:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:53:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 17:53:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 17:53:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 17:53:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 17:53:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 17:53:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 17:53:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 17:53:07 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:53:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 17:53:07 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 17:53:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 17:53:07 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 17:53:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 17:53:07 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 17:53:07 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 17:53:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 17:53:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-30 17:53:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 17:53:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 17:53:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v62884
2022-03-30 17:53:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v62884
2022-03-30 17:53:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=62884&allowWatchBookmarks=true&watch=true...
2022-03-30 17:53:07 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 17:53:07 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 62885
2022-03-30 17:53:12 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 62900
2022-03-30 17:53:12 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 62901
2022-03-30 17:53:12 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v62900 in namespace default
2022-03-30 17:53:12 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@6bba30e4
2022-03-30 17:53:12 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 17:53:12 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@6f85b94b
2022-03-30 17:53:12 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@6f85b94b
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5e99e8d5
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace,second-metrics-cluster-test
bindingsNamespaces=[infra-namespace, second-metrics-cluster-test]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-30 17:53:12 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@6f85b94b
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@cbb87a8, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5e99e8d5, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='infra-namespace,second-metrics-cluster-test', bindingsNamespaces=[infra-namespace, second-metrics-cluster-test], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-30 17:53:12 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 17:53:12 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T17:53:12Z",
        "name": "infra-namespace",
        "resourceVersion": "62902",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "5bbd62dd-7298-415b-96c9-752c461186e2"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: second-metrics-cluster-test
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace second-metrics-cluster-test
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace second-metrics-cluster-test -o json
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace second-metrics-cluster-test -o json
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T17:53:12Z",
        "name": "second-metrics-cluster-test",
        "resourceVersion": "62906",
        "selfLink": "/api/v1/namespaces/second-metrics-cluster-test",
        "uid": "46b48a88-1171-4583-acff-ae1ee8831155"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace, second-metrics-cluster-test]}
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:53:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=second-metrics-cluster-test, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: second-metrics-cluster-test
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace second-metrics-cluster-test
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace second-metrics-cluster-test
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 17:53:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479997ms till timeout)
2022-03-30 17:53:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478994ms till timeout)
2022-03-30 17:53:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477991ms till timeout)
2022-03-30 17:53:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476988ms till timeout)
2022-03-30 17:53:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475985ms till timeout)
2022-03-30 17:53:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474982ms till timeout)
2022-03-30 17:53:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473979ms till timeout)
2022-03-30 17:53:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472975ms till timeout)
2022-03-30 17:53:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471972ms till timeout)
2022-03-30 17:53:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470970ms till timeout)
2022-03-30 17:53:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469966ms till timeout)
2022-03-30 17:53:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468964ms till timeout)
2022-03-30 17:53:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467961ms till timeout)
2022-03-30 17:53:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466958ms till timeout)
2022-03-30 17:53:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465955ms till timeout)
2022-03-30 17:53:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464952ms till timeout)
2022-03-30 17:53:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463949ms till timeout)
2022-03-30 17:53:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462946ms till timeout)
2022-03-30 17:53:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461943ms till timeout)
2022-03-30 17:53:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (460940ms till timeout)
2022-03-30 17:53:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (459937ms till timeout)
2022-03-30 17:53:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (458934ms till timeout)
2022-03-30 17:53:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (457931ms till timeout)
2022-03-30 17:53:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (456929ms till timeout)
2022-03-30 17:53:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (455926ms till timeout)
2022-03-30 17:53:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (454923ms till timeout)
2022-03-30 17:53:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (453920ms till timeout)
2022-03-30 17:53:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (452916ms till timeout)
2022-03-30 17:53:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (451913ms till timeout)
2022-03-30 17:53:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (450910ms till timeout)
2022-03-30 17:53:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (449907ms till timeout)
2022-03-30 17:53:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (448905ms till timeout)
2022-03-30 17:53:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (447902ms till timeout)
2022-03-30 17:53:46 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-30 17:53:46 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-30 17:53:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-30 17:53:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-pp6rn not ready: strimzi-cluster-operator)
2022-03-30 17:53:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-pp6rn are ready
2022-03-30 17:53:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 17:53:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-pp6rn not ready: strimzi-cluster-operator)
2022-03-30 17:53:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-pp6rn are ready
2022-03-30 17:53:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-30 17:53:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-pp6rn not ready: strimzi-cluster-operator)
2022-03-30 17:53:48 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-pp6rn are ready
2022-03-30 17:53:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597991ms till timeout)
2022-03-30 17:53:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-pp6rn not ready: strimzi-cluster-operator)
2022-03-30 17:53:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-pp6rn are ready
2022-03-30 17:53:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596987ms till timeout)
2022-03-30 17:53:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-pp6rn not ready: strimzi-cluster-operator)
2022-03-30 17:53:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-pp6rn are ready
2022-03-30 17:53:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595983ms till timeout)
2022-03-30 17:53:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-pp6rn not ready: strimzi-cluster-operator)
2022-03-30 17:53:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-pp6rn are ready
2022-03-30 17:53:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594980ms till timeout)
2022-03-30 17:53:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-pp6rn not ready: strimzi-cluster-operator)
2022-03-30 17:53:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-pp6rn are ready
2022-03-30 17:53:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593976ms till timeout)
2022-03-30 17:53:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-pp6rn not ready: strimzi-cluster-operator)
2022-03-30 17:53:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-pp6rn are ready
2022-03-30 17:53:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592973ms till timeout)
2022-03-30 17:53:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-pp6rn not ready: strimzi-cluster-operator)
2022-03-30 17:53:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-pp6rn are ready
2022-03-30 17:53:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591969ms till timeout)
2022-03-30 17:53:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-pp6rn not ready: strimzi-cluster-operator)
2022-03-30 17:53:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-pp6rn are ready
2022-03-30 17:53:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590966ms till timeout)
2022-03-30 17:53:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-pp6rn not ready: strimzi-cluster-operator)
2022-03-30 17:53:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-pp6rn are ready
2022-03-30 17:53:56 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-30 17:53:56 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 17:53:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka metrics-cluster-name in namespace infra-namespace
2022-03-30 17:53:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka second-kafka-cluster in namespace second-metrics-cluster-test
2022-03-30 17:53:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment infra-namespace-kafka-clients in namespace infra-namespace
2022-03-30 17:53:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment second-metrics-cluster-test-kafka-clients in namespace second-metrics-cluster-test
2022-03-30 17:53:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:metrics-cluster-name
2022-03-30 17:53:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: metrics-cluster-name will have desired state: Ready
2022-03-30 17:53:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: metrics-cluster-name will have desired state: Ready
2022-03-30 17:53:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1799992ms till timeout)
2022-03-30 17:53:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1798989ms till timeout)
2022-03-30 17:53:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1797985ms till timeout)
2022-03-30 17:53:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1796982ms till timeout)
2022-03-30 17:54:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1795978ms till timeout)
2022-03-30 17:54:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1794975ms till timeout)
2022-03-30 17:54:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1793972ms till timeout)
2022-03-30 17:54:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1792968ms till timeout)
2022-03-30 17:54:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1791964ms till timeout)
2022-03-30 17:54:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1790960ms till timeout)
2022-03-30 17:54:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1789957ms till timeout)
2022-03-30 17:54:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1788954ms till timeout)
2022-03-30 17:54:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1787950ms till timeout)
2022-03-30 17:54:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1786946ms till timeout)
2022-03-30 17:54:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1785942ms till timeout)
2022-03-30 17:54:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1784938ms till timeout)
2022-03-30 17:54:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1783935ms till timeout)
2022-03-30 17:54:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1782932ms till timeout)
2022-03-30 17:54:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1781928ms till timeout)
2022-03-30 17:54:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1780925ms till timeout)
2022-03-30 17:54:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1779921ms till timeout)
2022-03-30 17:54:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1778918ms till timeout)
2022-03-30 17:54:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1777915ms till timeout)
2022-03-30 17:54:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1776911ms till timeout)
2022-03-30 17:54:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1775908ms till timeout)
2022-03-30 17:54:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1774905ms till timeout)
2022-03-30 17:54:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1773902ms till timeout)
2022-03-30 17:54:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1772899ms till timeout)
2022-03-30 17:54:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1771896ms till timeout)
2022-03-30 17:54:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1770893ms till timeout)
2022-03-30 17:54:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1769889ms till timeout)
2022-03-30 17:54:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1768886ms till timeout)
2022-03-30 17:54:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1767883ms till timeout)
2022-03-30 17:54:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1766880ms till timeout)
2022-03-30 17:54:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1765876ms till timeout)
2022-03-30 17:54:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1764873ms till timeout)
2022-03-30 17:54:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1763870ms till timeout)
2022-03-30 17:54:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1762867ms till timeout)
2022-03-30 17:54:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1761863ms till timeout)
2022-03-30 17:54:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1760860ms till timeout)
2022-03-30 17:54:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1759856ms till timeout)
2022-03-30 17:54:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1758853ms till timeout)
2022-03-30 17:54:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1757849ms till timeout)
2022-03-30 17:54:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1756846ms till timeout)
2022-03-30 17:54:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1755842ms till timeout)
2022-03-30 17:54:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1754838ms till timeout)
2022-03-30 17:54:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1753834ms till timeout)
2022-03-30 17:54:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1752831ms till timeout)
2022-03-30 17:54:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1751828ms till timeout)
2022-03-30 17:54:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1750824ms till timeout)
2022-03-30 17:54:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1749821ms till timeout)
2022-03-30 17:54:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1748818ms till timeout)
2022-03-30 17:54:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1747815ms till timeout)
2022-03-30 17:54:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1746812ms till timeout)
2022-03-30 17:54:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1745808ms till timeout)
2022-03-30 17:54:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1744805ms till timeout)
2022-03-30 17:54:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1743802ms till timeout)
2022-03-30 17:54:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1742798ms till timeout)
2022-03-30 17:54:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1741795ms till timeout)
2022-03-30 17:54:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1740791ms till timeout)
2022-03-30 17:54:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1739788ms till timeout)
2022-03-30 17:54:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1738785ms till timeout)
2022-03-30 17:54:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1737782ms till timeout)
2022-03-30 17:54:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1736779ms till timeout)
2022-03-30 17:55:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1735775ms till timeout)
2022-03-30 17:55:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1734772ms till timeout)
2022-03-30 17:55:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1733769ms till timeout)
2022-03-30 17:55:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1732766ms till timeout)
2022-03-30 17:55:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1731762ms till timeout)
2022-03-30 17:55:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1730759ms till timeout)
2022-03-30 17:55:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1729755ms till timeout)
2022-03-30 17:55:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1728752ms till timeout)
2022-03-30 17:55:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1727748ms till timeout)
2022-03-30 17:55:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1726745ms till timeout)
2022-03-30 17:55:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1725741ms till timeout)
2022-03-30 17:55:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1724738ms till timeout)
2022-03-30 17:55:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1723734ms till timeout)
2022-03-30 17:55:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1722731ms till timeout)
2022-03-30 17:55:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1721728ms till timeout)
2022-03-30 17:55:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1720721ms till timeout)
2022-03-30 17:55:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1719718ms till timeout)
2022-03-30 17:55:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1718715ms till timeout)
2022-03-30 17:55:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1717711ms till timeout)
2022-03-30 17:55:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1716708ms till timeout)
2022-03-30 17:55:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1715704ms till timeout)
2022-03-30 17:55:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1714701ms till timeout)
2022-03-30 17:55:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1713698ms till timeout)
2022-03-30 17:55:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1712694ms till timeout)
2022-03-30 17:55:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1711691ms till timeout)
2022-03-30 17:55:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1710687ms till timeout)
2022-03-30 17:55:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1709684ms till timeout)
2022-03-30 17:55:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1708680ms till timeout)
2022-03-30 17:55:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1707677ms till timeout)
2022-03-30 17:55:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1706674ms till timeout)
2022-03-30 17:55:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1705671ms till timeout)
2022-03-30 17:55:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1704667ms till timeout)
2022-03-30 17:55:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1703663ms till timeout)
2022-03-30 17:55:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1702660ms till timeout)
2022-03-30 17:55:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1701653ms till timeout)
2022-03-30 17:55:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1700650ms till timeout)
2022-03-30 17:55:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1699645ms till timeout)
2022-03-30 17:55:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1698641ms till timeout)
2022-03-30 17:55:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1697638ms till timeout)
2022-03-30 17:55:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1696634ms till timeout)
2022-03-30 17:55:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1695631ms till timeout)
2022-03-30 17:55:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1694628ms till timeout)
2022-03-30 17:55:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1693625ms till timeout)
2022-03-30 17:55:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1692622ms till timeout)
2022-03-30 17:55:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1691618ms till timeout)
2022-03-30 17:55:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1690615ms till timeout)
2022-03-30 17:55:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1689611ms till timeout)
2022-03-30 17:55:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1688608ms till timeout)
2022-03-30 17:55:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1687604ms till timeout)
2022-03-30 17:55:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1686601ms till timeout)
2022-03-30 17:55:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1685598ms till timeout)
2022-03-30 17:55:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1684594ms till timeout)
2022-03-30 17:55:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1683591ms till timeout)
2022-03-30 17:55:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1682587ms till timeout)
2022-03-30 17:55:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1681584ms till timeout)
2022-03-30 17:55:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1680580ms till timeout)
2022-03-30 17:55:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1679577ms till timeout)
2022-03-30 17:55:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1678573ms till timeout)
2022-03-30 17:55:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1677570ms till timeout)
2022-03-30 17:56:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1676566ms till timeout)
2022-03-30 17:56:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1675563ms till timeout)
2022-03-30 17:56:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1674560ms till timeout)
2022-03-30 17:56:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1673557ms till timeout)
2022-03-30 17:56:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1672553ms till timeout)
2022-03-30 17:56:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1671550ms till timeout)
2022-03-30 17:56:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1670547ms till timeout)
2022-03-30 17:56:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1669544ms till timeout)
2022-03-30 17:56:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1668541ms till timeout)
2022-03-30 17:56:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1667538ms till timeout)
2022-03-30 17:56:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1666534ms till timeout)
2022-03-30 17:56:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1665531ms till timeout)
2022-03-30 17:56:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1664528ms till timeout)
2022-03-30 17:56:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1663524ms till timeout)
2022-03-30 17:56:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1662521ms till timeout)
2022-03-30 17:56:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1661518ms till timeout)
2022-03-30 17:56:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1660515ms till timeout)
2022-03-30 17:56:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1659511ms till timeout)
2022-03-30 17:56:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1658508ms till timeout)
2022-03-30 17:56:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1657505ms till timeout)
2022-03-30 17:56:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1656502ms till timeout)
2022-03-30 17:56:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1655499ms till timeout)
2022-03-30 17:56:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1654496ms till timeout)
2022-03-30 17:56:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1653493ms till timeout)
2022-03-30 17:56:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1652489ms till timeout)
2022-03-30 17:56:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1651486ms till timeout)
2022-03-30 17:56:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1650483ms till timeout)
2022-03-30 17:56:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1649480ms till timeout)
2022-03-30 17:56:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1648477ms till timeout)
2022-03-30 17:56:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1647474ms till timeout)
2022-03-30 17:56:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1646471ms till timeout)
2022-03-30 17:56:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1645467ms till timeout)
2022-03-30 17:56:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1644464ms till timeout)
2022-03-30 17:56:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1643461ms till timeout)
2022-03-30 17:56:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1642458ms till timeout)
2022-03-30 17:56:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1641454ms till timeout)
2022-03-30 17:56:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1640451ms till timeout)
2022-03-30 17:56:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1639448ms till timeout)
2022-03-30 17:56:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1638444ms till timeout)
2022-03-30 17:56:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1637441ms till timeout)
2022-03-30 17:56:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1636437ms till timeout)
2022-03-30 17:56:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1635433ms till timeout)
2022-03-30 17:56:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1634430ms till timeout)
2022-03-30 17:56:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1633427ms till timeout)
2022-03-30 17:56:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1632424ms till timeout)
2022-03-30 17:56:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1631420ms till timeout)
2022-03-30 17:56:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1630417ms till timeout)
2022-03-30 17:56:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1629414ms till timeout)
2022-03-30 17:56:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1628410ms till timeout)
2022-03-30 17:56:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1627407ms till timeout)
2022-03-30 17:56:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1626403ms till timeout)
2022-03-30 17:56:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1625399ms till timeout)
2022-03-30 17:56:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1624396ms till timeout)
2022-03-30 17:56:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1623393ms till timeout)
2022-03-30 17:56:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1622390ms till timeout)
2022-03-30 17:56:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1621386ms till timeout)
2022-03-30 17:56:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1620383ms till timeout)
2022-03-30 17:56:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1619380ms till timeout)
2022-03-30 17:56:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1618376ms till timeout)
2022-03-30 17:56:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1617373ms till timeout)
2022-03-30 17:57:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1616369ms till timeout)
2022-03-30 17:57:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1615366ms till timeout)
2022-03-30 17:57:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1614362ms till timeout)
2022-03-30 17:57:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1613359ms till timeout)
2022-03-30 17:57:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1612356ms till timeout)
2022-03-30 17:57:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1611352ms till timeout)
2022-03-30 17:57:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1610349ms till timeout)
2022-03-30 17:57:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1609345ms till timeout)
2022-03-30 17:57:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1608342ms till timeout)
2022-03-30 17:57:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1607338ms till timeout)
2022-03-30 17:57:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1606335ms till timeout)
2022-03-30 17:57:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1605331ms till timeout)
2022-03-30 17:57:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1604328ms till timeout)
2022-03-30 17:57:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1603325ms till timeout)
2022-03-30 17:57:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1602322ms till timeout)
2022-03-30 17:57:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1601318ms till timeout)
2022-03-30 17:57:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1600315ms till timeout)
2022-03-30 17:57:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1599312ms till timeout)
2022-03-30 17:57:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: metrics-cluster-name is in desired state: Ready
2022-03-30 17:57:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:second-kafka-cluster
2022-03-30 17:57:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: second-kafka-cluster will have desired state: Ready
2022-03-30 17:57:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: second-kafka-cluster will have desired state: Ready
2022-03-30 17:57:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: second-kafka-cluster is in desired state: Ready
2022-03-30 17:57:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:infra-namespace-kafka-clients
2022-03-30 17:57:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: infra-namespace-kafka-clients will be ready
2022-03-30 17:57:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: infra-namespace-kafka-clients will be ready
2022-03-30 17:57:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: infra-namespace-kafka-clients is ready
2022-03-30 17:57:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:second-metrics-cluster-test-kafka-clients
2022-03-30 17:57:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: second-metrics-cluster-test-kafka-clients will be ready
2022-03-30 17:57:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: second-metrics-cluster-test-kafka-clients will be ready
2022-03-30 17:57:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: second-metrics-cluster-test-kafka-clients is ready
2022-03-30 17:57:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge my-bridge in namespace infra-namespace
2022-03-30 17:57:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaBridge:my-bridge
2022-03-30 17:57:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: my-bridge will have desired state: Ready
2022-03-30 17:57:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaBridge: my-bridge will have desired state: Ready
2022-03-30 17:57:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (479999ms till timeout)
2022-03-30 17:57:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (478996ms till timeout)
2022-03-30 17:57:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (477993ms till timeout)
2022-03-30 17:57:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (476990ms till timeout)
2022-03-30 17:57:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (475987ms till timeout)
2022-03-30 17:57:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (474984ms till timeout)
2022-03-30 17:57:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (473981ms till timeout)
2022-03-30 17:57:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (472978ms till timeout)
2022-03-30 17:57:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (471976ms till timeout)
2022-03-30 17:57:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (470972ms till timeout)
2022-03-30 17:57:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (469969ms till timeout)
2022-03-30 17:57:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (468966ms till timeout)
2022-03-30 17:57:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (467963ms till timeout)
2022-03-30 17:57:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (466960ms till timeout)
2022-03-30 17:57:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (465957ms till timeout)
2022-03-30 17:57:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (464954ms till timeout)
2022-03-30 17:57:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (463951ms till timeout)
2022-03-30 17:57:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (462948ms till timeout)
2022-03-30 17:57:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (461946ms till timeout)
2022-03-30 17:57:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (460943ms till timeout)
2022-03-30 17:57:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (459940ms till timeout)
2022-03-30 17:57:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (458937ms till timeout)
2022-03-30 17:57:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (457934ms till timeout)
2022-03-30 17:57:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (456931ms till timeout)
2022-03-30 17:57:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: my-bridge is in desired state: Ready
2022-03-30 17:57:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 mm2-cluster in namespace infra-namespace
2022-03-30 17:57:42 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaMirrorMaker2:mm2-cluster
2022-03-30 17:57:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: mm2-cluster will have desired state: Ready
2022-03-30 17:57:42 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaMirrorMaker2: mm2-cluster will have desired state: Ready
2022-03-30 17:57:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 17:57:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (598995ms till timeout)
2022-03-30 17:57:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (597992ms till timeout)
2022-03-30 17:57:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (596989ms till timeout)
2022-03-30 17:57:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (595985ms till timeout)
2022-03-30 17:57:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (594982ms till timeout)
2022-03-30 17:57:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (593979ms till timeout)
2022-03-30 17:57:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (592972ms till timeout)
2022-03-30 17:57:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (591968ms till timeout)
2022-03-30 17:57:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (590965ms till timeout)
2022-03-30 17:57:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (589962ms till timeout)
2022-03-30 17:57:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (588959ms till timeout)
2022-03-30 17:57:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (587956ms till timeout)
2022-03-30 17:57:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (586953ms till timeout)
2022-03-30 17:57:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (585949ms till timeout)
2022-03-30 17:57:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (584946ms till timeout)
2022-03-30 17:57:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (583943ms till timeout)
2022-03-30 17:57:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (582939ms till timeout)
2022-03-30 17:58:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (581936ms till timeout)
2022-03-30 17:58:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (580933ms till timeout)
2022-03-30 17:58:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (579930ms till timeout)
2022-03-30 17:58:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (578926ms till timeout)
2022-03-30 17:58:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (577923ms till timeout)
2022-03-30 17:58:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (576920ms till timeout)
2022-03-30 17:58:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (575917ms till timeout)
2022-03-30 17:58:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (574914ms till timeout)
2022-03-30 17:58:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (573910ms till timeout)
2022-03-30 17:58:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (572907ms till timeout)
2022-03-30 17:58:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (571904ms till timeout)
2022-03-30 17:58:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (570901ms till timeout)
2022-03-30 17:58:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (569898ms till timeout)
2022-03-30 17:58:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (568894ms till timeout)
2022-03-30 17:58:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (567891ms till timeout)
2022-03-30 17:58:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (566888ms till timeout)
2022-03-30 17:58:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (565885ms till timeout)
2022-03-30 17:58:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (564881ms till timeout)
2022-03-30 17:58:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (563878ms till timeout)
2022-03-30 17:58:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (562874ms till timeout)
2022-03-30 17:58:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (561871ms till timeout)
2022-03-30 17:58:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (560868ms till timeout)
2022-03-30 17:58:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (559865ms till timeout)
2022-03-30 17:58:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (558862ms till timeout)
2022-03-30 17:58:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (557858ms till timeout)
2022-03-30 17:58:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (556855ms till timeout)
2022-03-30 17:58:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (555852ms till timeout)
2022-03-30 17:58:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (554848ms till timeout)
2022-03-30 17:58:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (553845ms till timeout)
2022-03-30 17:58:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (552841ms till timeout)
2022-03-30 17:58:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (551838ms till timeout)
2022-03-30 17:58:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (550835ms till timeout)
2022-03-30 17:58:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (549832ms till timeout)
2022-03-30 17:58:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (548829ms till timeout)
2022-03-30 17:58:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (547825ms till timeout)
2022-03-30 17:58:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (546822ms till timeout)
2022-03-30 17:58:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (545819ms till timeout)
2022-03-30 17:58:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (544816ms till timeout)
2022-03-30 17:58:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (543812ms till timeout)
2022-03-30 17:58:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (542810ms till timeout)
2022-03-30 17:58:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (541806ms till timeout)
2022-03-30 17:58:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (540803ms till timeout)
2022-03-30 17:58:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (539800ms till timeout)
2022-03-30 17:58:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (538796ms till timeout)
2022-03-30 17:58:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (537793ms till timeout)
2022-03-30 17:58:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (536790ms till timeout)
2022-03-30 17:58:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (535786ms till timeout)
2022-03-30 17:58:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (534783ms till timeout)
2022-03-30 17:58:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (533780ms till timeout)
2022-03-30 17:58:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (532776ms till timeout)
2022-03-30 17:58:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (531773ms till timeout)
2022-03-30 17:58:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (530770ms till timeout)
2022-03-30 17:58:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (529767ms till timeout)
2022-03-30 17:58:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (528761ms till timeout)
2022-03-30 17:58:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: mm2-cluster is in desired state: Ready
2022-03-30 17:58:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1642297574-660862374 in namespace infra-namespace
2022-03-30 17:58:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1642297574-660862374
2022-03-30 17:58:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1642297574-660862374 will have desired state: Ready
2022-03-30 17:58:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1642297574-660862374 will have desired state: Ready
2022-03-30 17:58:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1642297574-660862374 will have desired state: Ready not ready, will try again in 1000 ms (179997ms till timeout)
2022-03-30 17:58:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1642297574-660862374 is in desired state: Ready
2022-03-30 17:58:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-840173747-1931076155 in namespace infra-namespace
2022-03-30 17:58:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-840173747-1931076155
2022-03-30 17:58:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-840173747-1931076155 will have desired state: Ready
2022-03-30 17:58:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-840173747-1931076155 will have desired state: Ready
2022-03-30 17:58:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-840173747-1931076155 will have desired state: Ready not ready, will try again in 1000 ms (179997ms till timeout)
2022-03-30 17:58:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-840173747-1931076155 is in desired state: Ready
2022-03-30 17:58:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1177668122-1476973967 in namespace infra-namespace
2022-03-30 17:58:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1177668122-1476973967
2022-03-30 17:58:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1177668122-1476973967 will have desired state: Ready
2022-03-30 17:58:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1177668122-1476973967 will have desired state: Ready
2022-03-30 17:58:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1177668122-1476973967 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 17:58:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1177668122-1476973967 is in desired state: Ready
2022-03-30 17:58:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-180171383-280558322 in namespace infra-namespace
2022-03-30 17:58:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-180171383-280558322
2022-03-30 17:58:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-180171383-280558322 will have desired state: Ready
2022-03-30 17:58:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-180171383-280558322 will have desired state: Ready
2022-03-30 17:58:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-180171383-280558322 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 17:58:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-180171383-280558322 is in desired state: Ready
2022-03-30 17:58:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-39163752-1058894052 in namespace infra-namespace
2022-03-30 17:58:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-39163752-1058894052
2022-03-30 17:58:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-39163752-1058894052 will have desired state: Ready
2022-03-30 17:58:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-39163752-1058894052 will have desired state: Ready
2022-03-30 17:58:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-39163752-1058894052 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 17:58:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-39163752-1058894052 is in desired state: Ready
2022-03-30 17:58:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect metrics-cluster-name in namespace infra-namespace
2022-03-30 17:58:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaConnect:metrics-cluster-name
2022-03-30 17:58:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: metrics-cluster-name will have desired state: Ready
2022-03-30 17:58:59 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaConnect: metrics-cluster-name will have desired state: Ready
2022-03-30 17:58:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 17:59:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (598995ms till timeout)
2022-03-30 17:59:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (597991ms till timeout)
2022-03-30 17:59:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (596988ms till timeout)
2022-03-30 17:59:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (595985ms till timeout)
2022-03-30 17:59:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (594982ms till timeout)
2022-03-30 17:59:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (593978ms till timeout)
2022-03-30 17:59:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (592975ms till timeout)
2022-03-30 17:59:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (591972ms till timeout)
2022-03-30 17:59:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (590969ms till timeout)
2022-03-30 17:59:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (589966ms till timeout)
2022-03-30 17:59:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (588963ms till timeout)
2022-03-30 17:59:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (587960ms till timeout)
2022-03-30 17:59:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (586956ms till timeout)
2022-03-30 17:59:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (585953ms till timeout)
2022-03-30 17:59:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (584950ms till timeout)
2022-03-30 17:59:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (583947ms till timeout)
2022-03-30 17:59:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (582944ms till timeout)
2022-03-30 17:59:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (581940ms till timeout)
2022-03-30 17:59:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (580937ms till timeout)
2022-03-30 17:59:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (579934ms till timeout)
2022-03-30 17:59:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (578931ms till timeout)
2022-03-30 17:59:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (577928ms till timeout)
2022-03-30 17:59:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (576925ms till timeout)
2022-03-30 17:59:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (575922ms till timeout)
2022-03-30 17:59:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (574919ms till timeout)
2022-03-30 17:59:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (573915ms till timeout)
2022-03-30 17:59:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (572912ms till timeout)
2022-03-30 17:59:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (571909ms till timeout)
2022-03-30 17:59:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (570906ms till timeout)
2022-03-30 17:59:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (569902ms till timeout)
2022-03-30 17:59:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (568899ms till timeout)
2022-03-30 17:59:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (567896ms till timeout)
2022-03-30 17:59:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (566893ms till timeout)
2022-03-30 17:59:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (565889ms till timeout)
2022-03-30 17:59:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (564886ms till timeout)
2022-03-30 17:59:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (563883ms till timeout)
2022-03-30 17:59:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (562880ms till timeout)
2022-03-30 17:59:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (561877ms till timeout)
2022-03-30 17:59:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (560874ms till timeout)
2022-03-30 17:59:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (559870ms till timeout)
2022-03-30 17:59:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (558867ms till timeout)
2022-03-30 17:59:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (557864ms till timeout)
2022-03-30 17:59:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (556861ms till timeout)
2022-03-30 17:59:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (555858ms till timeout)
2022-03-30 17:59:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (554855ms till timeout)
2022-03-30 17:59:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (553852ms till timeout)
2022-03-30 17:59:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (552848ms till timeout)
2022-03-30 17:59:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (551845ms till timeout)
2022-03-30 17:59:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (550842ms till timeout)
2022-03-30 17:59:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (549839ms till timeout)
2022-03-30 17:59:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (548836ms till timeout)
2022-03-30 17:59:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (547833ms till timeout)
2022-03-30 17:59:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (546829ms till timeout)
2022-03-30 17:59:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (545826ms till timeout)
2022-03-30 17:59:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (544823ms till timeout)
2022-03-30 17:59:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (543820ms till timeout)
2022-03-30 17:59:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (542816ms till timeout)
2022-03-30 17:59:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (541813ms till timeout)
2022-03-30 17:59:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (540810ms till timeout)
2022-03-30 18:00:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (539807ms till timeout)
2022-03-30 18:00:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (538804ms till timeout)
2022-03-30 18:00:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (537800ms till timeout)
2022-03-30 18:00:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (536797ms till timeout)
2022-03-30 18:00:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: metrics-cluster-name is in desired state: Ready
2022-03-30 18:00:04 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:72] Apply NetworkPolicy access to cluster-operator from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={})
2022-03-30 18:00:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyResource:88] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=cluster-operator-allow, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8080, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/kind=cluster-operator}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 18:00:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy cluster-operator-allow in namespace infra-namespace
2022-03-30 18:00:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:cluster-operator-allow
2022-03-30 18:00:04 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:90] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}) successfully created
2022-03-30 18:00:04 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:104] Apply NetworkPolicy access to metrics-cluster-name-entity-operator from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={})
2022-03-30 18:00:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyResource:126] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=metrics-cluster-name-entity-operator-allow, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8080, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8081, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-entity-operator}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 18:00:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy metrics-cluster-name-entity-operator-allow in namespace infra-namespace
2022-03-30 18:00:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:metrics-cluster-name-entity-operator-allow
2022-03-30 18:00:04 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:128] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}) successfully created
2022-03-30 18:00:04 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:104] Apply NetworkPolicy access to second-kafka-cluster-entity-operator from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={})
2022-03-30 18:00:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyResource:126] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=second-kafka-cluster-entity-operator-allow, namespace=second-metrics-cluster-test, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8080, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8081, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=second-kafka-cluster, strimzi.io/kind=Kafka, strimzi.io/name=second-kafka-cluster-entity-operator}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 18:00:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy second-kafka-cluster-entity-operator-allow in namespace second-metrics-cluster-test
2022-03-30 18:00:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:second-kafka-cluster-entity-operator-allow
2022-03-30 18:00:04 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:128] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}) successfully created
2022-03-30 18:00:04 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:141] Apply NetworkPolicy access to metrics-cluster-name-kafka-exporter from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={})
2022-03-30 18:00:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyResource:159] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=metrics-cluster-name-kafka-exporter-allow, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9404, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 18:00:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy metrics-cluster-name-kafka-exporter-allow in namespace infra-namespace
2022-03-30 18:00:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:metrics-cluster-name-kafka-exporter-allow
2022-03-30 18:00:04 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:161] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}) successfully created
2022-03-30 18:00:04 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:141] Apply NetworkPolicy access to second-kafka-cluster-kafka-exporter from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={})
2022-03-30 18:00:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyResource:159] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=second-kafka-cluster-kafka-exporter-allow, namespace=second-metrics-cluster-test, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9404, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=second-kafka-cluster, strimzi.io/kind=Kafka, strimzi.io/name=second-kafka-cluster-kafka-exporter}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 18:00:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy second-kafka-cluster-kafka-exporter-allow in namespace second-metrics-cluster-test
2022-03-30 18:00:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:second-kafka-cluster-kafka-exporter-allow
2022-03-30 18:00:04 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:161] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}) successfully created
2022-03-30 18:01:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 18:01:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-s25n7 -n infra-namespace -- curl 172.17.0.18:9404
2022-03-30 18:01:26 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.18 from Pod infra-namespace-kafka-clients-748578f786-s25n7 finished with return code: 0
2022-03-30 18:01:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-s25n7 -n infra-namespace -- curl 172.17.0.16:9404
2022-03-30 18:01:28 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.16 from Pod infra-namespace-kafka-clients-748578f786-s25n7 finished with return code: 0
2022-03-30 18:01:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-s25n7 -n infra-namespace -- curl 172.17.0.17:9404
2022-03-30 18:01:30 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.17 from Pod infra-namespace-kafka-clients-748578f786-s25n7 finished with return code: 0
2022-03-30 18:01:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 18:01:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-s25n7 -n infra-namespace -- curl 172.17.0.9:9404
2022-03-30 18:01:30 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.9 from Pod infra-namespace-kafka-clients-748578f786-s25n7 finished with return code: 0
2022-03-30 18:01:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-s25n7 -n infra-namespace -- curl 172.17.0.10:9404
2022-03-30 18:01:31 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.10 from Pod infra-namespace-kafka-clients-748578f786-s25n7 finished with return code: 0
2022-03-30 18:01:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-s25n7 -n infra-namespace -- curl 172.17.0.11:9404
2022-03-30 18:01:31 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.11 from Pod infra-namespace-kafka-clients-748578f786-s25n7 finished with return code: 0
2022-03-30 18:01:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 18:01:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-s25n7 -n infra-namespace -- curl 172.17.0.24:9404
2022-03-30 18:01:32 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.24 from Pod infra-namespace-kafka-clients-748578f786-s25n7 finished with return code: 0
2022-03-30 18:01:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 18:01:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-s25n7 -n infra-namespace -- curl 172.17.0.21:9404/metrics
2022-03-30 18:01:32 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.21 from Pod infra-namespace-kafka-clients-748578f786-s25n7 finished with return code: 0
2022-03-30 18:01:32 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 18:01:32 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testUserOperatorMetrics-STARTED
2022-03-30 18:01:32 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 18:01:32 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 18:01:32 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testUserOperatorMetrics
2022-03-30 18:01:32 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 1
2022-03-30 18:01:32 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:230] testUserOperatorMetrics test now can proceed its execution
2022-03-30 18:01:32 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 18:01:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1, testSendSimpleMessageTls=my-cluster-128d9a40, testUpdateUser=my-cluster-75998b92, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e, testCruiseControlBasicAPIRequests=my-cluster-6da231a3, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2, testSendMessagesTlsScramSha=my-cluster-a11b9173, testReceiveSimpleMessageTls=my-cluster-90021a9f, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7, testUserOperatorMetrics=my-cluster-7692a28e, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e}
2022-03-30 18:01:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1975958319-512406578, testSendSimpleMessageTls=my-user-323988616-1355569855, testUpdateUser=my-user-275427717-1183444028, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-695287754-1004738297, testKafkaInDifferentNsThanClusterOperator=my-user-908767181-1543724508, testSendMessagesCustomListenerTlsScramSha=my-user-1653975062-1727371851, testCruiseControlBasicAPIRequests=my-user-918568860-36694764, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1337805046-1917759354, testAutoRenewAllCaCertsTriggeredByAnno=my-user-308582398-741623568, testSendMessagesTlsScramSha=my-user-1791509549-1350003609, testReceiveSimpleMessageTls=my-user-491397521-1196361913, testMirrorMaker2TlsAndTlsClientAuth=my-user-711655589-495294801, testUserOperatorMetrics=my-user-92263365-1802597441, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-83332781-1273282344, testRackAwareConnectCorrectDeployment=my-user-1839832768-1329691231, testMirrorMakerTlsAuthenticated=my-user-1552986594-1641210083}
2022-03-30 18:01:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-1496158521-938246156, testSendSimpleMessageTls=my-topic-1040221322-670398147, testUpdateUser=my-topic-275186280-765877519, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1609923788-1785060017, testKafkaInDifferentNsThanClusterOperator=my-topic-2051437288-2087424913, testSendMessagesCustomListenerTlsScramSha=my-topic-629448478-708445494, testCruiseControlBasicAPIRequests=my-topic-710218582-658397020, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-604305255-1585536778, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1728678022-515015095, testSendMessagesTlsScramSha=my-topic-1482193652-649135537, testReceiveSimpleMessageTls=my-topic-941665209-436577257, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1152366002-738531989, testUserOperatorMetrics=my-topic-1414123631-687906301, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1536476030-1482182833, testRackAwareConnectCorrectDeployment=my-topic-1526242410-482838543, testMirrorMakerTlsAuthenticated=my-topic-1936321612-505632869}
2022-03-30 18:01:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1-kafka-clients, testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-6da231a3-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2-kafka-clients, testSendMessagesTlsScramSha=my-cluster-a11b9173-kafka-clients, testReceiveSimpleMessageTls=my-cluster-90021a9f-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7-kafka-clients, testUserOperatorMetrics=my-cluster-7692a28e-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e-kafka-clients}
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testZookeeperWatchersCount-STARTED
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testZookeeperWatchersCount
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 2
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:230] testZookeeperWatchersCount test now can proceed its execution
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1, testSendSimpleMessageTls=my-cluster-128d9a40, testUpdateUser=my-cluster-75998b92, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e, testCruiseControlBasicAPIRequests=my-cluster-6da231a3, testZookeeperWatchersCount=my-cluster-a66ecd5e, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2, testSendMessagesTlsScramSha=my-cluster-a11b9173, testReceiveSimpleMessageTls=my-cluster-90021a9f, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7, testUserOperatorMetrics=my-cluster-7692a28e, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e}
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1975958319-512406578, testSendSimpleMessageTls=my-user-323988616-1355569855, testUpdateUser=my-user-275427717-1183444028, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-695287754-1004738297, testKafkaInDifferentNsThanClusterOperator=my-user-908767181-1543724508, testSendMessagesCustomListenerTlsScramSha=my-user-1653975062-1727371851, testCruiseControlBasicAPIRequests=my-user-918568860-36694764, testZookeeperWatchersCount=my-user-1252011538-504314473, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1337805046-1917759354, testAutoRenewAllCaCertsTriggeredByAnno=my-user-308582398-741623568, testSendMessagesTlsScramSha=my-user-1791509549-1350003609, testReceiveSimpleMessageTls=my-user-491397521-1196361913, testMirrorMaker2TlsAndTlsClientAuth=my-user-711655589-495294801, testUserOperatorMetrics=my-user-92263365-1802597441, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-83332781-1273282344, testRackAwareConnectCorrectDeployment=my-user-1839832768-1329691231, testMirrorMakerTlsAuthenticated=my-user-1552986594-1641210083}
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-1496158521-938246156, testSendSimpleMessageTls=my-topic-1040221322-670398147, testUpdateUser=my-topic-275186280-765877519, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1609923788-1785060017, testKafkaInDifferentNsThanClusterOperator=my-topic-2051437288-2087424913, testSendMessagesCustomListenerTlsScramSha=my-topic-629448478-708445494, testCruiseControlBasicAPIRequests=my-topic-710218582-658397020, testZookeeperWatchersCount=my-topic-1225869180-847659507, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-604305255-1585536778, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1728678022-515015095, testSendMessagesTlsScramSha=my-topic-1482193652-649135537, testReceiveSimpleMessageTls=my-topic-941665209-436577257, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1152366002-738531989, testUserOperatorMetrics=my-topic-1414123631-687906301, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1536476030-1482182833, testRackAwareConnectCorrectDeployment=my-topic-1526242410-482838543, testMirrorMakerTlsAuthenticated=my-topic-1936321612-505632869}
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1-kafka-clients, testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-6da231a3-kafka-clients, testZookeeperWatchersCount=my-cluster-a66ecd5e-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2-kafka-clients, testSendMessagesTlsScramSha=my-cluster-a11b9173-kafka-clients, testReceiveSimpleMessageTls=my-cluster-90021a9f-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7-kafka-clients, testUserOperatorMetrics=my-cluster-7692a28e-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e-kafka-clients}
2022-03-30 18:01:32 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:346] In context testZookeeperWatchersCount is everything deleted.
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:267] testZookeeperWatchersCount - Notifies waiting test cases:[testUpdateUser, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testCruiseControlBasicAPIRequests, testSendMessagesTlsScramSha, testSendMessagesCustomListenerTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMaker2TlsAndTlsClientAuth, testMirrorMakerTlsAuthenticated, testUserOperatorMetrics, testZookeeperWatchersCount] to and randomly select one to start execution
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testZookeeperWatchersCount
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 1
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testZookeeperWatchersCount-FINISHED
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 18:01:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-s25n7 -n infra-namespace -- curl 172.17.0.19:8081/metrics
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaConnectIoNetwork-STARTED
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaConnectIoNetwork
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 2
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:230] testKafkaConnectIoNetwork test now can proceed its execution
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1, testSendSimpleMessageTls=my-cluster-128d9a40, testKafkaConnectIoNetwork=my-cluster-580bbc7b, testUpdateUser=my-cluster-75998b92, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e, testCruiseControlBasicAPIRequests=my-cluster-6da231a3, testZookeeperWatchersCount=my-cluster-a66ecd5e, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2, testSendMessagesTlsScramSha=my-cluster-a11b9173, testReceiveSimpleMessageTls=my-cluster-90021a9f, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7, testUserOperatorMetrics=my-cluster-7692a28e, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e}
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1975958319-512406578, testSendSimpleMessageTls=my-user-323988616-1355569855, testKafkaConnectIoNetwork=my-user-1092781470-1895809353, testUpdateUser=my-user-275427717-1183444028, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-695287754-1004738297, testKafkaInDifferentNsThanClusterOperator=my-user-908767181-1543724508, testSendMessagesCustomListenerTlsScramSha=my-user-1653975062-1727371851, testCruiseControlBasicAPIRequests=my-user-918568860-36694764, testZookeeperWatchersCount=my-user-1252011538-504314473, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1337805046-1917759354, testAutoRenewAllCaCertsTriggeredByAnno=my-user-308582398-741623568, testSendMessagesTlsScramSha=my-user-1791509549-1350003609, testReceiveSimpleMessageTls=my-user-491397521-1196361913, testMirrorMaker2TlsAndTlsClientAuth=my-user-711655589-495294801, testUserOperatorMetrics=my-user-92263365-1802597441, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-83332781-1273282344, testRackAwareConnectCorrectDeployment=my-user-1839832768-1329691231, testMirrorMakerTlsAuthenticated=my-user-1552986594-1641210083}
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-1496158521-938246156, testSendSimpleMessageTls=my-topic-1040221322-670398147, testKafkaConnectIoNetwork=my-topic-2145871889-1295201610, testUpdateUser=my-topic-275186280-765877519, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1609923788-1785060017, testKafkaInDifferentNsThanClusterOperator=my-topic-2051437288-2087424913, testSendMessagesCustomListenerTlsScramSha=my-topic-629448478-708445494, testCruiseControlBasicAPIRequests=my-topic-710218582-658397020, testZookeeperWatchersCount=my-topic-1225869180-847659507, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-604305255-1585536778, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1728678022-515015095, testSendMessagesTlsScramSha=my-topic-1482193652-649135537, testReceiveSimpleMessageTls=my-topic-941665209-436577257, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1152366002-738531989, testUserOperatorMetrics=my-topic-1414123631-687906301, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1536476030-1482182833, testRackAwareConnectCorrectDeployment=my-topic-1526242410-482838543, testMirrorMakerTlsAuthenticated=my-topic-1936321612-505632869}
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1-kafka-clients, testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testKafkaConnectIoNetwork=my-cluster-580bbc7b-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-6da231a3-kafka-clients, testZookeeperWatchersCount=my-cluster-a66ecd5e-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2-kafka-clients, testSendMessagesTlsScramSha=my-cluster-a11b9173-kafka-clients, testReceiveSimpleMessageTls=my-cluster-90021a9f-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7-kafka-clients, testUserOperatorMetrics=my-cluster-7692a28e-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e-kafka-clients}
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 18:01:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-s25n7 -n infra-namespace -- curl 172.17.0.24:9404
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.19 from Pod infra-namespace-kafka-clients-748578f786-s25n7 finished with return code: 0
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:346] In context testUserOperatorMetrics is everything deleted.
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:267] testUserOperatorMetrics - Notifies waiting test cases:[testUpdateUser, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testCruiseControlBasicAPIRequests, testSendMessagesTlsScramSha, testSendMessagesCustomListenerTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMaker2TlsAndTlsClientAuth, testMirrorMakerTlsAuthenticated, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaConnectIoNetwork] to and randomly select one to start execution
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testUserOperatorMetrics
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 1
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testUserOperatorMetrics-FINISHED
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaBrokersCount-STARTED
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaBrokersCount
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 2
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:230] testKafkaBrokersCount test now can proceed its execution
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1, testSendSimpleMessageTls=my-cluster-128d9a40, testKafkaConnectIoNetwork=my-cluster-580bbc7b, testUpdateUser=my-cluster-75998b92, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e, testCruiseControlBasicAPIRequests=my-cluster-6da231a3, testZookeeperWatchersCount=my-cluster-a66ecd5e, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2, testSendMessagesTlsScramSha=my-cluster-a11b9173, testReceiveSimpleMessageTls=my-cluster-90021a9f, testKafkaBrokersCount=my-cluster-e91a8cef, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7, testUserOperatorMetrics=my-cluster-7692a28e, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e}
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1975958319-512406578, testSendSimpleMessageTls=my-user-323988616-1355569855, testKafkaConnectIoNetwork=my-user-1092781470-1895809353, testUpdateUser=my-user-275427717-1183444028, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-695287754-1004738297, testKafkaInDifferentNsThanClusterOperator=my-user-908767181-1543724508, testSendMessagesCustomListenerTlsScramSha=my-user-1653975062-1727371851, testCruiseControlBasicAPIRequests=my-user-918568860-36694764, testZookeeperWatchersCount=my-user-1252011538-504314473, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1337805046-1917759354, testAutoRenewAllCaCertsTriggeredByAnno=my-user-308582398-741623568, testSendMessagesTlsScramSha=my-user-1791509549-1350003609, testReceiveSimpleMessageTls=my-user-491397521-1196361913, testKafkaBrokersCount=my-user-699069229-1729992055, testMirrorMaker2TlsAndTlsClientAuth=my-user-711655589-495294801, testUserOperatorMetrics=my-user-92263365-1802597441, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-83332781-1273282344, testRackAwareConnectCorrectDeployment=my-user-1839832768-1329691231, testMirrorMakerTlsAuthenticated=my-user-1552986594-1641210083}
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-1496158521-938246156, testSendSimpleMessageTls=my-topic-1040221322-670398147, testKafkaConnectIoNetwork=my-topic-2145871889-1295201610, testUpdateUser=my-topic-275186280-765877519, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1609923788-1785060017, testKafkaInDifferentNsThanClusterOperator=my-topic-2051437288-2087424913, testSendMessagesCustomListenerTlsScramSha=my-topic-629448478-708445494, testCruiseControlBasicAPIRequests=my-topic-710218582-658397020, testZookeeperWatchersCount=my-topic-1225869180-847659507, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-604305255-1585536778, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1728678022-515015095, testSendMessagesTlsScramSha=my-topic-1482193652-649135537, testReceiveSimpleMessageTls=my-topic-941665209-436577257, testKafkaBrokersCount=my-topic-708967095-808914391, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1152366002-738531989, testUserOperatorMetrics=my-topic-1414123631-687906301, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1536476030-1482182833, testRackAwareConnectCorrectDeployment=my-topic-1526242410-482838543, testMirrorMakerTlsAuthenticated=my-topic-1936321612-505632869}
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1-kafka-clients, testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testKafkaConnectIoNetwork=my-cluster-580bbc7b-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-6da231a3-kafka-clients, testZookeeperWatchersCount=my-cluster-a66ecd5e-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2-kafka-clients, testSendMessagesTlsScramSha=my-cluster-a11b9173-kafka-clients, testReceiveSimpleMessageTls=my-cluster-90021a9f-kafka-clients, testKafkaBrokersCount=my-cluster-e91a8cef-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7-kafka-clients, testUserOperatorMetrics=my-cluster-7692a28e-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e-kafka-clients}
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:346] In context testKafkaBrokersCount is everything deleted.
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:267] testKafkaBrokersCount - Notifies waiting test cases:[testUpdateUser, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testCruiseControlBasicAPIRequests, testSendMessagesTlsScramSha, testSendMessagesCustomListenerTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMaker2TlsAndTlsClientAuth, testMirrorMakerTlsAuthenticated, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaConnectIoNetwork, testKafkaBrokersCount] to and randomly select one to start execution
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaBrokersCount
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 1
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaBrokersCount-FINISHED
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaActiveControllers-STARTED
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaActiveControllers
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 2
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:230] testKafkaActiveControllers test now can proceed its execution
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1, testSendSimpleMessageTls=my-cluster-128d9a40, testKafkaConnectIoNetwork=my-cluster-580bbc7b, testUpdateUser=my-cluster-75998b92, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e, testCruiseControlBasicAPIRequests=my-cluster-6da231a3, testKafkaActiveControllers=my-cluster-948e4115, testZookeeperWatchersCount=my-cluster-a66ecd5e, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2, testSendMessagesTlsScramSha=my-cluster-a11b9173, testReceiveSimpleMessageTls=my-cluster-90021a9f, testKafkaBrokersCount=my-cluster-e91a8cef, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7, testUserOperatorMetrics=my-cluster-7692a28e, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e}
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1975958319-512406578, testSendSimpleMessageTls=my-user-323988616-1355569855, testKafkaConnectIoNetwork=my-user-1092781470-1895809353, testUpdateUser=my-user-275427717-1183444028, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-695287754-1004738297, testKafkaInDifferentNsThanClusterOperator=my-user-908767181-1543724508, testSendMessagesCustomListenerTlsScramSha=my-user-1653975062-1727371851, testCruiseControlBasicAPIRequests=my-user-918568860-36694764, testKafkaActiveControllers=my-user-971800472-962265772, testZookeeperWatchersCount=my-user-1252011538-504314473, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1337805046-1917759354, testAutoRenewAllCaCertsTriggeredByAnno=my-user-308582398-741623568, testSendMessagesTlsScramSha=my-user-1791509549-1350003609, testReceiveSimpleMessageTls=my-user-491397521-1196361913, testKafkaBrokersCount=my-user-699069229-1729992055, testMirrorMaker2TlsAndTlsClientAuth=my-user-711655589-495294801, testUserOperatorMetrics=my-user-92263365-1802597441, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-83332781-1273282344, testRackAwareConnectCorrectDeployment=my-user-1839832768-1329691231, testMirrorMakerTlsAuthenticated=my-user-1552986594-1641210083}
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-1496158521-938246156, testSendSimpleMessageTls=my-topic-1040221322-670398147, testKafkaConnectIoNetwork=my-topic-2145871889-1295201610, testUpdateUser=my-topic-275186280-765877519, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1609923788-1785060017, testKafkaInDifferentNsThanClusterOperator=my-topic-2051437288-2087424913, testSendMessagesCustomListenerTlsScramSha=my-topic-629448478-708445494, testCruiseControlBasicAPIRequests=my-topic-710218582-658397020, testKafkaActiveControllers=my-topic-916932648-500952606, testZookeeperWatchersCount=my-topic-1225869180-847659507, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-604305255-1585536778, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1728678022-515015095, testSendMessagesTlsScramSha=my-topic-1482193652-649135537, testReceiveSimpleMessageTls=my-topic-941665209-436577257, testKafkaBrokersCount=my-topic-708967095-808914391, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1152366002-738531989, testUserOperatorMetrics=my-topic-1414123631-687906301, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1536476030-1482182833, testRackAwareConnectCorrectDeployment=my-topic-1526242410-482838543, testMirrorMakerTlsAuthenticated=my-topic-1936321612-505632869}
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1-kafka-clients, testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testKafkaConnectIoNetwork=my-cluster-580bbc7b-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-6da231a3-kafka-clients, testKafkaActiveControllers=my-cluster-948e4115-kafka-clients, testZookeeperWatchersCount=my-cluster-a66ecd5e-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2-kafka-clients, testSendMessagesTlsScramSha=my-cluster-a11b9173-kafka-clients, testReceiveSimpleMessageTls=my-cluster-90021a9f-kafka-clients, testKafkaBrokersCount=my-cluster-e91a8cef-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7-kafka-clients, testUserOperatorMetrics=my-cluster-7692a28e-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e-kafka-clients}
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:346] In context testKafkaActiveControllers is everything deleted.
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:267] testKafkaActiveControllers - Notifies waiting test cases:[testUpdateUser, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testCruiseControlBasicAPIRequests, testSendMessagesTlsScramSha, testSendMessagesCustomListenerTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMaker2TlsAndTlsClientAuth, testMirrorMakerTlsAuthenticated, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaConnectIoNetwork, testKafkaBrokersCount, testKafkaActiveControllers] to and randomly select one to start execution
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaActiveControllers
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 1
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaActiveControllers-FINISHED
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaTopicUnderReplicatedPartitions-STARTED
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaTopicUnderReplicatedPartitions
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 2
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:230] testKafkaTopicUnderReplicatedPartitions test now can proceed its execution
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1, testSendSimpleMessageTls=my-cluster-128d9a40, testKafkaConnectIoNetwork=my-cluster-580bbc7b, testUpdateUser=my-cluster-75998b92, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e, testCruiseControlBasicAPIRequests=my-cluster-6da231a3, testKafkaActiveControllers=my-cluster-948e4115, testZookeeperWatchersCount=my-cluster-a66ecd5e, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2, testKafkaTopicUnderReplicatedPartitions=my-cluster-fa981fe3, testSendMessagesTlsScramSha=my-cluster-a11b9173, testReceiveSimpleMessageTls=my-cluster-90021a9f, testKafkaBrokersCount=my-cluster-e91a8cef, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7, testUserOperatorMetrics=my-cluster-7692a28e, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e}
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-1975958319-512406578, testSendSimpleMessageTls=my-user-323988616-1355569855, testKafkaConnectIoNetwork=my-user-1092781470-1895809353, testUpdateUser=my-user-275427717-1183444028, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-695287754-1004738297, testKafkaInDifferentNsThanClusterOperator=my-user-908767181-1543724508, testSendMessagesCustomListenerTlsScramSha=my-user-1653975062-1727371851, testCruiseControlBasicAPIRequests=my-user-918568860-36694764, testKafkaActiveControllers=my-user-971800472-962265772, testZookeeperWatchersCount=my-user-1252011538-504314473, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1337805046-1917759354, testAutoRenewAllCaCertsTriggeredByAnno=my-user-308582398-741623568, testKafkaTopicUnderReplicatedPartitions=my-user-1487552272-1931929933, testSendMessagesTlsScramSha=my-user-1791509549-1350003609, testReceiveSimpleMessageTls=my-user-491397521-1196361913, testKafkaBrokersCount=my-user-699069229-1729992055, testMirrorMaker2TlsAndTlsClientAuth=my-user-711655589-495294801, testUserOperatorMetrics=my-user-92263365-1802597441, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-83332781-1273282344, testRackAwareConnectCorrectDeployment=my-user-1839832768-1329691231, testMirrorMakerTlsAuthenticated=my-user-1552986594-1641210083}
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-1496158521-938246156, testSendSimpleMessageTls=my-topic-1040221322-670398147, testKafkaConnectIoNetwork=my-topic-2145871889-1295201610, testUpdateUser=my-topic-275186280-765877519, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1609923788-1785060017, testKafkaInDifferentNsThanClusterOperator=my-topic-2051437288-2087424913, testSendMessagesCustomListenerTlsScramSha=my-topic-629448478-708445494, testCruiseControlBasicAPIRequests=my-topic-710218582-658397020, testKafkaActiveControllers=my-topic-916932648-500952606, testZookeeperWatchersCount=my-topic-1225869180-847659507, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-604305255-1585536778, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1728678022-515015095, testKafkaTopicUnderReplicatedPartitions=my-topic-893837582-953830717, testSendMessagesTlsScramSha=my-topic-1482193652-649135537, testReceiveSimpleMessageTls=my-topic-941665209-436577257, testKafkaBrokersCount=my-topic-708967095-808914391, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1152366002-738531989, testUserOperatorMetrics=my-topic-1414123631-687906301, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1536476030-1482182833, testRackAwareConnectCorrectDeployment=my-topic-1526242410-482838543, testMirrorMakerTlsAuthenticated=my-topic-1936321612-505632869}
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1-kafka-clients, testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testKafkaConnectIoNetwork=my-cluster-580bbc7b-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-6da231a3-kafka-clients, testKafkaActiveControllers=my-cluster-948e4115-kafka-clients, testZookeeperWatchersCount=my-cluster-a66ecd5e-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-fa981fe3-kafka-clients, testSendMessagesTlsScramSha=my-cluster-a11b9173-kafka-clients, testReceiveSimpleMessageTls=my-cluster-90021a9f-kafka-clients, testKafkaBrokersCount=my-cluster-e91a8cef-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7-kafka-clients, testUserOperatorMetrics=my-cluster-7692a28e-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e-kafka-clients}
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:346] In context testKafkaTopicUnderReplicatedPartitions is everything deleted.
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:267] testKafkaTopicUnderReplicatedPartitions - Notifies waiting test cases:[testUpdateUser, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testCruiseControlBasicAPIRequests, testSendMessagesTlsScramSha, testSendMessagesCustomListenerTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMaker2TlsAndTlsClientAuth, testMirrorMakerTlsAuthenticated, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaConnectIoNetwork, testKafkaBrokersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions] to and randomly select one to start execution
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaTopicUnderReplicatedPartitions
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 1
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaTopicUnderReplicatedPartitions-FINISHED
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testZookeeperQuorumSize-STARTED
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testZookeeperQuorumSize
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 2
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:230] testZookeeperQuorumSize test now can proceed its execution
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testZookeeperQuorumSize=my-cluster-5a26ca2a, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1, testSendSimpleMessageTls=my-cluster-128d9a40, testKafkaConnectIoNetwork=my-cluster-580bbc7b, testUpdateUser=my-cluster-75998b92, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e, testCruiseControlBasicAPIRequests=my-cluster-6da231a3, testKafkaActiveControllers=my-cluster-948e4115, testZookeeperWatchersCount=my-cluster-a66ecd5e, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2, testKafkaTopicUnderReplicatedPartitions=my-cluster-fa981fe3, testSendMessagesTlsScramSha=my-cluster-a11b9173, testReceiveSimpleMessageTls=my-cluster-90021a9f, testKafkaBrokersCount=my-cluster-e91a8cef, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7, testUserOperatorMetrics=my-cluster-7692a28e, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e}
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testZookeeperQuorumSize=my-user-1387929819-2059775559, testKafkaAndZookeeperScaleUpScaleDown=my-user-1975958319-512406578, testSendSimpleMessageTls=my-user-323988616-1355569855, testKafkaConnectIoNetwork=my-user-1092781470-1895809353, testUpdateUser=my-user-275427717-1183444028, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-695287754-1004738297, testKafkaInDifferentNsThanClusterOperator=my-user-908767181-1543724508, testSendMessagesCustomListenerTlsScramSha=my-user-1653975062-1727371851, testCruiseControlBasicAPIRequests=my-user-918568860-36694764, testKafkaActiveControllers=my-user-971800472-962265772, testZookeeperWatchersCount=my-user-1252011538-504314473, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1337805046-1917759354, testAutoRenewAllCaCertsTriggeredByAnno=my-user-308582398-741623568, testKafkaTopicUnderReplicatedPartitions=my-user-1487552272-1931929933, testSendMessagesTlsScramSha=my-user-1791509549-1350003609, testReceiveSimpleMessageTls=my-user-491397521-1196361913, testKafkaBrokersCount=my-user-699069229-1729992055, testMirrorMaker2TlsAndTlsClientAuth=my-user-711655589-495294801, testUserOperatorMetrics=my-user-92263365-1802597441, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-83332781-1273282344, testRackAwareConnectCorrectDeployment=my-user-1839832768-1329691231, testMirrorMakerTlsAuthenticated=my-user-1552986594-1641210083}
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testZookeeperQuorumSize=my-topic-1242495538-1808467385, testKafkaAndZookeeperScaleUpScaleDown=my-topic-1496158521-938246156, testSendSimpleMessageTls=my-topic-1040221322-670398147, testKafkaConnectIoNetwork=my-topic-2145871889-1295201610, testUpdateUser=my-topic-275186280-765877519, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1609923788-1785060017, testKafkaInDifferentNsThanClusterOperator=my-topic-2051437288-2087424913, testSendMessagesCustomListenerTlsScramSha=my-topic-629448478-708445494, testCruiseControlBasicAPIRequests=my-topic-710218582-658397020, testKafkaActiveControllers=my-topic-916932648-500952606, testZookeeperWatchersCount=my-topic-1225869180-847659507, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-604305255-1585536778, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1728678022-515015095, testKafkaTopicUnderReplicatedPartitions=my-topic-893837582-953830717, testSendMessagesTlsScramSha=my-topic-1482193652-649135537, testReceiveSimpleMessageTls=my-topic-941665209-436577257, testKafkaBrokersCount=my-topic-708967095-808914391, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1152366002-738531989, testUserOperatorMetrics=my-topic-1414123631-687906301, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1536476030-1482182833, testRackAwareConnectCorrectDeployment=my-topic-1526242410-482838543, testMirrorMakerTlsAuthenticated=my-topic-1936321612-505632869}
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testZookeeperQuorumSize=my-cluster-5a26ca2a-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1-kafka-clients, testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testKafkaConnectIoNetwork=my-cluster-580bbc7b-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-6da231a3-kafka-clients, testKafkaActiveControllers=my-cluster-948e4115-kafka-clients, testZookeeperWatchersCount=my-cluster-a66ecd5e-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-fa981fe3-kafka-clients, testSendMessagesTlsScramSha=my-cluster-a11b9173-kafka-clients, testReceiveSimpleMessageTls=my-cluster-90021a9f-kafka-clients, testKafkaBrokersCount=my-cluster-e91a8cef-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7-kafka-clients, testUserOperatorMetrics=my-cluster-7692a28e-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e-kafka-clients}
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:346] In context testZookeeperQuorumSize is everything deleted.
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:267] testZookeeperQuorumSize - Notifies waiting test cases:[testUpdateUser, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testCruiseControlBasicAPIRequests, testSendMessagesTlsScramSha, testSendMessagesCustomListenerTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMaker2TlsAndTlsClientAuth, testMirrorMakerTlsAuthenticated, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaConnectIoNetwork, testKafkaBrokersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize] to and randomly select one to start execution
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testZookeeperQuorumSize
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 1
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testZookeeperQuorumSize-FINISHED
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testZookeeperAliveConnections-STARTED
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testZookeeperAliveConnections
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 2
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:230] testZookeeperAliveConnections test now can proceed its execution
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testZookeeperQuorumSize=my-cluster-5a26ca2a, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1, testSendSimpleMessageTls=my-cluster-128d9a40, testKafkaConnectIoNetwork=my-cluster-580bbc7b, testUpdateUser=my-cluster-75998b92, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e, testCruiseControlBasicAPIRequests=my-cluster-6da231a3, testKafkaActiveControllers=my-cluster-948e4115, testZookeeperWatchersCount=my-cluster-a66ecd5e, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0, testZookeeperAliveConnections=my-cluster-01c0acb0, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2, testKafkaTopicUnderReplicatedPartitions=my-cluster-fa981fe3, testSendMessagesTlsScramSha=my-cluster-a11b9173, testReceiveSimpleMessageTls=my-cluster-90021a9f, testKafkaBrokersCount=my-cluster-e91a8cef, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7, testUserOperatorMetrics=my-cluster-7692a28e, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e}
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testZookeeperQuorumSize=my-user-1387929819-2059775559, testKafkaAndZookeeperScaleUpScaleDown=my-user-1975958319-512406578, testSendSimpleMessageTls=my-user-323988616-1355569855, testKafkaConnectIoNetwork=my-user-1092781470-1895809353, testUpdateUser=my-user-275427717-1183444028, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-695287754-1004738297, testKafkaInDifferentNsThanClusterOperator=my-user-908767181-1543724508, testSendMessagesCustomListenerTlsScramSha=my-user-1653975062-1727371851, testCruiseControlBasicAPIRequests=my-user-918568860-36694764, testKafkaActiveControllers=my-user-971800472-962265772, testZookeeperWatchersCount=my-user-1252011538-504314473, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1337805046-1917759354, testZookeeperAliveConnections=my-user-704084348-2034238998, testAutoRenewAllCaCertsTriggeredByAnno=my-user-308582398-741623568, testKafkaTopicUnderReplicatedPartitions=my-user-1487552272-1931929933, testSendMessagesTlsScramSha=my-user-1791509549-1350003609, testReceiveSimpleMessageTls=my-user-491397521-1196361913, testKafkaBrokersCount=my-user-699069229-1729992055, testMirrorMaker2TlsAndTlsClientAuth=my-user-711655589-495294801, testUserOperatorMetrics=my-user-92263365-1802597441, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-83332781-1273282344, testRackAwareConnectCorrectDeployment=my-user-1839832768-1329691231, testMirrorMakerTlsAuthenticated=my-user-1552986594-1641210083}
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testZookeeperQuorumSize=my-topic-1242495538-1808467385, testKafkaAndZookeeperScaleUpScaleDown=my-topic-1496158521-938246156, testSendSimpleMessageTls=my-topic-1040221322-670398147, testKafkaConnectIoNetwork=my-topic-2145871889-1295201610, testUpdateUser=my-topic-275186280-765877519, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1609923788-1785060017, testKafkaInDifferentNsThanClusterOperator=my-topic-2051437288-2087424913, testSendMessagesCustomListenerTlsScramSha=my-topic-629448478-708445494, testCruiseControlBasicAPIRequests=my-topic-710218582-658397020, testKafkaActiveControllers=my-topic-916932648-500952606, testZookeeperWatchersCount=my-topic-1225869180-847659507, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-604305255-1585536778, testZookeeperAliveConnections=my-topic-1222053841-372865493, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1728678022-515015095, testKafkaTopicUnderReplicatedPartitions=my-topic-893837582-953830717, testSendMessagesTlsScramSha=my-topic-1482193652-649135537, testReceiveSimpleMessageTls=my-topic-941665209-436577257, testKafkaBrokersCount=my-topic-708967095-808914391, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1152366002-738531989, testUserOperatorMetrics=my-topic-1414123631-687906301, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1536476030-1482182833, testRackAwareConnectCorrectDeployment=my-topic-1526242410-482838543, testMirrorMakerTlsAuthenticated=my-topic-1936321612-505632869}
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testZookeeperQuorumSize=my-cluster-5a26ca2a-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1-kafka-clients, testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testKafkaConnectIoNetwork=my-cluster-580bbc7b-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-6da231a3-kafka-clients, testKafkaActiveControllers=my-cluster-948e4115-kafka-clients, testZookeeperWatchersCount=my-cluster-a66ecd5e-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0-kafka-clients, testZookeeperAliveConnections=my-cluster-01c0acb0-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-fa981fe3-kafka-clients, testSendMessagesTlsScramSha=my-cluster-a11b9173-kafka-clients, testReceiveSimpleMessageTls=my-cluster-90021a9f-kafka-clients, testKafkaBrokersCount=my-cluster-e91a8cef-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7-kafka-clients, testUserOperatorMetrics=my-cluster-7692a28e-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e-kafka-clients}
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:346] In context testZookeeperAliveConnections is everything deleted.
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:267] testZookeeperAliveConnections - Notifies waiting test cases:[testUpdateUser, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testCruiseControlBasicAPIRequests, testSendMessagesTlsScramSha, testSendMessagesCustomListenerTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMaker2TlsAndTlsClientAuth, testMirrorMakerTlsAuthenticated, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaConnectIoNetwork, testKafkaBrokersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize, testZookeeperAliveConnections] to and randomly select one to start execution
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testZookeeperAliveConnections
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 1
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testZookeeperAliveConnections-FINISHED
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaTopicPartitions-STARTED
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaTopicPartitions
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 2
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:230] testKafkaTopicPartitions test now can proceed its execution
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testZookeeperQuorumSize=my-cluster-5a26ca2a, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1, testSendSimpleMessageTls=my-cluster-128d9a40, testKafkaConnectIoNetwork=my-cluster-580bbc7b, testUpdateUser=my-cluster-75998b92, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e, testCruiseControlBasicAPIRequests=my-cluster-6da231a3, testKafkaActiveControllers=my-cluster-948e4115, testZookeeperWatchersCount=my-cluster-a66ecd5e, testKafkaTopicPartitions=my-cluster-1625722f, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0, testZookeeperAliveConnections=my-cluster-01c0acb0, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2, testKafkaTopicUnderReplicatedPartitions=my-cluster-fa981fe3, testSendMessagesTlsScramSha=my-cluster-a11b9173, testReceiveSimpleMessageTls=my-cluster-90021a9f, testKafkaBrokersCount=my-cluster-e91a8cef, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7, testUserOperatorMetrics=my-cluster-7692a28e, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e}
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testZookeeperQuorumSize=my-user-1387929819-2059775559, testKafkaAndZookeeperScaleUpScaleDown=my-user-1975958319-512406578, testSendSimpleMessageTls=my-user-323988616-1355569855, testKafkaConnectIoNetwork=my-user-1092781470-1895809353, testUpdateUser=my-user-275427717-1183444028, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-695287754-1004738297, testKafkaInDifferentNsThanClusterOperator=my-user-908767181-1543724508, testSendMessagesCustomListenerTlsScramSha=my-user-1653975062-1727371851, testCruiseControlBasicAPIRequests=my-user-918568860-36694764, testKafkaActiveControllers=my-user-971800472-962265772, testZookeeperWatchersCount=my-user-1252011538-504314473, testKafkaTopicPartitions=my-user-394346586-105422980, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1337805046-1917759354, testZookeeperAliveConnections=my-user-704084348-2034238998, testAutoRenewAllCaCertsTriggeredByAnno=my-user-308582398-741623568, testKafkaTopicUnderReplicatedPartitions=my-user-1487552272-1931929933, testSendMessagesTlsScramSha=my-user-1791509549-1350003609, testReceiveSimpleMessageTls=my-user-491397521-1196361913, testKafkaBrokersCount=my-user-699069229-1729992055, testMirrorMaker2TlsAndTlsClientAuth=my-user-711655589-495294801, testUserOperatorMetrics=my-user-92263365-1802597441, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-83332781-1273282344, testRackAwareConnectCorrectDeployment=my-user-1839832768-1329691231, testMirrorMakerTlsAuthenticated=my-user-1552986594-1641210083}
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testZookeeperQuorumSize=my-topic-1242495538-1808467385, testKafkaAndZookeeperScaleUpScaleDown=my-topic-1496158521-938246156, testSendSimpleMessageTls=my-topic-1040221322-670398147, testKafkaConnectIoNetwork=my-topic-2145871889-1295201610, testUpdateUser=my-topic-275186280-765877519, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1609923788-1785060017, testKafkaInDifferentNsThanClusterOperator=my-topic-2051437288-2087424913, testSendMessagesCustomListenerTlsScramSha=my-topic-629448478-708445494, testCruiseControlBasicAPIRequests=my-topic-710218582-658397020, testKafkaActiveControllers=my-topic-916932648-500952606, testZookeeperWatchersCount=my-topic-1225869180-847659507, testKafkaTopicPartitions=my-topic-1086071779-2133194430, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-604305255-1585536778, testZookeeperAliveConnections=my-topic-1222053841-372865493, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1728678022-515015095, testKafkaTopicUnderReplicatedPartitions=my-topic-893837582-953830717, testSendMessagesTlsScramSha=my-topic-1482193652-649135537, testReceiveSimpleMessageTls=my-topic-941665209-436577257, testKafkaBrokersCount=my-topic-708967095-808914391, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1152366002-738531989, testUserOperatorMetrics=my-topic-1414123631-687906301, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1536476030-1482182833, testRackAwareConnectCorrectDeployment=my-topic-1526242410-482838543, testMirrorMakerTlsAuthenticated=my-topic-1936321612-505632869}
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testZookeeperQuorumSize=my-cluster-5a26ca2a-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1-kafka-clients, testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testKafkaConnectIoNetwork=my-cluster-580bbc7b-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-6da231a3-kafka-clients, testKafkaActiveControllers=my-cluster-948e4115-kafka-clients, testZookeeperWatchersCount=my-cluster-a66ecd5e-kafka-clients, testKafkaTopicPartitions=my-cluster-1625722f-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0-kafka-clients, testZookeeperAliveConnections=my-cluster-01c0acb0-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-fa981fe3-kafka-clients, testSendMessagesTlsScramSha=my-cluster-a11b9173-kafka-clients, testReceiveSimpleMessageTls=my-cluster-90021a9f-kafka-clients, testKafkaBrokersCount=my-cluster-e91a8cef-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7-kafka-clients, testUserOperatorMetrics=my-cluster-7692a28e-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e-kafka-clients}
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:346] In context testKafkaTopicPartitions is everything deleted.
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:267] testKafkaTopicPartitions - Notifies waiting test cases:[testUpdateUser, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testCruiseControlBasicAPIRequests, testSendMessagesTlsScramSha, testSendMessagesCustomListenerTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMaker2TlsAndTlsClientAuth, testMirrorMakerTlsAuthenticated, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaConnectIoNetwork, testKafkaBrokersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions] to and randomly select one to start execution
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaTopicPartitions
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 1
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaTopicPartitions-FINISHED
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testMirrorMaker2Metrics-STARTED
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testMirrorMaker2Metrics
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 2
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:230] testMirrorMaker2Metrics test now can proceed its execution
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-128d9a40, testKafkaConnectIoNetwork=my-cluster-580bbc7b, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e, testCruiseControlBasicAPIRequests=my-cluster-6da231a3, testZookeeperWatchersCount=my-cluster-a66ecd5e, testKafkaTopicPartitions=my-cluster-1625722f, testMirrorMaker2Metrics=my-cluster-a4258561, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0, testZookeeperAliveConnections=my-cluster-01c0acb0, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2, testKafkaTopicUnderReplicatedPartitions=my-cluster-fa981fe3, testSendMessagesTlsScramSha=my-cluster-a11b9173, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc, testZookeeperQuorumSize=my-cluster-5a26ca2a, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1, testUpdateUser=my-cluster-75998b92, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8, testKafkaActiveControllers=my-cluster-948e4115, testReceiveSimpleMessageTls=my-cluster-90021a9f, testKafkaBrokersCount=my-cluster-e91a8cef, testUserOperatorMetrics=my-cluster-7692a28e, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e}
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-323988616-1355569855, testKafkaConnectIoNetwork=my-user-1092781470-1895809353, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-695287754-1004738297, testSendMessagesCustomListenerTlsScramSha=my-user-1653975062-1727371851, testCruiseControlBasicAPIRequests=my-user-918568860-36694764, testZookeeperWatchersCount=my-user-1252011538-504314473, testKafkaTopicPartitions=my-user-394346586-105422980, testMirrorMaker2Metrics=my-user-926413343-1872437319, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1337805046-1917759354, testZookeeperAliveConnections=my-user-704084348-2034238998, testAutoRenewAllCaCertsTriggeredByAnno=my-user-308582398-741623568, testKafkaTopicUnderReplicatedPartitions=my-user-1487552272-1931929933, testSendMessagesTlsScramSha=my-user-1791509549-1350003609, testMirrorMaker2TlsAndTlsClientAuth=my-user-711655589-495294801, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-83332781-1273282344, testRackAwareConnectCorrectDeployment=my-user-1839832768-1329691231, testZookeeperQuorumSize=my-user-1387929819-2059775559, testKafkaAndZookeeperScaleUpScaleDown=my-user-1975958319-512406578, testUpdateUser=my-user-275427717-1183444028, testKafkaInDifferentNsThanClusterOperator=my-user-908767181-1543724508, testKafkaActiveControllers=my-user-971800472-962265772, testReceiveSimpleMessageTls=my-user-491397521-1196361913, testKafkaBrokersCount=my-user-699069229-1729992055, testUserOperatorMetrics=my-user-92263365-1802597441, testMirrorMakerTlsAuthenticated=my-user-1552986594-1641210083}
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-1040221322-670398147, testKafkaConnectIoNetwork=my-topic-2145871889-1295201610, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1609923788-1785060017, testSendMessagesCustomListenerTlsScramSha=my-topic-629448478-708445494, testCruiseControlBasicAPIRequests=my-topic-710218582-658397020, testZookeeperWatchersCount=my-topic-1225869180-847659507, testKafkaTopicPartitions=my-topic-1086071779-2133194430, testMirrorMaker2Metrics=my-topic-1779646810-1685449566, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-604305255-1585536778, testZookeeperAliveConnections=my-topic-1222053841-372865493, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1728678022-515015095, testKafkaTopicUnderReplicatedPartitions=my-topic-893837582-953830717, testSendMessagesTlsScramSha=my-topic-1482193652-649135537, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1152366002-738531989, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1536476030-1482182833, testRackAwareConnectCorrectDeployment=my-topic-1526242410-482838543, testZookeeperQuorumSize=my-topic-1242495538-1808467385, testKafkaAndZookeeperScaleUpScaleDown=my-topic-1496158521-938246156, testUpdateUser=my-topic-275186280-765877519, testKafkaInDifferentNsThanClusterOperator=my-topic-2051437288-2087424913, testKafkaActiveControllers=my-topic-916932648-500952606, testReceiveSimpleMessageTls=my-topic-941665209-436577257, testKafkaBrokersCount=my-topic-708967095-808914391, testUserOperatorMetrics=my-topic-1414123631-687906301, testMirrorMakerTlsAuthenticated=my-topic-1936321612-505632869}
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testKafkaConnectIoNetwork=my-cluster-580bbc7b-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-6da231a3-kafka-clients, testZookeeperWatchersCount=my-cluster-a66ecd5e-kafka-clients, testKafkaTopicPartitions=my-cluster-1625722f-kafka-clients, testMirrorMaker2Metrics=my-cluster-a4258561-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0-kafka-clients, testZookeeperAliveConnections=my-cluster-01c0acb0-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-fa981fe3-kafka-clients, testSendMessagesTlsScramSha=my-cluster-a11b9173-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc-kafka-clients, testZookeeperQuorumSize=my-cluster-5a26ca2a-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8-kafka-clients, testKafkaActiveControllers=my-cluster-948e4115-kafka-clients, testReceiveSimpleMessageTls=my-cluster-90021a9f-kafka-clients, testKafkaBrokersCount=my-cluster-e91a8cef-kafka-clients, testUserOperatorMetrics=my-cluster-7692a28e-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e-kafka-clients}
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 18:01:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-s25n7 -n infra-namespace -- curl 172.17.0.23:9404
2022-03-30 18:01:33 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.24 from Pod infra-namespace-kafka-clients-748578f786-s25n7 finished with return code: 0
2022-03-30 18:01:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 18:01:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 18:01:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 18:01:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:346] In context testKafkaConnectIoNetwork is everything deleted.
2022-03-30 18:01:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 18:01:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:267] testKafkaConnectIoNetwork - Notifies waiting test cases:[testUpdateUser, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testCruiseControlBasicAPIRequests, testSendMessagesTlsScramSha, testSendMessagesCustomListenerTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMaker2TlsAndTlsClientAuth, testMirrorMakerTlsAuthenticated, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaConnectIoNetwork, testKafkaBrokersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics] to and randomly select one to start execution
2022-03-30 18:01:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaConnectIoNetwork
2022-03-30 18:01:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 1
2022-03-30 18:01:33 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaConnectIoNetwork-FINISHED
2022-03-30 18:01:33 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 18:01:33 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 18:01:33 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaBridgeMetrics-STARTED
2022-03-30 18:01:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 18:01:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 18:01:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaBridgeMetrics
2022-03-30 18:01:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 2
2022-03-30 18:01:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:230] testKafkaBridgeMetrics test now can proceed its execution
2022-03-30 18:01:33 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 18:01:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-128d9a40, testKafkaConnectIoNetwork=my-cluster-580bbc7b, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e, testCruiseControlBasicAPIRequests=my-cluster-6da231a3, testZookeeperWatchersCount=my-cluster-a66ecd5e, testKafkaTopicPartitions=my-cluster-1625722f, testMirrorMaker2Metrics=my-cluster-a4258561, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0, testZookeeperAliveConnections=my-cluster-01c0acb0, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2, testKafkaTopicUnderReplicatedPartitions=my-cluster-fa981fe3, testSendMessagesTlsScramSha=my-cluster-a11b9173, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc, testZookeeperQuorumSize=my-cluster-5a26ca2a, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1, testKafkaBridgeMetrics=my-cluster-0eb8fc89, testUpdateUser=my-cluster-75998b92, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8, testKafkaActiveControllers=my-cluster-948e4115, testReceiveSimpleMessageTls=my-cluster-90021a9f, testKafkaBrokersCount=my-cluster-e91a8cef, testUserOperatorMetrics=my-cluster-7692a28e, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e}
2022-03-30 18:01:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-323988616-1355569855, testKafkaConnectIoNetwork=my-user-1092781470-1895809353, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-695287754-1004738297, testSendMessagesCustomListenerTlsScramSha=my-user-1653975062-1727371851, testCruiseControlBasicAPIRequests=my-user-918568860-36694764, testZookeeperWatchersCount=my-user-1252011538-504314473, testKafkaTopicPartitions=my-user-394346586-105422980, testMirrorMaker2Metrics=my-user-926413343-1872437319, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1337805046-1917759354, testZookeeperAliveConnections=my-user-704084348-2034238998, testAutoRenewAllCaCertsTriggeredByAnno=my-user-308582398-741623568, testKafkaTopicUnderReplicatedPartitions=my-user-1487552272-1931929933, testSendMessagesTlsScramSha=my-user-1791509549-1350003609, testMirrorMaker2TlsAndTlsClientAuth=my-user-711655589-495294801, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-83332781-1273282344, testRackAwareConnectCorrectDeployment=my-user-1839832768-1329691231, testZookeeperQuorumSize=my-user-1387929819-2059775559, testKafkaAndZookeeperScaleUpScaleDown=my-user-1975958319-512406578, testKafkaBridgeMetrics=my-user-1215630877-369211451, testUpdateUser=my-user-275427717-1183444028, testKafkaInDifferentNsThanClusterOperator=my-user-908767181-1543724508, testKafkaActiveControllers=my-user-971800472-962265772, testReceiveSimpleMessageTls=my-user-491397521-1196361913, testKafkaBrokersCount=my-user-699069229-1729992055, testUserOperatorMetrics=my-user-92263365-1802597441, testMirrorMakerTlsAuthenticated=my-user-1552986594-1641210083}
2022-03-30 18:01:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-1040221322-670398147, testKafkaConnectIoNetwork=my-topic-2145871889-1295201610, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1609923788-1785060017, testSendMessagesCustomListenerTlsScramSha=my-topic-629448478-708445494, testCruiseControlBasicAPIRequests=my-topic-710218582-658397020, testZookeeperWatchersCount=my-topic-1225869180-847659507, testKafkaTopicPartitions=my-topic-1086071779-2133194430, testMirrorMaker2Metrics=my-topic-1779646810-1685449566, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-604305255-1585536778, testZookeeperAliveConnections=my-topic-1222053841-372865493, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1728678022-515015095, testKafkaTopicUnderReplicatedPartitions=my-topic-893837582-953830717, testSendMessagesTlsScramSha=my-topic-1482193652-649135537, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1152366002-738531989, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1536476030-1482182833, testRackAwareConnectCorrectDeployment=my-topic-1526242410-482838543, testZookeeperQuorumSize=my-topic-1242495538-1808467385, testKafkaAndZookeeperScaleUpScaleDown=my-topic-1496158521-938246156, testKafkaBridgeMetrics=my-topic-90145110-388295797, testUpdateUser=my-topic-275186280-765877519, testKafkaInDifferentNsThanClusterOperator=my-topic-2051437288-2087424913, testKafkaActiveControllers=my-topic-916932648-500952606, testReceiveSimpleMessageTls=my-topic-941665209-436577257, testKafkaBrokersCount=my-topic-708967095-808914391, testUserOperatorMetrics=my-topic-1414123631-687906301, testMirrorMakerTlsAuthenticated=my-topic-1936321612-505632869}
2022-03-30 18:01:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testKafkaConnectIoNetwork=my-cluster-580bbc7b-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-6da231a3-kafka-clients, testZookeeperWatchersCount=my-cluster-a66ecd5e-kafka-clients, testKafkaTopicPartitions=my-cluster-1625722f-kafka-clients, testMirrorMaker2Metrics=my-cluster-a4258561-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0-kafka-clients, testZookeeperAliveConnections=my-cluster-01c0acb0-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-fa981fe3-kafka-clients, testSendMessagesTlsScramSha=my-cluster-a11b9173-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc-kafka-clients, testZookeeperQuorumSize=my-cluster-5a26ca2a-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1-kafka-clients, testKafkaBridgeMetrics=my-cluster-0eb8fc89-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8-kafka-clients, testKafkaActiveControllers=my-cluster-948e4115-kafka-clients, testReceiveSimpleMessageTls=my-cluster-90021a9f-kafka-clients, testKafkaBrokersCount=my-cluster-e91a8cef-kafka-clients, testUserOperatorMetrics=my-cluster-7692a28e-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e-kafka-clients}
2022-03-30 18:01:33 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-30 18:01:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job bridge-producer in namespace infra-namespace
2022-03-30 18:01:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:bridge-producer
2022-03-30 18:01:33 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: bridge-producer will be in active state
2022-03-30 18:01:33 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for job active
2022-03-30 18:01:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] job active not ready, will try again in 1000 ms (179992ms till timeout)
2022-03-30 18:01:34 [ForkJoinPool-3-worker-5] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.23 from Pod infra-namespace-kafka-clients-748578f786-s25n7 finished with return code: 0
2022-03-30 18:01:34 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 18:01:34 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaMetricsSettings-STARTED
2022-03-30 18:01:34 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 18:01:34 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 18:01:34 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaMetricsSettings
2022-03-30 18:01:34 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 3
2022-03-30 18:01:34 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:205] [testKafkaMetricsSettings] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:01:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:210] testKafkaMetricsSettings is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:01:34 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 18:01:34 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 18:01:34 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 18:01:34 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:346] In context testMirrorMaker2Metrics is everything deleted.
2022-03-30 18:01:34 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 18:01:34 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:267] testMirrorMaker2Metrics - Notifies waiting test cases:[testUpdateUser, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testCruiseControlBasicAPIRequests, testSendMessagesTlsScramSha, testSendMessagesCustomListenerTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMaker2TlsAndTlsClientAuth, testMirrorMakerTlsAuthenticated, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaConnectIoNetwork, testKafkaBrokersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics, testKafkaMetricsSettings] to and randomly select one to start execution
2022-03-30 18:01:34 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testMirrorMaker2Metrics
2022-03-30 18:01:34 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 2
2022-03-30 18:01:34 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testMirrorMaker2Metrics-FINISHED
2022-03-30 18:01:34 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 18:01:34 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 18:01:34 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testClusterOperatorMetrics-STARTED
2022-03-30 18:01:34 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 18:01:34 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 18:01:34 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testClusterOperatorMetrics
2022-03-30 18:01:34 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 3
2022-03-30 18:01:34 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:205] [testClusterOperatorMetrics] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:01:34 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:210] testClusterOperatorMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:01:34 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job bridge-consumer in namespace infra-namespace
2022-03-30 18:01:34 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:bridge-consumer
2022-03-30 18:01:34 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: bridge-consumer will be in active state
2022-03-30 18:01:34 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for job active
2022-03-30 18:01:34 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] job active not ready, will try again in 1000 ms (179984ms till timeout)
2022-03-30 18:01:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for KafkaProducer metrics will be available
2022-03-30 18:01:35 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsIsolatedST:420] Looking for 'strimzi_bridge_kafka_producer_count' in bridge metrics
2022-03-30 18:01:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 18:01:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-s25n7 -n infra-namespace -- curl 172.17.0.22:8080/metrics
2022-03-30 18:01:35 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.22 from Pod infra-namespace-kafka-clients-748578f786-s25n7 finished with return code: 0
2022-03-30 18:01:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] KafkaProducer metrics will be available not ready, will try again in 1000 ms (299573ms till timeout)
2022-03-30 18:01:36 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsIsolatedST:420] Looking for 'strimzi_bridge_kafka_producer_count' in bridge metrics
2022-03-30 18:01:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 18:01:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-s25n7 -n infra-namespace -- curl 172.17.0.22:8080/metrics
2022-03-30 18:01:37 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.22 from Pod infra-namespace-kafka-clients-748578f786-s25n7 finished with return code: 0
2022-03-30 18:01:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for KafkaConsumer metrics will be available
2022-03-30 18:01:37 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsIsolatedST:428] Looking for 'strimzi_bridge_kafka_consumer_connection_count' in bridge metrics
2022-03-30 18:01:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 18:01:37 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-s25n7 -n infra-namespace -- curl 172.17.0.22:8080/metrics
2022-03-30 18:01:37 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.22 from Pod infra-namespace-kafka-clients-748578f786-s25n7 finished with return code: 0
2022-03-30 18:01:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 18:01:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 18:01:37 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 18:01:37 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaBridgeMetrics
2022-03-30 18:01:37 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job bridge-consumer in namespace infra-namespace
2022-03-30 18:01:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:bridge-consumer
2022-03-30 18:01:37 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job bridge-producer in namespace infra-namespace
2022-03-30 18:01:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:bridge-producer
2022-03-30 18:01:37 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 18:01:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:267] testKafkaBridgeMetrics - Notifies waiting test cases:[testUpdateUser, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testCruiseControlBasicAPIRequests, testSendMessagesTlsScramSha, testSendMessagesCustomListenerTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMaker2TlsAndTlsClientAuth, testMirrorMakerTlsAuthenticated, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaConnectIoNetwork, testKafkaBrokersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics, testKafkaMetricsSettings, testClusterOperatorMetrics] to and randomly select one to start execution
2022-03-30 18:01:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaBridgeMetrics
2022-03-30 18:01:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 2
2022-03-30 18:01:37 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaBridgeMetrics-FINISHED
2022-03-30 18:01:37 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 18:01:37 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 18:01:37 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testCruiseControlMetrics-STARTED
2022-03-30 18:01:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 18:01:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 18:01:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testCruiseControlMetrics
2022-03-30 18:01:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 3
2022-03-30 18:01:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:205] [testCruiseControlMetrics] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:01:37 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:210] testCruiseControlMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:01:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:230] testKafkaMetricsSettings test now can proceed its execution
2022-03-30 18:01:39 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 18:01:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-128d9a40, testKafkaConnectIoNetwork=my-cluster-580bbc7b, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e, testCruiseControlBasicAPIRequests=my-cluster-6da231a3, testZookeeperWatchersCount=my-cluster-a66ecd5e, testKafkaTopicPartitions=my-cluster-1625722f, testMirrorMaker2Metrics=my-cluster-a4258561, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0, testZookeeperAliveConnections=my-cluster-01c0acb0, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2, testKafkaTopicUnderReplicatedPartitions=my-cluster-fa981fe3, testSendMessagesTlsScramSha=my-cluster-a11b9173, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc, testZookeeperQuorumSize=my-cluster-5a26ca2a, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1, testKafkaBridgeMetrics=my-cluster-0eb8fc89, testUpdateUser=my-cluster-75998b92, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8, testKafkaActiveControllers=my-cluster-948e4115, testReceiveSimpleMessageTls=my-cluster-90021a9f, testKafkaBrokersCount=my-cluster-e91a8cef, testUserOperatorMetrics=my-cluster-7692a28e, testKafkaMetricsSettings=my-cluster-f556a04e, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e}
2022-03-30 18:01:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-323988616-1355569855, testKafkaConnectIoNetwork=my-user-1092781470-1895809353, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-695287754-1004738297, testSendMessagesCustomListenerTlsScramSha=my-user-1653975062-1727371851, testCruiseControlBasicAPIRequests=my-user-918568860-36694764, testZookeeperWatchersCount=my-user-1252011538-504314473, testKafkaTopicPartitions=my-user-394346586-105422980, testMirrorMaker2Metrics=my-user-926413343-1872437319, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1337805046-1917759354, testZookeeperAliveConnections=my-user-704084348-2034238998, testAutoRenewAllCaCertsTriggeredByAnno=my-user-308582398-741623568, testKafkaTopicUnderReplicatedPartitions=my-user-1487552272-1931929933, testSendMessagesTlsScramSha=my-user-1791509549-1350003609, testMirrorMaker2TlsAndTlsClientAuth=my-user-711655589-495294801, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-83332781-1273282344, testRackAwareConnectCorrectDeployment=my-user-1839832768-1329691231, testZookeeperQuorumSize=my-user-1387929819-2059775559, testKafkaAndZookeeperScaleUpScaleDown=my-user-1975958319-512406578, testKafkaBridgeMetrics=my-user-1215630877-369211451, testUpdateUser=my-user-275427717-1183444028, testKafkaInDifferentNsThanClusterOperator=my-user-908767181-1543724508, testKafkaActiveControllers=my-user-971800472-962265772, testReceiveSimpleMessageTls=my-user-491397521-1196361913, testKafkaBrokersCount=my-user-699069229-1729992055, testUserOperatorMetrics=my-user-92263365-1802597441, testKafkaMetricsSettings=my-user-10583859-1000138651, testMirrorMakerTlsAuthenticated=my-user-1552986594-1641210083}
2022-03-30 18:01:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-1040221322-670398147, testKafkaConnectIoNetwork=my-topic-2145871889-1295201610, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1609923788-1785060017, testSendMessagesCustomListenerTlsScramSha=my-topic-629448478-708445494, testCruiseControlBasicAPIRequests=my-topic-710218582-658397020, testZookeeperWatchersCount=my-topic-1225869180-847659507, testKafkaTopicPartitions=my-topic-1086071779-2133194430, testMirrorMaker2Metrics=my-topic-1779646810-1685449566, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-604305255-1585536778, testZookeeperAliveConnections=my-topic-1222053841-372865493, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1728678022-515015095, testKafkaTopicUnderReplicatedPartitions=my-topic-893837582-953830717, testSendMessagesTlsScramSha=my-topic-1482193652-649135537, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1152366002-738531989, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1536476030-1482182833, testRackAwareConnectCorrectDeployment=my-topic-1526242410-482838543, testZookeeperQuorumSize=my-topic-1242495538-1808467385, testKafkaAndZookeeperScaleUpScaleDown=my-topic-1496158521-938246156, testKafkaBridgeMetrics=my-topic-90145110-388295797, testUpdateUser=my-topic-275186280-765877519, testKafkaInDifferentNsThanClusterOperator=my-topic-2051437288-2087424913, testKafkaActiveControllers=my-topic-916932648-500952606, testReceiveSimpleMessageTls=my-topic-941665209-436577257, testKafkaBrokersCount=my-topic-708967095-808914391, testUserOperatorMetrics=my-topic-1414123631-687906301, testKafkaMetricsSettings=my-topic-1375496082-553518335, testMirrorMakerTlsAuthenticated=my-topic-1936321612-505632869}
2022-03-30 18:01:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testKafkaConnectIoNetwork=my-cluster-580bbc7b-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-6da231a3-kafka-clients, testZookeeperWatchersCount=my-cluster-a66ecd5e-kafka-clients, testKafkaTopicPartitions=my-cluster-1625722f-kafka-clients, testMirrorMaker2Metrics=my-cluster-a4258561-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0-kafka-clients, testZookeeperAliveConnections=my-cluster-01c0acb0-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-fa981fe3-kafka-clients, testSendMessagesTlsScramSha=my-cluster-a11b9173-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc-kafka-clients, testZookeeperQuorumSize=my-cluster-5a26ca2a-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1-kafka-clients, testKafkaBridgeMetrics=my-cluster-0eb8fc89-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8-kafka-clients, testKafkaActiveControllers=my-cluster-948e4115-kafka-clients, testReceiveSimpleMessageTls=my-cluster-90021a9f-kafka-clients, testKafkaBrokersCount=my-cluster-e91a8cef-kafka-clients, testUserOperatorMetrics=my-cluster-7692a28e-kafka-clients, testKafkaMetricsSettings=my-cluster-f556a04e-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e-kafka-clients}
2022-03-30 18:01:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:210] testClusterOperatorMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:01:39 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: second-kafka-cluster are stable
2022-03-30 18:01:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for pod with prefixsecond-kafka-cluster is present.
2022-03-30 18:01:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Pods stability in phase Running
2022-03-30 18:01:39 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 50
2022-03-30 18:01:39 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 50
2022-03-30 18:01:39 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 50
2022-03-30 18:01:39 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 50
2022-03-30 18:01:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (299910ms till timeout)
2022-03-30 18:01:40 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 49
2022-03-30 18:01:40 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 49
2022-03-30 18:01:40 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 49
2022-03-30 18:01:40 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 49
2022-03-30 18:01:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (298900ms till timeout)
2022-03-30 18:01:41 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 48
2022-03-30 18:01:41 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 48
2022-03-30 18:01:41 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 48
2022-03-30 18:01:41 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 48
2022-03-30 18:01:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (297890ms till timeout)
2022-03-30 18:01:42 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 47
2022-03-30 18:01:42 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 47
2022-03-30 18:01:42 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 47
2022-03-30 18:01:42 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 47
2022-03-30 18:01:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (296881ms till timeout)
2022-03-30 18:01:42 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:210] testCruiseControlMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:01:43 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 46
2022-03-30 18:01:43 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 46
2022-03-30 18:01:43 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 46
2022-03-30 18:01:43 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 46
2022-03-30 18:01:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (295871ms till timeout)
2022-03-30 18:01:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:210] testClusterOperatorMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:01:44 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 45
2022-03-30 18:01:44 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 45
2022-03-30 18:01:44 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 45
2022-03-30 18:01:44 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 45
2022-03-30 18:01:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (294861ms till timeout)
2022-03-30 18:01:45 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 44
2022-03-30 18:01:45 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 44
2022-03-30 18:01:45 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 44
2022-03-30 18:01:45 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 44
2022-03-30 18:01:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (293852ms till timeout)
2022-03-30 18:01:46 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 43
2022-03-30 18:01:46 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 43
2022-03-30 18:01:46 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 43
2022-03-30 18:01:46 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 43
2022-03-30 18:01:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (292843ms till timeout)
2022-03-30 18:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 42
2022-03-30 18:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 42
2022-03-30 18:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 42
2022-03-30 18:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 42
2022-03-30 18:01:47 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (291833ms till timeout)
2022-03-30 18:01:47 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:210] testCruiseControlMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:01:48 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 41
2022-03-30 18:01:48 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 41
2022-03-30 18:01:48 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 41
2022-03-30 18:01:48 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 41
2022-03-30 18:01:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (290824ms till timeout)
2022-03-30 18:01:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:210] testClusterOperatorMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:01:49 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 40
2022-03-30 18:01:49 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 40
2022-03-30 18:01:49 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 40
2022-03-30 18:01:49 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 40
2022-03-30 18:01:49 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (289814ms till timeout)
2022-03-30 18:01:50 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 39
2022-03-30 18:01:50 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 39
2022-03-30 18:01:50 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 39
2022-03-30 18:01:50 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 39
2022-03-30 18:01:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (288801ms till timeout)
2022-03-30 18:01:51 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 38
2022-03-30 18:01:51 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 38
2022-03-30 18:01:51 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 38
2022-03-30 18:01:51 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 38
2022-03-30 18:01:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (287792ms till timeout)
2022-03-30 18:01:52 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 37
2022-03-30 18:01:52 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 37
2022-03-30 18:01:52 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 37
2022-03-30 18:01:52 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 37
2022-03-30 18:01:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (286783ms till timeout)
2022-03-30 18:01:52 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:210] testCruiseControlMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:01:53 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 36
2022-03-30 18:01:53 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 36
2022-03-30 18:01:53 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 36
2022-03-30 18:01:53 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 36
2022-03-30 18:01:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (285774ms till timeout)
2022-03-30 18:01:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:210] testClusterOperatorMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:01:54 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 35
2022-03-30 18:01:54 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 35
2022-03-30 18:01:54 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 35
2022-03-30 18:01:54 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 35
2022-03-30 18:01:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (284765ms till timeout)
2022-03-30 18:01:55 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 34
2022-03-30 18:01:55 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 34
2022-03-30 18:01:55 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 34
2022-03-30 18:01:55 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 34
2022-03-30 18:01:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (283756ms till timeout)
2022-03-30 18:01:56 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 33
2022-03-30 18:01:56 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 33
2022-03-30 18:01:56 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 33
2022-03-30 18:01:56 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 33
2022-03-30 18:01:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (282746ms till timeout)
2022-03-30 18:01:57 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 32
2022-03-30 18:01:57 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 32
2022-03-30 18:01:57 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 32
2022-03-30 18:01:57 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 32
2022-03-30 18:01:57 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (281737ms till timeout)
2022-03-30 18:01:57 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:210] testCruiseControlMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:01:58 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 31
2022-03-30 18:01:58 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 31
2022-03-30 18:01:58 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 31
2022-03-30 18:01:58 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 31
2022-03-30 18:01:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (280727ms till timeout)
2022-03-30 18:01:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:210] testClusterOperatorMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:01:59 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 30
2022-03-30 18:01:59 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 30
2022-03-30 18:01:59 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 30
2022-03-30 18:01:59 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 30
2022-03-30 18:01:59 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (279718ms till timeout)
2022-03-30 18:02:00 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 29
2022-03-30 18:02:00 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 29
2022-03-30 18:02:00 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 29
2022-03-30 18:02:00 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 29
2022-03-30 18:02:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (278708ms till timeout)
2022-03-30 18:02:01 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 28
2022-03-30 18:02:01 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 28
2022-03-30 18:02:01 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 28
2022-03-30 18:02:01 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 28
2022-03-30 18:02:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (277699ms till timeout)
2022-03-30 18:02:02 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 27
2022-03-30 18:02:02 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 27
2022-03-30 18:02:02 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 27
2022-03-30 18:02:02 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 27
2022-03-30 18:02:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (276690ms till timeout)
2022-03-30 18:02:02 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:210] testCruiseControlMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:02:03 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 26
2022-03-30 18:02:03 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 26
2022-03-30 18:02:03 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 26
2022-03-30 18:02:03 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 26
2022-03-30 18:02:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (275681ms till timeout)
2022-03-30 18:02:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:210] testClusterOperatorMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:02:04 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 25
2022-03-30 18:02:04 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 25
2022-03-30 18:02:04 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 25
2022-03-30 18:02:04 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 25
2022-03-30 18:02:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (274672ms till timeout)
2022-03-30 18:02:05 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 24
2022-03-30 18:02:05 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 24
2022-03-30 18:02:05 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 24
2022-03-30 18:02:05 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 24
2022-03-30 18:02:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (273663ms till timeout)
2022-03-30 18:02:06 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 23
2022-03-30 18:02:06 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 23
2022-03-30 18:02:06 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 23
2022-03-30 18:02:06 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 23
2022-03-30 18:02:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (272653ms till timeout)
2022-03-30 18:02:07 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 22
2022-03-30 18:02:07 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 22
2022-03-30 18:02:07 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 22
2022-03-30 18:02:07 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 22
2022-03-30 18:02:07 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (271644ms till timeout)
2022-03-30 18:02:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:210] testCruiseControlMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:02:08 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 21
2022-03-30 18:02:08 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 21
2022-03-30 18:02:08 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 21
2022-03-30 18:02:08 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 21
2022-03-30 18:02:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (270634ms till timeout)
2022-03-30 18:02:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:210] testClusterOperatorMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:02:09 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 20
2022-03-30 18:02:09 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 20
2022-03-30 18:02:09 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 20
2022-03-30 18:02:09 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 20
2022-03-30 18:02:09 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (269625ms till timeout)
2022-03-30 18:02:10 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 19
2022-03-30 18:02:10 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 19
2022-03-30 18:02:10 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 19
2022-03-30 18:02:10 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 19
2022-03-30 18:02:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (268616ms till timeout)
2022-03-30 18:02:11 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 18
2022-03-30 18:02:11 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 18
2022-03-30 18:02:11 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 18
2022-03-30 18:02:11 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 18
2022-03-30 18:02:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (267607ms till timeout)
2022-03-30 18:02:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:210] testCruiseControlMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:02:12 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 17
2022-03-30 18:02:12 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 17
2022-03-30 18:02:12 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 17
2022-03-30 18:02:12 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 17
2022-03-30 18:02:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (266598ms till timeout)
2022-03-30 18:02:13 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 16
2022-03-30 18:02:13 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 16
2022-03-30 18:02:13 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 16
2022-03-30 18:02:13 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 16
2022-03-30 18:02:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (265589ms till timeout)
2022-03-30 18:02:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:210] testClusterOperatorMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:02:14 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 15
2022-03-30 18:02:14 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 15
2022-03-30 18:02:14 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 15
2022-03-30 18:02:14 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 15
2022-03-30 18:02:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (264579ms till timeout)
2022-03-30 18:02:15 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 14
2022-03-30 18:02:15 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 14
2022-03-30 18:02:15 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 14
2022-03-30 18:02:15 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 14
2022-03-30 18:02:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (263570ms till timeout)
2022-03-30 18:02:16 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 13
2022-03-30 18:02:16 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 13
2022-03-30 18:02:16 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 13
2022-03-30 18:02:16 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 13
2022-03-30 18:02:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (262561ms till timeout)
2022-03-30 18:02:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:210] testCruiseControlMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:02:17 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 12
2022-03-30 18:02:17 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 12
2022-03-30 18:02:17 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 12
2022-03-30 18:02:17 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 12
2022-03-30 18:02:17 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (261551ms till timeout)
2022-03-30 18:02:18 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 11
2022-03-30 18:02:18 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 11
2022-03-30 18:02:18 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 11
2022-03-30 18:02:18 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 11
2022-03-30 18:02:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (260542ms till timeout)
2022-03-30 18:02:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:210] testClusterOperatorMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:02:19 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 10
2022-03-30 18:02:19 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 10
2022-03-30 18:02:19 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 10
2022-03-30 18:02:19 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 10
2022-03-30 18:02:19 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (259533ms till timeout)
2022-03-30 18:02:20 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 9
2022-03-30 18:02:20 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 9
2022-03-30 18:02:20 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 9
2022-03-30 18:02:20 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 9
2022-03-30 18:02:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (258523ms till timeout)
2022-03-30 18:02:21 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 8
2022-03-30 18:02:21 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 8
2022-03-30 18:02:21 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 8
2022-03-30 18:02:21 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 8
2022-03-30 18:02:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (257514ms till timeout)
2022-03-30 18:02:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:210] testCruiseControlMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:02:22 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 7
2022-03-30 18:02:22 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 7
2022-03-30 18:02:22 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 7
2022-03-30 18:02:22 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 7
2022-03-30 18:02:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (256504ms till timeout)
2022-03-30 18:02:23 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 6
2022-03-30 18:02:23 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 6
2022-03-30 18:02:23 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 6
2022-03-30 18:02:23 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 6
2022-03-30 18:02:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (255495ms till timeout)
2022-03-30 18:02:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:210] testClusterOperatorMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:02:24 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 5
2022-03-30 18:02:24 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 5
2022-03-30 18:02:24 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 5
2022-03-30 18:02:24 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 5
2022-03-30 18:02:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (254485ms till timeout)
2022-03-30 18:02:25 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 4
2022-03-30 18:02:25 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 4
2022-03-30 18:02:25 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 4
2022-03-30 18:02:25 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 4
2022-03-30 18:02:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (253476ms till timeout)
2022-03-30 18:02:26 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 3
2022-03-30 18:02:26 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 3
2022-03-30 18:02:26 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 3
2022-03-30 18:02:26 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 3
2022-03-30 18:02:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (252467ms till timeout)
2022-03-30 18:02:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:210] testCruiseControlMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:02:27 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 2
2022-03-30 18:02:27 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 2
2022-03-30 18:02:27 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 2
2022-03-30 18:02:27 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 2
2022-03-30 18:02:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (251457ms till timeout)
2022-03-30 18:02:28 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 1
2022-03-30 18:02:28 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 1
2022-03-30 18:02:28 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 1
2022-03-30 18:02:28 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 1
2022-03-30 18:02:28 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:335] All pods are stable second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd ,second-kafka-cluster-kafka-0 ,second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q ,second-kafka-cluster-zookeeper-0
2022-03-30 18:02:28 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: second-kafka-cluster are stable
2022-03-30 18:02:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for pod with prefixsecond-kafka-cluster is present.
2022-03-30 18:02:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Pods stability in phase Running
2022-03-30 18:02:28 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 50
2022-03-30 18:02:28 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 50
2022-03-30 18:02:28 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 50
2022-03-30 18:02:28 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 50
2022-03-30 18:02:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (299990ms till timeout)
2022-03-30 18:02:29 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:210] testClusterOperatorMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:02:29 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 49
2022-03-30 18:02:29 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 49
2022-03-30 18:02:29 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 49
2022-03-30 18:02:29 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 49
2022-03-30 18:02:29 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (298981ms till timeout)
2022-03-30 18:02:30 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 48
2022-03-30 18:02:30 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 48
2022-03-30 18:02:30 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 48
2022-03-30 18:02:30 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 48
2022-03-30 18:02:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (297972ms till timeout)
2022-03-30 18:02:31 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 47
2022-03-30 18:02:31 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 47
2022-03-30 18:02:31 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 47
2022-03-30 18:02:31 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 47
2022-03-30 18:02:31 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (296962ms till timeout)
2022-03-30 18:02:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:210] testCruiseControlMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:02:32 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 46
2022-03-30 18:02:32 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 46
2022-03-30 18:02:32 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 46
2022-03-30 18:02:32 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 46
2022-03-30 18:02:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (295953ms till timeout)
2022-03-30 18:02:33 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 45
2022-03-30 18:02:33 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 45
2022-03-30 18:02:33 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 45
2022-03-30 18:02:33 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 45
2022-03-30 18:02:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (294944ms till timeout)
2022-03-30 18:02:34 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:210] testClusterOperatorMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:02:34 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 44
2022-03-30 18:02:34 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 44
2022-03-30 18:02:34 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 44
2022-03-30 18:02:34 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 44
2022-03-30 18:02:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (293933ms till timeout)
2022-03-30 18:02:35 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 43
2022-03-30 18:02:35 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 43
2022-03-30 18:02:35 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 43
2022-03-30 18:02:35 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 43
2022-03-30 18:02:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (292923ms till timeout)
2022-03-30 18:02:36 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 42
2022-03-30 18:02:36 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 42
2022-03-30 18:02:36 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 42
2022-03-30 18:02:36 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 42
2022-03-30 18:02:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (291914ms till timeout)
2022-03-30 18:02:37 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:210] testCruiseControlMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:02:37 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 41
2022-03-30 18:02:37 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 41
2022-03-30 18:02:37 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 41
2022-03-30 18:02:37 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 41
2022-03-30 18:02:37 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (290905ms till timeout)
2022-03-30 18:02:38 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 40
2022-03-30 18:02:38 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 40
2022-03-30 18:02:38 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 40
2022-03-30 18:02:38 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 40
2022-03-30 18:02:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (289895ms till timeout)
2022-03-30 18:02:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:210] testClusterOperatorMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:02:39 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 39
2022-03-30 18:02:39 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 39
2022-03-30 18:02:39 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 39
2022-03-30 18:02:39 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 39
2022-03-30 18:02:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (288886ms till timeout)
2022-03-30 18:02:40 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 38
2022-03-30 18:02:40 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 38
2022-03-30 18:02:40 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 38
2022-03-30 18:02:40 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 38
2022-03-30 18:02:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (287876ms till timeout)
2022-03-30 18:02:41 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 37
2022-03-30 18:02:41 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 37
2022-03-30 18:02:41 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 37
2022-03-30 18:02:41 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 37
2022-03-30 18:02:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (286867ms till timeout)
2022-03-30 18:02:42 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:210] testCruiseControlMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:02:42 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 36
2022-03-30 18:02:42 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 36
2022-03-30 18:02:42 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 36
2022-03-30 18:02:42 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 36
2022-03-30 18:02:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (285857ms till timeout)
2022-03-30 18:02:43 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 35
2022-03-30 18:02:43 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 35
2022-03-30 18:02:43 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 35
2022-03-30 18:02:43 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 35
2022-03-30 18:02:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (284847ms till timeout)
2022-03-30 18:02:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:210] testClusterOperatorMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:02:44 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 34
2022-03-30 18:02:44 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 34
2022-03-30 18:02:44 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 34
2022-03-30 18:02:44 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 34
2022-03-30 18:02:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (283837ms till timeout)
2022-03-30 18:02:45 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 33
2022-03-30 18:02:45 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 33
2022-03-30 18:02:45 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 33
2022-03-30 18:02:45 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 33
2022-03-30 18:02:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (282827ms till timeout)
2022-03-30 18:02:46 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 32
2022-03-30 18:02:46 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 32
2022-03-30 18:02:46 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 32
2022-03-30 18:02:46 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 32
2022-03-30 18:02:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (281818ms till timeout)
2022-03-30 18:02:47 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:210] testCruiseControlMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:02:47 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 31
2022-03-30 18:02:47 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 31
2022-03-30 18:02:47 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 31
2022-03-30 18:02:47 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 31
2022-03-30 18:02:47 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (280809ms till timeout)
2022-03-30 18:02:48 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 30
2022-03-30 18:02:48 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 30
2022-03-30 18:02:48 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 30
2022-03-30 18:02:48 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 30
2022-03-30 18:02:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (279800ms till timeout)
2022-03-30 18:02:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:210] testClusterOperatorMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:02:49 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 29
2022-03-30 18:02:49 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 29
2022-03-30 18:02:49 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 29
2022-03-30 18:02:49 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 29
2022-03-30 18:02:49 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (278788ms till timeout)
2022-03-30 18:02:50 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 28
2022-03-30 18:02:50 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 28
2022-03-30 18:02:50 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 28
2022-03-30 18:02:50 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 28
2022-03-30 18:02:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (277779ms till timeout)
2022-03-30 18:02:51 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 27
2022-03-30 18:02:51 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 27
2022-03-30 18:02:51 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 27
2022-03-30 18:02:51 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 27
2022-03-30 18:02:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (276729ms till timeout)
2022-03-30 18:02:52 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:210] testCruiseControlMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:02:52 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 26
2022-03-30 18:02:52 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 26
2022-03-30 18:02:52 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 26
2022-03-30 18:02:52 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 26
2022-03-30 18:02:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (275720ms till timeout)
2022-03-30 18:02:53 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 25
2022-03-30 18:02:53 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 25
2022-03-30 18:02:53 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 25
2022-03-30 18:02:53 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 25
2022-03-30 18:02:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (274711ms till timeout)
2022-03-30 18:02:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:210] testClusterOperatorMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:02:54 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 24
2022-03-30 18:02:54 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 24
2022-03-30 18:02:54 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 24
2022-03-30 18:02:54 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 24
2022-03-30 18:02:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (273701ms till timeout)
2022-03-30 18:02:55 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 23
2022-03-30 18:02:55 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 23
2022-03-30 18:02:55 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 23
2022-03-30 18:02:55 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 23
2022-03-30 18:02:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (272692ms till timeout)
2022-03-30 18:02:56 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 22
2022-03-30 18:02:56 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 22
2022-03-30 18:02:56 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 22
2022-03-30 18:02:56 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 22
2022-03-30 18:02:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (271683ms till timeout)
2022-03-30 18:02:57 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:210] testCruiseControlMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:02:57 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 21
2022-03-30 18:02:57 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 21
2022-03-30 18:02:57 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 21
2022-03-30 18:02:57 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 21
2022-03-30 18:02:57 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (270674ms till timeout)
2022-03-30 18:02:59 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 20
2022-03-30 18:02:59 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 20
2022-03-30 18:02:59 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 20
2022-03-30 18:02:59 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 20
2022-03-30 18:02:59 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (269664ms till timeout)
2022-03-30 18:02:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:210] testClusterOperatorMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:03:00 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 19
2022-03-30 18:03:00 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 19
2022-03-30 18:03:00 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 19
2022-03-30 18:03:00 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 19
2022-03-30 18:03:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (268655ms till timeout)
2022-03-30 18:03:01 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 18
2022-03-30 18:03:01 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 18
2022-03-30 18:03:01 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 18
2022-03-30 18:03:01 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 18
2022-03-30 18:03:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (267646ms till timeout)
2022-03-30 18:03:02 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 17
2022-03-30 18:03:02 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 17
2022-03-30 18:03:02 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 17
2022-03-30 18:03:02 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 17
2022-03-30 18:03:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (266636ms till timeout)
2022-03-30 18:03:02 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:210] testCruiseControlMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:03:03 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 16
2022-03-30 18:03:03 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 16
2022-03-30 18:03:03 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 16
2022-03-30 18:03:03 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 16
2022-03-30 18:03:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (265627ms till timeout)
2022-03-30 18:03:04 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 15
2022-03-30 18:03:04 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 15
2022-03-30 18:03:04 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 15
2022-03-30 18:03:04 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 15
2022-03-30 18:03:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (264618ms till timeout)
2022-03-30 18:03:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:210] testClusterOperatorMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:03:05 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 14
2022-03-30 18:03:05 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 14
2022-03-30 18:03:05 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 14
2022-03-30 18:03:05 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 14
2022-03-30 18:03:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (263609ms till timeout)
2022-03-30 18:03:06 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 13
2022-03-30 18:03:06 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 13
2022-03-30 18:03:06 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 13
2022-03-30 18:03:06 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 13
2022-03-30 18:03:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (262599ms till timeout)
2022-03-30 18:03:07 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 12
2022-03-30 18:03:07 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 12
2022-03-30 18:03:07 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 12
2022-03-30 18:03:07 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 12
2022-03-30 18:03:07 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (261548ms till timeout)
2022-03-30 18:03:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:210] testCruiseControlMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:03:08 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 11
2022-03-30 18:03:08 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 11
2022-03-30 18:03:08 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 11
2022-03-30 18:03:08 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 11
2022-03-30 18:03:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (260539ms till timeout)
2022-03-30 18:03:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:210] testClusterOperatorMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:03:09 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 10
2022-03-30 18:03:09 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 10
2022-03-30 18:03:09 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 10
2022-03-30 18:03:09 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 10
2022-03-30 18:03:09 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (259530ms till timeout)
2022-03-30 18:03:10 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 9
2022-03-30 18:03:10 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 9
2022-03-30 18:03:10 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 9
2022-03-30 18:03:10 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 9
2022-03-30 18:03:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (258520ms till timeout)
2022-03-30 18:03:11 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 8
2022-03-30 18:03:11 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 8
2022-03-30 18:03:11 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 8
2022-03-30 18:03:11 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 8
2022-03-30 18:03:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (257511ms till timeout)
2022-03-30 18:03:12 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 7
2022-03-30 18:03:12 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 7
2022-03-30 18:03:12 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 7
2022-03-30 18:03:12 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 7
2022-03-30 18:03:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (256502ms till timeout)
2022-03-30 18:03:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:210] testCruiseControlMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:03:13 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 6
2022-03-30 18:03:13 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 6
2022-03-30 18:03:13 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 6
2022-03-30 18:03:13 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 6
2022-03-30 18:03:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (255492ms till timeout)
2022-03-30 18:03:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:210] testClusterOperatorMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:03:14 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 5
2022-03-30 18:03:14 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 5
2022-03-30 18:03:14 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 5
2022-03-30 18:03:14 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 5
2022-03-30 18:03:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (254483ms till timeout)
2022-03-30 18:03:15 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 4
2022-03-30 18:03:15 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 4
2022-03-30 18:03:15 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 4
2022-03-30 18:03:15 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 4
2022-03-30 18:03:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (253474ms till timeout)
2022-03-30 18:03:16 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 3
2022-03-30 18:03:16 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 3
2022-03-30 18:03:16 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 3
2022-03-30 18:03:16 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 3
2022-03-30 18:03:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (252465ms till timeout)
2022-03-30 18:03:17 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 2
2022-03-30 18:03:17 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 2
2022-03-30 18:03:17 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 2
2022-03-30 18:03:17 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 2
2022-03-30 18:03:17 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (251456ms till timeout)
2022-03-30 18:03:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:210] testCruiseControlMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:03:18 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd is in the Running state. Remaining seconds pod to be stable 1
2022-03-30 18:03:18 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 1
2022-03-30 18:03:18 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q is in the Running state. Remaining seconds pod to be stable 1
2022-03-30 18:03:18 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 1
2022-03-30 18:03:18 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:335] All pods are stable second-kafka-cluster-entity-operator-5f8949dc9c-tdkrd ,second-kafka-cluster-kafka-0 ,second-kafka-cluster-kafka-exporter-6dfb7ccc69-nvr8q ,second-kafka-cluster-zookeeper-0
2022-03-30 18:03:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 18:03:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 18:03:18 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 18:03:18 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:346] In context testKafkaMetricsSettings is everything deleted.
2022-03-30 18:03:18 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 18:03:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:267] testKafkaMetricsSettings - Notifies waiting test cases:[testUpdateUser, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testCruiseControlBasicAPIRequests, testSendMessagesTlsScramSha, testSendMessagesCustomListenerTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMaker2TlsAndTlsClientAuth, testMirrorMakerTlsAuthenticated, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaConnectIoNetwork, testKafkaBrokersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics, testClusterOperatorMetrics, testCruiseControlMetrics] to and randomly select one to start execution
2022-03-30 18:03:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaMetricsSettings
2022-03-30 18:03:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 2
2022-03-30 18:03:18 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaMetricsSettings-FINISHED
2022-03-30 18:03:18 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 18:03:18 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 18:03:18 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testTopicOperatorMetrics-STARTED
2022-03-30 18:03:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 18:03:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 18:03:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testTopicOperatorMetrics
2022-03-30 18:03:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 3
2022-03-30 18:03:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:205] [testTopicOperatorMetrics] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:03:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:210] testTopicOperatorMetrics is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:03:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:230] testClusterOperatorMetrics test now can proceed its execution
2022-03-30 18:03:19 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 18:03:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-128d9a40, testKafkaConnectIoNetwork=my-cluster-580bbc7b, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e, testCruiseControlBasicAPIRequests=my-cluster-6da231a3, testZookeeperWatchersCount=my-cluster-a66ecd5e, testKafkaTopicPartitions=my-cluster-1625722f, testMirrorMaker2Metrics=my-cluster-a4258561, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0, testZookeeperAliveConnections=my-cluster-01c0acb0, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2, testKafkaTopicUnderReplicatedPartitions=my-cluster-fa981fe3, testSendMessagesTlsScramSha=my-cluster-a11b9173, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc, testZookeeperQuorumSize=my-cluster-5a26ca2a, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1, testKafkaBridgeMetrics=my-cluster-0eb8fc89, testUpdateUser=my-cluster-75998b92, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8, testKafkaActiveControllers=my-cluster-948e4115, testReceiveSimpleMessageTls=my-cluster-90021a9f, testKafkaBrokersCount=my-cluster-e91a8cef, testClusterOperatorMetrics=my-cluster-2b296952, testUserOperatorMetrics=my-cluster-7692a28e, testKafkaMetricsSettings=my-cluster-f556a04e, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e}
2022-03-30 18:03:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-323988616-1355569855, testKafkaConnectIoNetwork=my-user-1092781470-1895809353, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-695287754-1004738297, testSendMessagesCustomListenerTlsScramSha=my-user-1653975062-1727371851, testCruiseControlBasicAPIRequests=my-user-918568860-36694764, testZookeeperWatchersCount=my-user-1252011538-504314473, testKafkaTopicPartitions=my-user-394346586-105422980, testMirrorMaker2Metrics=my-user-926413343-1872437319, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1337805046-1917759354, testZookeeperAliveConnections=my-user-704084348-2034238998, testAutoRenewAllCaCertsTriggeredByAnno=my-user-308582398-741623568, testKafkaTopicUnderReplicatedPartitions=my-user-1487552272-1931929933, testSendMessagesTlsScramSha=my-user-1791509549-1350003609, testMirrorMaker2TlsAndTlsClientAuth=my-user-711655589-495294801, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-83332781-1273282344, testRackAwareConnectCorrectDeployment=my-user-1839832768-1329691231, testZookeeperQuorumSize=my-user-1387929819-2059775559, testKafkaAndZookeeperScaleUpScaleDown=my-user-1975958319-512406578, testKafkaBridgeMetrics=my-user-1215630877-369211451, testUpdateUser=my-user-275427717-1183444028, testKafkaInDifferentNsThanClusterOperator=my-user-908767181-1543724508, testKafkaActiveControllers=my-user-971800472-962265772, testReceiveSimpleMessageTls=my-user-491397521-1196361913, testKafkaBrokersCount=my-user-699069229-1729992055, testClusterOperatorMetrics=my-user-1075673018-1047529285, testUserOperatorMetrics=my-user-92263365-1802597441, testKafkaMetricsSettings=my-user-10583859-1000138651, testMirrorMakerTlsAuthenticated=my-user-1552986594-1641210083}
2022-03-30 18:03:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-1040221322-670398147, testKafkaConnectIoNetwork=my-topic-2145871889-1295201610, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1609923788-1785060017, testSendMessagesCustomListenerTlsScramSha=my-topic-629448478-708445494, testCruiseControlBasicAPIRequests=my-topic-710218582-658397020, testZookeeperWatchersCount=my-topic-1225869180-847659507, testKafkaTopicPartitions=my-topic-1086071779-2133194430, testMirrorMaker2Metrics=my-topic-1779646810-1685449566, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-604305255-1585536778, testZookeeperAliveConnections=my-topic-1222053841-372865493, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1728678022-515015095, testKafkaTopicUnderReplicatedPartitions=my-topic-893837582-953830717, testSendMessagesTlsScramSha=my-topic-1482193652-649135537, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1152366002-738531989, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1536476030-1482182833, testRackAwareConnectCorrectDeployment=my-topic-1526242410-482838543, testZookeeperQuorumSize=my-topic-1242495538-1808467385, testKafkaAndZookeeperScaleUpScaleDown=my-topic-1496158521-938246156, testKafkaBridgeMetrics=my-topic-90145110-388295797, testUpdateUser=my-topic-275186280-765877519, testKafkaInDifferentNsThanClusterOperator=my-topic-2051437288-2087424913, testKafkaActiveControllers=my-topic-916932648-500952606, testReceiveSimpleMessageTls=my-topic-941665209-436577257, testKafkaBrokersCount=my-topic-708967095-808914391, testClusterOperatorMetrics=my-topic-1725435997-1780998333, testUserOperatorMetrics=my-topic-1414123631-687906301, testKafkaMetricsSettings=my-topic-1375496082-553518335, testMirrorMakerTlsAuthenticated=my-topic-1936321612-505632869}
2022-03-30 18:03:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testKafkaConnectIoNetwork=my-cluster-580bbc7b-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-6da231a3-kafka-clients, testZookeeperWatchersCount=my-cluster-a66ecd5e-kafka-clients, testKafkaTopicPartitions=my-cluster-1625722f-kafka-clients, testMirrorMaker2Metrics=my-cluster-a4258561-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0-kafka-clients, testZookeeperAliveConnections=my-cluster-01c0acb0-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-fa981fe3-kafka-clients, testSendMessagesTlsScramSha=my-cluster-a11b9173-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc-kafka-clients, testZookeeperQuorumSize=my-cluster-5a26ca2a-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1-kafka-clients, testKafkaBridgeMetrics=my-cluster-0eb8fc89-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8-kafka-clients, testKafkaActiveControllers=my-cluster-948e4115-kafka-clients, testReceiveSimpleMessageTls=my-cluster-90021a9f-kafka-clients, testKafkaBrokersCount=my-cluster-e91a8cef-kafka-clients, testClusterOperatorMetrics=my-cluster-2b296952-kafka-clients, testUserOperatorMetrics=my-cluster-7692a28e-kafka-clients, testKafkaMetricsSettings=my-cluster-f556a04e-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e-kafka-clients}
2022-03-30 18:03:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 18:03:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-s25n7 -n infra-namespace -- curl 172.17.0.5:8080/metrics
2022-03-30 18:03:19 [ForkJoinPool-3-worker-5] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.5 from Pod infra-namespace-kafka-clients-748578f786-s25n7 finished with return code: 0
2022-03-30 18:03:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 18:03:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 18:03:19 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 18:03:19 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:346] In context testClusterOperatorMetrics is everything deleted.
2022-03-30 18:03:19 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 18:03:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:267] testClusterOperatorMetrics - Notifies waiting test cases:[testUpdateUser, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testCruiseControlBasicAPIRequests, testSendMessagesTlsScramSha, testSendMessagesCustomListenerTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMaker2TlsAndTlsClientAuth, testMirrorMakerTlsAuthenticated, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaConnectIoNetwork, testKafkaBrokersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics, testCruiseControlMetrics, testTopicOperatorMetrics] to and randomly select one to start execution
2022-03-30 18:03:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testClusterOperatorMetrics
2022-03-30 18:03:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 2
2022-03-30 18:03:19 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testClusterOperatorMetrics-FINISHED
2022-03-30 18:03:19 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 18:03:19 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 18:03:19 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaConnectRequests-STARTED
2022-03-30 18:03:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 18:03:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 18:03:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaConnectRequests
2022-03-30 18:03:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 3
2022-03-30 18:03:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:205] [testKafkaConnectRequests] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:03:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [SuiteThreadController:210] testKafkaConnectRequests is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:03:22 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:230] testCruiseControlMetrics test now can proceed its execution
2022-03-30 18:03:22 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 18:03:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-128d9a40, testKafkaConnectIoNetwork=my-cluster-580bbc7b, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e, testCruiseControlBasicAPIRequests=my-cluster-6da231a3, testZookeeperWatchersCount=my-cluster-a66ecd5e, testKafkaTopicPartitions=my-cluster-1625722f, testMirrorMaker2Metrics=my-cluster-a4258561, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0, testZookeeperAliveConnections=my-cluster-01c0acb0, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2, testKafkaTopicUnderReplicatedPartitions=my-cluster-fa981fe3, testSendMessagesTlsScramSha=my-cluster-a11b9173, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc, testZookeeperQuorumSize=my-cluster-5a26ca2a, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1, testKafkaBridgeMetrics=my-cluster-0eb8fc89, testUpdateUser=my-cluster-75998b92, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8, testKafkaActiveControllers=my-cluster-948e4115, testCruiseControlMetrics=my-cluster-f2271154, testReceiveSimpleMessageTls=my-cluster-90021a9f, testKafkaBrokersCount=my-cluster-e91a8cef, testClusterOperatorMetrics=my-cluster-2b296952, testUserOperatorMetrics=my-cluster-7692a28e, testKafkaMetricsSettings=my-cluster-f556a04e, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e}
2022-03-30 18:03:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-323988616-1355569855, testKafkaConnectIoNetwork=my-user-1092781470-1895809353, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-695287754-1004738297, testSendMessagesCustomListenerTlsScramSha=my-user-1653975062-1727371851, testCruiseControlBasicAPIRequests=my-user-918568860-36694764, testZookeeperWatchersCount=my-user-1252011538-504314473, testKafkaTopicPartitions=my-user-394346586-105422980, testMirrorMaker2Metrics=my-user-926413343-1872437319, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1337805046-1917759354, testZookeeperAliveConnections=my-user-704084348-2034238998, testAutoRenewAllCaCertsTriggeredByAnno=my-user-308582398-741623568, testKafkaTopicUnderReplicatedPartitions=my-user-1487552272-1931929933, testSendMessagesTlsScramSha=my-user-1791509549-1350003609, testMirrorMaker2TlsAndTlsClientAuth=my-user-711655589-495294801, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-83332781-1273282344, testRackAwareConnectCorrectDeployment=my-user-1839832768-1329691231, testZookeeperQuorumSize=my-user-1387929819-2059775559, testKafkaAndZookeeperScaleUpScaleDown=my-user-1975958319-512406578, testKafkaBridgeMetrics=my-user-1215630877-369211451, testUpdateUser=my-user-275427717-1183444028, testKafkaInDifferentNsThanClusterOperator=my-user-908767181-1543724508, testKafkaActiveControllers=my-user-971800472-962265772, testCruiseControlMetrics=my-user-155132995-1606538153, testReceiveSimpleMessageTls=my-user-491397521-1196361913, testKafkaBrokersCount=my-user-699069229-1729992055, testClusterOperatorMetrics=my-user-1075673018-1047529285, testUserOperatorMetrics=my-user-92263365-1802597441, testKafkaMetricsSettings=my-user-10583859-1000138651, testMirrorMakerTlsAuthenticated=my-user-1552986594-1641210083}
2022-03-30 18:03:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-1040221322-670398147, testKafkaConnectIoNetwork=my-topic-2145871889-1295201610, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1609923788-1785060017, testSendMessagesCustomListenerTlsScramSha=my-topic-629448478-708445494, testCruiseControlBasicAPIRequests=my-topic-710218582-658397020, testZookeeperWatchersCount=my-topic-1225869180-847659507, testKafkaTopicPartitions=my-topic-1086071779-2133194430, testMirrorMaker2Metrics=my-topic-1779646810-1685449566, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-604305255-1585536778, testZookeeperAliveConnections=my-topic-1222053841-372865493, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1728678022-515015095, testKafkaTopicUnderReplicatedPartitions=my-topic-893837582-953830717, testSendMessagesTlsScramSha=my-topic-1482193652-649135537, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1152366002-738531989, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1536476030-1482182833, testRackAwareConnectCorrectDeployment=my-topic-1526242410-482838543, testZookeeperQuorumSize=my-topic-1242495538-1808467385, testKafkaAndZookeeperScaleUpScaleDown=my-topic-1496158521-938246156, testKafkaBridgeMetrics=my-topic-90145110-388295797, testUpdateUser=my-topic-275186280-765877519, testKafkaInDifferentNsThanClusterOperator=my-topic-2051437288-2087424913, testKafkaActiveControllers=my-topic-916932648-500952606, testCruiseControlMetrics=my-topic-1862902924-1176608323, testReceiveSimpleMessageTls=my-topic-941665209-436577257, testKafkaBrokersCount=my-topic-708967095-808914391, testClusterOperatorMetrics=my-topic-1725435997-1780998333, testUserOperatorMetrics=my-topic-1414123631-687906301, testKafkaMetricsSettings=my-topic-1375496082-553518335, testMirrorMakerTlsAuthenticated=my-topic-1936321612-505632869}
2022-03-30 18:03:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testKafkaConnectIoNetwork=my-cluster-580bbc7b-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-6da231a3-kafka-clients, testZookeeperWatchersCount=my-cluster-a66ecd5e-kafka-clients, testKafkaTopicPartitions=my-cluster-1625722f-kafka-clients, testMirrorMaker2Metrics=my-cluster-a4258561-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0-kafka-clients, testZookeeperAliveConnections=my-cluster-01c0acb0-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-fa981fe3-kafka-clients, testSendMessagesTlsScramSha=my-cluster-a11b9173-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc-kafka-clients, testZookeeperQuorumSize=my-cluster-5a26ca2a-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1-kafka-clients, testKafkaBridgeMetrics=my-cluster-0eb8fc89-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8-kafka-clients, testKafkaActiveControllers=my-cluster-948e4115-kafka-clients, testCruiseControlMetrics=my-cluster-f2271154-kafka-clients, testReceiveSimpleMessageTls=my-cluster-90021a9f-kafka-clients, testKafkaBrokersCount=my-cluster-e91a8cef-kafka-clients, testClusterOperatorMetrics=my-cluster-2b296952-kafka-clients, testUserOperatorMetrics=my-cluster-7692a28e-kafka-clients, testKafkaMetricsSettings=my-cluster-f556a04e-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e-kafka-clients}
2022-03-30 18:03:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace exec metrics-cluster-name-cruise-control-55d98795b-27kmf -c cruise-control -- /bin/bash -c curl -XGET localhost:9404/metrics
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace exec metrics-cluster-name-cruise-control-55d98795b-27kmf -c cruise-control -- /bin/bash -c curl -XGET localhost:9404/metrics
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsIsolatedST:450] Verifying that we have more than 0 groups
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_info{runtime="OpenJDK Runtime Environment",vendor="Red Hat, Inc.",version="11.0.14.1+1-LTS",} -> 1.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_total{pool="CodeHeap 'profiled nmethods'",} -> 1.5251584E7
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_total{pool="G1 Old Gen",} -> 2.4277672E7
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_total{pool="G1 Eden Space",} -> 5.9244544E8
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_total{pool="CodeHeap 'non-profiled nmethods'",} -> 3678848.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_total{pool="G1 Survivor Space",} -> 1.2582912E7
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_total{pool="Compressed Class Space",} -> 5499880.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_total{pool="Metaspace",} -> 4.8414136E7
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_total{pool="CodeHeap 'non-nmethods'",} -> 1471488.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_current -> 55.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_daemon -> 36.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_peak -> 55.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_started_total -> 70.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_deadlocked -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_deadlocked_monitor -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_state{state="RUNNABLE",} -> 14.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_state{state="TIMED_WAITING",} -> 24.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_state{state="WAITING",} -> 17.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_state{state="BLOCKED",} -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_state{state="TERMINATED",} -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_state{state="NEW",} -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_objects_pending_finalization -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_bytes_used{area="heap",} -> 4.6560768E7
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_bytes_used{area="nonheap",} -> 7.504312E7
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_bytes_committed{area="heap",} -> 8.98629632E8
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_bytes_committed{area="nonheap",} -> 7.86432E7
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_bytes_max{area="heap",} -> 8.37812224E9
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_bytes_max{area="nonheap",} -> -1.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_bytes_init{area="heap",} -> 1.34217728E8
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_bytes_init{area="nonheap",} -> 7667712.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_used{pool="CodeHeap 'non-nmethods'",} -> 1485056.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_used{pool="Metaspace",} -> 4.8659464E7
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_used{pool="CodeHeap 'profiled nmethods'",} -> 1.5608704E7
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_used{pool="Compressed Class Space",} -> 5519912.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_used{pool="G1 Eden Space",} -> 2.2020096E7
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_used{pool="G1 Old Gen",} -> 2.244352E7
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_used{pool="G1 Survivor Space",} -> 2097152.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_used{pool="CodeHeap 'non-profiled nmethods'",} -> 3769984.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_committed{pool="CodeHeap 'non-nmethods'",} -> 2555904.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_committed{pool="Metaspace",} -> 5.046272E7
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_committed{pool="CodeHeap 'profiled nmethods'",} -> 1.5663104E7
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_committed{pool="Compressed Class Space",} -> 6160384.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_committed{pool="G1 Eden Space",} -> 9.8566144E7
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_committed{pool="G1 Old Gen",} -> 7.97966336E8
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_committed{pool="G1 Survivor Space",} -> 2097152.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_committed{pool="CodeHeap 'non-profiled nmethods'",} -> 3801088.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_max{pool="CodeHeap 'non-nmethods'",} -> 5828608.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_max{pool="Metaspace",} -> -1.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_max{pool="CodeHeap 'profiled nmethods'",} -> 1.22912768E8
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_max{pool="Compressed Class Space",} -> 1.073741824E9
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_max{pool="G1 Eden Space",} -> -1.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_max{pool="G1 Old Gen",} -> 8.37812224E9
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_max{pool="G1 Survivor Space",} -> -1.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_max{pool="CodeHeap 'non-profiled nmethods'",} -> 1.22916864E8
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_init{pool="CodeHeap 'non-nmethods'",} -> 2555904.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_init{pool="Metaspace",} -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_init{pool="CodeHeap 'profiled nmethods'",} -> 2555904.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_init{pool="Compressed Class Space",} -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_init{pool="G1 Eden Space",} -> 7340032.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_init{pool="G1 Old Gen",} -> 1.26877696E8
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_init{pool="G1 Survivor Space",} -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_init{pool="CodeHeap 'non-profiled nmethods'",} -> 2555904.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_used_bytes{pool="G1 Eden Space",} -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_used_bytes{pool="G1 Old Gen",} -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_used_bytes{pool="G1 Survivor Space",} -> 2097152.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_committed_bytes{pool="G1 Eden Space",} -> 9.8566144E7
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_committed_bytes{pool="G1 Old Gen",} -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_committed_bytes{pool="G1 Survivor Space",} -> 2097152.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_max_bytes{pool="G1 Eden Space",} -> -1.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_max_bytes{pool="G1 Old Gen",} -> 8.37812224E9
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_max_bytes{pool="G1 Survivor Space",} -> -1.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_init_bytes{pool="G1 Eden Space",} -> 7340032.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_init_bytes{pool="G1 Old Gen",} -> 1.26877696E8
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_init_bytes{pool="G1 Survivor Space",} -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jmx_exporter_build_info{version="0.16.1",name="jmx_prometheus_javaagent",} -> 1.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_buffer_pool_used_bytes{pool="mapped",} -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_buffer_pool_used_bytes{pool="direct",} -> 331444.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_buffer_pool_capacity_bytes{pool="mapped",} -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_buffer_pool_capacity_bytes{pool="direct",} -> 331444.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_buffer_pool_used_buffers{pool="mapped",} -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_buffer_pool_used_buffers{pool="direct",} -> 24.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] process_cpu_seconds_total -> 20.85
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] process_start_time_seconds -> 1.648662976514E9
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] process_open_fds -> 169.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] process_max_fds -> 1048576.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] process_virtual_memory_bytes -> 1.4654283776E10
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] process_resident_memory_bytes -> 3.47918336E8
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_gc_collection_seconds_count{gc="G1 Young Generation",} -> 26.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_gc_collection_seconds_sum{gc="G1 Young Generation",} -> 0.275
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_gc_collection_seconds_count{gc="G1 Old Generation",} -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_gc_collection_seconds_sum{gc="G1 Old Generation",} -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jmx_config_reload_success_total -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jmx_config_reload_failure_total -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_mean -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_max -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_75thpercentile -> 50.406917
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_request_rate_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_aborting_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_execution_started_kafka_assigner_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_min -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_has_partitions_with_isr_greater_than_replicas_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_count -> 1.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_min -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_failure_rate_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_mean -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_request_rate_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_request_rate_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_stddev -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_rate_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_request_rate_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_stddev -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_enabled_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_request_rate_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_request_rate_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_ongoing_anomaly_duration_ms_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_request_rate_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_request_rate_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_min -> 28.443149
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_stddev -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_request_rate_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_max -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_stddev -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_min -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_min -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_request_rate_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_aborted_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_request_rate_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_request_rate_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_mean -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_usertaskmanager_num_active_sessions_number -> 12.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_request_rate_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_mean -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_inter_broker_partition_movements_per_broker_cap_value -> 5.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_request_rate_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_min -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_min -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_automated_rightsizing_rate_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_request_rate_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_total_monitored_windows_value -> 2.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_request_rate_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_mean -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_95thpercentile -> 50.406917
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_request_rate_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_number_of_self_healing_failed_to_start_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_fiveminuterate -> 0.010787345794585342
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_movements_global_cap_value -> 1000.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_min -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_99thpercentile -> 50.406917
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_request_rate_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_mean -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_rate_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_request_rate_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_metadata_factor_number -> 1512.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_failure_rate_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_min -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_automated_rightsizing_rate_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_999thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_min -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_max -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_max -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_request_rate_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_max -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_request_rate_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_75thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_98thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_999thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_max -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_request_rate_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_stddev -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_rate_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_mean -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_request_rate_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_999thpercentile -> 1.494227
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_dead_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_number_of_self_healing_failed_to_start_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_75thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_999thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_min -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_mean -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_oneminuterate -> 2.4792393800201324E-4
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_98thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_request_rate_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_total_monitored_windows_number -> 2.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_in_progress_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_request_rate_meanrate -> 0.20204551355131614
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_95thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_50thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_rate_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_request_rate_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_request_rate_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_stddev -> 15.302721705321773
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_stddev -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_request_rate_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_right_sized_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_request_rate_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_50thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_enabled_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_count -> 1.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_rate_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_request_rate_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_95thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_99thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_mean -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_rate_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_request_rate_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_execution_started_kafka_assigner_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_max -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_request_rate_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_mean -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_valid_windows_number -> 1.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_999thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_min -> 32.588045
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_99thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_completed_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_request_rate_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_rate_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_stddev -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_rate_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_request_rate_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_request_rate_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_mean -> 32.588045
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_dead_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_request_rate_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_completed_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_failure_rate_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_95thpercentile -> 213.132251
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_enabled_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_valid_windows_value -> 1.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_mean -> 1.494227
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_meanrate -> 0.002405913313101559
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_request_rate_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_ongoing_execution_non_kafka_assigner_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_fiveminuterate -> 0.0022909907785187192
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_request_rate_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_98thpercentile -> 32.588045
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_stddev -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_request_rate_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_50thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_stddev -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_has_unfixable_goals_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_stddev -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_meanrate -> 0.0024054347739080393
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_pending_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_count -> 6.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_oneminuterate -> 0.0025566557373434187
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_request_rate_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_stddev -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_stddev -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_monitored_partitions_percentage_value -> 1.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_rate_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_rate_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_num_suspect_metric_anomalies_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_mean -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_max -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_request_rate_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_max -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_number_of_self_healing_started_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_request_rate_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_mean -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_75thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_enabled_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_mean -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_request_rate_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_rate_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_98thpercentile -> 50.406917
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_num_topics_number -> 17.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_dead_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_has_partitions_with_isr_greater_than_replicas_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_aborted_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_mean_time_to_start_fix_ms_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_monitored_partitions_percentage_number -> 1.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_75thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_stddev -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_mean -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_stddev -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_request_rate_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_dead_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_98thpercentile -> 1.494227
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_max -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_execution_stopped_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_dead_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_failure_rate_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_pending_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_request_rate_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_max -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_request_rate_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_request_rate_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_fiveminuterate -> 0.0014366553116703102
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_request_rate_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_request_rate_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_min -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_mean -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_98thpercentile -> 213.132251
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_aborted_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_rate_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_in_progress_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_stddev -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_stddev -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_aborting_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_stddev -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_98thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_fiveminuterate -> 0.0022909907785187192
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_rate_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_rate_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_50thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_min -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_under_provisioned_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_enabled_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_pending_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_stddev -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_max -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_mean -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_aborted_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_50thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_request_rate_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_request_rate_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_request_rate_fifteenminuterate -> 0.32702239865138905
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_num_topics_value -> 17.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_aborting_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_partition_movements_per_broker_cap_number -> 2.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_max -> 32.588045
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_request_rate_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_enabled_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_stddev -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_mean -> 213.132251
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_request_rate_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_failure_rate_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_request_rate_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_ongoing_execution_kafka_assigner_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_movements_global_cap_number -> 1000.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_request_rate_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_stddev -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_rate_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_max -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_count -> 1.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_has_partitions_with_replication_factor_greater_than_num_racks_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_rate_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_dead_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_98thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_aborting_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_max -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_99thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_request_rate_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_completed_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_min -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_75thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_50thpercentile -> 213.132251
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_enabled_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_request_rate_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_min -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_request_rate_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_request_rate_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_request_rate_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_request_rate_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_75thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_min -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_in_progress_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_ongoing_execution_non_kafka_assigner_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_75thpercentile -> 32.588045
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_enabled_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_95thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_min -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_request_rate_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_num_recent_metric_anomalies_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_stddev -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_mean -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_in_progress_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_enabled_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_request_rate_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_over_provisioned_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_999thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_rate_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_request_rate_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_completed_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_usertaskmanager_num_active_sessions_value -> 12.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_under_provisioned_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_999thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_in_progress_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_has_partitions_with_replication_factor_greater_than_num_racks_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_max -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_min -> 1.494227
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_metadata_factor_value -> 1512.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_min -> 213.132251
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_stddev -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_999thpercentile -> 213.132251
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_request_rate_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_in_progress_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_request_rate_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_request_rate_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_right_sized_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_meanrate -> 0.002354372950372468
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_stddev -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_completed_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_fifteenminuterate -> 0.005358016726253495
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_request_rate_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_50thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_request_rate_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_95thpercentile -> 1.494227
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_request_rate_oneminuterate -> 0.2085349944145525
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_rate_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_usertaskmanager_num_active_user_tasks_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_num_persistent_metric_anomalies_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_95thpercentile -> 32.588045
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_balancedness_score_number -> 100.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_max -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_failure_rate_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_rate_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_automated_rightsizing_rate_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_98thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_99thpercentile -> 1.494227
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_rate_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_request_rate_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_stddev -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_request_rate_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_aborted_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_request_rate_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_request_rate_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_mean -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_mean -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_fifteenminuterate -> 9.805533749814622E-4
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_mean -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_fifteenminuterate -> 8.392943884619204E-4
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_request_rate_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_99thpercentile -> 213.132251
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_request_rate_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_usertaskmanager_num_active_user_tasks_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_request_rate_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_enabled_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_request_rate_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_request_rate_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_95thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_mean -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_stddev -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_stddev -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_min -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_request_rate_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_99thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_request_rate_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_mean -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_max -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_num_persistent_metric_anomalies_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_max -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_completed_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_oneminuterate -> 0.010967028559299336
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_request_rate_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_stddev -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_inter_broker_partition_movements_per_broker_cap_number -> 5.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_rate_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_mean -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_98thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_ongoing_anomaly_duration_ms_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_aborting_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_request_rate_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_failure_rate_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_request_rate_count -> 84.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_request_rate_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_98thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_request_rate_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_fifteenminuterate -> 9.805533749814622E-4
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_min -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_75thpercentile -> 1.494227
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_min -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_min -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_mean_time_to_start_fix_ms_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_execution_started_non_kafka_assigner_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_over_provisioned_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_max -> 191.216582
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_min -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_rate_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_rate_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_999thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_execution_stopped_by_user_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_max -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_rate_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_min -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_max -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_stddev -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_aborted_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_ongoing_execution_kafka_assigner_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_mean -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_request_rate_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_request_rate_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_number_of_self_healing_started_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_num_suspect_metric_anomalies_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_execution_stopped_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_failure_rate_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_99thpercentile -> 32.588045
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_failure_rate_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_max -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_max -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_max -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_max -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_request_rate_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_max -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_mean -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_min -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_request_rate_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_rate_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_50thpercentile -> 50.406917
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_request_rate_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_mean -> 43.75782210423739
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_request_rate_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_partition_movements_per_broker_cap_value -> 2.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_rate_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_rate_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_max -> 213.132251
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_min -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_request_rate_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_999thpercentile -> 32.588045
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_mean -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_balancedness_score_value -> 100.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_oneminuterate -> 0.0025566557373434187
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_failure_rate_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_50thpercentile -> 1.494227
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_95thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_mean -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_has_unfixable_goals_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_50thpercentile -> 32.588045
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_pending_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_min -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_99thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_enabled_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_automated_rightsizing_rate_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_rate_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_request_rate_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_pending_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_request_rate_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_min -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_stddev -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_num_partitions_with_extrapolations_number -> 79.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_execution_started_non_kafka_assigner_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_request_rate_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_mean -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_max -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_min -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_max -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_75thpercentile -> 213.132251
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_99thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_rate_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_stddev -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_request_rate_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_95thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_min -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_request_rate_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_50thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_request_rate_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_num_recent_metric_anomalies_value -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_99thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_request_rate_fiveminuterate -> 0.2522327914687181
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_75thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_execution_stopped_by_user_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_count -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_rate_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_request_rate_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_automated_rightsizing_rate_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_aborting_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_num_partitions_with_extrapolations_value -> 79.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_999thpercentile -> 191.216582
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_stddev -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_request_rate_oneminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_max -> 1.494227
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_pending_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_meanrate -> 0.014334478123018144
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_enabled_number -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_95thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_request_rate_fifteenminuterate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_request_rate_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_max -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jmx_scrape_duration_seconds -> 0.26674141
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jmx_scrape_error -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jmx_scrape_cached_beans -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_classes_loaded -> 8751.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_classes_loaded_total -> 8751.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_classes_unloaded_total -> 0.0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jmx_config_reload_failure_created -> 1.648662976688E9
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jmx_config_reload_success_created -> 1.648662976687E9
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_created{pool="CodeHeap 'profiled nmethods'",} -> 1.648662976968E9
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_created{pool="G1 Old Gen",} -> 1.648662976972E9
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_created{pool="G1 Eden Space",} -> 1.648662976972E9
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_created{pool="CodeHeap 'non-profiled nmethods'",} -> 1.648662976972E9
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_created{pool="G1 Survivor Space",} -> 1.648662976972E9
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_created{pool="Compressed Class Space",} -> 1.648662976972E9
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_created{pool="Metaspace",} -> 1.648662976972E9
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_created{pool="CodeHeap 'non-nmethods'",} 1.648662976972E9 -> 
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:346] In context testCruiseControlMetrics is everything deleted.
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:267] testCruiseControlMetrics - Notifies waiting test cases:[testUpdateUser, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testCruiseControlBasicAPIRequests, testSendMessagesTlsScramSha, testSendMessagesCustomListenerTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMaker2TlsAndTlsClientAuth, testMirrorMakerTlsAuthenticated, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaConnectIoNetwork, testKafkaBrokersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics, testTopicOperatorMetrics, testKafkaConnectRequests] to and randomly select one to start execution
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testCruiseControlMetrics
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 2
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testCruiseControlMetrics-FINISHED
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaConnectResponse-STARTED
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaConnectResponse
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 3
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:205] [testKafkaConnectResponse] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:03:23 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:210] testKafkaConnectResponse is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:230] testTopicOperatorMetrics test now can proceed its execution
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-128d9a40, testKafkaConnectIoNetwork=my-cluster-580bbc7b, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e, testCruiseControlBasicAPIRequests=my-cluster-6da231a3, testZookeeperWatchersCount=my-cluster-a66ecd5e, testKafkaTopicPartitions=my-cluster-1625722f, testMirrorMaker2Metrics=my-cluster-a4258561, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0, testZookeeperAliveConnections=my-cluster-01c0acb0, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2, testKafkaTopicUnderReplicatedPartitions=my-cluster-fa981fe3, testSendMessagesTlsScramSha=my-cluster-a11b9173, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc, testZookeeperQuorumSize=my-cluster-5a26ca2a, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1, testKafkaBridgeMetrics=my-cluster-0eb8fc89, testUpdateUser=my-cluster-75998b92, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8, testTopicOperatorMetrics=my-cluster-40f822af, testKafkaActiveControllers=my-cluster-948e4115, testCruiseControlMetrics=my-cluster-f2271154, testReceiveSimpleMessageTls=my-cluster-90021a9f, testKafkaBrokersCount=my-cluster-e91a8cef, testClusterOperatorMetrics=my-cluster-2b296952, testUserOperatorMetrics=my-cluster-7692a28e, testKafkaMetricsSettings=my-cluster-f556a04e, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e}
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-323988616-1355569855, testKafkaConnectIoNetwork=my-user-1092781470-1895809353, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-695287754-1004738297, testSendMessagesCustomListenerTlsScramSha=my-user-1653975062-1727371851, testCruiseControlBasicAPIRequests=my-user-918568860-36694764, testZookeeperWatchersCount=my-user-1252011538-504314473, testKafkaTopicPartitions=my-user-394346586-105422980, testMirrorMaker2Metrics=my-user-926413343-1872437319, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1337805046-1917759354, testZookeeperAliveConnections=my-user-704084348-2034238998, testAutoRenewAllCaCertsTriggeredByAnno=my-user-308582398-741623568, testKafkaTopicUnderReplicatedPartitions=my-user-1487552272-1931929933, testSendMessagesTlsScramSha=my-user-1791509549-1350003609, testMirrorMaker2TlsAndTlsClientAuth=my-user-711655589-495294801, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-83332781-1273282344, testRackAwareConnectCorrectDeployment=my-user-1839832768-1329691231, testZookeeperQuorumSize=my-user-1387929819-2059775559, testKafkaAndZookeeperScaleUpScaleDown=my-user-1975958319-512406578, testKafkaBridgeMetrics=my-user-1215630877-369211451, testUpdateUser=my-user-275427717-1183444028, testKafkaInDifferentNsThanClusterOperator=my-user-908767181-1543724508, testTopicOperatorMetrics=my-user-717431267-943604128, testKafkaActiveControllers=my-user-971800472-962265772, testCruiseControlMetrics=my-user-155132995-1606538153, testReceiveSimpleMessageTls=my-user-491397521-1196361913, testKafkaBrokersCount=my-user-699069229-1729992055, testClusterOperatorMetrics=my-user-1075673018-1047529285, testUserOperatorMetrics=my-user-92263365-1802597441, testKafkaMetricsSettings=my-user-10583859-1000138651, testMirrorMakerTlsAuthenticated=my-user-1552986594-1641210083}
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-1040221322-670398147, testKafkaConnectIoNetwork=my-topic-2145871889-1295201610, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1609923788-1785060017, testSendMessagesCustomListenerTlsScramSha=my-topic-629448478-708445494, testCruiseControlBasicAPIRequests=my-topic-710218582-658397020, testZookeeperWatchersCount=my-topic-1225869180-847659507, testKafkaTopicPartitions=my-topic-1086071779-2133194430, testMirrorMaker2Metrics=my-topic-1779646810-1685449566, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-604305255-1585536778, testZookeeperAliveConnections=my-topic-1222053841-372865493, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1728678022-515015095, testKafkaTopicUnderReplicatedPartitions=my-topic-893837582-953830717, testSendMessagesTlsScramSha=my-topic-1482193652-649135537, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1152366002-738531989, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1536476030-1482182833, testRackAwareConnectCorrectDeployment=my-topic-1526242410-482838543, testZookeeperQuorumSize=my-topic-1242495538-1808467385, testKafkaAndZookeeperScaleUpScaleDown=my-topic-1496158521-938246156, testKafkaBridgeMetrics=my-topic-90145110-388295797, testUpdateUser=my-topic-275186280-765877519, testKafkaInDifferentNsThanClusterOperator=my-topic-2051437288-2087424913, testTopicOperatorMetrics=my-topic-1059872505-1735356044, testKafkaActiveControllers=my-topic-916932648-500952606, testCruiseControlMetrics=my-topic-1862902924-1176608323, testReceiveSimpleMessageTls=my-topic-941665209-436577257, testKafkaBrokersCount=my-topic-708967095-808914391, testClusterOperatorMetrics=my-topic-1725435997-1780998333, testUserOperatorMetrics=my-topic-1414123631-687906301, testKafkaMetricsSettings=my-topic-1375496082-553518335, testMirrorMakerTlsAuthenticated=my-topic-1936321612-505632869}
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testKafkaConnectIoNetwork=my-cluster-580bbc7b-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-6da231a3-kafka-clients, testZookeeperWatchersCount=my-cluster-a66ecd5e-kafka-clients, testKafkaTopicPartitions=my-cluster-1625722f-kafka-clients, testMirrorMaker2Metrics=my-cluster-a4258561-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0-kafka-clients, testZookeeperAliveConnections=my-cluster-01c0acb0-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-fa981fe3-kafka-clients, testSendMessagesTlsScramSha=my-cluster-a11b9173-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc-kafka-clients, testZookeeperQuorumSize=my-cluster-5a26ca2a-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1-kafka-clients, testKafkaBridgeMetrics=my-cluster-0eb8fc89-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8-kafka-clients, testTopicOperatorMetrics=my-cluster-40f822af-kafka-clients, testKafkaActiveControllers=my-cluster-948e4115-kafka-clients, testCruiseControlMetrics=my-cluster-f2271154-kafka-clients, testReceiveSimpleMessageTls=my-cluster-90021a9f-kafka-clients, testKafkaBrokersCount=my-cluster-e91a8cef-kafka-clients, testClusterOperatorMetrics=my-cluster-2b296952-kafka-clients, testUserOperatorMetrics=my-cluster-7692a28e-kafka-clients, testKafkaMetricsSettings=my-cluster-f556a04e-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e-kafka-clients}
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-s25n7 -n infra-namespace -- curl 172.17.0.19:8080/metrics
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.19 from Pod infra-namespace-kafka-clients-748578f786-s25n7 finished with return code: 0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get KafkaTopic -o jsonpath={range .items[*]}{.metadata.name} 
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get KafkaTopic -o jsonpath={range .items[*]}{.metadata.name} 
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: consumer-offsets---84e7a678d08f4bd226872e5cdd4eb527fadc1c6a
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: heartbeats
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: metrics-cluster-name-connect-config
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: metrics-cluster-name-connect-offsets
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: metrics-cluster-name-connect-status
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: mirrormaker2-cluster-configs
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: mirrormaker2-cluster-offsets
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: mirrormaker2-cluster-status
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: my-topic-1177668122-1476973967
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: my-topic-1642297574-660862374
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: my-topic-840173747-1931076155
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: second-kafka-cluster.checkpoints.internal
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: strimzi-store-topic---effb8e3e057afce1ecf67c3f5d8e4e3ff177fc55
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: strimzi-topic-operator-kstreams-topic-store-changelog---b75e702040b99be8a9263134de3507fc0cc4017b
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: strimzi.cruisecontrol.metrics
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: strimzi.cruisecontrol.modeltrainingsamples
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: strimzi.cruisecontrol.partitionmetricsamples
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:346] In context testTopicOperatorMetrics is everything deleted.
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:267] testTopicOperatorMetrics - Notifies waiting test cases:[testUpdateUser, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testCruiseControlBasicAPIRequests, testSendMessagesTlsScramSha, testSendMessagesCustomListenerTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMaker2TlsAndTlsClientAuth, testMirrorMakerTlsAuthenticated, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaConnectIoNetwork, testKafkaBrokersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics, testKafkaConnectRequests, testKafkaConnectResponse] to and randomly select one to start execution
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testTopicOperatorMetrics
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 2
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testTopicOperatorMetrics-FINISHED
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaExporterDifferentSetting-STARTED
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaExporterDifferentSetting
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 3
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:205] [testKafkaExporterDifferentSetting] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:03:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:210] testKafkaExporterDifferentSetting is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (3/2)
2022-03-30 18:03:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:230] testKafkaConnectRequests test now can proceed its execution
2022-03-30 18:03:24 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 18:03:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-128d9a40, testKafkaConnectIoNetwork=my-cluster-580bbc7b, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e, testCruiseControlBasicAPIRequests=my-cluster-6da231a3, testZookeeperWatchersCount=my-cluster-a66ecd5e, testKafkaTopicPartitions=my-cluster-1625722f, testMirrorMaker2Metrics=my-cluster-a4258561, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0, testZookeeperAliveConnections=my-cluster-01c0acb0, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2, testKafkaTopicUnderReplicatedPartitions=my-cluster-fa981fe3, testSendMessagesTlsScramSha=my-cluster-a11b9173, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc, testZookeeperQuorumSize=my-cluster-5a26ca2a, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1, testKafkaBridgeMetrics=my-cluster-0eb8fc89, testUpdateUser=my-cluster-75998b92, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8, testTopicOperatorMetrics=my-cluster-40f822af, testKafkaActiveControllers=my-cluster-948e4115, testKafkaConnectRequests=my-cluster-fc5c2305, testCruiseControlMetrics=my-cluster-f2271154, testReceiveSimpleMessageTls=my-cluster-90021a9f, testKafkaBrokersCount=my-cluster-e91a8cef, testClusterOperatorMetrics=my-cluster-2b296952, testUserOperatorMetrics=my-cluster-7692a28e, testKafkaMetricsSettings=my-cluster-f556a04e, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e}
2022-03-30 18:03:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-323988616-1355569855, testKafkaConnectIoNetwork=my-user-1092781470-1895809353, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-695287754-1004738297, testSendMessagesCustomListenerTlsScramSha=my-user-1653975062-1727371851, testCruiseControlBasicAPIRequests=my-user-918568860-36694764, testZookeeperWatchersCount=my-user-1252011538-504314473, testKafkaTopicPartitions=my-user-394346586-105422980, testMirrorMaker2Metrics=my-user-926413343-1872437319, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1337805046-1917759354, testZookeeperAliveConnections=my-user-704084348-2034238998, testAutoRenewAllCaCertsTriggeredByAnno=my-user-308582398-741623568, testKafkaTopicUnderReplicatedPartitions=my-user-1487552272-1931929933, testSendMessagesTlsScramSha=my-user-1791509549-1350003609, testMirrorMaker2TlsAndTlsClientAuth=my-user-711655589-495294801, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-83332781-1273282344, testRackAwareConnectCorrectDeployment=my-user-1839832768-1329691231, testZookeeperQuorumSize=my-user-1387929819-2059775559, testKafkaAndZookeeperScaleUpScaleDown=my-user-1975958319-512406578, testKafkaBridgeMetrics=my-user-1215630877-369211451, testUpdateUser=my-user-275427717-1183444028, testKafkaInDifferentNsThanClusterOperator=my-user-908767181-1543724508, testTopicOperatorMetrics=my-user-717431267-943604128, testKafkaActiveControllers=my-user-971800472-962265772, testKafkaConnectRequests=my-user-146416398-1606246430, testCruiseControlMetrics=my-user-155132995-1606538153, testReceiveSimpleMessageTls=my-user-491397521-1196361913, testKafkaBrokersCount=my-user-699069229-1729992055, testClusterOperatorMetrics=my-user-1075673018-1047529285, testUserOperatorMetrics=my-user-92263365-1802597441, testKafkaMetricsSettings=my-user-10583859-1000138651, testMirrorMakerTlsAuthenticated=my-user-1552986594-1641210083}
2022-03-30 18:03:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-1040221322-670398147, testKafkaConnectIoNetwork=my-topic-2145871889-1295201610, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1609923788-1785060017, testSendMessagesCustomListenerTlsScramSha=my-topic-629448478-708445494, testCruiseControlBasicAPIRequests=my-topic-710218582-658397020, testZookeeperWatchersCount=my-topic-1225869180-847659507, testKafkaTopicPartitions=my-topic-1086071779-2133194430, testMirrorMaker2Metrics=my-topic-1779646810-1685449566, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-604305255-1585536778, testZookeeperAliveConnections=my-topic-1222053841-372865493, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1728678022-515015095, testKafkaTopicUnderReplicatedPartitions=my-topic-893837582-953830717, testSendMessagesTlsScramSha=my-topic-1482193652-649135537, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1152366002-738531989, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1536476030-1482182833, testRackAwareConnectCorrectDeployment=my-topic-1526242410-482838543, testZookeeperQuorumSize=my-topic-1242495538-1808467385, testKafkaAndZookeeperScaleUpScaleDown=my-topic-1496158521-938246156, testKafkaBridgeMetrics=my-topic-90145110-388295797, testUpdateUser=my-topic-275186280-765877519, testKafkaInDifferentNsThanClusterOperator=my-topic-2051437288-2087424913, testTopicOperatorMetrics=my-topic-1059872505-1735356044, testKafkaActiveControllers=my-topic-916932648-500952606, testKafkaConnectRequests=my-topic-825879374-1943650740, testCruiseControlMetrics=my-topic-1862902924-1176608323, testReceiveSimpleMessageTls=my-topic-941665209-436577257, testKafkaBrokersCount=my-topic-708967095-808914391, testClusterOperatorMetrics=my-topic-1725435997-1780998333, testUserOperatorMetrics=my-topic-1414123631-687906301, testKafkaMetricsSettings=my-topic-1375496082-553518335, testMirrorMakerTlsAuthenticated=my-topic-1936321612-505632869}
2022-03-30 18:03:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testKafkaConnectIoNetwork=my-cluster-580bbc7b-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-6da231a3-kafka-clients, testZookeeperWatchersCount=my-cluster-a66ecd5e-kafka-clients, testKafkaTopicPartitions=my-cluster-1625722f-kafka-clients, testMirrorMaker2Metrics=my-cluster-a4258561-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0-kafka-clients, testZookeeperAliveConnections=my-cluster-01c0acb0-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-fa981fe3-kafka-clients, testSendMessagesTlsScramSha=my-cluster-a11b9173-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc-kafka-clients, testZookeeperQuorumSize=my-cluster-5a26ca2a-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1-kafka-clients, testKafkaBridgeMetrics=my-cluster-0eb8fc89-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8-kafka-clients, testTopicOperatorMetrics=my-cluster-40f822af-kafka-clients, testKafkaActiveControllers=my-cluster-948e4115-kafka-clients, testKafkaConnectRequests=my-cluster-fc5c2305-kafka-clients, testCruiseControlMetrics=my-cluster-f2271154-kafka-clients, testReceiveSimpleMessageTls=my-cluster-90021a9f-kafka-clients, testKafkaBrokersCount=my-cluster-e91a8cef-kafka-clients, testClusterOperatorMetrics=my-cluster-2b296952-kafka-clients, testUserOperatorMetrics=my-cluster-7692a28e-kafka-clients, testKafkaMetricsSettings=my-cluster-f556a04e-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e-kafka-clients}
2022-03-30 18:03:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 18:03:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-s25n7 -n infra-namespace -- curl 172.17.0.24:9404
2022-03-30 18:03:24 [ForkJoinPool-3-worker-5] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.24 from Pod infra-namespace-kafka-clients-748578f786-s25n7 finished with return code: 0
2022-03-30 18:03:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 18:03:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 18:03:24 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 18:03:24 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:346] In context testKafkaConnectRequests is everything deleted.
2022-03-30 18:03:24 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 18:03:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:267] testKafkaConnectRequests - Notifies waiting test cases:[testUpdateUser, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testCruiseControlBasicAPIRequests, testSendMessagesTlsScramSha, testSendMessagesCustomListenerTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMaker2TlsAndTlsClientAuth, testMirrorMakerTlsAuthenticated, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaConnectIoNetwork, testKafkaBrokersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics, testKafkaConnectResponse, testKafkaExporterDifferentSetting] to and randomly select one to start execution
2022-03-30 18:03:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaConnectRequests
2022-03-30 18:03:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 2
2022-03-30 18:03:24 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaConnectRequests-FINISHED
2022-03-30 18:03:24 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 18:03:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:230] testKafkaConnectResponse test now can proceed its execution
2022-03-30 18:03:28 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 18:03:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaConnectResponse=my-cluster-9669a216, testSendSimpleMessageTls=my-cluster-128d9a40, testKafkaConnectIoNetwork=my-cluster-580bbc7b, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e, testCruiseControlBasicAPIRequests=my-cluster-6da231a3, testZookeeperWatchersCount=my-cluster-a66ecd5e, testKafkaTopicPartitions=my-cluster-1625722f, testMirrorMaker2Metrics=my-cluster-a4258561, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0, testZookeeperAliveConnections=my-cluster-01c0acb0, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2, testKafkaTopicUnderReplicatedPartitions=my-cluster-fa981fe3, testSendMessagesTlsScramSha=my-cluster-a11b9173, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc, testZookeeperQuorumSize=my-cluster-5a26ca2a, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1, testKafkaBridgeMetrics=my-cluster-0eb8fc89, testUpdateUser=my-cluster-75998b92, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8, testTopicOperatorMetrics=my-cluster-40f822af, testKafkaActiveControllers=my-cluster-948e4115, testKafkaConnectRequests=my-cluster-fc5c2305, testCruiseControlMetrics=my-cluster-f2271154, testReceiveSimpleMessageTls=my-cluster-90021a9f, testKafkaBrokersCount=my-cluster-e91a8cef, testClusterOperatorMetrics=my-cluster-2b296952, testUserOperatorMetrics=my-cluster-7692a28e, testKafkaMetricsSettings=my-cluster-f556a04e, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e}
2022-03-30 18:03:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaConnectResponse=my-user-156248344-120370488, testSendSimpleMessageTls=my-user-323988616-1355569855, testKafkaConnectIoNetwork=my-user-1092781470-1895809353, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-695287754-1004738297, testSendMessagesCustomListenerTlsScramSha=my-user-1653975062-1727371851, testCruiseControlBasicAPIRequests=my-user-918568860-36694764, testZookeeperWatchersCount=my-user-1252011538-504314473, testKafkaTopicPartitions=my-user-394346586-105422980, testMirrorMaker2Metrics=my-user-926413343-1872437319, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1337805046-1917759354, testZookeeperAliveConnections=my-user-704084348-2034238998, testAutoRenewAllCaCertsTriggeredByAnno=my-user-308582398-741623568, testKafkaTopicUnderReplicatedPartitions=my-user-1487552272-1931929933, testSendMessagesTlsScramSha=my-user-1791509549-1350003609, testMirrorMaker2TlsAndTlsClientAuth=my-user-711655589-495294801, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-83332781-1273282344, testRackAwareConnectCorrectDeployment=my-user-1839832768-1329691231, testZookeeperQuorumSize=my-user-1387929819-2059775559, testKafkaAndZookeeperScaleUpScaleDown=my-user-1975958319-512406578, testKafkaBridgeMetrics=my-user-1215630877-369211451, testUpdateUser=my-user-275427717-1183444028, testKafkaInDifferentNsThanClusterOperator=my-user-908767181-1543724508, testTopicOperatorMetrics=my-user-717431267-943604128, testKafkaActiveControllers=my-user-971800472-962265772, testKafkaConnectRequests=my-user-146416398-1606246430, testCruiseControlMetrics=my-user-155132995-1606538153, testReceiveSimpleMessageTls=my-user-491397521-1196361913, testKafkaBrokersCount=my-user-699069229-1729992055, testClusterOperatorMetrics=my-user-1075673018-1047529285, testUserOperatorMetrics=my-user-92263365-1802597441, testKafkaMetricsSettings=my-user-10583859-1000138651, testMirrorMakerTlsAuthenticated=my-user-1552986594-1641210083}
2022-03-30 18:03:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaConnectResponse=my-topic-130879205-3932964, testSendSimpleMessageTls=my-topic-1040221322-670398147, testKafkaConnectIoNetwork=my-topic-2145871889-1295201610, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1609923788-1785060017, testSendMessagesCustomListenerTlsScramSha=my-topic-629448478-708445494, testCruiseControlBasicAPIRequests=my-topic-710218582-658397020, testZookeeperWatchersCount=my-topic-1225869180-847659507, testKafkaTopicPartitions=my-topic-1086071779-2133194430, testMirrorMaker2Metrics=my-topic-1779646810-1685449566, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-604305255-1585536778, testZookeeperAliveConnections=my-topic-1222053841-372865493, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1728678022-515015095, testKafkaTopicUnderReplicatedPartitions=my-topic-893837582-953830717, testSendMessagesTlsScramSha=my-topic-1482193652-649135537, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1152366002-738531989, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1536476030-1482182833, testRackAwareConnectCorrectDeployment=my-topic-1526242410-482838543, testZookeeperQuorumSize=my-topic-1242495538-1808467385, testKafkaAndZookeeperScaleUpScaleDown=my-topic-1496158521-938246156, testKafkaBridgeMetrics=my-topic-90145110-388295797, testUpdateUser=my-topic-275186280-765877519, testKafkaInDifferentNsThanClusterOperator=my-topic-2051437288-2087424913, testTopicOperatorMetrics=my-topic-1059872505-1735356044, testKafkaActiveControllers=my-topic-916932648-500952606, testKafkaConnectRequests=my-topic-825879374-1943650740, testCruiseControlMetrics=my-topic-1862902924-1176608323, testReceiveSimpleMessageTls=my-topic-941665209-436577257, testKafkaBrokersCount=my-topic-708967095-808914391, testClusterOperatorMetrics=my-topic-1725435997-1780998333, testUserOperatorMetrics=my-topic-1414123631-687906301, testKafkaMetricsSettings=my-topic-1375496082-553518335, testMirrorMakerTlsAuthenticated=my-topic-1936321612-505632869}
2022-03-30 18:03:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaConnectResponse=my-cluster-9669a216-kafka-clients, testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testKafkaConnectIoNetwork=my-cluster-580bbc7b-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-6da231a3-kafka-clients, testZookeeperWatchersCount=my-cluster-a66ecd5e-kafka-clients, testKafkaTopicPartitions=my-cluster-1625722f-kafka-clients, testMirrorMaker2Metrics=my-cluster-a4258561-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0-kafka-clients, testZookeeperAliveConnections=my-cluster-01c0acb0-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-fa981fe3-kafka-clients, testSendMessagesTlsScramSha=my-cluster-a11b9173-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc-kafka-clients, testZookeeperQuorumSize=my-cluster-5a26ca2a-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1-kafka-clients, testKafkaBridgeMetrics=my-cluster-0eb8fc89-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8-kafka-clients, testTopicOperatorMetrics=my-cluster-40f822af-kafka-clients, testKafkaActiveControllers=my-cluster-948e4115-kafka-clients, testKafkaConnectRequests=my-cluster-fc5c2305-kafka-clients, testCruiseControlMetrics=my-cluster-f2271154-kafka-clients, testReceiveSimpleMessageTls=my-cluster-90021a9f-kafka-clients, testKafkaBrokersCount=my-cluster-e91a8cef-kafka-clients, testClusterOperatorMetrics=my-cluster-2b296952-kafka-clients, testUserOperatorMetrics=my-cluster-7692a28e-kafka-clients, testKafkaMetricsSettings=my-cluster-f556a04e-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e-kafka-clients}
2022-03-30 18:03:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 18:03:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-s25n7 -n infra-namespace -- curl 172.17.0.24:9404
2022-03-30 18:03:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:230] testKafkaExporterDifferentSetting test now can proceed its execution
2022-03-30 18:03:28 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 18:03:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaConnectResponse=my-cluster-9669a216, testSendSimpleMessageTls=my-cluster-128d9a40, testKafkaConnectIoNetwork=my-cluster-580bbc7b, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e, testCruiseControlBasicAPIRequests=my-cluster-6da231a3, testZookeeperWatchersCount=my-cluster-a66ecd5e, testKafkaTopicPartitions=my-cluster-1625722f, testMirrorMaker2Metrics=my-cluster-a4258561, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0, testZookeeperAliveConnections=my-cluster-01c0acb0, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2, testKafkaTopicUnderReplicatedPartitions=my-cluster-fa981fe3, testSendMessagesTlsScramSha=my-cluster-a11b9173, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc, testZookeeperQuorumSize=my-cluster-5a26ca2a, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1, testKafkaBridgeMetrics=my-cluster-0eb8fc89, testUpdateUser=my-cluster-75998b92, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8, testTopicOperatorMetrics=my-cluster-40f822af, testKafkaActiveControllers=my-cluster-948e4115, testKafkaExporterDifferentSetting=my-cluster-9e83be11, testKafkaConnectRequests=my-cluster-fc5c2305, testCruiseControlMetrics=my-cluster-f2271154, testReceiveSimpleMessageTls=my-cluster-90021a9f, testKafkaBrokersCount=my-cluster-e91a8cef, testClusterOperatorMetrics=my-cluster-2b296952, testUserOperatorMetrics=my-cluster-7692a28e, testKafkaMetricsSettings=my-cluster-f556a04e, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e}
2022-03-30 18:03:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaConnectResponse=my-user-156248344-120370488, testSendSimpleMessageTls=my-user-323988616-1355569855, testKafkaConnectIoNetwork=my-user-1092781470-1895809353, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-695287754-1004738297, testSendMessagesCustomListenerTlsScramSha=my-user-1653975062-1727371851, testCruiseControlBasicAPIRequests=my-user-918568860-36694764, testZookeeperWatchersCount=my-user-1252011538-504314473, testKafkaTopicPartitions=my-user-394346586-105422980, testMirrorMaker2Metrics=my-user-926413343-1872437319, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1337805046-1917759354, testZookeeperAliveConnections=my-user-704084348-2034238998, testAutoRenewAllCaCertsTriggeredByAnno=my-user-308582398-741623568, testKafkaTopicUnderReplicatedPartitions=my-user-1487552272-1931929933, testSendMessagesTlsScramSha=my-user-1791509549-1350003609, testMirrorMaker2TlsAndTlsClientAuth=my-user-711655589-495294801, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-83332781-1273282344, testRackAwareConnectCorrectDeployment=my-user-1839832768-1329691231, testZookeeperQuorumSize=my-user-1387929819-2059775559, testKafkaAndZookeeperScaleUpScaleDown=my-user-1975958319-512406578, testKafkaBridgeMetrics=my-user-1215630877-369211451, testUpdateUser=my-user-275427717-1183444028, testKafkaInDifferentNsThanClusterOperator=my-user-908767181-1543724508, testTopicOperatorMetrics=my-user-717431267-943604128, testKafkaActiveControllers=my-user-971800472-962265772, testKafkaExporterDifferentSetting=my-user-1271920772-1306432157, testKafkaConnectRequests=my-user-146416398-1606246430, testCruiseControlMetrics=my-user-155132995-1606538153, testReceiveSimpleMessageTls=my-user-491397521-1196361913, testKafkaBrokersCount=my-user-699069229-1729992055, testClusterOperatorMetrics=my-user-1075673018-1047529285, testUserOperatorMetrics=my-user-92263365-1802597441, testKafkaMetricsSettings=my-user-10583859-1000138651, testMirrorMakerTlsAuthenticated=my-user-1552986594-1641210083}
2022-03-30 18:03:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaConnectResponse=my-topic-130879205-3932964, testSendSimpleMessageTls=my-topic-1040221322-670398147, testKafkaConnectIoNetwork=my-topic-2145871889-1295201610, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1609923788-1785060017, testSendMessagesCustomListenerTlsScramSha=my-topic-629448478-708445494, testCruiseControlBasicAPIRequests=my-topic-710218582-658397020, testZookeeperWatchersCount=my-topic-1225869180-847659507, testKafkaTopicPartitions=my-topic-1086071779-2133194430, testMirrorMaker2Metrics=my-topic-1779646810-1685449566, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-604305255-1585536778, testZookeeperAliveConnections=my-topic-1222053841-372865493, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1728678022-515015095, testKafkaTopicUnderReplicatedPartitions=my-topic-893837582-953830717, testSendMessagesTlsScramSha=my-topic-1482193652-649135537, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1152366002-738531989, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1536476030-1482182833, testRackAwareConnectCorrectDeployment=my-topic-1526242410-482838543, testZookeeperQuorumSize=my-topic-1242495538-1808467385, testKafkaAndZookeeperScaleUpScaleDown=my-topic-1496158521-938246156, testKafkaBridgeMetrics=my-topic-90145110-388295797, testUpdateUser=my-topic-275186280-765877519, testKafkaInDifferentNsThanClusterOperator=my-topic-2051437288-2087424913, testTopicOperatorMetrics=my-topic-1059872505-1735356044, testKafkaActiveControllers=my-topic-916932648-500952606, testKafkaExporterDifferentSetting=my-topic-1899997873-490699538, testKafkaConnectRequests=my-topic-825879374-1943650740, testCruiseControlMetrics=my-topic-1862902924-1176608323, testReceiveSimpleMessageTls=my-topic-941665209-436577257, testKafkaBrokersCount=my-topic-708967095-808914391, testClusterOperatorMetrics=my-topic-1725435997-1780998333, testUserOperatorMetrics=my-topic-1414123631-687906301, testKafkaMetricsSettings=my-topic-1375496082-553518335, testMirrorMakerTlsAuthenticated=my-topic-1936321612-505632869}
2022-03-30 18:03:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaConnectResponse=my-cluster-9669a216-kafka-clients, testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testKafkaConnectIoNetwork=my-cluster-580bbc7b-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-6da231a3-kafka-clients, testZookeeperWatchersCount=my-cluster-a66ecd5e-kafka-clients, testKafkaTopicPartitions=my-cluster-1625722f-kafka-clients, testMirrorMaker2Metrics=my-cluster-a4258561-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0-kafka-clients, testZookeeperAliveConnections=my-cluster-01c0acb0-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-fa981fe3-kafka-clients, testSendMessagesTlsScramSha=my-cluster-a11b9173-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc-kafka-clients, testZookeeperQuorumSize=my-cluster-5a26ca2a-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1-kafka-clients, testKafkaBridgeMetrics=my-cluster-0eb8fc89-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8-kafka-clients, testTopicOperatorMetrics=my-cluster-40f822af-kafka-clients, testKafkaActiveControllers=my-cluster-948e4115-kafka-clients, testKafkaExporterDifferentSetting=my-cluster-9e83be11-kafka-clients, testKafkaConnectRequests=my-cluster-fc5c2305-kafka-clients, testCruiseControlMetrics=my-cluster-f2271154-kafka-clients, testReceiveSimpleMessageTls=my-cluster-90021a9f-kafka-clients, testKafkaBrokersCount=my-cluster-e91a8cef-kafka-clients, testClusterOperatorMetrics=my-cluster-2b296952-kafka-clients, testUserOperatorMetrics=my-cluster-7692a28e-kafka-clients, testKafkaMetricsSettings=my-cluster-f556a04e-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e-kafka-clients}
2022-03-30 18:03:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl exec metrics-cluster-name-kafka-exporter-8454677f49-p72xw -n infra-namespace -- cat /tmp/run.sh
2022-03-30 18:03:28 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.24 from Pod infra-namespace-kafka-clients-748578f786-s25n7 finished with return code: 0
2022-03-30 18:03:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 18:03:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 18:03:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 18:03:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:346] In context testKafkaConnectResponse is everything deleted.
2022-03-30 18:03:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 18:03:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:267] testKafkaConnectResponse - Notifies waiting test cases:[testUpdateUser, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testCruiseControlBasicAPIRequests, testSendMessagesTlsScramSha, testSendMessagesCustomListenerTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMaker2TlsAndTlsClientAuth, testMirrorMakerTlsAuthenticated, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaConnectIoNetwork, testKafkaBrokersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics, testKafkaExporterDifferentSetting] to and randomly select one to start execution
2022-03-30 18:03:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaConnectResponse
2022-03-30 18:03:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 1
2022-03-30 18:03:28 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaConnectResponse-FINISHED
2022-03-30 18:03:28 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 18:03:28 [ForkJoinPool-3-worker-7] [32mINFO [m [MetricsIsolatedST:608] Metrics collection for pod metrics-cluster-name-kafka-exporter-8454677f49-p72xw return code - 0
2022-03-30 18:03:28 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment metrics-cluster-name-kafka-exporter rolling update
2022-03-30 18:03:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace
2022-03-30 18:03:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-p72xw=a80ba23a-d5ca-4917-989e-4445775caba0}
2022-03-30 18:03:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-p72xw=a80ba23a-d5ca-4917-989e-4445775caba0}
2022-03-30 18:03:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {metrics-cluster-name-kafka-exporter-8454677f49-p72xw=a80ba23a-d5ca-4917-989e-4445775caba0}
2022-03-30 18:03:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace not ready, will try again in 5000 ms (599954ms till timeout)
2022-03-30 18:03:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-p72xw=a80ba23a-d5ca-4917-989e-4445775caba0}
2022-03-30 18:03:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-687d9cc8b-cpwtw=1a16d5d4-b529-43d4-895c-74dc273f3b99, metrics-cluster-name-kafka-exporter-8454677f49-p72xw=a80ba23a-d5ca-4917-989e-4445775caba0}
2022-03-30 18:03:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {metrics-cluster-name-kafka-exporter-8454677f49-p72xw=a80ba23a-d5ca-4917-989e-4445775caba0}
2022-03-30 18:03:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace not ready, will try again in 5000 ms (594947ms till timeout)
2022-03-30 18:03:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-p72xw=a80ba23a-d5ca-4917-989e-4445775caba0}
2022-03-30 18:03:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-687d9cc8b-cpwtw=1a16d5d4-b529-43d4-895c-74dc273f3b99, metrics-cluster-name-kafka-exporter-8454677f49-p72xw=a80ba23a-d5ca-4917-989e-4445775caba0}
2022-03-30 18:03:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {metrics-cluster-name-kafka-exporter-8454677f49-p72xw=a80ba23a-d5ca-4917-989e-4445775caba0}
2022-03-30 18:03:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace not ready, will try again in 5000 ms (589941ms till timeout)
2022-03-30 18:03:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-p72xw=a80ba23a-d5ca-4917-989e-4445775caba0}
2022-03-30 18:03:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-687d9cc8b-cpwtw=1a16d5d4-b529-43d4-895c-74dc273f3b99, metrics-cluster-name-kafka-exporter-8454677f49-p72xw=a80ba23a-d5ca-4917-989e-4445775caba0}
2022-03-30 18:03:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {metrics-cluster-name-kafka-exporter-8454677f49-p72xw=a80ba23a-d5ca-4917-989e-4445775caba0}
2022-03-30 18:03:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace not ready, will try again in 5000 ms (584934ms till timeout)
2022-03-30 18:03:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-p72xw=a80ba23a-d5ca-4917-989e-4445775caba0}
2022-03-30 18:03:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-687d9cc8b-cpwtw=1a16d5d4-b529-43d4-895c-74dc273f3b99, metrics-cluster-name-kafka-exporter-8454677f49-p72xw=a80ba23a-d5ca-4917-989e-4445775caba0}
2022-03-30 18:03:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {metrics-cluster-name-kafka-exporter-8454677f49-p72xw=a80ba23a-d5ca-4917-989e-4445775caba0}
2022-03-30 18:03:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace not ready, will try again in 5000 ms (579928ms till timeout)
2022-03-30 18:03:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-p72xw=a80ba23a-d5ca-4917-989e-4445775caba0}
2022-03-30 18:03:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-687d9cc8b-cpwtw=1a16d5d4-b529-43d4-895c-74dc273f3b99, metrics-cluster-name-kafka-exporter-8454677f49-p72xw=a80ba23a-d5ca-4917-989e-4445775caba0}
2022-03-30 18:03:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {metrics-cluster-name-kafka-exporter-8454677f49-p72xw=a80ba23a-d5ca-4917-989e-4445775caba0}
2022-03-30 18:03:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace not ready, will try again in 5000 ms (574921ms till timeout)
2022-03-30 18:03:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-p72xw=a80ba23a-d5ca-4917-989e-4445775caba0}
2022-03-30 18:03:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-687d9cc8b-cpwtw=1a16d5d4-b529-43d4-895c-74dc273f3b99, metrics-cluster-name-kafka-exporter-8454677f49-p72xw=a80ba23a-d5ca-4917-989e-4445775caba0}
2022-03-30 18:03:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {metrics-cluster-name-kafka-exporter-8454677f49-p72xw=a80ba23a-d5ca-4917-989e-4445775caba0}
2022-03-30 18:03:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace not ready, will try again in 5000 ms (569915ms till timeout)
2022-03-30 18:04:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-p72xw=a80ba23a-d5ca-4917-989e-4445775caba0}
2022-03-30 18:04:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-687d9cc8b-cpwtw=1a16d5d4-b529-43d4-895c-74dc273f3b99}
2022-03-30 18:04:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DeploymentUtils:119] All pods seem to have rolled
2022-03-30 18:04:03 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: metrics-cluster-name-kafka-exporter will be ready
2022-03-30 18:04:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: metrics-cluster-name-kafka-exporter will be ready
2022-03-30 18:04:03 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: metrics-cluster-name-kafka-exporter is ready
2022-03-30 18:04:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready
2022-03-30 18:04:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-687d9cc8b-cpwtw not ready: metrics-cluster-name-kafka-exporter)
2022-03-30 18:04:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-687d9cc8b-cpwtw are ready
2022-03-30 18:04:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599996ms till timeout)
2022-03-30 18:04:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-687d9cc8b-cpwtw not ready: metrics-cluster-name-kafka-exporter)
2022-03-30 18:04:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-687d9cc8b-cpwtw are ready
2022-03-30 18:04:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598992ms till timeout)
2022-03-30 18:04:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-687d9cc8b-cpwtw not ready: metrics-cluster-name-kafka-exporter)
2022-03-30 18:04:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-687d9cc8b-cpwtw are ready
2022-03-30 18:04:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597988ms till timeout)
2022-03-30 18:04:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-687d9cc8b-cpwtw not ready: metrics-cluster-name-kafka-exporter)
2022-03-30 18:04:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-687d9cc8b-cpwtw are ready
2022-03-30 18:04:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596983ms till timeout)
2022-03-30 18:04:07 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-687d9cc8b-cpwtw not ready: metrics-cluster-name-kafka-exporter)
2022-03-30 18:04:07 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-687d9cc8b-cpwtw are ready
2022-03-30 18:04:07 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595978ms till timeout)
2022-03-30 18:04:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-687d9cc8b-cpwtw not ready: metrics-cluster-name-kafka-exporter)
2022-03-30 18:04:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-687d9cc8b-cpwtw are ready
2022-03-30 18:04:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594974ms till timeout)
2022-03-30 18:04:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-687d9cc8b-cpwtw not ready: metrics-cluster-name-kafka-exporter)
2022-03-30 18:04:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-687d9cc8b-cpwtw are ready
2022-03-30 18:04:09 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593969ms till timeout)
2022-03-30 18:04:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-687d9cc8b-cpwtw not ready: metrics-cluster-name-kafka-exporter)
2022-03-30 18:04:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-687d9cc8b-cpwtw are ready
2022-03-30 18:04:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592965ms till timeout)
2022-03-30 18:04:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-687d9cc8b-cpwtw not ready: metrics-cluster-name-kafka-exporter)
2022-03-30 18:04:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-687d9cc8b-cpwtw are ready
2022-03-30 18:04:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591960ms till timeout)
2022-03-30 18:04:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-687d9cc8b-cpwtw not ready: metrics-cluster-name-kafka-exporter)
2022-03-30 18:04:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-687d9cc8b-cpwtw are ready
2022-03-30 18:04:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590955ms till timeout)
2022-03-30 18:04:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-687d9cc8b-cpwtw not ready: metrics-cluster-name-kafka-exporter)
2022-03-30 18:04:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-687d9cc8b-cpwtw are ready
2022-03-30 18:04:13 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:141] Deployment metrics-cluster-name-kafka-exporter rolling update finished
2022-03-30 18:04:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl exec metrics-cluster-name-kafka-exporter-687d9cc8b-cpwtw -n infra-namespace -- cat /tmp/run.sh
2022-03-30 18:04:14 [ForkJoinPool-3-worker-7] [32mINFO [m [MetricsIsolatedST:608] Metrics collection for pod metrics-cluster-name-kafka-exporter-687d9cc8b-cpwtw return code - 0
2022-03-30 18:04:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 18:04:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 18:04:14 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 18:04:14 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:346] In context testKafkaExporterDifferentSetting is everything deleted.
2022-03-30 18:04:14 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 18:04:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:267] testKafkaExporterDifferentSetting - Notifies waiting test cases:[testUpdateUser, testSendSimpleMessageTls, testReceiveSimpleMessageTls, testCruiseControlBasicAPIRequestsWithSecurityDisabled, testCruiseControlBasicAPIRequests, testSendMessagesTlsScramSha, testSendMessagesCustomListenerTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testMultiNodeKafkaConnectWithConnectorCreation, testMirrorMaker2TlsAndTlsClientAuth, testMirrorMakerTlsAuthenticated, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaConnectIoNetwork, testKafkaBrokersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testKafkaBridgeMetrics, testKafkaExporterDifferentSetting] to and randomly select one to start execution
2022-03-30 18:04:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaExporterDifferentSetting
2022-03-30 18:04:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 0
2022-03-30 18:04:14 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaExporterDifferentSetting-FINISHED
2022-03-30 18:04:14 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 18:04:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 18:04:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testReconcileStateMetricInTopicOperator-STARTED
2022-03-30 18:04:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 18:04:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 18:04:14 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 18:04:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaConnectResponse=my-cluster-9669a216, testSendSimpleMessageTls=my-cluster-128d9a40, testKafkaConnectIoNetwork=my-cluster-580bbc7b, testReconcileStateMetricInTopicOperator=my-cluster-42adc6a9, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e, testCruiseControlBasicAPIRequests=my-cluster-6da231a3, testZookeeperWatchersCount=my-cluster-a66ecd5e, testKafkaTopicPartitions=my-cluster-1625722f, testMirrorMaker2Metrics=my-cluster-a4258561, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0, testZookeeperAliveConnections=my-cluster-01c0acb0, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2, testKafkaTopicUnderReplicatedPartitions=my-cluster-fa981fe3, testSendMessagesTlsScramSha=my-cluster-a11b9173, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc, testZookeeperQuorumSize=my-cluster-5a26ca2a, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1, testKafkaBridgeMetrics=my-cluster-0eb8fc89, testUpdateUser=my-cluster-75998b92, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8, testTopicOperatorMetrics=my-cluster-40f822af, testKafkaActiveControllers=my-cluster-948e4115, testKafkaExporterDifferentSetting=my-cluster-9e83be11, testKafkaConnectRequests=my-cluster-fc5c2305, testCruiseControlMetrics=my-cluster-f2271154, testReceiveSimpleMessageTls=my-cluster-90021a9f, testKafkaBrokersCount=my-cluster-e91a8cef, testClusterOperatorMetrics=my-cluster-2b296952, testUserOperatorMetrics=my-cluster-7692a28e, testKafkaMetricsSettings=my-cluster-f556a04e, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e}
2022-03-30 18:04:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaConnectResponse=my-user-156248344-120370488, testSendSimpleMessageTls=my-user-323988616-1355569855, testKafkaConnectIoNetwork=my-user-1092781470-1895809353, testReconcileStateMetricInTopicOperator=my-user-643234685-1598469208, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-695287754-1004738297, testSendMessagesCustomListenerTlsScramSha=my-user-1653975062-1727371851, testCruiseControlBasicAPIRequests=my-user-918568860-36694764, testZookeeperWatchersCount=my-user-1252011538-504314473, testKafkaTopicPartitions=my-user-394346586-105422980, testMirrorMaker2Metrics=my-user-926413343-1872437319, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1337805046-1917759354, testZookeeperAliveConnections=my-user-704084348-2034238998, testAutoRenewAllCaCertsTriggeredByAnno=my-user-308582398-741623568, testKafkaTopicUnderReplicatedPartitions=my-user-1487552272-1931929933, testSendMessagesTlsScramSha=my-user-1791509549-1350003609, testMirrorMaker2TlsAndTlsClientAuth=my-user-711655589-495294801, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-83332781-1273282344, testRackAwareConnectCorrectDeployment=my-user-1839832768-1329691231, testZookeeperQuorumSize=my-user-1387929819-2059775559, testKafkaAndZookeeperScaleUpScaleDown=my-user-1975958319-512406578, testKafkaBridgeMetrics=my-user-1215630877-369211451, testUpdateUser=my-user-275427717-1183444028, testKafkaInDifferentNsThanClusterOperator=my-user-908767181-1543724508, testTopicOperatorMetrics=my-user-717431267-943604128, testKafkaActiveControllers=my-user-971800472-962265772, testKafkaExporterDifferentSetting=my-user-1271920772-1306432157, testKafkaConnectRequests=my-user-146416398-1606246430, testCruiseControlMetrics=my-user-155132995-1606538153, testReceiveSimpleMessageTls=my-user-491397521-1196361913, testKafkaBrokersCount=my-user-699069229-1729992055, testClusterOperatorMetrics=my-user-1075673018-1047529285, testUserOperatorMetrics=my-user-92263365-1802597441, testKafkaMetricsSettings=my-user-10583859-1000138651, testMirrorMakerTlsAuthenticated=my-user-1552986594-1641210083}
2022-03-30 18:04:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaConnectResponse=my-topic-130879205-3932964, testSendSimpleMessageTls=my-topic-1040221322-670398147, testKafkaConnectIoNetwork=my-topic-2145871889-1295201610, testReconcileStateMetricInTopicOperator=my-topic-1542111309-257341397, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1609923788-1785060017, testSendMessagesCustomListenerTlsScramSha=my-topic-629448478-708445494, testCruiseControlBasicAPIRequests=my-topic-710218582-658397020, testZookeeperWatchersCount=my-topic-1225869180-847659507, testKafkaTopicPartitions=my-topic-1086071779-2133194430, testMirrorMaker2Metrics=my-topic-1779646810-1685449566, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-604305255-1585536778, testZookeeperAliveConnections=my-topic-1222053841-372865493, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1728678022-515015095, testKafkaTopicUnderReplicatedPartitions=my-topic-893837582-953830717, testSendMessagesTlsScramSha=my-topic-1482193652-649135537, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1152366002-738531989, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1536476030-1482182833, testRackAwareConnectCorrectDeployment=my-topic-1526242410-482838543, testZookeeperQuorumSize=my-topic-1242495538-1808467385, testKafkaAndZookeeperScaleUpScaleDown=my-topic-1496158521-938246156, testKafkaBridgeMetrics=my-topic-90145110-388295797, testUpdateUser=my-topic-275186280-765877519, testKafkaInDifferentNsThanClusterOperator=my-topic-2051437288-2087424913, testTopicOperatorMetrics=my-topic-1059872505-1735356044, testKafkaActiveControllers=my-topic-916932648-500952606, testKafkaExporterDifferentSetting=my-topic-1899997873-490699538, testKafkaConnectRequests=my-topic-825879374-1943650740, testCruiseControlMetrics=my-topic-1862902924-1176608323, testReceiveSimpleMessageTls=my-topic-941665209-436577257, testKafkaBrokersCount=my-topic-708967095-808914391, testClusterOperatorMetrics=my-topic-1725435997-1780998333, testUserOperatorMetrics=my-topic-1414123631-687906301, testKafkaMetricsSettings=my-topic-1375496082-553518335, testMirrorMakerTlsAuthenticated=my-topic-1936321612-505632869}
2022-03-30 18:04:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaConnectResponse=my-cluster-9669a216-kafka-clients, testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testKafkaConnectIoNetwork=my-cluster-580bbc7b-kafka-clients, testReconcileStateMetricInTopicOperator=my-cluster-42adc6a9-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-6da231a3-kafka-clients, testZookeeperWatchersCount=my-cluster-a66ecd5e-kafka-clients, testKafkaTopicPartitions=my-cluster-1625722f-kafka-clients, testMirrorMaker2Metrics=my-cluster-a4258561-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0-kafka-clients, testZookeeperAliveConnections=my-cluster-01c0acb0-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-fa981fe3-kafka-clients, testSendMessagesTlsScramSha=my-cluster-a11b9173-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc-kafka-clients, testZookeeperQuorumSize=my-cluster-5a26ca2a-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1-kafka-clients, testKafkaBridgeMetrics=my-cluster-0eb8fc89-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8-kafka-clients, testTopicOperatorMetrics=my-cluster-40f822af-kafka-clients, testKafkaActiveControllers=my-cluster-948e4115-kafka-clients, testKafkaExporterDifferentSetting=my-cluster-9e83be11-kafka-clients, testKafkaConnectRequests=my-cluster-fc5c2305-kafka-clients, testCruiseControlMetrics=my-cluster-f2271154-kafka-clients, testReceiveSimpleMessageTls=my-cluster-90021a9f-kafka-clients, testKafkaBrokersCount=my-cluster-e91a8cef-kafka-clients, testClusterOperatorMetrics=my-cluster-2b296952-kafka-clients, testUserOperatorMetrics=my-cluster-7692a28e-kafka-clients, testKafkaMetricsSettings=my-cluster-f556a04e-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e-kafka-clients}
2022-03-30 18:04:14 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-metrics-cluster-test
2022-03-30 18:04:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1542111309-257341397 in namespace second-metrics-cluster-test
2022-03-30 18:04:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1542111309-257341397
2022-03-30 18:04:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1542111309-257341397 will have desired state: Ready
2022-03-30 18:04:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1542111309-257341397 will have desired state: Ready
2022-03-30 18:04:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1542111309-257341397 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 18:04:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1542111309-257341397 is in desired state: Ready
2022-03-30 18:04:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 18:04:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec second-metrics-cluster-test-kafka-clients-6c57d6898-pk49z -n second-metrics-cluster-test -- curl 172.17.0.14:8080/metrics
2022-03-30 18:04:15 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.14 from Pod second-metrics-cluster-test-kafka-clients-6c57d6898-pk49z finished with return code: 0
2022-03-30 18:04:15 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:553] Checking if resource state metric reason message is "none" and KafkaTopic is ready
2022-03-30 18:04:15 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:556] Changing topic name in spec.topicName
2022-03-30 18:04:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1542111309-257341397 will have desired state: NotReady
2022-03-30 18:04:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1542111309-257341397 will have desired state: NotReady
2022-03-30 18:04:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1542111309-257341397 will have desired state: NotReady not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 18:04:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1542111309-257341397 is in desired state: NotReady
2022-03-30 18:04:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 18:04:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec second-metrics-cluster-test-kafka-clients-6c57d6898-pk49z -n second-metrics-cluster-test -- curl 172.17.0.14:8080/metrics
2022-03-30 18:04:16 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.14 from Pod second-metrics-cluster-test-kafka-clients-6c57d6898-pk49z finished with return code: 0
2022-03-30 18:04:16 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:564] Changing back to it's original name and scaling replicas to be higher number
2022-03-30 18:04:16 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:132] Waiting for KafkaTopic change my-topic-1542111309-257341397
2022-03-30 18:04:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic change my-topic-1542111309-257341397
2022-03-30 18:04:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 18:04:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec second-metrics-cluster-test-kafka-clients-6c57d6898-pk49z -n second-metrics-cluster-test -- curl 172.17.0.14:8080/metrics
2022-03-30 18:04:16 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.14 from Pod second-metrics-cluster-test-kafka-clients-6c57d6898-pk49z finished with return code: 0
2022-03-30 18:04:16 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:576] Scaling replicas to be higher than before
2022-03-30 18:04:16 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:132] Waiting for KafkaTopic change my-topic-1542111309-257341397
2022-03-30 18:04:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic change my-topic-1542111309-257341397
2022-03-30 18:04:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 18:04:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec second-metrics-cluster-test-kafka-clients-6c57d6898-pk49z -n second-metrics-cluster-test -- curl 172.17.0.14:8080/metrics
2022-03-30 18:04:16 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.14 from Pod second-metrics-cluster-test-kafka-clients-6c57d6898-pk49z finished with return code: 0
2022-03-30 18:04:16 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:584] Changing KafkaTopic's spec to correct state
2022-03-30 18:04:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1542111309-257341397 will have desired state: Ready
2022-03-30 18:04:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1542111309-257341397 will have desired state: Ready
2022-03-30 18:04:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1542111309-257341397 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 18:04:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1542111309-257341397 is in desired state: Ready
2022-03-30 18:04:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 18:04:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec second-metrics-cluster-test-kafka-clients-6c57d6898-pk49z -n second-metrics-cluster-test -- curl 172.17.0.14:8080/metrics
2022-03-30 18:04:18 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.14 from Pod second-metrics-cluster-test-kafka-clients-6c57d6898-pk49z finished with return code: 0
2022-03-30 18:04:18 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 18:04:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 18:04:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 18:04:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 18:04:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testReconcileStateMetricInTopicOperator
2022-03-30 18:04:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1542111309-257341397 in namespace second-metrics-cluster-test
2022-03-30 18:04:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1542111309-257341397
2022-03-30 18:04:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1542111309-257341397 not ready, will try again in 10000 ms (179995ms till timeout)
2022-03-30 18:04:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 18:04:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testReconcileStateMetricInTopicOperator-FINISHED
2022-03-30 18:04:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 18:04:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 18:04:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaExporterDataAfterExchange-STARTED
2022-03-30 18:04:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 18:04:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 18:04:28 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 18:04:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaConnectResponse=my-cluster-9669a216, testSendSimpleMessageTls=my-cluster-128d9a40, testKafkaConnectIoNetwork=my-cluster-580bbc7b, testReconcileStateMetricInTopicOperator=my-cluster-42adc6a9, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e, testCruiseControlBasicAPIRequests=my-cluster-6da231a3, testZookeeperWatchersCount=my-cluster-a66ecd5e, testKafkaTopicPartitions=my-cluster-1625722f, testMirrorMaker2Metrics=my-cluster-a4258561, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0, testZookeeperAliveConnections=my-cluster-01c0acb0, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2, testKafkaTopicUnderReplicatedPartitions=my-cluster-fa981fe3, testSendMessagesTlsScramSha=my-cluster-a11b9173, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc, testZookeeperQuorumSize=my-cluster-5a26ca2a, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1, testKafkaBridgeMetrics=my-cluster-0eb8fc89, testKafkaExporterDataAfterExchange=my-cluster-651df0cd, testUpdateUser=my-cluster-75998b92, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8, testTopicOperatorMetrics=my-cluster-40f822af, testKafkaActiveControllers=my-cluster-948e4115, testKafkaExporterDifferentSetting=my-cluster-9e83be11, testKafkaConnectRequests=my-cluster-fc5c2305, testCruiseControlMetrics=my-cluster-f2271154, testReceiveSimpleMessageTls=my-cluster-90021a9f, testKafkaBrokersCount=my-cluster-e91a8cef, testClusterOperatorMetrics=my-cluster-2b296952, testUserOperatorMetrics=my-cluster-7692a28e, testKafkaMetricsSettings=my-cluster-f556a04e, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e}
2022-03-30 18:04:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaConnectResponse=my-user-156248344-120370488, testSendSimpleMessageTls=my-user-323988616-1355569855, testKafkaConnectIoNetwork=my-user-1092781470-1895809353, testReconcileStateMetricInTopicOperator=my-user-643234685-1598469208, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-695287754-1004738297, testSendMessagesCustomListenerTlsScramSha=my-user-1653975062-1727371851, testCruiseControlBasicAPIRequests=my-user-918568860-36694764, testZookeeperWatchersCount=my-user-1252011538-504314473, testKafkaTopicPartitions=my-user-394346586-105422980, testMirrorMaker2Metrics=my-user-926413343-1872437319, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1337805046-1917759354, testZookeeperAliveConnections=my-user-704084348-2034238998, testAutoRenewAllCaCertsTriggeredByAnno=my-user-308582398-741623568, testKafkaTopicUnderReplicatedPartitions=my-user-1487552272-1931929933, testSendMessagesTlsScramSha=my-user-1791509549-1350003609, testMirrorMaker2TlsAndTlsClientAuth=my-user-711655589-495294801, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-83332781-1273282344, testRackAwareConnectCorrectDeployment=my-user-1839832768-1329691231, testZookeeperQuorumSize=my-user-1387929819-2059775559, testKafkaAndZookeeperScaleUpScaleDown=my-user-1975958319-512406578, testKafkaBridgeMetrics=my-user-1215630877-369211451, testKafkaExporterDataAfterExchange=my-user-1331947501-1407239216, testUpdateUser=my-user-275427717-1183444028, testKafkaInDifferentNsThanClusterOperator=my-user-908767181-1543724508, testTopicOperatorMetrics=my-user-717431267-943604128, testKafkaActiveControllers=my-user-971800472-962265772, testKafkaExporterDifferentSetting=my-user-1271920772-1306432157, testKafkaConnectRequests=my-user-146416398-1606246430, testCruiseControlMetrics=my-user-155132995-1606538153, testReceiveSimpleMessageTls=my-user-491397521-1196361913, testKafkaBrokersCount=my-user-699069229-1729992055, testClusterOperatorMetrics=my-user-1075673018-1047529285, testUserOperatorMetrics=my-user-92263365-1802597441, testKafkaMetricsSettings=my-user-10583859-1000138651, testMirrorMakerTlsAuthenticated=my-user-1552986594-1641210083}
2022-03-30 18:04:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaConnectResponse=my-topic-130879205-3932964, testSendSimpleMessageTls=my-topic-1040221322-670398147, testKafkaConnectIoNetwork=my-topic-2145871889-1295201610, testReconcileStateMetricInTopicOperator=my-topic-1542111309-257341397, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-1609923788-1785060017, testSendMessagesCustomListenerTlsScramSha=my-topic-629448478-708445494, testCruiseControlBasicAPIRequests=my-topic-710218582-658397020, testZookeeperWatchersCount=my-topic-1225869180-847659507, testKafkaTopicPartitions=my-topic-1086071779-2133194430, testMirrorMaker2Metrics=my-topic-1779646810-1685449566, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-604305255-1585536778, testZookeeperAliveConnections=my-topic-1222053841-372865493, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1728678022-515015095, testKafkaTopicUnderReplicatedPartitions=my-topic-893837582-953830717, testSendMessagesTlsScramSha=my-topic-1482193652-649135537, testMirrorMaker2TlsAndTlsClientAuth=my-topic-1152366002-738531989, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-1536476030-1482182833, testRackAwareConnectCorrectDeployment=my-topic-1526242410-482838543, testZookeeperQuorumSize=my-topic-1242495538-1808467385, testKafkaAndZookeeperScaleUpScaleDown=my-topic-1496158521-938246156, testKafkaBridgeMetrics=my-topic-90145110-388295797, testKafkaExporterDataAfterExchange=my-topic-1806238814-200216369, testUpdateUser=my-topic-275186280-765877519, testKafkaInDifferentNsThanClusterOperator=my-topic-2051437288-2087424913, testTopicOperatorMetrics=my-topic-1059872505-1735356044, testKafkaActiveControllers=my-topic-916932648-500952606, testKafkaExporterDifferentSetting=my-topic-1899997873-490699538, testKafkaConnectRequests=my-topic-825879374-1943650740, testCruiseControlMetrics=my-topic-1862902924-1176608323, testReceiveSimpleMessageTls=my-topic-941665209-436577257, testKafkaBrokersCount=my-topic-708967095-808914391, testClusterOperatorMetrics=my-topic-1725435997-1780998333, testUserOperatorMetrics=my-topic-1414123631-687906301, testKafkaMetricsSettings=my-topic-1375496082-553518335, testMirrorMakerTlsAuthenticated=my-topic-1936321612-505632869}
2022-03-30 18:04:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaConnectResponse=my-cluster-9669a216-kafka-clients, testSendSimpleMessageTls=my-cluster-128d9a40-kafka-clients, testKafkaConnectIoNetwork=my-cluster-580bbc7b-kafka-clients, testReconcileStateMetricInTopicOperator=my-cluster-42adc6a9-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-8d8daaa5-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-31e27e7e-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-6da231a3-kafka-clients, testZookeeperWatchersCount=my-cluster-a66ecd5e-kafka-clients, testKafkaTopicPartitions=my-cluster-1625722f-kafka-clients, testMirrorMaker2Metrics=my-cluster-a4258561-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-138b60b0-kafka-clients, testZookeeperAliveConnections=my-cluster-01c0acb0-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-60aa0ca2-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-fa981fe3-kafka-clients, testSendMessagesTlsScramSha=my-cluster-a11b9173-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-499ae2e7-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-427f35de-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-b2005abc-kafka-clients, testZookeeperQuorumSize=my-cluster-5a26ca2a-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-e6ec2fe1-kafka-clients, testKafkaBridgeMetrics=my-cluster-0eb8fc89-kafka-clients, testKafkaExporterDataAfterExchange=my-cluster-651df0cd-kafka-clients, testUpdateUser=my-cluster-75998b92-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-7e64bad8-kafka-clients, testTopicOperatorMetrics=my-cluster-40f822af-kafka-clients, testKafkaActiveControllers=my-cluster-948e4115-kafka-clients, testKafkaExporterDifferentSetting=my-cluster-9e83be11-kafka-clients, testKafkaConnectRequests=my-cluster-fc5c2305-kafka-clients, testCruiseControlMetrics=my-cluster-f2271154-kafka-clients, testReceiveSimpleMessageTls=my-cluster-90021a9f-kafka-clients, testKafkaBrokersCount=my-cluster-e91a8cef-kafka-clients, testClusterOperatorMetrics=my-cluster-2b296952-kafka-clients, testUserOperatorMetrics=my-cluster-7692a28e-kafka-clients, testKafkaMetricsSettings=my-cluster-f556a04e-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-4ad8959e-kafka-clients}
2022-03-30 18:04:28 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-30 18:04:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5b88eb81, which are set.
2022-03-30 18:04:28 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@1d2e23dc, messages=[], arguments=[--topic, my-topic-840173747-1931076155, --bootstrap-server, metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092, --max-messages, 5000], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='infra-namespace-kafka-clients-748578f786-s25n7', podNamespace='infra-namespace', bootstrapServer='metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092', topicName='my-topic-840173747-1931076155', maxMessages=5000, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5b88eb81}
2022-03-30 18:04:28 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:94] Producing 5000 messages to metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092:my-topic-840173747-1931076155 from pod infra-namespace-kafka-clients-748578f786-s25n7
2022-03-30 18:04:28 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec infra-namespace-kafka-clients-748578f786-s25n7 -n infra-namespace -- /opt/kafka/producer.sh --topic my-topic-840173747-1931076155 --bootstrap-server metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092 --max-messages 5000
2022-03-30 18:04:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-s25n7 -n infra-namespace -- /opt/kafka/producer.sh --topic my-topic-840173747-1931076155 --bootstrap-server metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092 --max-messages 5000
2022-03-30 18:04:30 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-03-30 18:04:30 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:101] Producer produced 5000 messages
2022-03-30 18:04:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3d57b5, which are set.
2022-03-30 18:04:30 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@43d16537, messages=[], arguments=[--group-id, my-consumer-group-1633611811, --topic, my-topic-840173747-1931076155, --bootstrap-server, metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092, --max-messages, 5000, --group-instance-id, instance2009541324], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='infra-namespace-kafka-clients-748578f786-s25n7', podNamespace='infra-namespace', bootstrapServer='metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092', topicName='my-topic-840173747-1931076155', maxMessages=5000, kafkaUsername='null', consumerGroupName='my-consumer-group-1633611811', consumerInstanceId='instance2009541324', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3d57b5}
2022-03-30 18:04:30 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:157] Consuming 5000 messages from metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092#my-topic-840173747-1931076155 from pod infra-namespace-kafka-clients-748578f786-s25n7
2022-03-30 18:04:30 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec infra-namespace-kafka-clients-748578f786-s25n7 -n infra-namespace -- /opt/kafka/consumer.sh --group-id my-consumer-group-1633611811 --topic my-topic-840173747-1931076155 --bootstrap-server metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092 --max-messages 5000 --group-instance-id instance2009541324
2022-03-30 18:04:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-s25n7 -n infra-namespace -- /opt/kafka/consumer.sh --group-id my-consumer-group-1633611811 --topic my-topic-840173747-1931076155 --bootstrap-server metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092 --max-messages 5000 --group-instance-id instance2009541324
2022-03-30 18:04:36 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-30 18:04:36 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 5000 messages
2022-03-30 18:04:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 18:04:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-s25n7 -n infra-namespace -- curl 172.17.0.25:9404/metrics
2022-03-30 18:04:36 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.25 from Pod infra-namespace-kafka-clients-748578f786-s25n7 finished with return code: 0
2022-03-30 18:04:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 18:04:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 18:04:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 18:04:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context testKafkaExporterDataAfterExchange is everything deleted.
2022-03-30 18:04:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 18:04:36 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaExporterDataAfterExchange-FINISHED
2022-03-30 18:04:36 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 18:04:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 18:04:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:690] [metrics.MetricsIsolatedST - After All] - Clean up after test suite
2022-03-30 18:04:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 18:04:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for MetricsIsolatedST
2022-03-30 18:04:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-39163752-1058894052 in namespace infra-namespace
2022-03-30 18:04:36 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge my-bridge in namespace infra-namespace
2022-03-30 18:04:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaBridge:my-bridge
2022-03-30 18:04:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-39163752-1058894052
2022-03-30 18:04:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaBridge:my-bridge not ready, will try again in 10000 ms (479993ms till timeout)
2022-03-30 18:04:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-39163752-1058894052 not ready, will try again in 10000 ms (179994ms till timeout)
2022-03-30 18:04:46 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 mm2-cluster in namespace infra-namespace
2022-03-30 18:04:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect metrics-cluster-name in namespace infra-namespace
2022-03-30 18:04:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaMirrorMaker2:mm2-cluster
2022-03-30 18:04:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaConnect:metrics-cluster-name
2022-03-30 18:04:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaMirrorMaker2:mm2-cluster not ready, will try again in 10000 ms (599986ms till timeout)
2022-03-30 18:04:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaConnect:metrics-cluster-name not ready, will try again in 10000 ms (599986ms till timeout)
2022-03-30 18:04:56 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1642297574-660862374 in namespace infra-namespace
2022-03-30 18:04:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1177668122-1476973967 in namespace infra-namespace
2022-03-30 18:04:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1642297574-660862374
2022-03-30 18:04:56 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1177668122-1476973967
2022-03-30 18:04:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1177668122-1476973967 not ready, will try again in 10000 ms (179994ms till timeout)
2022-03-30 18:04:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1642297574-660862374 not ready, will try again in 10000 ms (179994ms till timeout)
2022-03-30 18:05:06 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-840173747-1931076155 in namespace infra-namespace
2022-03-30 18:05:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-180171383-280558322 in namespace infra-namespace
2022-03-30 18:05:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-840173747-1931076155
2022-03-30 18:05:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-180171383-280558322
2022-03-30 18:05:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-840173747-1931076155 not ready, will try again in 10000 ms (179990ms till timeout)
2022-03-30 18:05:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-180171383-280558322 not ready, will try again in 10000 ms (179991ms till timeout)
2022-03-30 18:05:16 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment infra-namespace-kafka-clients in namespace infra-namespace
2022-03-30 18:05:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy second-kafka-cluster-entity-operator-allow in namespace second-metrics-cluster-test
2022-03-30 18:05:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:infra-namespace-kafka-clients
2022-03-30 18:05:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:second-kafka-cluster-entity-operator-allow
2022-03-30 18:05:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy metrics-cluster-name-kafka-exporter-allow in namespace infra-namespace
2022-03-30 18:05:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:metrics-cluster-name-kafka-exporter-allow
2022-03-30 18:05:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:infra-namespace-kafka-clients not ready, will try again in 10000 ms (479988ms till timeout)
2022-03-30 18:05:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy second-kafka-cluster-kafka-exporter-allow in namespace second-metrics-cluster-test
2022-03-30 18:05:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:second-kafka-cluster-kafka-exporter-allow
2022-03-30 18:05:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy cluster-operator-allow in namespace infra-namespace
2022-03-30 18:05:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:cluster-operator-allow
2022-03-30 18:05:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy metrics-cluster-name-entity-operator-allow in namespace infra-namespace
2022-03-30 18:05:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:metrics-cluster-name-entity-operator-allow
2022-03-30 18:05:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka metrics-cluster-name in namespace infra-namespace
2022-03-30 18:05:16 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace infra-namespace, for cruise control Kafka cluster metrics-cluster-name
2022-03-30 18:05:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:metrics-cluster-name
2022-03-30 18:05:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:metrics-cluster-name not ready, will try again in 10000 ms (839997ms till timeout)
2022-03-30 18:05:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:infra-namespace-kafka-clients not ready, will try again in 10000 ms (469979ms till timeout)
2022-03-30 18:05:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka second-kafka-cluster in namespace second-metrics-cluster-test
2022-03-30 18:05:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:second-kafka-cluster
2022-03-30 18:05:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:second-kafka-cluster not ready, will try again in 10000 ms (839957ms till timeout)
2022-03-30 18:05:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:infra-namespace-kafka-clients not ready, will try again in 10000 ms (459970ms till timeout)
2022-03-30 18:05:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:infra-namespace-kafka-clients not ready, will try again in 10000 ms (449961ms till timeout)
2022-03-30 18:05:56 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment second-metrics-cluster-test-kafka-clients in namespace second-metrics-cluster-test
2022-03-30 18:05:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:second-metrics-cluster-test-kafka-clients
2022-03-30 18:05:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:second-metrics-cluster-test-kafka-clients not ready, will try again in 10000 ms (479993ms till timeout)
2022-03-30 18:06:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:second-metrics-cluster-test-kafka-clients not ready, will try again in 10000 ms (469986ms till timeout)
2022-03-30 18:06:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:second-metrics-cluster-test-kafka-clients not ready, will try again in 10000 ms (459978ms till timeout)
2022-03-30 18:06:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:second-metrics-cluster-test-kafka-clients not ready, will try again in 10000 ms (449970ms till timeout)
2022-03-30 18:06:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 20, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 864.74 s - in io.strimzi.systemtest.metrics.MetricsIsolatedST
2022-03-30 18:06:36 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-30 18:06:36 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-30 18:06:36 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-30 18:06:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 18:06:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-30 18:06:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 18:06:36 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 18:06:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 18:06:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 18:06:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 18:06:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 18:06:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 18:06:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 18:06:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 18:06:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179949ms till timeout)
2022-03-30 18:06:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 18:06:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 18:06:36 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 18:06:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 18:06:37 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 18:06:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io not ready, will try again in 10000 ms (179955ms till timeout)
2022-03-30 18:06:46 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 18:06:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 18:06:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io not ready, will try again in 10000 ms (179992ms till timeout)
2022-03-30 18:06:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 18:06:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 18:06:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 18:06:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 18:06:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace second-metrics-cluster-test
2022-03-30 18:06:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 18:06:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace second-metrics-cluster-test
2022-03-30 18:06:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 18:06:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 18:06:47 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 18:06:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator not ready, will try again in 10000 ms (479953ms till timeout)
2022-03-30 18:06:56 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 18:06:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 18:06:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 18:06:57 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 18:06:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 18:06:57 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 18:06:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 18:06:57 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 18:06:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 18:06:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 18:06:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 18:06:57 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 18:06:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 18:06:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 18:06:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 18:06:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 18:06:57 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io not ready, will try again in 10000 ms (179977ms till timeout)
2022-03-30 18:06:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 18:06:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 18:06:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 18:06:57 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 18:06:57 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 18:06:57 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 18:06:57 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 18:06:57 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 18:06:57 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 18:06:57 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 18:06:57 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 18:06:57 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 18:06:57 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 18:06:57 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 18:06:57 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179886ms till timeout)
2022-03-30 18:07:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 18:07:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-30 18:07:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 18:07:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 18:07:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v64947
2022-03-30 18:07:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v64947
2022-03-30 18:07:07 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=64947&allowWatchBookmarks=true&watch=true...
2022-03-30 18:07:07 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 18:07:07 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 64948
2022-03-30 18:07:13 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 65152
2022-03-30 18:07:18 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 65157
2022-03-30 18:07:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: second-metrics-cluster-test
2022-03-30 18:07:18 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v65152 in namespace default
2022-03-30 18:07:18 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@364f27a5
2022-03-30 18:07:18 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 18:07:18 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@3270a5b4
2022-03-30 18:07:18 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@3270a5b4
2022-03-30 18:07:18 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@3270a5b4
2022-03-30 18:07:18 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 18:07:18 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 18:07:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 18:07:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 18:07:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v65158
2022-03-30 18:07:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v65158
2022-03-30 18:07:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dsecond-metrics-cluster-test&resourceVersion=65158&allowWatchBookmarks=true&watch=true...
2022-03-30 18:07:18 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 18:07:18 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 65159
2022-03-30 18:07:23 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 65235
2022-03-30 18:07:28 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 65240
2022-03-30 18:07:28 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v65235 in namespace default
2022-03-30 18:07:28 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@972335a
2022-03-30 18:07:28 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 18:07:28 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@464f5fde
2022-03-30 18:07:28 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@464f5fde
2022-03-30 18:07:28 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@464f5fde
2022-03-30 18:07:28 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 18:07:28 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 18:07:28 [main] [32mINFO [m [TestExecutionListener:40] =======================================================================
2022-03-30 18:07:28 [main] [32mINFO [m [TestExecutionListener:41] =======================================================================
2022-03-30 18:07:28 [main] [32mINFO [m [TestExecutionListener:42]                         Test run finished
2022-03-30 18:07:28 [main] [32mINFO [m [TestExecutionListener:43] =======================================================================
2022-03-30 18:07:28 [main] [32mINFO [m [TestExecutionListener:44] =======================================================================
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 35, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] [1mReactor Summary for Strimzi - Apache Kafka on Kubernetes and OpenShift 0.29.0-SNAPSHOT:[m
[[1;34mINFO[m] 
[[1;34mINFO[m] Strimzi - Apache Kafka on Kubernetes and OpenShift . [1;32mSUCCESS[m [  2.563 s]
[[1;34mINFO[m] test ............................................... [1;32mSUCCESS[m [  1.014 s]
[[1;34mINFO[m] crd-annotations .................................... [1;32mSUCCESS[m [  1.039 s]
[[1;34mINFO[m] crd-generator ...................................... [1;32mSUCCESS[m [  2.552 s]
[[1;34mINFO[m] api ................................................ [1;32mSUCCESS[m [  6.777 s]
[[1;34mINFO[m] mockkube ........................................... [1;32mSUCCESS[m [  0.900 s]
[[1;34mINFO[m] config-model ....................................... [1;32mSUCCESS[m [  0.730 s]
[[1;34mINFO[m] certificate-manager ................................ [1;32mSUCCESS[m [  0.776 s]
[[1;34mINFO[m] operator-common .................................... [1;32mSUCCESS[m [  1.836 s]
[[1;34mINFO[m] systemtest ......................................... [1;32mSUCCESS[m [  01:30 h]
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] [1;32mBUILD SUCCESS[m
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] Total time:  01:31 h
[[1;34mINFO[m] Finished at: 2022-03-30T18:07:28Z
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
