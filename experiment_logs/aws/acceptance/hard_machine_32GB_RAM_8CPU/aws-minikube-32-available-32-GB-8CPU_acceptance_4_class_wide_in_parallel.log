[[1;34mINFO[m] Scanning for projects...
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] [1mReactor Build Order:[m
[[1;34mINFO[m] 
[[1;34mINFO[m] Strimzi - Apache Kafka on Kubernetes and OpenShift                 [pom]
[[1;34mINFO[m] test                                                               [jar]
[[1;34mINFO[m] crd-annotations                                                    [jar]
[[1;34mINFO[m] crd-generator                                                      [jar]
[[1;34mINFO[m] api                                                                [jar]
[[1;34mINFO[m] mockkube                                                           [jar]
[[1;34mINFO[m] config-model                                                       [jar]
[[1;34mINFO[m] certificate-manager                                                [jar]
[[1;34mINFO[m] operator-common                                                    [jar]
[[1;34mINFO[m] systemtest                                                         [jar]
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m-------------------------< [0;36mio.strimzi:strimzi[0;1m >-------------------------[m
[[1;34mINFO[m] [1mBuilding Strimzi - Apache Kafka on Kubernetes and OpenShift 0.29.0-SNAPSHOT [1/10][m
[[1;34mINFO[m] [1m--------------------------------[ pom ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mstrimzi[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mstrimzi[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] No tests to run.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Skipping pom project
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--------------------------< [0;36mio.strimzi:test[0;1m >---------------------------[m
[[1;34mINFO[m] [1mBuilding test 0.29.0-SNAPSHOT                                     [2/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/test/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/test/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/test/src/test/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] No sources to compile
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] No tests to run.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mtest[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/test/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mtest[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/test/target/test-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] No tests to run.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m---------------------< [0;36mio.strimzi:crd-annotations[0;1m >---------------------[m
[[1;34mINFO[m] [1mBuilding crd-annotations 0.29.0-SNAPSHOT                          [3/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/crd-annotations/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/crd-annotations/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/crd-annotations/src/test/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mcrd-annotations[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/crd-annotations/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mcrd-annotations[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/crd-annotations/target/crd-annotations-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mcrd-annotations[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m----------------------< [0;36mio.strimzi:crd-generator[0;1m >----------------------[m
[[1;34mINFO[m] [1mBuilding crd-generator 0.29.0-SNAPSHOT                            [4/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/crd-generator/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/crd-generator/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 7 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mcrd-generator[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/crd-generator/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mcrd-generator[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-shade-plugin:3.1.0:shade[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Including io.strimzi:crd-annotations:jar:0.29.0-SNAPSHOT in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.core:jackson-core:jar:2.12.6 in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.core:jackson-databind:jar:2.12.6 in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:jar:2.12.6 in the shaded jar.
[[1;34mINFO[m] Including org.yaml:snakeyaml:jar:1.27 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-client:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-rbac:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-admissionregistration:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-apps:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-autoscaling:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-apiextensions:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-batch:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-certificates:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-coordination:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-discovery:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-events:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-extensions:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-flowcontrol:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-networking:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-metrics:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-policy:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-scheduling:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-storageclass:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-node:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including com.squareup.okhttp3:okhttp:jar:3.12.12 in the shaded jar.
[[1;34mINFO[m] Including com.squareup.okio:okio:jar:1.15.0 in the shaded jar.
[[1;34mINFO[m] Including com.squareup.okhttp3:logging-interceptor:jar:3.12.12 in the shaded jar.
[[1;34mINFO[m] Including org.slf4j:slf4j-api:jar:1.7.36 in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.datatype:jackson-datatype-jsr310:jar:2.13.1 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:zjsonpatch:jar:0.3.0 in the shaded jar.
[[1;34mINFO[m] Including com.github.mifmif:generex:jar:1.0.2 in the shaded jar.
[[1;34mINFO[m] Including dk.brics.automaton:automaton:jar:1.11-8 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-core:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-common:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.core:jackson-annotations:jar:2.12.6 in the shaded jar.
[[1;33mWARNING[m] Discovered module-info.class. Shading will break its strong encapsulation.
[[1;33mWARNING[m] Discovered module-info.class. Shading will break its strong encapsulation.
[[1;33mWARNING[m] Discovered module-info.class. Shading will break its strong encapsulation.
[[1;33mWARNING[m] Discovered module-info.class. Shading will break its strong encapsulation.
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, generex-1.0.2.jar define 7 overlapping classes: 
[[1;33mWARNING[m]   - com.mifmif.common.regex.GenerexIterator
[[1;33mWARNING[m]   - com.mifmif.common.regex.Generex
[[1;33mWARNING[m]   - com.mifmif.common.regex.GenerexIterator$Step
[[1;33mWARNING[m]   - com.mifmif.common.regex.Node
[[1;33mWARNING[m]   - com.mifmif.common.regex.Main
[[1;33mWARNING[m]   - com.mifmif.common.regex.util.Iterable
[[1;33mWARNING[m]   - com.mifmif.common.regex.util.Iterator
[[1;33mWARNING[m] kubernetes-model-rbac-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 80 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.SubjectBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.RoleListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingFluent$MetadataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.RoleBindingBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.AggregationRuleFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.SubjectFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.PolicyRuleFluent
[[1;33mWARNING[m]   - 70 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, jackson-annotations-2.12.6.jar define 71 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonAutoDetect
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonInclude
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.ObjectIdGenerators
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonFormat$Features
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonFormat$Feature
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonIgnore
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonSetter
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonTypeInfo$None
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonFormat$Shape
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonSubTypes
[[1;33mWARNING[m]   - 61 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-extensions-5.12.0.jar define 264 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetConditionBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicyListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.DeploymentStrategyFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.IngressListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicySpecFluent$IngressNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.IngressStatus
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetFluentImpl$SpecNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.IngressSpecFluent$RulesNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetSpecFluent$UpdateStrategyNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicyPeerBuilder
[[1;33mWARNING[m]   - 254 more...
[[1;33mWARNING[m] kubernetes-model-autoscaling-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 350 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.MetricSpecFluentImpl$ObjectNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.CrossVersionObjectReferenceBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerStatusFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerFluent$SpecNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.ContainerResourceMetricStatusFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerStatus
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.MetricStatusFluent$ObjectNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerSpecFluent$ScaleTargetRefNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerFluent$SpecNested
[[1;33mWARNING[m]   - 340 more...
[[1;33mWARNING[m] kubernetes-model-storageclass-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 172 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSIStorageCapacityListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.CSINodeFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.CSINodeDriverFluentImpl$AllocatableNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.StorageClass
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSourceBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSourceFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.TokenRequestFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSINodeDriverFluent$AllocatableNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSIDriverSpecBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSpecFluent
[[1;33mWARNING[m]   - 162 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-batch-5.12.0.jar define 112 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.JobFluentImpl$StatusNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.CronJobStatusFluentImpl$ActiveNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.JobSpecFluent$TemplateNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobSpecFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.JobSpecFluentImpl$TemplateNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.Job
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobListFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.CronJobListFluent
[[1;33mWARNING[m]   - 102 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-apiextensions-5.12.0.jar define 350 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionVersionBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrBoolBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionSpecFluent$ValidationNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionVersionFluentImpl$SchemaNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrStringArraySerDe$Deserializer$1
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceValidationFluentImpl$OpenAPIV3SchemaNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsFluentImpl$NotNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.WebhookClientConfigFluentImpl$ServiceNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrArrayFluent$SchemaNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1.JSONSchemaPropsOrBoolSerDe
[[1;33mWARNING[m]   - 340 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-discovery-5.12.0.jar define 88 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointSliceFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointSliceBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointPort
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.ForZoneBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointFluent$TargetRefNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointFluentImpl$ConditionsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointSliceListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointConditionsFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointSliceListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - 78 more...
[[1;33mWARNING[m] okhttp-3.12.12.jar, crd-generator-0.29.0-SNAPSHOT.jar define 208 overlapping classes: 
[[1;33mWARNING[m]   - okhttp3.WebSocket
[[1;33mWARNING[m]   - okhttp3.Cookie$Builder
[[1;33mWARNING[m]   - okhttp3.internal.http.HttpHeaders
[[1;33mWARNING[m]   - okhttp3.internal.http2.Http2Connection$ReaderRunnable
[[1;33mWARNING[m]   - okhttp3.internal.http2.Http2Reader$ContinuationSource
[[1;33mWARNING[m]   - okhttp3.internal.tls.OkHostnameVerifier
[[1;33mWARNING[m]   - okhttp3.Cache$Entry
[[1;33mWARNING[m]   - okhttp3.internal.http2.Http2Connection$3
[[1;33mWARNING[m]   - okhttp3.internal.ws.RealWebSocket$Streams
[[1;33mWARNING[m]   - okhttp3.CacheControl$Builder
[[1;33mWARNING[m]   - 198 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-metrics-5.12.0.jar define 30 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.ContainerMetricsFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetrics
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsFluent$MetadataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsFluentImpl$ContainersNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsListBuilder
[[1;33mWARNING[m]   - 20 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-flowcontrol-5.12.0.jar define 132 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationFluentImpl$SpecNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.FlowSchemaConditionFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.FlowDistinguisherMethodBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationReferenceBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.QueuingConfiguration
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.QueuingConfigurationFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfiguration
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationReference
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PolicyRulesWithSubjects
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationListFluent$ItemsNested
[[1;33mWARNING[m]   - 122 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-events-5.12.0.jar define 44 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$MetadataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$SeriesNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1.EventList
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1.EventListBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$RegardingNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1.EventSeriesFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - 34 more...
[[1;33mWARNING[m] automaton-1.11-8.jar, crd-generator-0.29.0-SNAPSHOT.jar define 25 overlapping classes: 
[[1;33mWARNING[m]   - dk.brics.automaton.AutomatonMatcher
[[1;33mWARNING[m]   - dk.brics.automaton.ShuffleOperations$ShuffleConfiguration
[[1;33mWARNING[m]   - dk.brics.automaton.RegExp$Kind
[[1;33mWARNING[m]   - dk.brics.automaton.RunAutomaton
[[1;33mWARNING[m]   - dk.brics.automaton.Automaton
[[1;33mWARNING[m]   - dk.brics.automaton.RegExp
[[1;33mWARNING[m]   - dk.brics.automaton.AutomatonProvider
[[1;33mWARNING[m]   - dk.brics.automaton.RegExp$1
[[1;33mWARNING[m]   - dk.brics.automaton.MinimizationOperations$StateListNode
[[1;33mWARNING[m]   - dk.brics.automaton.State
[[1;33mWARNING[m]   - 15 more...
[[1;33mWARNING[m] jackson-core-2.12.6.jar, crd-generator-0.29.0-SNAPSHOT.jar define 124 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.JsonGenerator$Feature
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.json.JsonReadFeature
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.util.ThreadLocalBufferManager$ThreadLocalBufferManagerHolder
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.util.Separators
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.io.SegmentedStringWriter
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.TreeNode
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.sym.Name
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.util.RequestPayload
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.util.JsonGeneratorDelegate
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.async.NonBlockingInputFeeder
[[1;33mWARNING[m]   - 114 more...
[[1;33mWARNING[m] kubernetes-model-networking-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 234 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.IngressServiceBackend
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.NetworkPolicyPort
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressFluentImpl$StatusNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.IngressStatus
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.IngressClassFluent$SpecNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.NetworkPolicyBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.IngressRuleFluentImpl$HttpNestedImpl
[[1;33mWARNING[m]   - 224 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-coordination-5.12.0.jar define 18 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluentImpl$SpecNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluent$SpecNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpecBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpecFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluentImpl$MetadataNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluent
[[1;33mWARNING[m]   - 8 more...
[[1;33mWARNING[m] zjsonpatch-0.3.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 24 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.sequence.InsertCommand
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.Operation
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.sequence.CommandVisitor
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.guava.Strings
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.sequence.EditCommand
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.JsonDiff$EncodePathFunction
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.sequence.SequencesComparator
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.Diff
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.ListUtils
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.JsonPatch
[[1;33mWARNING[m]   - 14 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-common-5.12.0.jar define 16 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.Plural
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.Group
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.jackson.JsonUnwrappedDeserializer$CancelUnwrapped
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.jackson.JsonUnwrappedDeserializer
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.PrinterColumn
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.jackson.UnwrappedTypeResolverBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.Singular
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.StatusReplicas
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.SpecReplicas
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.Version
[[1;33mWARNING[m]   - 6 more...
[[1;33mWARNING[m] kubernetes-model-admissionregistration-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 362 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookFluent$ObjectSelectorNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authorization.v1.SubjectAccessReviewSpecFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SubjectRulesReviewStatusBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1.ValidatingWebhookConfigurationBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authentication.TokenReviewFluentImpl$MetadataNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SelfSubjectRulesReviewSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookFluentImpl$NamespaceSelectorNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookConfigurationFluentImpl$WebhooksNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SelfSubjectAccessReviewFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1.MutatingWebhookFluent$ClientConfigNested
[[1;33mWARNING[m]   - 352 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, okio-1.15.0.jar define 44 overlapping classes: 
[[1;33mWARNING[m]   - okio.ByteString
[[1;33mWARNING[m]   - okio.Source
[[1;33mWARNING[m]   - okio.ForwardingSink
[[1;33mWARNING[m]   - okio.BufferedSource
[[1;33mWARNING[m]   - okio.Util
[[1;33mWARNING[m]   - okio.AsyncTimeout$1
[[1;33mWARNING[m]   - okio.HashingSource
[[1;33mWARNING[m]   - okio.GzipSink
[[1;33mWARNING[m]   - okio.Okio$1
[[1;33mWARNING[m]   - okio.Pipe$PipeSink
[[1;33mWARNING[m]   - 34 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-certificates-5.12.0.jar define 60 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestStatusFluent$ConditionsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluent$SpecNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluentImpl$StatusNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestStatusFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestFluent$MetadataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestConditionFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestStatusBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestStatusFluent$ConditionsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluentImpl
[[1;33mWARNING[m]   - 50 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, jackson-datatype-jsr310-2.13.1.jar define 59 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.deser.LocalDateDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.deser.key.Jsr310KeyDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.PackageVersion
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.deser.YearDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.ser.key.Jsr310NullKeySerializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.deser.key.LocalDateTimeKeyDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.util.DurationUnitConverter
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.ser.InstantSerializerBase
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.ser.LocalDateTimeSerializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.ser.OffsetDateTimeSerializer
[[1;33mWARNING[m]   - 49 more...
[[1;33mWARNING[m] crd-annotations-0.29.0-SNAPSHOT.jar, crd-generator-0.29.0-SNAPSHOT.jar define 8 overlapping classes: 
[[1;33mWARNING[m]   - io.strimzi.api.annotations.VersionRange
[[1;33mWARNING[m]   - io.strimzi.api.annotations.ApiVersion
[[1;33mWARNING[m]   - io.strimzi.api.annotations.ApiVersion$Stability
[[1;33mWARNING[m]   - io.strimzi.api.annotations.ApiVersion$1
[[1;33mWARNING[m]   - io.strimzi.api.annotations.DeprecatedType
[[1;33mWARNING[m]   - io.strimzi.api.annotations.DeprecatedProperty
[[1;33mWARNING[m]   - io.strimzi.api.annotations.VersionRange$VersionParser
[[1;33mWARNING[m]   - io.strimzi.api.annotations.KubeVersion
[[1;33mWARNING[m] kubernetes-model-apps-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 212 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.StatefulSetConditionFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.DeploymentStrategyFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ControllerRevisionFluent$DeploymentDataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetSpecFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.DeploymentFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetStatusFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ControllerRevisionFluentImpl$PersistentVolumeClaimDataNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetCondition
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.StatefulSetSpecFluent$UpdateStrategyNested
[[1;33mWARNING[m]   - 202 more...
[[1;33mWARNING[m] logging-interceptor-3.12.12.jar, crd-generator-0.29.0-SNAPSHOT.jar define 8 overlapping classes: 
[[1;33mWARNING[m]   - okhttp3.logging.HttpLoggingInterceptor$Logger$1
[[1;33mWARNING[m]   - okhttp3.logging.LoggingEventListener$Factory
[[1;33mWARNING[m]   - okhttp3.logging.HttpLoggingInterceptor$Level
[[1;33mWARNING[m]   - okhttp3.logging.HttpLoggingInterceptor
[[1;33mWARNING[m]   - okhttp3.logging.package-info
[[1;33mWARNING[m]   - okhttp3.logging.LoggingEventListener
[[1;33mWARNING[m]   - okhttp3.logging.LoggingEventListener$1
[[1;33mWARNING[m]   - okhttp3.logging.HttpLoggingInterceptor$Logger
[[1;33mWARNING[m] jackson-databind-2.12.6.jar, crd-generator-0.29.0-SNAPSHOT.jar define 700 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.introspect.AnnotationCollector$NoAnnotations
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.jsontype.BasicPolymorphicTypeValidator$Builder
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.BeanDescription
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.deser.impl.BeanAsArrayBuilderDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.introspect.AnnotatedMethodMap
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.SerializerProvider
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.introspect.AnnotationCollector$OneAnnotation
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.ser.std.StaticListSerializerBase
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.ser.std.NumberSerializers$ShortSerializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.ser.BeanSerializerFactory
[[1;33mWARNING[m]   - 690 more...
[[1;33mWARNING[m] jackson-dataformat-yaml-2.12.6.jar, crd-generator-0.29.0-SNAPSHOT.jar define 17 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLMapper$Builder
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.snakeyaml.error.Mark
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLGenerator
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.UTF8Reader
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLParser
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.util.StringQuotingChecker
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLGenerator$Feature
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.JacksonYAMLParseException
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLParser$Feature
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.util.StringQuotingChecker$Default
[[1;33mWARNING[m]   - 7 more...
[[1;33mWARNING[m] kubernetes-model-core-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 2394 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.BaseKubernetesListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.StatusBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.KubeSchemaFluentImpl$APIResourceNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.NodeListBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.ResourceQuotaListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.WatchEventFluentImpl$APIServiceStatusObjectNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.WatchEventFluent$VsphereVirtualDiskVolumeSourceObjectNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.ProbeFluentImpl$HttpGetNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.PatchOptionsFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.ServerAddressByClientCIDRFluentImpl
[[1;33mWARNING[m]   - 2384 more...
[[1;33mWARNING[m] slf4j-api-1.7.36.jar, crd-generator-0.29.0-SNAPSHOT.jar define 34 overlapping classes: 
[[1;33mWARNING[m]   - org.slf4j.helpers.SubstituteLogger
[[1;33mWARNING[m]   - org.slf4j.helpers.NamedLoggerBase
[[1;33mWARNING[m]   - org.slf4j.helpers.NOPMDCAdapter
[[1;33mWARNING[m]   - org.slf4j.MarkerFactory
[[1;33mWARNING[m]   - org.slf4j.helpers.BasicMarker
[[1;33mWARNING[m]   - org.slf4j.spi.LoggerFactoryBinder
[[1;33mWARNING[m]   - org.slf4j.MDC$MDCCloseable
[[1;33mWARNING[m]   - org.slf4j.spi.LocationAwareLogger
[[1;33mWARNING[m]   - org.slf4j.helpers.MessageFormatter
[[1;33mWARNING[m]   - org.slf4j.helpers.Util$ClassContextSecurityManager
[[1;33mWARNING[m]   - 24 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-node-5.12.0.jar define 78 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1beta1.OverheadBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1alpha1.Scheduling
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1beta1.RuntimeClassFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1beta1.RuntimeClassListBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1alpha1.SchedulingFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1alpha1.RuntimeClassSpecFluent$OverheadNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassFluentImpl
[[1;33mWARNING[m]   - 68 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, snakeyaml-1.27.jar define 216 overlapping classes: 
[[1;33mWARNING[m]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockMappingValue
[[1;33mWARNING[m]   - org.yaml.snakeyaml.parser.ParserImpl$ParseBlockNode
[[1;33mWARNING[m]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockMappingSimpleValue
[[1;33mWARNING[m]   - org.yaml.snakeyaml.emitter.Emitter$ExpectDocumentEnd
[[1;33mWARNING[m]   - org.yaml.snakeyaml.Yaml$3
[[1;33mWARNING[m]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockSequenceItem
[[1;33mWARNING[m]   - org.yaml.snakeyaml.parser.ParserImpl$ParseBlockSequenceEntry
[[1;33mWARNING[m]   - org.yaml.snakeyaml.util.ArrayUtils
[[1;33mWARNING[m]   - org.yaml.snakeyaml.tokens.Token$ID
[[1;33mWARNING[m]   - org.yaml.snakeyaml.reader.StreamReader
[[1;33mWARNING[m]   - 206 more...
[[1;33mWARNING[m] kubernetes-client-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 536 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.internal.CertUtils
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.CustomResource
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.osgi.ManagedKubernetesClient
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.V1beta1ApiextensionAPIGroupDSL
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.internal.PatchUtils$SingletonHolder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.VersionInfo$1
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.utils.ReplaceValueStream
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.dsl.CreateFromServerGettable
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.dsl.ApiextensionsAPIGroupDSL
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.dsl.Containerable
[[1;33mWARNING[m]   - 526 more...
[[1;33mWARNING[m] kubernetes-model-scheduling-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 24 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassFluentImpl$MetadataNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassList
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClass
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassFluent$MetadataNested
[[1;33mWARNING[m]   - 14 more...
[[1;33mWARNING[m] kubernetes-model-policy-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 162 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1.PodDisruptionBudgetList
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.HostPortRangeBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.EvictionFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicyFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicySpecFluentImpl$AllowedCSIDriversNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicyListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.AllowedFlexVolumeBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.IDRangeFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.SELinuxStrategyOptionsFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicySpecFluentImpl$FsGroupNestedImpl
[[1;33mWARNING[m]   - 152 more...
[[1;33mWARNING[m] maven-shade-plugin has detected that some class files are
[[1;33mWARNING[m] present in two or more JARs. When this happens, only one
[[1;33mWARNING[m] single version of the class is copied to the uber jar.
[[1;33mWARNING[m] Usually this is not harmful and you can skip these warnings,
[[1;33mWARNING[m] otherwise try to manually exclude artifacts based on
[[1;33mWARNING[m] mvn dependency:tree -Ddetail=true and the above output.
[[1;33mWARNING[m] See http://maven.apache.org/plugins/maven-shade-plugin/
[[1;34mINFO[m] Replacing original artifact with shaded artifact.
[[1;34mINFO[m] Replacing /home/ec2-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT.jar with /home/ec2-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT-shaded.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m---------------------------< [0;36mio.strimzi:api[0;1m >---------------------------[m
[[1;34mINFO[m] [1mBuilding api 0.29.0-SNAPSHOT                                      [5/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/api/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/api/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mexec-maven-plugin:1.6.0:exec[m [1m(generate-crd-co-install-v1)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mexec-maven-plugin:1.6.0:exec[m [1m(generate-crd-co-install-v1-eo)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mexec-maven-plugin:1.6.0:exec[m [1m(generate-doc)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 99 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-test-compile)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mapi[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/api/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mapi[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:test-jar[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m------------------------< [0;36mio.strimzi:mockkube[0;1m >-------------------------[m
[[1;34mINFO[m] [1mBuilding mockkube 0.29.0-SNAPSHOT                                 [6/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/mockkube/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/mockkube/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 1 resource
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mmockkube[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/mockkube/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mmockkube[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/mockkube/target/mockkube-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mmockkube[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m----------------------< [0;36mio.strimzi:config-model[0;1m >-----------------------[m
[[1;34mINFO[m] [1mBuilding config-model 0.29.0-SNAPSHOT                             [7/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/config-model/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/config-model/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/config-model/src/test/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mconfig-model[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/config-model/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mconfig-model[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/config-model/target/config-model-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mconfig-model[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m-------------------< [0;36mio.strimzi:certificate-manager[0;1m >-------------------[m
[[1;34mINFO[m] [1mBuilding certificate-manager 0.29.0-SNAPSHOT                      [8/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/certificate-manager/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 1 resource
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 1 resource
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mcertificate-manager[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/certificate-manager/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mcertificate-manager[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/certificate-manager/target/certificate-manager-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mcertificate-manager[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m---------------------< [0;36mio.strimzi:operator-common[0;1m >---------------------[m
[[1;34mINFO[m] [1mBuilding operator-common 0.29.0-SNAPSHOT                          [9/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/operator-common/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/operator-common/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 9 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36moperator-common[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/operator-common/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36moperator-common[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/operator-common/target/operator-common-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:test-jar[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m-----------------------< [0;36mio.strimzi:systemtest[0;1m >------------------------[m
[[1;34mINFO[m] [1mBuilding systemtest 0.29.0-SNAPSHOT                              [10/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/systemtest/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] Copying 2 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 32 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;33mWARNING[m] Corrupted STDOUT by directly writing to native stream in forked JVM 1. See FAQ web page and the dump file /home/ec2-user/strimzi-kafka-operator/systemtest/target/surefire-reports/2022-03-30T19-59-51_487-jvmRun1.dumpstream
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36msystemtest[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/systemtest/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36msystemtest[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36msystemtest[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
2022-03-30 20:00:09 [main] [32mINFO [m [TestExecutionListener:29] =======================================================================
2022-03-30 20:00:09 [main] [32mINFO [m [TestExecutionListener:30] =======================================================================
2022-03-30 20:00:09 [main] [32mINFO [m [TestExecutionListener:31]                         Test run started
2022-03-30 20:00:09 [main] [32mINFO [m [TestExecutionListener:32] =======================================================================
2022-03-30 20:00:09 [main] [32mINFO [m [TestExecutionListener:33] =======================================================================
2022-03-30 20:00:09 [main] [32mINFO [m [TestExecutionListener:48] Following testclasses are selected for run:
2022-03-30 20:00:09 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.operators.user.UserST
2022-03-30 20:00:09 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.bridge.HttpBridgeTlsST
2022-03-30 20:00:09 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.cruisecontrol.CruiseControlApiST
2022-03-30 20:00:09 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.cruisecontrol.CruiseControlST
2022-03-30 20:00:09 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.kafka.listeners.ListenersST
2022-03-30 20:00:09 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.security.SecurityST
2022-03-30 20:00:09 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.rollingupdate.RollingUpdateST
2022-03-30 20:00:09 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.specific.SpecificIsolatedST
2022-03-30 20:00:09 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.watcher.AllNamespaceIsolatedST
2022-03-30 20:00:09 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.connect.ConnectIsolatedST
2022-03-30 20:00:09 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST
2022-03-30 20:00:09 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST
2022-03-30 20:00:09 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.metrics.MetricsIsolatedST
2022-03-30 20:00:09 [main] [32mINFO [m [TestExecutionListener:52] =======================================================================
2022-03-30 20:00:09 [main] [32mINFO [m [TestExecutionListener:53] =======================================================================
[[1;34mINFO[m] Running io.strimzi.systemtest.cruisecontrol.CruiseControlApiST
[[1;34mINFO[m] Running io.strimzi.systemtest.metrics.MetricsIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.bridge.HttpBridgeTlsST
[[1;34mINFO[m] Running io.strimzi.systemtest.operators.user.UserST
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Environment:271] Json configuration is not provided or cannot be processed!
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:219] Used environment variables:
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:220] CONFIG: /home/ec2-user/strimzi-kafka-operator/systemtest/config.json
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:221] STRIMZI_RBAC_SCOPE: CLUSTER
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:221] OLM_APP_BUNDLE_PREFIX: strimzi-cluster-operator
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:221] TEST_CLIENTS_VERSION: 0.2.0
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:221] OLM_SOURCE_NAMESPACE: openshift-marketplace
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:221] CLUSTER_OPERATOR_INSTALL_TYPE: BUNDLE
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:221] STRIMZI_COMPONENTS_LOG_LEVEL: INFO
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:221] SKIP_TEARDOWN: false
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:221] LB_FINALIZERS: false
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:221] OLM_OPERATOR_DEPLOYMENT_NAME: strimzi-cluster-operator
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:221] DOCKER_ORG: strimzi
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:221] TEST_LOG_DIR: /home/ec2-user/strimzi-kafka-operator/systemtest/../systemtest/target/logs/
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:221] COMPONENTS_IMAGE_PULL_POLICY: IfNotPresent
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:221] DOCKER_REGISTRY: quay.io
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:221] TEST_CLIENT_IMAGE: quay.io/strimzi/test-client:latest-kafka-3.1.0
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:221] SYSTEM_TEST_STRIMZI_IMAGE_PULL_SECRET: 
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:221] TEST_ADMIN_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-admin:0.2.0-kafka-3.1.0
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:221] TEST_HTTP_PRODUCER_IMAGE: quay.io/strimzi-test-clients/test-client-http-producer:0.2.0
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:221] OLM_OPERATOR_NAME: strimzi-kafka-operator
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:221] DOCKER_TAG: latest
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:221] OLM_SOURCE_NAME: community-operators
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:221] STRIMZI_FEATURE_GATES: 
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:221] CLIENTS_KAFKA_VERSION: 3.1.0
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:221] TEST_HTTP_CONSUMER_IMAGE: quay.io/strimzi-test-clients/test-client-http-consumer:0.2.0
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:221] STRIMZI_LOG_LEVEL: DEBUG
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:221] ST_KAFKA_VERSION: 3.1.0
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:221] OPERATOR_IMAGE_PULL_POLICY: Always
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:221] DEFAULT_TO_DENY_NETWORK_POLICIES: true
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:221] TEST_PRODUCER_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-producer:0.2.0-kafka-3.1.0
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:221] BRIDGE_IMAGE: latest-released
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:221] TEST_STREAMS_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-streams:0.2.0-kafka-3.1.0
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:221] TEST_CONSUMER_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-consumer:0.2.0-kafka-3.1.0
2022-03-30 20:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Environment:221] OLM_OPERATOR_VERSION: 
2022-03-30 20:00:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [BeforeAllOnce:51] ============================================================================
2022-03-30 20:00:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [BeforeAllOnce:52] [io.strimzi.systemtest.bridge.HttpBridgeTlsST - Before Suite] - Setup Suite environment
2022-03-30 20:00:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Config:540] Trying to configure client from Kubernetes config...
2022-03-30 20:00:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Config:549] Found for Kubernetes config at: [/home/ec2-user/.kube/config].
2022-03-30 20:00:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Config:540] Trying to configure client from Kubernetes config...
2022-03-30 20:00:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Config:549] Found for Kubernetes config at: [/home/ec2-user/.kube/config].
2022-03-30 20:00:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [KubeCluster:71] Cluster minikube is installed
2022-03-30 20:00:09 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - minikube status
2022-03-30 20:00:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: minikube status
2022-03-30 20:00:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:00:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [KubeCluster:73] Cluster minikube is running
2022-03-30 20:00:10 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeCluster:87] Using cluster: minikube
2022-03-30 20:00:10 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:60] Cluster default namespace is 'default'
2022-03-30 20:00:10 [ForkJoinPool-3-worker-7] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5a34beae
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-30 20:00:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@5314833a, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5a34beae, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-30 20:00:10 [ForkJoinPool-3-worker-7] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-30 20:00:11 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-30 20:00:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-30 20:00:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace default get Namespace infra-namespace -o json
2022-03-30 20:00:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace default get Namespace infra-namespace -o json
2022-03-30 20:00:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:00:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T20:00:11Z",
        "name": "infra-namespace",
        "resourceVersion": "81780",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "764df83b-791a-48e0-9c95-fd4f59d58fe1"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 20:00:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-30 20:00:11 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 20:00:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-30 20:00:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:00:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 20:00:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 20:00:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 20:00:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 20:00:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 20:00:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 20:00:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 20:00:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 20:00:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 20:00:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 20:00:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 20:00:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 20:00:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 20:00:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 20:00:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 20:00:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 20:00:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:00:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 20:00:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 20:00:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 20:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 20:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 20:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 20:00:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479991ms till timeout)
2022-03-30 20:00:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478986ms till timeout)
2022-03-30 20:00:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477979ms till timeout)
2022-03-30 20:00:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476974ms till timeout)
2022-03-30 20:00:17 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475969ms till timeout)
2022-03-30 20:00:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474964ms till timeout)
2022-03-30 20:00:19 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473960ms till timeout)
2022-03-30 20:00:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472954ms till timeout)
2022-03-30 20:00:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471949ms till timeout)
2022-03-30 20:00:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470944ms till timeout)
2022-03-30 20:00:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469932ms till timeout)
2022-03-30 20:00:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468926ms till timeout)
2022-03-30 20:00:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467918ms till timeout)
2022-03-30 20:00:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466913ms till timeout)
2022-03-30 20:00:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465908ms till timeout)
2022-03-30 20:00:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464903ms till timeout)
2022-03-30 20:00:29 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-30 20:00:29 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-30 20:00:29 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-30 20:00:29 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-lzxnv not ready: strimzi-cluster-operator)
2022-03-30 20:00:29 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-lzxnv are ready
2022-03-30 20:00:29 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599981ms till timeout)
2022-03-30 20:00:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-lzxnv not ready: strimzi-cluster-operator)
2022-03-30 20:00:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-lzxnv are ready
2022-03-30 20:00:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598976ms till timeout)
2022-03-30 20:00:31 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-lzxnv not ready: strimzi-cluster-operator)
2022-03-30 20:00:31 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-lzxnv are ready
2022-03-30 20:00:31 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597970ms till timeout)
2022-03-30 20:00:32 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-lzxnv not ready: strimzi-cluster-operator)
2022-03-30 20:00:32 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-lzxnv are ready
2022-03-30 20:00:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596964ms till timeout)
2022-03-30 20:00:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-lzxnv not ready: strimzi-cluster-operator)
2022-03-30 20:00:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-lzxnv are ready
2022-03-30 20:00:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595958ms till timeout)
2022-03-30 20:00:34 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-lzxnv not ready: strimzi-cluster-operator)
2022-03-30 20:00:34 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-lzxnv are ready
2022-03-30 20:00:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594952ms till timeout)
2022-03-30 20:00:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-lzxnv not ready: strimzi-cluster-operator)
2022-03-30 20:00:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-lzxnv are ready
2022-03-30 20:00:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593947ms till timeout)
2022-03-30 20:00:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-lzxnv not ready: strimzi-cluster-operator)
2022-03-30 20:00:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-lzxnv are ready
2022-03-30 20:00:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592940ms till timeout)
2022-03-30 20:00:37 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-lzxnv not ready: strimzi-cluster-operator)
2022-03-30 20:00:37 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-lzxnv are ready
2022-03-30 20:00:37 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591935ms till timeout)
2022-03-30 20:00:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-lzxnv not ready: strimzi-cluster-operator)
2022-03-30 20:00:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-lzxnv are ready
2022-03-30 20:00:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590930ms till timeout)
2022-03-30 20:00:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-lzxnv not ready: strimzi-cluster-operator)
2022-03-30 20:00:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-lzxnv are ready
2022-03-30 20:00:39 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-30 20:00:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 20:00:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 20:00:39 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 20:00:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:667] [bridge.HttpBridgeTlsST - Before All] - Setup test suite environment
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:667] [cruisecontrol.CruiseControlApiST - Before All] - Setup test suite environment
2022-03-30 20:00:39 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:667] [metrics.MetricsIsolatedST - Before All] - Setup test suite environment
2022-03-30 20:00:39 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:667] [operators.user.UserST - Before All] - Setup test suite environment
2022-03-30 20:00:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:69] [bridge.HttpBridgeTlsST] - Adding parallel suite: HttpBridgeTlsST
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:69] [cruisecontrol.CruiseControlApiST] - Adding parallel suite: CruiseControlApiST
2022-03-30 20:00:39 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:69] [operators.user.UserST] - Adding parallel suite: UserST
2022-03-30 20:00:39 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 20:00:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:73] [bridge.HttpBridgeTlsST] - Parallel suites count: 1
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:73] [cruisecontrol.CruiseControlApiST] - Parallel suites count: 2
2022-03-30 20:00:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:00:39 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:73] [operators.user.UserST] - Parallel suites count: 3
2022-03-30 20:00:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:184] HttpBridgeTlsST suite now can proceed its execution
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:184] CruiseControlApiST suite now can proceed its execution
2022-03-30 20:00:39 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:184] UserST suite now can proceed its execution
2022-03-30 20:00:39 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-30 20:00:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-30 20:00:39 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestSuiteNamespaceManager:129] Test suite `UserST` creates these additional namespaces:[user-st]
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestSuiteNamespaceManager:129] Test suite `CruiseControlApiST` creates these additional namespaces:[cruise-control-api-st]
2022-03-30 20:00:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestSuiteNamespaceManager:129] Test suite `HttpBridgeTlsST` creates these additional namespaces:[http-bridge-tls-st]
2022-03-30 20:00:39 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: http-bridge-tls-st
2022-03-30 20:00:39 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: user-st
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: cruise-control-api-st
2022-03-30 20:00:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Namespace http-bridge-tls-st
2022-03-30 20:00:39 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Namespace user-st
2022-03-30 20:00:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace http-bridge-tls-st -o json
2022-03-30 20:00:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace user-st -o json
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Namespace cruise-control-api-st
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o json
2022-03-30 20:00:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace http-bridge-tls-st -o json
2022-03-30 20:00:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:00:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T20:00:39Z",
        "name": "http-bridge-tls-st",
        "resourceVersion": "81876",
        "selfLink": "/api/v1/namespaces/http-bridge-tls-st",
        "uid": "a6a0e3cf-2565-4e0f-be14-4eb43fb9ad5b"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o json
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:00:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st]}
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T20:00:39Z",
        "name": "cruise-control-api-st",
        "resourceVersion": "81878",
        "selfLink": "/api/v1/namespaces/cruise-control-api-st",
        "uid": "1bdd4a96-8d08-4626-bb77-2265de834561"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 20:00:39 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace user-st -o json
2022-03-30 20:00:39 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: http-bridge-tls-st
2022-03-30 20:00:39 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st]}
2022-03-30 20:00:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T20:00:39Z",
        "name": "user-st",
        "resourceVersion": "81877",
        "selfLink": "/api/v1/namespaces/user-st",
        "uid": "cf3f8c64-8453-4d16-9538-186ef85de17d"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: cruise-control-api-st
2022-03-30 20:00:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=http-bridge-tls-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 20:00:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st]}
2022-03-30 20:00:39 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: http-bridge-tls-st
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=cruise-control-api-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 20:00:39 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: user-st
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: cruise-control-api-st
2022-03-30 20:00:39 [ForkJoinPool-3-worker-5] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=user-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 20:00:39 [ForkJoinPool-3-worker-7] [32mINFO [m [HttpBridgeTlsST:129] Deploy Kafka and KafkaBridge before tests
2022-03-30 20:00:39 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: user-st
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlApiST.testCruiseControlBasicAPIRequestsWithSecurityDisabled-STARTED
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:659] [cruisecontrol.CruiseControlApiST - Before Each] - Setup test case environment
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:77] [cruisecontrol.CruiseControlApiST] - Adding parallel test: testCruiseControlBasicAPIRequestsWithSecurityDisabled
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:81] [cruisecontrol.CruiseControlApiST] - Parallel test count: 1
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:230] testCruiseControlBasicAPIRequestsWithSecurityDisabled test now can proceed its execution
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140}
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677}
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206}
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients}
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-0 for test case:testCruiseControlBasicAPIRequestsWithSecurityDisabled
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-0
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-0
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace user-st get Namespace namespace-0 -o json
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace user-st get Namespace namespace-0 -o json
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T20:00:39Z",
        "name": "namespace-0",
        "resourceVersion": "81888",
        "selfLink": "/api/v1/namespaces/namespace-0",
        "uid": "346ae93e-465f-4575-9d6f-4f5c140e7d19"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@12282135=[namespace-0], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st]}
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-0
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-0, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-0
2022-03-30 20:00:39 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka user-cluster-name in namespace user-st
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka cruise-control-api-cluster-name in namespace namespace-0
2022-03-30 20:00:39 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-0
2022-03-30 20:00:39 [ForkJoinPool-3-worker-5] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkas' with unstable version 'v1beta2'
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:cruise-control-api-cluster-name
2022-03-30 20:00:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:http-bridge-tls-cluster-name
2022-03-30 20:00:39 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:user-cluster-name
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: cruise-control-api-cluster-name will have desired state: Ready
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: cruise-control-api-cluster-name will have desired state: Ready
2022-03-30 20:00:39 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-30 20:00:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-30 20:00:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1319996ms till timeout)
2022-03-30 20:00:39 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: user-cluster-name will have desired state: Ready
2022-03-30 20:00:39 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: user-cluster-name will have desired state: Ready
2022-03-30 20:00:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (839979ms till timeout)
2022-03-30 20:00:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (839980ms till timeout)
2022-03-30 20:00:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1318991ms till timeout)
2022-03-30 20:00:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (838973ms till timeout)
2022-03-30 20:00:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (838976ms till timeout)
2022-03-30 20:00:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1317985ms till timeout)
2022-03-30 20:00:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (837969ms till timeout)
2022-03-30 20:00:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (837971ms till timeout)
2022-03-30 20:00:42 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1316981ms till timeout)
2022-03-30 20:00:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (836966ms till timeout)
2022-03-30 20:00:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (836968ms till timeout)
2022-03-30 20:00:43 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1315976ms till timeout)
2022-03-30 20:00:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (835963ms till timeout)
2022-03-30 20:00:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (835964ms till timeout)
2022-03-30 20:00:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:00:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:00:44 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1314971ms till timeout)
2022-03-30 20:00:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (834959ms till timeout)
2022-03-30 20:00:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (834961ms till timeout)
2022-03-30 20:00:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1313965ms till timeout)
2022-03-30 20:00:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (833956ms till timeout)
2022-03-30 20:00:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (833957ms till timeout)
2022-03-30 20:00:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1312960ms till timeout)
2022-03-30 20:00:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (832953ms till timeout)
2022-03-30 20:00:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (832954ms till timeout)
2022-03-30 20:00:47 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1311956ms till timeout)
2022-03-30 20:00:47 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (831949ms till timeout)
2022-03-30 20:00:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (831951ms till timeout)
2022-03-30 20:00:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1310952ms till timeout)
2022-03-30 20:00:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (830946ms till timeout)
2022-03-30 20:00:48 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (830948ms till timeout)
2022-03-30 20:00:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:00:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:00:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1309947ms till timeout)
2022-03-30 20:00:49 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (829942ms till timeout)
2022-03-30 20:00:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (829944ms till timeout)
2022-03-30 20:00:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1308941ms till timeout)
2022-03-30 20:00:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (828937ms till timeout)
2022-03-30 20:00:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (828937ms till timeout)
2022-03-30 20:00:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1307935ms till timeout)
2022-03-30 20:00:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (827932ms till timeout)
2022-03-30 20:00:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (827933ms till timeout)
2022-03-30 20:00:52 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1306929ms till timeout)
2022-03-30 20:00:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (826924ms till timeout)
2022-03-30 20:00:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (826917ms till timeout)
2022-03-30 20:00:53 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1305925ms till timeout)
2022-03-30 20:00:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (825914ms till timeout)
2022-03-30 20:00:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (825907ms till timeout)
2022-03-30 20:00:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:00:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:00:54 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1304921ms till timeout)
2022-03-30 20:00:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (824910ms till timeout)
2022-03-30 20:00:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (824893ms till timeout)
2022-03-30 20:00:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1303917ms till timeout)
2022-03-30 20:00:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (823907ms till timeout)
2022-03-30 20:00:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (823889ms till timeout)
2022-03-30 20:00:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1302913ms till timeout)
2022-03-30 20:00:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (822904ms till timeout)
2022-03-30 20:00:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (822886ms till timeout)
2022-03-30 20:00:57 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1301908ms till timeout)
2022-03-30 20:00:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (821900ms till timeout)
2022-03-30 20:00:57 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (821883ms till timeout)
2022-03-30 20:00:58 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1300904ms till timeout)
2022-03-30 20:00:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (820896ms till timeout)
2022-03-30 20:00:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (820879ms till timeout)
2022-03-30 20:00:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:00:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:00:59 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1299898ms till timeout)
2022-03-30 20:00:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (819892ms till timeout)
2022-03-30 20:00:59 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (819876ms till timeout)
2022-03-30 20:01:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1298894ms till timeout)
2022-03-30 20:01:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (818889ms till timeout)
2022-03-30 20:01:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (818872ms till timeout)
2022-03-30 20:01:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1297889ms till timeout)
2022-03-30 20:01:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (817886ms till timeout)
2022-03-30 20:01:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (817868ms till timeout)
2022-03-30 20:01:02 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1296885ms till timeout)
2022-03-30 20:01:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (816883ms till timeout)
2022-03-30 20:01:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (816865ms till timeout)
2022-03-30 20:01:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1295881ms till timeout)
2022-03-30 20:01:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (815880ms till timeout)
2022-03-30 20:01:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (815862ms till timeout)
2022-03-30 20:01:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:01:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:01:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1294877ms till timeout)
2022-03-30 20:01:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (814876ms till timeout)
2022-03-30 20:01:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (814859ms till timeout)
2022-03-30 20:01:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1293873ms till timeout)
2022-03-30 20:01:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (813873ms till timeout)
2022-03-30 20:01:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (813856ms till timeout)
2022-03-30 20:01:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1292869ms till timeout)
2022-03-30 20:01:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (812870ms till timeout)
2022-03-30 20:01:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (812853ms till timeout)
2022-03-30 20:01:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1291865ms till timeout)
2022-03-30 20:01:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (811867ms till timeout)
2022-03-30 20:01:07 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (811849ms till timeout)
2022-03-30 20:01:08 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1290861ms till timeout)
2022-03-30 20:01:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (810865ms till timeout)
2022-03-30 20:01:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (810846ms till timeout)
2022-03-30 20:01:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:01:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:01:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1289857ms till timeout)
2022-03-30 20:01:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (809861ms till timeout)
2022-03-30 20:01:09 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (809843ms till timeout)
2022-03-30 20:01:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1288853ms till timeout)
2022-03-30 20:01:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (808857ms till timeout)
2022-03-30 20:01:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (808840ms till timeout)
2022-03-30 20:01:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1287849ms till timeout)
2022-03-30 20:01:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (807852ms till timeout)
2022-03-30 20:01:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (807837ms till timeout)
2022-03-30 20:01:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1286844ms till timeout)
2022-03-30 20:01:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (806848ms till timeout)
2022-03-30 20:01:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (806834ms till timeout)
2022-03-30 20:01:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1285840ms till timeout)
2022-03-30 20:01:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (805843ms till timeout)
2022-03-30 20:01:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (805831ms till timeout)
2022-03-30 20:01:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:01:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:01:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1284836ms till timeout)
2022-03-30 20:01:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (804840ms till timeout)
2022-03-30 20:01:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (804828ms till timeout)
2022-03-30 20:01:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1283832ms till timeout)
2022-03-30 20:01:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (803835ms till timeout)
2022-03-30 20:01:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (803825ms till timeout)
2022-03-30 20:01:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1282828ms till timeout)
2022-03-30 20:01:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (802831ms till timeout)
2022-03-30 20:01:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (802822ms till timeout)
2022-03-30 20:01:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1281824ms till timeout)
2022-03-30 20:01:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (801827ms till timeout)
2022-03-30 20:01:17 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (801819ms till timeout)
2022-03-30 20:01:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1280820ms till timeout)
2022-03-30 20:01:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (800823ms till timeout)
2022-03-30 20:01:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (800816ms till timeout)
2022-03-30 20:01:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:01:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:01:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1279815ms till timeout)
2022-03-30 20:01:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (799819ms till timeout)
2022-03-30 20:01:19 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (799813ms till timeout)
2022-03-30 20:01:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1278811ms till timeout)
2022-03-30 20:01:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (798815ms till timeout)
2022-03-30 20:01:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (798810ms till timeout)
2022-03-30 20:01:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1277807ms till timeout)
2022-03-30 20:01:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (797811ms till timeout)
2022-03-30 20:01:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (797807ms till timeout)
2022-03-30 20:01:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1276803ms till timeout)
2022-03-30 20:01:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (796803ms till timeout)
2022-03-30 20:01:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (796806ms till timeout)
2022-03-30 20:01:23 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1275798ms till timeout)
2022-03-30 20:01:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (795799ms till timeout)
2022-03-30 20:01:23 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (795802ms till timeout)
2022-03-30 20:01:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:01:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:01:24 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1274794ms till timeout)
2022-03-30 20:01:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (794795ms till timeout)
2022-03-30 20:01:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (794798ms till timeout)
2022-03-30 20:01:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1273790ms till timeout)
2022-03-30 20:01:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (793791ms till timeout)
2022-03-30 20:01:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (793794ms till timeout)
2022-03-30 20:01:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1272786ms till timeout)
2022-03-30 20:01:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (792790ms till timeout)
2022-03-30 20:01:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (792787ms till timeout)
2022-03-30 20:01:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1271782ms till timeout)
2022-03-30 20:01:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (791783ms till timeout)
2022-03-30 20:01:28 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (791785ms till timeout)
2022-03-30 20:01:29 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1270778ms till timeout)
2022-03-30 20:01:29 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (790779ms till timeout)
2022-03-30 20:01:29 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (790782ms till timeout)
2022-03-30 20:01:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:01:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:01:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1269774ms till timeout)
2022-03-30 20:01:30 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (789777ms till timeout)
2022-03-30 20:01:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (789774ms till timeout)
2022-03-30 20:01:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1268770ms till timeout)
2022-03-30 20:01:31 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (788774ms till timeout)
2022-03-30 20:01:31 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (788771ms till timeout)
2022-03-30 20:01:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1267766ms till timeout)
2022-03-30 20:01:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (787766ms till timeout)
2022-03-30 20:01:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (787769ms till timeout)
2022-03-30 20:01:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1266762ms till timeout)
2022-03-30 20:01:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (786762ms till timeout)
2022-03-30 20:01:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (786765ms till timeout)
2022-03-30 20:01:34 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1265758ms till timeout)
2022-03-30 20:01:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (785758ms till timeout)
2022-03-30 20:01:34 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (785761ms till timeout)
2022-03-30 20:01:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:01:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:01:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1264754ms till timeout)
2022-03-30 20:01:35 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (784757ms till timeout)
2022-03-30 20:01:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (784754ms till timeout)
2022-03-30 20:01:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1263750ms till timeout)
2022-03-30 20:01:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (783750ms till timeout)
2022-03-30 20:01:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (783753ms till timeout)
2022-03-30 20:01:37 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1262746ms till timeout)
2022-03-30 20:01:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (782750ms till timeout)
2022-03-30 20:01:37 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (782747ms till timeout)
2022-03-30 20:01:38 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1261742ms till timeout)
2022-03-30 20:01:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (781746ms till timeout)
2022-03-30 20:01:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (781742ms till timeout)
2022-03-30 20:01:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1260738ms till timeout)
2022-03-30 20:01:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (780739ms till timeout)
2022-03-30 20:01:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (780742ms till timeout)
2022-03-30 20:01:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:01:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:01:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1259734ms till timeout)
2022-03-30 20:01:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (779738ms till timeout)
2022-03-30 20:01:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (779735ms till timeout)
2022-03-30 20:01:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1258730ms till timeout)
2022-03-30 20:01:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (778734ms till timeout)
2022-03-30 20:01:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (778731ms till timeout)
2022-03-30 20:01:42 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1257726ms till timeout)
2022-03-30 20:01:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (777730ms till timeout)
2022-03-30 20:01:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (777727ms till timeout)
2022-03-30 20:01:43 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1256722ms till timeout)
2022-03-30 20:01:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (776723ms till timeout)
2022-03-30 20:01:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (776726ms till timeout)
2022-03-30 20:01:44 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1255718ms till timeout)
2022-03-30 20:01:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (775721ms till timeout)
2022-03-30 20:01:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (775718ms till timeout)
2022-03-30 20:01:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:01:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:01:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1254714ms till timeout)
2022-03-30 20:01:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (774718ms till timeout)
2022-03-30 20:01:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (774715ms till timeout)
2022-03-30 20:01:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1253710ms till timeout)
2022-03-30 20:01:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (773714ms till timeout)
2022-03-30 20:01:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (773711ms till timeout)
2022-03-30 20:01:47 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1252706ms till timeout)
2022-03-30 20:01:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (772710ms till timeout)
2022-03-30 20:01:47 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (772707ms till timeout)
2022-03-30 20:01:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1251702ms till timeout)
2022-03-30 20:01:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (771703ms till timeout)
2022-03-30 20:01:48 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (771706ms till timeout)
2022-03-30 20:01:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1250698ms till timeout)
2022-03-30 20:01:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (770702ms till timeout)
2022-03-30 20:01:49 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (770698ms till timeout)
2022-03-30 20:01:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:01:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:01:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1249695ms till timeout)
2022-03-30 20:01:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (769698ms till timeout)
2022-03-30 20:01:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (769693ms till timeout)
2022-03-30 20:01:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1248691ms till timeout)
2022-03-30 20:01:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (768694ms till timeout)
2022-03-30 20:01:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (768689ms till timeout)
2022-03-30 20:01:52 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1247687ms till timeout)
2022-03-30 20:01:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (767690ms till timeout)
2022-03-30 20:01:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (767687ms till timeout)
2022-03-30 20:01:53 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1246683ms till timeout)
2022-03-30 20:01:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (766686ms till timeout)
2022-03-30 20:01:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (766683ms till timeout)
2022-03-30 20:01:54 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1245678ms till timeout)
2022-03-30 20:01:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (765682ms till timeout)
2022-03-30 20:01:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (765679ms till timeout)
2022-03-30 20:01:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:01:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:01:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1244674ms till timeout)
2022-03-30 20:01:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (764675ms till timeout)
2022-03-30 20:01:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (764678ms till timeout)
2022-03-30 20:01:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1243670ms till timeout)
2022-03-30 20:01:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (763674ms till timeout)
2022-03-30 20:01:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (763671ms till timeout)
2022-03-30 20:01:57 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1242666ms till timeout)
2022-03-30 20:01:57 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (762667ms till timeout)
2022-03-30 20:01:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (762670ms till timeout)
2022-03-30 20:01:58 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1241662ms till timeout)
2022-03-30 20:01:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (761662ms till timeout)
2022-03-30 20:01:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (761665ms till timeout)
2022-03-30 20:01:59 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1240658ms till timeout)
2022-03-30 20:01:59 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (760659ms till timeout)
2022-03-30 20:01:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (760661ms till timeout)
2022-03-30 20:01:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:01:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:02:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1239654ms till timeout)
2022-03-30 20:02:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (759657ms till timeout)
2022-03-30 20:02:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (759654ms till timeout)
2022-03-30 20:02:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1238650ms till timeout)
2022-03-30 20:02:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (758653ms till timeout)
2022-03-30 20:02:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (758650ms till timeout)
2022-03-30 20:02:02 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1237646ms till timeout)
2022-03-30 20:02:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (757650ms till timeout)
2022-03-30 20:02:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (757647ms till timeout)
2022-03-30 20:02:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1236642ms till timeout)
2022-03-30 20:02:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (756646ms till timeout)
2022-03-30 20:02:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (756643ms till timeout)
2022-03-30 20:02:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1235638ms till timeout)
2022-03-30 20:02:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (755638ms till timeout)
2022-03-30 20:02:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (755629ms till timeout)
2022-03-30 20:02:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:02:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:02:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1234634ms till timeout)
2022-03-30 20:02:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (754635ms till timeout)
2022-03-30 20:02:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (754626ms till timeout)
2022-03-30 20:02:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1233631ms till timeout)
2022-03-30 20:02:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (753630ms till timeout)
2022-03-30 20:02:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (753623ms till timeout)
2022-03-30 20:02:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1232626ms till timeout)
2022-03-30 20:02:07 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (752627ms till timeout)
2022-03-30 20:02:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (752621ms till timeout)
2022-03-30 20:02:08 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1231622ms till timeout)
2022-03-30 20:02:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (751622ms till timeout)
2022-03-30 20:02:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (751617ms till timeout)
2022-03-30 20:02:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1230618ms till timeout)
2022-03-30 20:02:09 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (750619ms till timeout)
2022-03-30 20:02:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (750613ms till timeout)
2022-03-30 20:02:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:02:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:02:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1229610ms till timeout)
2022-03-30 20:02:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (749611ms till timeout)
2022-03-30 20:02:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (749611ms till timeout)
2022-03-30 20:02:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1228606ms till timeout)
2022-03-30 20:02:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (748607ms till timeout)
2022-03-30 20:02:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (748608ms till timeout)
2022-03-30 20:02:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1227603ms till timeout)
2022-03-30 20:02:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (747603ms till timeout)
2022-03-30 20:02:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (747605ms till timeout)
2022-03-30 20:02:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1226599ms till timeout)
2022-03-30 20:02:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (746599ms till timeout)
2022-03-30 20:02:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (746602ms till timeout)
2022-03-30 20:02:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1225595ms till timeout)
2022-03-30 20:02:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (745595ms till timeout)
2022-03-30 20:02:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (745598ms till timeout)
2022-03-30 20:02:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:02:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:02:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1224590ms till timeout)
2022-03-30 20:02:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (744591ms till timeout)
2022-03-30 20:02:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (744594ms till timeout)
2022-03-30 20:02:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1223586ms till timeout)
2022-03-30 20:02:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (743586ms till timeout)
2022-03-30 20:02:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (743588ms till timeout)
2022-03-30 20:02:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1222582ms till timeout)
2022-03-30 20:02:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (742585ms till timeout)
2022-03-30 20:02:17 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (742582ms till timeout)
2022-03-30 20:02:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1221578ms till timeout)
2022-03-30 20:02:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (741581ms till timeout)
2022-03-30 20:02:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (741578ms till timeout)
2022-03-30 20:02:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1220574ms till timeout)
2022-03-30 20:02:19 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (740574ms till timeout)
2022-03-30 20:02:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (740577ms till timeout)
2022-03-30 20:02:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:02:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:02:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1219570ms till timeout)
2022-03-30 20:02:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (739573ms till timeout)
2022-03-30 20:02:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (739570ms till timeout)
2022-03-30 20:02:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1218566ms till timeout)
2022-03-30 20:02:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (738566ms till timeout)
2022-03-30 20:02:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (738569ms till timeout)
2022-03-30 20:02:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1217557ms till timeout)
2022-03-30 20:02:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (737559ms till timeout)
2022-03-30 20:02:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (737556ms till timeout)
2022-03-30 20:02:23 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1216553ms till timeout)
2022-03-30 20:02:23 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (736556ms till timeout)
2022-03-30 20:02:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (736551ms till timeout)
2022-03-30 20:02:24 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1215549ms till timeout)
2022-03-30 20:02:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (735552ms till timeout)
2022-03-30 20:02:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (735549ms till timeout)
2022-03-30 20:02:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:02:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:02:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1214545ms till timeout)
2022-03-30 20:02:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (734544ms till timeout)
2022-03-30 20:02:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (734547ms till timeout)
2022-03-30 20:02:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1213539ms till timeout)
2022-03-30 20:02:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (733539ms till timeout)
2022-03-30 20:02:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (733541ms till timeout)
2022-03-30 20:02:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1212530ms till timeout)
2022-03-30 20:02:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (732533ms till timeout)
2022-03-30 20:02:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (732530ms till timeout)
2022-03-30 20:02:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1211526ms till timeout)
2022-03-30 20:02:28 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (731529ms till timeout)
2022-03-30 20:02:28 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: http-bridge-tls-cluster-name is in desired state: Ready
2022-03-30 20:02:28 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-675329606-2074817820 in namespace http-bridge-tls-st
2022-03-30 20:02:28 [ForkJoinPool-3-worker-7] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkausers' with unstable version 'v1beta2'
2022-03-30 20:02:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-675329606-2074817820
2022-03-30 20:02:28 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-675329606-2074817820 will have desired state: Ready
2022-03-30 20:02:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-675329606-2074817820 will have desired state: Ready
2022-03-30 20:02:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-675329606-2074817820 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 20:02:29 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1210521ms till timeout)
2022-03-30 20:02:29 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (730524ms till timeout)
2022-03-30 20:02:29 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-675329606-2074817820 is in desired state: Ready
2022-03-30 20:02:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:02:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:02:29 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment http-bridge-tls-st-kafka-clients in namespace http-bridge-tls-st
2022-03-30 20:02:29 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients
2022-03-30 20:02:29 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready
2022-03-30 20:02:29 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready
2022-03-30 20:02:29 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready not ready, will try again in 1000 ms (479996ms till timeout)
2022-03-30 20:02:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1209518ms till timeout)
2022-03-30 20:02:30 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (729521ms till timeout)
2022-03-30 20:02:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready not ready, will try again in 1000 ms (478993ms till timeout)
2022-03-30 20:02:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1208514ms till timeout)
2022-03-30 20:02:31 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (728517ms till timeout)
2022-03-30 20:02:31 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: http-bridge-tls-st-kafka-clients is ready
2022-03-30 20:02:31 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-30 20:02:31 [ForkJoinPool-3-worker-7] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkabridges' with unstable version 'v1beta2'
2022-03-30 20:02:31 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaBridge:http-bridge-tls-cluster-name
2022-03-30 20:02:31 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-30 20:02:31 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-30 20:02:31 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (479997ms till timeout)
2022-03-30 20:02:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1207510ms till timeout)
2022-03-30 20:02:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (727513ms till timeout)
2022-03-30 20:02:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (478994ms till timeout)
2022-03-30 20:02:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1206502ms till timeout)
2022-03-30 20:02:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (726506ms till timeout)
2022-03-30 20:02:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (477990ms till timeout)
2022-03-30 20:02:34 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1205498ms till timeout)
2022-03-30 20:02:34 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (725502ms till timeout)
2022-03-30 20:02:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:02:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:02:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (476986ms till timeout)
2022-03-30 20:02:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1204495ms till timeout)
2022-03-30 20:02:35 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (724498ms till timeout)
2022-03-30 20:02:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (475980ms till timeout)
2022-03-30 20:02:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1203491ms till timeout)
2022-03-30 20:02:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (723495ms till timeout)
2022-03-30 20:02:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (474976ms till timeout)
2022-03-30 20:02:37 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1202487ms till timeout)
2022-03-30 20:02:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (722491ms till timeout)
2022-03-30 20:02:37 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (473969ms till timeout)
2022-03-30 20:02:38 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1201483ms till timeout)
2022-03-30 20:02:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (721486ms till timeout)
2022-03-30 20:02:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (472965ms till timeout)
2022-03-30 20:02:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1200479ms till timeout)
2022-03-30 20:02:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (720482ms till timeout)
2022-03-30 20:02:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:02:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:02:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (471962ms till timeout)
2022-03-30 20:02:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1199475ms till timeout)
2022-03-30 20:02:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (719478ms till timeout)
2022-03-30 20:02:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (470957ms till timeout)
2022-03-30 20:02:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1198471ms till timeout)
2022-03-30 20:02:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (718475ms till timeout)
2022-03-30 20:02:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (469953ms till timeout)
2022-03-30 20:02:42 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1197468ms till timeout)
2022-03-30 20:02:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (717471ms till timeout)
2022-03-30 20:02:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (468948ms till timeout)
2022-03-30 20:02:43 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1196464ms till timeout)
2022-03-30 20:02:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (716467ms till timeout)
2022-03-30 20:02:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (467941ms till timeout)
2022-03-30 20:02:44 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1195459ms till timeout)
2022-03-30 20:02:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (715460ms till timeout)
2022-03-30 20:02:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:02:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:02:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (466938ms till timeout)
2022-03-30 20:02:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1194455ms till timeout)
2022-03-30 20:02:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (714458ms till timeout)
2022-03-30 20:02:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (465934ms till timeout)
2022-03-30 20:02:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1193451ms till timeout)
2022-03-30 20:02:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (713454ms till timeout)
2022-03-30 20:02:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (464931ms till timeout)
2022-03-30 20:02:47 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1192446ms till timeout)
2022-03-30 20:02:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (712450ms till timeout)
2022-03-30 20:02:47 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (463926ms till timeout)
2022-03-30 20:02:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1191442ms till timeout)
2022-03-30 20:02:48 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (711445ms till timeout)
2022-03-30 20:02:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (462922ms till timeout)
2022-03-30 20:02:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1190438ms till timeout)
2022-03-30 20:02:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (710441ms till timeout)
2022-03-30 20:02:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:02:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:02:49 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (461918ms till timeout)
2022-03-30 20:02:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1189434ms till timeout)
2022-03-30 20:02:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (709438ms till timeout)
2022-03-30 20:02:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (460914ms till timeout)
2022-03-30 20:02:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1188430ms till timeout)
2022-03-30 20:02:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (708433ms till timeout)
2022-03-30 20:02:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (459909ms till timeout)
2022-03-30 20:02:52 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1187425ms till timeout)
2022-03-30 20:02:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (707428ms till timeout)
2022-03-30 20:02:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (458904ms till timeout)
2022-03-30 20:02:53 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1186420ms till timeout)
2022-03-30 20:02:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (706424ms till timeout)
2022-03-30 20:02:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (457900ms till timeout)
2022-03-30 20:02:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:02:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:02:54 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1185416ms till timeout)
2022-03-30 20:02:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (705420ms till timeout)
2022-03-30 20:02:54 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaBridge: http-bridge-tls-cluster-name is in desired state: Ready
2022-03-30 20:02:54 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-30 20:02:54 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 20:02:54 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testSendSimpleMessageTls-STARTED
2022-03-30 20:02:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 20:02:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:659] [bridge.HttpBridgeTlsST - Before Each] - Setup test case environment
2022-03-30 20:02:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:77] [bridge.HttpBridgeTlsST] - Adding parallel test: testSendSimpleMessageTls
2022-03-30 20:02:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:81] [bridge.HttpBridgeTlsST] - Parallel test count: 2
2022-03-30 20:02:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:230] testSendSimpleMessageTls test now can proceed its execution
2022-03-30 20:02:54 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 20:02:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-ca7325d7, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140}
2022-03-30 20:02:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-1309491966-2015599610, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677}
2022-03-30 20:02:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-435427300-917779110, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206}
2022-03-30 20:02:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients}
2022-03-30 20:02:54 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-2038900051-31929187 in namespace http-bridge-tls-st
2022-03-30 20:02:54 [ForkJoinPool-3-worker-7] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkatopics' with unstable version 'v1beta2'
2022-03-30 20:02:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-2038900051-31929187
2022-03-30 20:02:54 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-2038900051-31929187 will have desired state: Ready
2022-03-30 20:02:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-2038900051-31929187 will have desired state: Ready
2022-03-30 20:02:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-2038900051-31929187 will have desired state: Ready not ready, will try again in 1000 ms (179997ms till timeout)
2022-03-30 20:02:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1184413ms till timeout)
2022-03-30 20:02:55 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: user-cluster-name is in desired state: Ready
2022-03-30 20:02:55 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 20:02:55 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testUpdateUser-STARTED
2022-03-30 20:02:55 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 20:02:55 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-30 20:02:55 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testUpdateUser
2022-03-30 20:02:55 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 3
2022-03-30 20:02:55 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:230] testUpdateUser test now can proceed its execution
2022-03-30 20:02:55 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 20:02:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-ca7325d7, testUpdateUser=my-cluster-6e6fa8fe, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140}
2022-03-30 20:02:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-1309491966-2015599610, testUpdateUser=my-user-1387197422-925410353, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677}
2022-03-30 20:02:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-435427300-917779110, testUpdateUser=my-topic-1875783706-1763804507, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206}
2022-03-30 20:02:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testUpdateUser=my-cluster-6e6fa8fe-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients}
2022-03-30 20:02:55 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1387197422-925410353 in namespace user-st
2022-03-30 20:02:55 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1387197422-925410353
2022-03-30 20:02:55 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1387197422-925410353 will have desired state: Ready
2022-03-30 20:02:55 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-1387197422-925410353 will have desired state: Ready
2022-03-30 20:02:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-1387197422-925410353 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 20:02:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-2038900051-31929187 is in desired state: Ready
2022-03-30 20:02:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job producer-604865206 in namespace http-bridge-tls-st
2022-03-30 20:02:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:producer-604865206
2022-03-30 20:02:55 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: producer-604865206 will be in active state
2022-03-30 20:02:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for job active
2022-03-30 20:02:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:producer-604865206 to finished
2022-03-30 20:02:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for job finished
2022-03-30 20:02:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job producer-604865206 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T20:02:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 20:02:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (219987ms till timeout)
2022-03-30 20:02:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1183409ms till timeout)
2022-03-30 20:02:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1387197422-925410353 is in desired state: Ready
2022-03-30 20:02:56 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['data']['ca.crt']
2022-03-30 20:02:56 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['data']['user.crt']
2022-03-30 20:02:56 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['data']['user.key']
2022-03-30 20:02:56 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['metadata']['name']
2022-03-30 20:02:56 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['metadata']['namespace']
2022-03-30 20:02:56 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['metadata']['name']
2022-03-30 20:02:56 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['metadata']['namespace']
2022-03-30 20:02:56 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['spec']['authentication']['type']
2022-03-30 20:02:56 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for increase observation generation from 1 for user my-user-1387197422-925410353
2022-03-30 20:02:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] increase observation generation from 1 for user my-user-1387197422-925410353 not ready, will try again in 1000 ms (179996ms till timeout)
2022-03-30 20:02:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job producer-604865206 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T20:02:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 20:02:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (218979ms till timeout)
2022-03-30 20:02:57 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1182403ms till timeout)
2022-03-30 20:02:57 [ForkJoinPool-3-worker-5] [32mINFO [m [SecretUtils:46] Waiting for Secret my-user-1387197422-925410353
2022-03-30 20:02:57 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Expected secret my-user-1387197422-925410353 exists
2022-03-30 20:02:57 [ForkJoinPool-3-worker-5] [32mINFO [m [SecretUtils:50] Secret my-user-1387197422-925410353 created
2022-03-30 20:02:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1387197422-925410353 will have desired state: Ready
2022-03-30 20:02:57 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-1387197422-925410353 will have desired state: Ready
2022-03-30 20:02:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1387197422-925410353 is in desired state: Ready
2022-03-30 20:02:57 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['data']['password']
2022-03-30 20:02:57 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['metadata']['name']
2022-03-30 20:02:57 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['metadata']['namespace']
2022-03-30 20:02:57 [ForkJoinPool-3-worker-5] [36mDEBUG[m [CompiledPath:93] Evaluating path: $['spec']['authentication']['type']
2022-03-30 20:02:57 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaUserUtils:62] Waiting for KafkaUser deletion my-user-1387197422-925410353
2022-03-30 20:02:57 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser deletion my-user-1387197422-925410353
2022-03-30 20:02:57 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaUserUtils:75] KafkaUser my-user-1387197422-925410353 deleted
2022-03-30 20:02:57 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 20:02:57 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-30 20:02:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:02:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testUpdateUser
2022-03-30 20:02:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1387197422-925410353 in namespace user-st
2022-03-30 20:02:57 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1387197422-925410353
2022-03-30 20:02:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:02:57 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:267] testUpdateUser - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendSimpleMessageTls, testUpdateUser] to and randomly select one to start execution
2022-03-30 20:02:57 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testUpdateUser
2022-03-30 20:02:57 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 2
2022-03-30 20:02:57 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testUpdateUser-FINISHED
2022-03-30 20:02:57 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 20:02:57 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 20:02:57 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:690] [operators.user.UserST - After All] - Clean up after test suite
2022-03-30 20:02:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:02:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for UserST
2022-03-30 20:02:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka user-cluster-name in namespace user-st
2022-03-30 20:02:57 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:user-cluster-name
2022-03-30 20:02:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:user-cluster-name not ready, will try again in 10000 ms (839993ms till timeout)
2022-03-30 20:02:57 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job producer-604865206 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T20:02:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 20:02:57 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (217969ms till timeout)
2022-03-30 20:02:58 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1181399ms till timeout)
2022-03-30 20:02:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job producer-604865206 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T20:02:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 20:02:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (216963ms till timeout)
2022-03-30 20:02:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:02:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:02:59 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1180396ms till timeout)
2022-03-30 20:02:59 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job producer-604865206 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T20:02:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 20:02:59 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (215958ms till timeout)
2022-03-30 20:03:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1179392ms till timeout)
2022-03-30 20:03:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job producer-604865206 in namespace http-bridge-tls-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-30T20:03:00Z, conditions=[JobCondition(lastProbeTime=2022-03-30T20:03:00Z, lastTransitionTime=2022-03-30T20:03:00Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-30T20:02:55Z, succeeded=1, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 20:03:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [JobUtils:37] Waiting for ReplicaSet of Deployment producer-604865206 deletion
2022-03-30 20:03:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for ReplicaSet producer-604865206 to be deleted
2022-03-30 20:03:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] ReplicaSet producer-604865206 to be deleted not ready, will try again in 5000 ms (179993ms till timeout)
2022-03-30 20:03:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1178388ms till timeout)
2022-03-30 20:03:02 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1177384ms till timeout)
2022-03-30 20:03:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1176381ms till timeout)
2022-03-30 20:03:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:03:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:03:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1175377ms till timeout)
2022-03-30 20:03:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1174374ms till timeout)
2022-03-30 20:03:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [JobUtils:40] Job producer-604865206 was deleted
2022-03-30 20:03:05 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-30 20:03:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job consumer-92922259 in namespace http-bridge-tls-st
2022-03-30 20:03:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:consumer-92922259
2022-03-30 20:03:05 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: consumer-92922259 will be in active state
2022-03-30 20:03:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for job active
2022-03-30 20:03:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job active not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 20:03:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1173370ms till timeout)
2022-03-30 20:03:06 [ForkJoinPool-3-worker-7] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:consumer-92922259 to finished
2022-03-30 20:03:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for job finished
2022-03-30 20:03:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job consumer-92922259 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T20:03:05Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 20:03:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (219994ms till timeout)
2022-03-30 20:03:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1172365ms till timeout)
2022-03-30 20:03:07 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:03:07 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Namespace user-st removal
2022-03-30 20:03:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 get Namespace user-st -o yaml
2022-03-30 20:03:07 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 get Namespace user-st -o yaml
2022-03-30 20:03:07 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:03:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (479916ms till timeout)
2022-03-30 20:03:07 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job consumer-92922259 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T20:03:05Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 20:03:07 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (218988ms till timeout)
2022-03-30 20:03:08 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1171361ms till timeout)
2022-03-30 20:03:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 get Namespace user-st -o yaml
2022-03-30 20:03:08 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 get Namespace user-st -o yaml
2022-03-30 20:03:08 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:03:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (478837ms till timeout)
2022-03-30 20:03:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job consumer-92922259 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T20:03:05Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 20:03:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (217983ms till timeout)
2022-03-30 20:03:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:03:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:03:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1170358ms till timeout)
2022-03-30 20:03:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 get Namespace user-st -o yaml
2022-03-30 20:03:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job consumer-92922259 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T20:03:05Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 20:03:09 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (216977ms till timeout)
2022-03-30 20:03:09 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 get Namespace user-st -o yaml
2022-03-30 20:03:09 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:03:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (477758ms till timeout)
2022-03-30 20:03:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1169354ms till timeout)
2022-03-30 20:03:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job consumer-92922259 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T20:03:05Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 20:03:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (215971ms till timeout)
2022-03-30 20:03:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 get Namespace user-st -o yaml
2022-03-30 20:03:10 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 get Namespace user-st -o yaml
2022-03-30 20:03:10 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:03:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (476678ms till timeout)
2022-03-30 20:03:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1168350ms till timeout)
2022-03-30 20:03:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job consumer-92922259 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-30T20:03:05Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 20:03:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job finished not ready, will try again in 1000 ms (214965ms till timeout)
2022-03-30 20:03:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 get Namespace user-st -o yaml
2022-03-30 20:03:12 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 get Namespace user-st -o yaml
2022-03-30 20:03:12 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:03:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (475601ms till timeout)
2022-03-30 20:03:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1167346ms till timeout)
2022-03-30 20:03:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [ClientUtils:79] Job consumer-92922259 in namespace http-bridge-tls-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-30T20:03:12Z, conditions=[JobCondition(lastProbeTime=2022-03-30T20:03:12Z, lastTransitionTime=2022-03-30T20:03:12Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-30T20:03:05Z, succeeded=1, uncountedTerminatedPods=null, additionalProperties={})
2022-03-30 20:03:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [JobUtils:37] Waiting for ReplicaSet of Deployment consumer-92922259 deletion
2022-03-30 20:03:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for ReplicaSet consumer-92922259 to be deleted
2022-03-30 20:03:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] ReplicaSet consumer-92922259 to be deleted not ready, will try again in 5000 ms (179997ms till timeout)
2022-03-30 20:03:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 get Namespace user-st -o yaml
2022-03-30 20:03:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-0 get Namespace user-st -o yaml
2022-03-30 20:03:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 20:03:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 20:03:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "user-st" not found
2022-03-30 20:03:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 20:03:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@12282135=[namespace-0], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st]}
2022-03-30 20:03:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:254] UserST - Notifies waiting test suites:[HttpBridgeTlsST, CruiseControlApiST, UserST] to and randomly select one to start execution
2022-03-30 20:03:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:85] [operators.user.UserST] - Removing parallel suite: UserST
2022-03-30 20:03:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:89] [operators.user.UserST] - Parallel suites count: 2
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 183.736 s - in io.strimzi.systemtest.operators.user.UserST
[[1;34mINFO[m] Running io.strimzi.systemtest.cruisecontrol.CruiseControlST
2022-03-30 20:03:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 20:03:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:667] [cruisecontrol.CruiseControlST - Before All] - Setup test suite environment
2022-03-30 20:03:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:69] [cruisecontrol.CruiseControlST] - Adding parallel suite: CruiseControlST
2022-03-30 20:03:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:73] [cruisecontrol.CruiseControlST] - Parallel suites count: 3
2022-03-30 20:03:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:184] CruiseControlST suite now can proceed its execution
2022-03-30 20:03:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-30 20:03:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestSuiteNamespaceManager:129] Test suite `CruiseControlST` creates these additional namespaces:[cruise-control-st]
2022-03-30 20:03:13 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: cruise-control-st
2022-03-30 20:03:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Namespace cruise-control-st
2022-03-30 20:03:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 get Namespace cruise-control-st -o json
2022-03-30 20:03:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 get Namespace cruise-control-st -o json
2022-03-30 20:03:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:03:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T20:03:13Z",
        "name": "cruise-control-st",
        "resourceVersion": "82748",
        "selfLink": "/api/v1/namespaces/cruise-control-st",
        "uid": "ce8bf44a-1512-4cfc-ba31-3d738e4d8f9f"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 20:03:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@12282135=[namespace-0], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st]}
2022-03-30 20:03:13 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: cruise-control-st
2022-03-30 20:03:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=cruise-control-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 20:03:13 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: cruise-control-st
2022-03-30 20:03:13 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 20:03:13 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlApiST.testCruiseControlBasicAPIRequests-STARTED
2022-03-30 20:03:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 20:03:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:659] [cruisecontrol.CruiseControlApiST - Before Each] - Setup test case environment
2022-03-30 20:03:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:77] [cruisecontrol.CruiseControlApiST] - Adding parallel test: testCruiseControlBasicAPIRequests
2022-03-30 20:03:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:81] [cruisecontrol.CruiseControlApiST] - Parallel test count: 3
2022-03-30 20:03:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:230] testCruiseControlBasicAPIRequests test now can proceed its execution
2022-03-30 20:03:13 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 20:03:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-ca7325d7, testUpdateUser=my-cluster-6e6fa8fe, testCruiseControlBasicAPIRequests=my-cluster-8aed4745, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140}
2022-03-30 20:03:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-1309491966-2015599610, testUpdateUser=my-user-1387197422-925410353, testCruiseControlBasicAPIRequests=my-user-1861484466-561878159, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677}
2022-03-30 20:03:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-435427300-917779110, testUpdateUser=my-topic-1875783706-1763804507, testCruiseControlBasicAPIRequests=my-topic-1631570507-1308241024, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206}
2022-03-30 20:03:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testUpdateUser=my-cluster-6e6fa8fe-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-8aed4745-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients}
2022-03-30 20:03:13 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-1 for test case:testCruiseControlBasicAPIRequests
2022-03-30 20:03:13 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-1
2022-03-30 20:03:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-1
2022-03-30 20:03:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st get Namespace namespace-1 -o json
2022-03-30 20:03:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st get Namespace namespace-1 -o json
2022-03-30 20:03:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:03:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T20:03:13Z",
        "name": "namespace-1",
        "resourceVersion": "82752",
        "selfLink": "/api/v1/namespaces/namespace-1",
        "uid": "17246190-c409-4379-81a5-86155456bf04"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 20:03:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@12282135=[namespace-0], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-1]}
2022-03-30 20:03:13 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-1
2022-03-30 20:03:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-1, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 20:03:13 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-1
2022-03-30 20:03:13 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-09b59e48 in namespace namespace-1
2022-03-30 20:03:13 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-1
2022-03-30 20:03:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-09b59e48
2022-03-30 20:03:13 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-09b59e48 will have desired state: Ready
2022-03-30 20:03:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-09b59e48 will have desired state: Ready
2022-03-30 20:03:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1319994ms till timeout)
2022-03-30 20:03:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1166342ms till timeout)
2022-03-30 20:03:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:03:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:03:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1318990ms till timeout)
2022-03-30 20:03:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1165338ms till timeout)
2022-03-30 20:03:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1317986ms till timeout)
2022-03-30 20:03:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1164335ms till timeout)
2022-03-30 20:03:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1316982ms till timeout)
2022-03-30 20:03:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1163331ms till timeout)
2022-03-30 20:03:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1315976ms till timeout)
2022-03-30 20:03:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1162327ms till timeout)
2022-03-30 20:03:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [JobUtils:40] Job consumer-92922259 was deleted
2022-03-30 20:03:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 20:03:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:675] [bridge.HttpBridgeTlsST - After Each] - Clean up after test
2022-03-30 20:03:17 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:03:17 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testSendSimpleMessageTls
2022-03-30 20:03:17 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job producer-604865206 in namespace http-bridge-tls-st
2022-03-30 20:03:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:producer-604865206
2022-03-30 20:03:17 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job consumer-92922259 in namespace http-bridge-tls-st
2022-03-30 20:03:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:consumer-92922259
2022-03-30 20:03:17 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-2038900051-31929187 in namespace http-bridge-tls-st
2022-03-30 20:03:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-2038900051-31929187
2022-03-30 20:03:17 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-2038900051-31929187 not ready, will try again in 10000 ms (179982ms till timeout)
2022-03-30 20:03:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1314972ms till timeout)
2022-03-30 20:03:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1161324ms till timeout)
2022-03-30 20:03:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:03:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:03:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1313968ms till timeout)
2022-03-30 20:03:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1160319ms till timeout)
2022-03-30 20:03:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1312962ms till timeout)
2022-03-30 20:03:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1159315ms till timeout)
2022-03-30 20:03:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1311957ms till timeout)
2022-03-30 20:03:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1158305ms till timeout)
2022-03-30 20:03:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1310953ms till timeout)
2022-03-30 20:03:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1157301ms till timeout)
2022-03-30 20:03:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1309950ms till timeout)
2022-03-30 20:03:23 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1156297ms till timeout)
2022-03-30 20:03:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:03:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:03:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1308946ms till timeout)
2022-03-30 20:03:24 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1155293ms till timeout)
2022-03-30 20:03:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1307942ms till timeout)
2022-03-30 20:03:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1154290ms till timeout)
2022-03-30 20:03:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1306938ms till timeout)
2022-03-30 20:03:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1153286ms till timeout)
2022-03-30 20:03:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1305934ms till timeout)
2022-03-30 20:03:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1152283ms till timeout)
2022-03-30 20:03:27 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:03:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:267] testSendSimpleMessageTls - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendSimpleMessageTls, testUpdateUser, testCruiseControlBasicAPIRequests] to and randomly select one to start execution
2022-03-30 20:03:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:93] [bridge.HttpBridgeTlsST] - Removing parallel test: testSendSimpleMessageTls
2022-03-30 20:03:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:97] [bridge.HttpBridgeTlsST] - Parallel test count: 2
2022-03-30 20:03:27 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testSendSimpleMessageTls-FINISHED
2022-03-30 20:03:27 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 20:03:27 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 20:03:27 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testReceiveSimpleMessageTls-STARTED
2022-03-30 20:03:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 20:03:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:659] [bridge.HttpBridgeTlsST - Before Each] - Setup test case environment
2022-03-30 20:03:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:77] [bridge.HttpBridgeTlsST] - Adding parallel test: testReceiveSimpleMessageTls
2022-03-30 20:03:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:81] [bridge.HttpBridgeTlsST] - Parallel test count: 3
2022-03-30 20:03:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:230] testReceiveSimpleMessageTls test now can proceed its execution
2022-03-30 20:03:27 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 20:03:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-ca7325d7, testUpdateUser=my-cluster-6e6fa8fe, testReceiveSimpleMessageTls=my-cluster-2e450144, testCruiseControlBasicAPIRequests=my-cluster-8aed4745, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140}
2022-03-30 20:03:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-1309491966-2015599610, testUpdateUser=my-user-1387197422-925410353, testReceiveSimpleMessageTls=my-user-1669668617-1031881775, testCruiseControlBasicAPIRequests=my-user-1861484466-561878159, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677}
2022-03-30 20:03:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-435427300-917779110, testUpdateUser=my-topic-1875783706-1763804507, testReceiveSimpleMessageTls=my-topic-368564676-637091599, testCruiseControlBasicAPIRequests=my-topic-1631570507-1308241024, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206}
2022-03-30 20:03:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testUpdateUser=my-cluster-6e6fa8fe-kafka-clients, testReceiveSimpleMessageTls=my-cluster-2e450144-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-8aed4745-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients}
2022-03-30 20:03:27 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-762457382-24121967 in namespace http-bridge-tls-st
2022-03-30 20:03:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-762457382-24121967
2022-03-30 20:03:27 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-762457382-24121967 will have desired state: Ready
2022-03-30 20:03:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-762457382-24121967 will have desired state: Ready
2022-03-30 20:03:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-762457382-24121967 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 20:03:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1304931ms till timeout)
2022-03-30 20:03:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1151279ms till timeout)
2022-03-30 20:03:28 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-762457382-24121967 is in desired state: Ready
2022-03-30 20:03:28 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job consumer-175504412 in namespace http-bridge-tls-st
2022-03-30 20:03:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:consumer-175504412
2022-03-30 20:03:28 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: consumer-175504412 will be in active state
2022-03-30 20:03:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for job active
2022-03-30 20:03:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] job active not ready, will try again in 1000 ms (179997ms till timeout)
2022-03-30 20:03:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:03:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:03:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1303927ms till timeout)
2022-03-30 20:03:29 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1150276ms till timeout)
2022-03-30 20:03:29 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-30 20:03:29 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job producer-1165001693 in namespace http-bridge-tls-st
2022-03-30 20:03:29 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:producer-1165001693
2022-03-30 20:03:29 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: producer-1165001693 will be in active state
2022-03-30 20:03:29 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for job active
2022-03-30 20:03:29 [ForkJoinPool-3-worker-7] [32mINFO [m [ClientUtils:61] Waiting till producer producer-1165001693 and consumer consumer-175504412 finish
2022-03-30 20:03:29 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for clients finished
2022-03-30 20:03:29 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (219998ms till timeout)
2022-03-30 20:03:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1302921ms till timeout)
2022-03-30 20:03:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1149272ms till timeout)
2022-03-30 20:03:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (218995ms till timeout)
2022-03-30 20:03:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1301918ms till timeout)
2022-03-30 20:03:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1148268ms till timeout)
2022-03-30 20:03:31 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (217991ms till timeout)
2022-03-30 20:03:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1300914ms till timeout)
2022-03-30 20:03:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1147264ms till timeout)
2022-03-30 20:03:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (216988ms till timeout)
2022-03-30 20:03:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1299910ms till timeout)
2022-03-30 20:03:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1146260ms till timeout)
2022-03-30 20:03:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (215984ms till timeout)
2022-03-30 20:03:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:03:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:03:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1298906ms till timeout)
2022-03-30 20:03:34 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1145257ms till timeout)
2022-03-30 20:03:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (214979ms till timeout)
2022-03-30 20:03:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1297903ms till timeout)
2022-03-30 20:03:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1144254ms till timeout)
2022-03-30 20:03:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [JobUtils:37] Waiting for ReplicaSet of Deployment producer-1165001693 deletion
2022-03-30 20:03:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for ReplicaSet producer-1165001693 to be deleted
2022-03-30 20:03:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] ReplicaSet producer-1165001693 to be deleted not ready, will try again in 5000 ms (179997ms till timeout)
2022-03-30 20:03:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1296899ms till timeout)
2022-03-30 20:03:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: cruise-control-api-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1143251ms till timeout)
2022-03-30 20:03:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1295895ms till timeout)
2022-03-30 20:03:37 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: cruise-control-api-cluster-name is in desired state: Ready
2022-03-30 20:03:37 [ForkJoinPool-3-worker-1] [32mINFO [m [CruiseControlApiST:153] ----> CRUISE CONTROL DEPLOYMENT STATE ENDPOINT <----
2022-03-30 20:03:37 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-0 exec cruise-control-api-cluster-name-cruise-control-568c7787f-qwhpp -c cruise-control -- /bin/bash -c curl -XGET -k  HTTP://localhost:9090/kafkacruisecontrol/state
2022-03-30 20:03:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-0 exec cruise-control-api-cluster-name-cruise-control-568c7787f-qwhpp -c cruise-control -- /bin/bash -c curl -XGET -k  HTTP://localhost:9090/kafkacruisecontrol/state
2022-03-30 20:03:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:03:37 [ForkJoinPool-3-worker-1] [32mINFO [m [CruiseControlApiST:157] Verifying that Cruise Control REST API is available using HTTP request without credentials
2022-03-30 20:03:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 20:03:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:675] [cruisecontrol.CruiseControlApiST - After Each] - Clean up after test
2022-03-30 20:03:37 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:03:37 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testCruiseControlBasicAPIRequestsWithSecurityDisabled
2022-03-30 20:03:37 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka cruise-control-api-cluster-name in namespace namespace-0
2022-03-30 20:03:37 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-0, for cruise control Kafka cluster cruise-control-api-cluster-name
2022-03-30 20:03:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:cruise-control-api-cluster-name
2022-03-30 20:03:37 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:cruise-control-api-cluster-name not ready, will try again in 10000 ms (839994ms till timeout)
2022-03-30 20:03:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1294891ms till timeout)
2022-03-30 20:03:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:03:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:03:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1293888ms till timeout)
2022-03-30 20:03:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1292884ms till timeout)
2022-03-30 20:03:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [JobUtils:40] Job producer-1165001693 was deleted
2022-03-30 20:03:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [JobUtils:37] Waiting for ReplicaSet of Deployment consumer-175504412 deletion
2022-03-30 20:03:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for ReplicaSet consumer-175504412 to be deleted
2022-03-30 20:03:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] ReplicaSet consumer-175504412 to be deleted not ready, will try again in 5000 ms (179996ms till timeout)
2022-03-30 20:03:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1291880ms till timeout)
2022-03-30 20:03:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1290877ms till timeout)
2022-03-30 20:03:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1289873ms till timeout)
2022-03-30 20:03:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:03:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:03:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1288870ms till timeout)
2022-03-30 20:03:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1287865ms till timeout)
2022-03-30 20:03:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [JobUtils:40] Job consumer-175504412 was deleted
2022-03-30 20:03:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 20:03:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:675] [bridge.HttpBridgeTlsST - After Each] - Clean up after test
2022-03-30 20:03:45 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:03:45 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testReceiveSimpleMessageTls
2022-03-30 20:03:45 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job consumer-175504412 in namespace http-bridge-tls-st
2022-03-30 20:03:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:consumer-175504412
2022-03-30 20:03:45 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job producer-1165001693 in namespace http-bridge-tls-st
2022-03-30 20:03:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:producer-1165001693
2022-03-30 20:03:45 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-762457382-24121967 in namespace http-bridge-tls-st
2022-03-30 20:03:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-762457382-24121967
2022-03-30 20:03:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-762457382-24121967 not ready, will try again in 10000 ms (179988ms till timeout)
2022-03-30 20:03:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1286855ms till timeout)
2022-03-30 20:03:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1285851ms till timeout)
2022-03-30 20:03:47 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:03:47 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-0 for test case:testCruiseControlBasicAPIRequestsWithSecurityDisabled
2022-03-30 20:03:47 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-0 removal
2022-03-30 20:03:47 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:03:47 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:03:47 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:03:47 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (479916ms till timeout)
2022-03-30 20:03:48 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1284846ms till timeout)
2022-03-30 20:03:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:03:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:03:49 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:03:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (478818ms till timeout)
[[1;34mINFO[m] Running io.strimzi.systemtest.kafka.listeners.ListenersST
2022-03-30 20:03:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:03:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:03:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1283820ms till timeout)
2022-03-30 20:03:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:03:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:03:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:03:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (477693ms till timeout)
2022-03-30 20:03:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1282812ms till timeout)
2022-03-30 20:03:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:03:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:03:51 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:03:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (476575ms till timeout)
2022-03-30 20:03:51 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1281803ms till timeout)
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:667] [kafka.listeners.ListenersST - Before All] - Setup test suite environment
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:69] [kafka.listeners.ListenersST] - Adding parallel suite: ListenersST
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:73] [kafka.listeners.ListenersST] - Parallel suites count: 4
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:184] ListenersST suite now can proceed its execution
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestSuiteNamespaceManager:129] Test suite `ListenersST` creates these additional namespaces:[listeners-st]
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:156] Creating Namespace: listeners-st
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Namespace listeners-st
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace listeners-st -o json
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace listeners-st -o json
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T20:03:52Z",
        "name": "listeners-st",
        "resourceVersion": "83114",
        "selfLink": "/api/v1/namespaces/listeners-st",
        "uid": "fa41e639-5b3d-4bd2-9ced-cb6cf7a150b2"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@12282135=[namespace-0], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-1]}
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:82] Client use Namespace: listeners-st
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=listeners-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: listeners-st
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesTlsScramSha-STARTED
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:659] [kafka.listeners.ListenersST - Before Each] - Setup test case environment
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:77] [kafka.listeners.ListenersST] - Adding parallel test: testSendMessagesTlsScramSha
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:81] [kafka.listeners.ListenersST] - Parallel test count: 4
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:230] testSendMessagesTlsScramSha test now can proceed its execution
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testSendSimpleMessageTls=my-cluster-ca7325d7, testUpdateUser=my-cluster-6e6fa8fe, testSendMessagesTlsScramSha=my-cluster-84882f67, testReceiveSimpleMessageTls=my-cluster-2e450144, testCruiseControlBasicAPIRequests=my-cluster-8aed4745, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140}
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testSendSimpleMessageTls=my-user-1309491966-2015599610, testUpdateUser=my-user-1387197422-925410353, testSendMessagesTlsScramSha=my-user-586569987-2105688079, testReceiveSimpleMessageTls=my-user-1669668617-1031881775, testCruiseControlBasicAPIRequests=my-user-1861484466-561878159, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677}
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testSendSimpleMessageTls=my-topic-435427300-917779110, testUpdateUser=my-topic-1875783706-1763804507, testSendMessagesTlsScramSha=my-topic-255809133-823300208, testReceiveSimpleMessageTls=my-topic-368564676-637091599, testCruiseControlBasicAPIRequests=my-topic-1631570507-1308241024, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206}
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testUpdateUser=my-cluster-6e6fa8fe-kafka-clients, testSendMessagesTlsScramSha=my-cluster-84882f67-kafka-clients, testReceiveSimpleMessageTls=my-cluster-2e450144-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-8aed4745-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients}
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-2 for test case:testSendMessagesTlsScramSha
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-2
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-2
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace listeners-st get Namespace namespace-2 -o json
2022-03-30 20:03:52 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace listeners-st get Namespace namespace-2 -o json
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T20:03:52Z",
        "name": "namespace-2",
        "resourceVersion": "83118",
        "selfLink": "/api/v1/namespaces/namespace-2",
        "uid": "4c793954-b5eb-4eb4-8ce0-c32a91912d36"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[namespace-2], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@12282135=[namespace-0], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-1]}
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-2
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-2, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-2
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-84882f67 in namespace namespace-2
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-2
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-84882f67
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-84882f67 will have desired state: Ready
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-84882f67 will have desired state: Ready
2022-03-30 20:03:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (839990ms till timeout)
2022-03-30 20:03:52 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:03:52 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:03:52 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (475461ms till timeout)
2022-03-30 20:03:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1280799ms till timeout)
2022-03-30 20:03:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (838986ms till timeout)
2022-03-30 20:03:53 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:03:53 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:03:53 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:03:53 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (474363ms till timeout)
2022-03-30 20:03:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1279793ms till timeout)
2022-03-30 20:03:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:03:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:03:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (837980ms till timeout)
2022-03-30 20:03:54 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:03:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1278787ms till timeout)
2022-03-30 20:03:54 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:03:54 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:03:54 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (473263ms till timeout)
2022-03-30 20:03:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (836977ms till timeout)
2022-03-30 20:03:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1277783ms till timeout)
2022-03-30 20:03:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:03:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:03:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:03:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (472182ms till timeout)
2022-03-30 20:03:56 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:03:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:267] testReceiveSimpleMessageTls - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendSimpleMessageTls, testUpdateUser, testCruiseControlBasicAPIRequests, testReceiveSimpleMessageTls, testSendMessagesTlsScramSha] to and randomly select one to start execution
2022-03-30 20:03:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:93] [bridge.HttpBridgeTlsST] - Removing parallel test: testReceiveSimpleMessageTls
2022-03-30 20:03:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:97] [bridge.HttpBridgeTlsST] - Parallel test count: 3
2022-03-30 20:03:56 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testReceiveSimpleMessageTls-FINISHED
2022-03-30 20:03:56 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 20:03:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 20:03:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:690] [bridge.HttpBridgeTlsST - After All] - Clean up after test suite
2022-03-30 20:03:56 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:03:56 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for HttpBridgeTlsST
2022-03-30 20:03:56 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment http-bridge-tls-st-kafka-clients in namespace http-bridge-tls-st
2022-03-30 20:03:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients
2022-03-30 20:03:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (479991ms till timeout)
2022-03-30 20:03:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (835972ms till timeout)
2022-03-30 20:03:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1276778ms till timeout)
2022-03-30 20:03:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:03:56 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:03:56 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:03:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (471088ms till timeout)
2022-03-30 20:03:57 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (834968ms till timeout)
2022-03-30 20:03:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1275774ms till timeout)
2022-03-30 20:03:57 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:03:57 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:03:57 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:03:57 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (470003ms till timeout)
2022-03-30 20:03:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (833964ms till timeout)
2022-03-30 20:03:58 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1274767ms till timeout)
2022-03-30 20:03:58 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:03:58 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:03:58 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:03:58 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (468907ms till timeout)
2022-03-30 20:03:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:03:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:03:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (832959ms till timeout)
2022-03-30 20:03:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1273760ms till timeout)
2022-03-30 20:03:59 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (467826ms till timeout)
2022-03-30 20:04:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (831956ms till timeout)
2022-03-30 20:04:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1272756ms till timeout)
2022-03-30 20:04:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:01 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:01 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (466753ms till timeout)
2022-03-30 20:04:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (830952ms till timeout)
2022-03-30 20:04:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1271753ms till timeout)
2022-03-30 20:04:02 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:02 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:02 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:02 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (465674ms till timeout)
2022-03-30 20:04:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (829948ms till timeout)
2022-03-30 20:04:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1270749ms till timeout)
2022-03-30 20:04:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:03 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:03 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (464598ms till timeout)
2022-03-30 20:04:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (828944ms till timeout)
2022-03-30 20:04:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1269745ms till timeout)
2022-03-30 20:04:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:04:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:04:04 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:04 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (463521ms till timeout)
2022-03-30 20:04:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (827940ms till timeout)
2022-03-30 20:04:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1268742ms till timeout)
2022-03-30 20:04:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (462445ms till timeout)
2022-03-30 20:04:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (826936ms till timeout)
2022-03-30 20:04:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1267738ms till timeout)
2022-03-30 20:04:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (469981ms till timeout)
2022-03-30 20:04:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (825933ms till timeout)
2022-03-30 20:04:06 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:06 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (461368ms till timeout)
2022-03-30 20:04:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1266734ms till timeout)
2022-03-30 20:04:07 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (824927ms till timeout)
2022-03-30 20:04:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:07 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (460293ms till timeout)
2022-03-30 20:04:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1265731ms till timeout)
2022-03-30 20:04:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (823923ms till timeout)
2022-03-30 20:04:08 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:08 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1264727ms till timeout)
2022-03-30 20:04:08 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:08 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:08 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (459215ms till timeout)
2022-03-30 20:04:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:04:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:04:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (822920ms till timeout)
2022-03-30 20:04:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1263723ms till timeout)
2022-03-30 20:04:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:09 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:09 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (458127ms till timeout)
2022-03-30 20:04:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (821916ms till timeout)
2022-03-30 20:04:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1262719ms till timeout)
2022-03-30 20:04:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (457039ms till timeout)
2022-03-30 20:04:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (820913ms till timeout)
2022-03-30 20:04:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1261715ms till timeout)
2022-03-30 20:04:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:11 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (455949ms till timeout)
2022-03-30 20:04:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (819909ms till timeout)
2022-03-30 20:04:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1260711ms till timeout)
2022-03-30 20:04:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (454863ms till timeout)
2022-03-30 20:04:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (818905ms till timeout)
2022-03-30 20:04:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1259706ms till timeout)
2022-03-30 20:04:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:14 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:14 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (453760ms till timeout)
2022-03-30 20:04:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:04:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:04:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (817901ms till timeout)
2022-03-30 20:04:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1258703ms till timeout)
2022-03-30 20:04:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (452683ms till timeout)
2022-03-30 20:04:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (816898ms till timeout)
2022-03-30 20:04:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1257699ms till timeout)
2022-03-30 20:04:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (459969ms till timeout)
2022-03-30 20:04:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (451597ms till timeout)
2022-03-30 20:04:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (815895ms till timeout)
2022-03-30 20:04:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1256696ms till timeout)
2022-03-30 20:04:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (450526ms till timeout)
2022-03-30 20:04:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (814891ms till timeout)
2022-03-30 20:04:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1255692ms till timeout)
2022-03-30 20:04:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (449447ms till timeout)
2022-03-30 20:04:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (813888ms till timeout)
2022-03-30 20:04:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1254687ms till timeout)
2022-03-30 20:04:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:04:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:04:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (448374ms till timeout)
2022-03-30 20:04:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (812885ms till timeout)
2022-03-30 20:04:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1253684ms till timeout)
2022-03-30 20:04:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (811881ms till timeout)
2022-03-30 20:04:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (447290ms till timeout)
2022-03-30 20:04:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1252681ms till timeout)
2022-03-30 20:04:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (810878ms till timeout)
2022-03-30 20:04:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1251676ms till timeout)
2022-03-30 20:04:21 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:21 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (446215ms till timeout)
2022-03-30 20:04:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (809874ms till timeout)
2022-03-30 20:04:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1250672ms till timeout)
2022-03-30 20:04:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:22 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:22 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (445143ms till timeout)
2022-03-30 20:04:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (808870ms till timeout)
2022-03-30 20:04:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1249668ms till timeout)
2022-03-30 20:04:23 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:23 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (444053ms till timeout)
2022-03-30 20:04:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:04:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:04:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (807865ms till timeout)
2022-03-30 20:04:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1248665ms till timeout)
2022-03-30 20:04:24 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:24 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:24 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:24 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (442975ms till timeout)
2022-03-30 20:04:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (806859ms till timeout)
2022-03-30 20:04:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1247660ms till timeout)
2022-03-30 20:04:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:26 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:26 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (441871ms till timeout)
2022-03-30 20:04:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (449958ms till timeout)
2022-03-30 20:04:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (805852ms till timeout)
2022-03-30 20:04:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1246656ms till timeout)
2022-03-30 20:04:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:27 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (440751ms till timeout)
2022-03-30 20:04:27 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (804849ms till timeout)
2022-03-30 20:04:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1245650ms till timeout)
2022-03-30 20:04:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:28 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (439644ms till timeout)
2022-03-30 20:04:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (803844ms till timeout)
2022-03-30 20:04:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1244646ms till timeout)
2022-03-30 20:04:29 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:29 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:29 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:29 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (438541ms till timeout)
2022-03-30 20:04:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:04:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:04:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (802840ms till timeout)
2022-03-30 20:04:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1243642ms till timeout)
2022-03-30 20:04:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (437422ms till timeout)
2022-03-30 20:04:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (801833ms till timeout)
2022-03-30 20:04:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1242637ms till timeout)
2022-03-30 20:04:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-30 20:04:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 20:04:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 20:04:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-0" not found
2022-03-30 20:04:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 20:04:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[namespace-2], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-1]}
2022-03-30 20:04:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:267] testCruiseControlBasicAPIRequestsWithSecurityDisabled - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendSimpleMessageTls, testUpdateUser, testCruiseControlBasicAPIRequests, testReceiveSimpleMessageTls, testSendMessagesTlsScramSha] to and randomly select one to start execution
2022-03-30 20:04:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:93] [cruisecontrol.CruiseControlApiST] - Removing parallel test: testCruiseControlBasicAPIRequestsWithSecurityDisabled
2022-03-30 20:04:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:97] [cruisecontrol.CruiseControlApiST] - Parallel test count: 2
2022-03-30 20:04:31 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlApiST.testCruiseControlBasicAPIRequestsWithSecurityDisabled-FINISHED
2022-03-30 20:04:31 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 20:04:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (800829ms till timeout)
2022-03-30 20:04:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1241633ms till timeout)
2022-03-30 20:04:32 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (799825ms till timeout)
2022-03-30 20:04:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1240629ms till timeout)
2022-03-30 20:04:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (798822ms till timeout)
2022-03-30 20:04:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1239626ms till timeout)
2022-03-30 20:04:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:04:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:04:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (797818ms till timeout)
2022-03-30 20:04:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1238622ms till timeout)
2022-03-30 20:04:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (796815ms till timeout)
2022-03-30 20:04:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1237619ms till timeout)
2022-03-30 20:04:36 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-30 20:04:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaBridge:http-bridge-tls-cluster-name
2022-03-30 20:04:36 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-675329606-2074817820 in namespace http-bridge-tls-st
2022-03-30 20:04:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-675329606-2074817820
2022-03-30 20:04:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-675329606-2074817820 not ready, will try again in 10000 ms (179980ms till timeout)
2022-03-30 20:04:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (795811ms till timeout)
2022-03-30 20:04:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1236615ms till timeout)
2022-03-30 20:04:37 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (794808ms till timeout)
2022-03-30 20:04:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1235611ms till timeout)
2022-03-30 20:04:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (793804ms till timeout)
2022-03-30 20:04:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1234607ms till timeout)
2022-03-30 20:04:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:04:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:04:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (792800ms till timeout)
2022-03-30 20:04:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1233602ms till timeout)
2022-03-30 20:04:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (791796ms till timeout)
2022-03-30 20:04:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1232598ms till timeout)
2022-03-30 20:04:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (790793ms till timeout)
2022-03-30 20:04:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1231595ms till timeout)
2022-03-30 20:04:42 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (789788ms till timeout)
2022-03-30 20:04:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1230591ms till timeout)
2022-03-30 20:04:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (788785ms till timeout)
2022-03-30 20:04:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1229588ms till timeout)
2022-03-30 20:04:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:04:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:04:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (787782ms till timeout)
2022-03-30 20:04:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1228585ms till timeout)
2022-03-30 20:04:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (786778ms till timeout)
2022-03-30 20:04:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1227581ms till timeout)
2022-03-30 20:04:46 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-30 20:04:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:http-bridge-tls-cluster-name
2022-03-30 20:04:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:http-bridge-tls-cluster-name not ready, will try again in 10000 ms (839958ms till timeout)
2022-03-30 20:04:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (785775ms till timeout)
2022-03-30 20:04:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1226578ms till timeout)
2022-03-30 20:04:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (784772ms till timeout)
2022-03-30 20:04:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1225575ms till timeout)
2022-03-30 20:04:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (783768ms till timeout)
2022-03-30 20:04:48 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1224572ms till timeout)
2022-03-30 20:04:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:04:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:04:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (782760ms till timeout)
2022-03-30 20:04:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1223567ms till timeout)
2022-03-30 20:04:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (781755ms till timeout)
2022-03-30 20:04:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-09b59e48 will have desired state: Ready not ready, will try again in 1000 ms (1222564ms till timeout)
2022-03-30 20:04:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (780749ms till timeout)
2022-03-30 20:04:51 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-09b59e48 is in desired state: Ready
2022-03-30 20:04:51 [ForkJoinPool-3-worker-9] [32mINFO [m [CruiseControlApiST:48] ----> CRUISE CONTROL DEPLOYMENT STATE ENDPOINT <----
2022-03-30 20:04:51 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/state
2022-03-30 20:04:52 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/state
2022-03-30 20:04:52 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XGET --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/state
2022-03-30 20:04:52 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XGET --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/state
2022-03-30 20:04:52 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:52 [ForkJoinPool-3-worker-9] [32mINFO [m [CruiseControlApiST:58] Verifying that Cruise Control REST API is available
2022-03-30 20:04:52 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-30 20:04:52 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-30 20:04:52 [ForkJoinPool-3-worker-9] [32mINFO [m [CruiseControlApiST:66] ----> KAFKA REBALANCE <----
2022-03-30 20:04:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XGET --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:04:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (779739ms till timeout)
2022-03-30 20:04:52 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XGET --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:04:52 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:52 [ForkJoinPool-3-worker-9] [32mINFO [m [CruiseControlApiST:73] Waiting for CC will have for enough metrics to be recorded to make a proposal 
2022-03-30 20:04:52 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Wait for rebalance endpoint is ready
2022-03-30 20:04:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:04:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:04:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648670693439] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-30 20:04:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (599411ms till timeout)
2022-03-30 20:04:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (778736ms till timeout)
2022-03-30 20:04:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:04:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:04:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (777733ms till timeout)
2022-03-30 20:04:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (776729ms till timeout)
2022-03-30 20:04:56 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:04:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Namespace http-bridge-tls-st removal
2022-03-30 20:04:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:04:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:04:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (479922ms till timeout)
2022-03-30 20:04:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (775726ms till timeout)
2022-03-30 20:04:57 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:04:57 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:04:57 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:57 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (478828ms till timeout)
2022-03-30 20:04:57 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (774723ms till timeout)
2022-03-30 20:04:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:04:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:04:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (477757ms till timeout)
2022-03-30 20:04:58 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:04:58 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:04:58 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:58 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648670698690] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-30 20:04:58 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (594167ms till timeout)
2022-03-30 20:04:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (773719ms till timeout)
2022-03-30 20:04:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:04:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:04:59 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:04:59 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:04:59 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:04:59 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (476688ms till timeout)
2022-03-30 20:04:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (772716ms till timeout)
2022-03-30 20:05:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (475603ms till timeout)
2022-03-30 20:05:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (771713ms till timeout)
2022-03-30 20:05:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:01 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:01 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (474522ms till timeout)
2022-03-30 20:05:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (770709ms till timeout)
2022-03-30 20:05:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (769706ms till timeout)
2022-03-30 20:05:02 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:02 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (473453ms till timeout)
2022-03-30 20:05:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:05:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (768702ms till timeout)
2022-03-30 20:05:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (472373ms till timeout)
2022-03-30 20:05:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:05:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648670703949] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-30 20:05:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (588908ms till timeout)
2022-03-30 20:05:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:05:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:05:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (767699ms till timeout)
2022-03-30 20:05:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (471300ms till timeout)
2022-03-30 20:05:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (766695ms till timeout)
2022-03-30 20:05:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (470228ms till timeout)
2022-03-30 20:05:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (765692ms till timeout)
2022-03-30 20:05:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:07 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:07 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:07 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (469148ms till timeout)
2022-03-30 20:05:07 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (764689ms till timeout)
2022-03-30 20:05:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (468070ms till timeout)
2022-03-30 20:05:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-84882f67 will have desired state: Ready not ready, will try again in 1000 ms (763685ms till timeout)
2022-03-30 20:05:08 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:05:09 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:09 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:05:09 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:09 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648670709202] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-30 20:05:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (583653ms till timeout)
2022-03-30 20:05:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:09 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (466981ms till timeout)
2022-03-30 20:05:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:05:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:05:09 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-84882f67 is in desired state: Ready
2022-03-30 20:05:09 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-255809133-823300208 in namespace namespace-2
2022-03-30 20:05:09 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-2
2022-03-30 20:05:09 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-255809133-823300208
2022-03-30 20:05:09 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-255809133-823300208 will have desired state: Ready
2022-03-30 20:05:09 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-255809133-823300208 will have desired state: Ready
2022-03-30 20:05:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-255809133-823300208 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 20:05:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (465909ms till timeout)
2022-03-30 20:05:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-255809133-823300208 is in desired state: Ready
2022-03-30 20:05:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-586569987-2105688079 in namespace namespace-2
2022-03-30 20:05:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-2
2022-03-30 20:05:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-586569987-2105688079
2022-03-30 20:05:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-586569987-2105688079 will have desired state: Ready
2022-03-30 20:05:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-586569987-2105688079 will have desired state: Ready
2022-03-30 20:05:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-586569987-2105688079 will have desired state: Ready not ready, will try again in 1000 ms (179997ms till timeout)
2022-03-30 20:05:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (464828ms till timeout)
2022-03-30 20:05:11 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-586569987-2105688079 is in desired state: Ready
2022-03-30 20:05:11 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-84882f67-kafka-clients in namespace namespace-2
2022-03-30 20:05:11 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-2
2022-03-30 20:05:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-84882f67-kafka-clients
2022-03-30 20:05:11 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-84882f67-kafka-clients will be ready
2022-03-30 20:05:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-84882f67-kafka-clients will be ready
2022-03-30 20:05:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-84882f67-kafka-clients will be ready not ready, will try again in 1000 ms (479988ms till timeout)
2022-03-30 20:05:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (463744ms till timeout)
2022-03-30 20:05:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-84882f67-kafka-clients will be ready not ready, will try again in 1000 ms (478984ms till timeout)
2022-03-30 20:05:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (462667ms till timeout)
2022-03-30 20:05:13 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-84882f67-kafka-clients is ready
2022-03-30 20:05:13 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-30 20:05:13 [ForkJoinPool-3-worker-13] [32mINFO [m [ListenersST:370] Checking produced and consumed messages to pod:my-cluster-84882f67-kafka-clients-779d6669b4-t2lrx
2022-03-30 20:05:13 [ForkJoinPool-3-worker-13] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@53d23e9c, which are set.
2022-03-30 20:05:13 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@517a0c85, messages=[], arguments=[USER=my_user_586569987_2105688079, --topic, my-topic-255809133-823300208, --bootstrap-server, my-cluster-84882f67-kafka-bootstrap.namespace-2.svc:9096, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-84882f67-kafka-clients-779d6669b4-t2lrx', podNamespace='namespace-2', bootstrapServer='my-cluster-84882f67-kafka-bootstrap.namespace-2.svc:9096', topicName='my-topic-255809133-823300208', maxMessages=100, kafkaUsername='my-user-586569987-2105688079', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@53d23e9c}
2022-03-30 20:05:13 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-84882f67-kafka-bootstrap.namespace-2.svc:9096:my-topic-255809133-823300208 from pod my-cluster-84882f67-kafka-clients-779d6669b4-t2lrx
2022-03-30 20:05:13 [ForkJoinPool-3-worker-13] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-84882f67-kafka-clients-779d6669b4-t2lrx -n namespace-2 -- /opt/kafka/producer.sh USER=my_user_586569987_2105688079 --topic my-topic-255809133-823300208 --bootstrap-server my-cluster-84882f67-kafka-bootstrap.namespace-2.svc:9096 --max-messages 100
2022-03-30 20:05:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-84882f67-kafka-clients-779d6669b4-t2lrx -n namespace-2 -- /opt/kafka/producer.sh USER=my_user_586569987_2105688079 --topic my-topic-255809133-823300208 --bootstrap-server my-cluster-84882f67-kafka-bootstrap.namespace-2.svc:9096 --max-messages 100
2022-03-30 20:05:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:05:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:05:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:05:14 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:05:14 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:14 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648670714461] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-30 20:05:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (578393ms till timeout)
[[1;34mINFO[m] Running io.strimzi.systemtest.security.SecurityST
2022-03-30 20:05:14 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 20:05:14 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:667] [security.SecurityST - Before All] - Setup test suite environment
2022-03-30 20:05:14 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:69] [security.SecurityST] - Adding parallel suite: SecurityST
2022-03-30 20:05:14 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:73] [security.SecurityST] - Parallel suites count: 5
2022-03-30 20:05:14 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:159] [SecurityST] moved to the WaitZone, because current thread exceed maximum of allowed test suites in parallel. (5/4)
2022-03-30 20:05:14 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:164] SecurityST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (5/4)
2022-03-30 20:05:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (461593ms till timeout)
2022-03-30 20:05:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (460524ms till timeout)
2022-03-30 20:05:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (459448ms till timeout)
2022-03-30 20:05:17 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-30 20:05:17 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-03-30 20:05:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@39b12923, which are set.
2022-03-30 20:05:17 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@5e4e8fcc, messages=[], arguments=[USER=my_user_586569987_2105688079, --group-id, my-consumer-group-1987081524, --topic, my-topic-255809133-823300208, --group-instance-id, instance548884799, --bootstrap-server, my-cluster-84882f67-kafka-bootstrap.namespace-2.svc:9096, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-84882f67-kafka-clients-779d6669b4-t2lrx', podNamespace='namespace-2', bootstrapServer='my-cluster-84882f67-kafka-bootstrap.namespace-2.svc:9096', topicName='my-topic-255809133-823300208', maxMessages=100, kafkaUsername='my-user-586569987-2105688079', consumerGroupName='my-consumer-group-1987081524', consumerInstanceId='instance548884799', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@39b12923}
2022-03-30 20:05:17 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-84882f67-kafka-bootstrap.namespace-2.svc:9096:my-topic-255809133-823300208 from pod my-cluster-84882f67-kafka-clients-779d6669b4-t2lrx
2022-03-30 20:05:17 [ForkJoinPool-3-worker-13] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-84882f67-kafka-clients-779d6669b4-t2lrx -n namespace-2 -- /opt/kafka/consumer.sh USER=my_user_586569987_2105688079 --group-id my-consumer-group-1987081524 --topic my-topic-255809133-823300208 --group-instance-id instance548884799 --bootstrap-server my-cluster-84882f67-kafka-bootstrap.namespace-2.svc:9096 --max-messages 100
2022-03-30 20:05:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-84882f67-kafka-clients-779d6669b4-t2lrx -n namespace-2 -- /opt/kafka/consumer.sh USER=my_user_586569987_2105688079 --group-id my-consumer-group-1987081524 --topic my-topic-255809133-823300208 --group-instance-id instance548884799 --bootstrap-server my-cluster-84882f67-kafka-bootstrap.namespace-2.svc:9096 --max-messages 100
2022-03-30 20:05:17 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:17 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (458374ms till timeout)
2022-03-30 20:05:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (457302ms till timeout)
2022-03-30 20:05:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:05:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:184] SecurityST suite now can proceed its execution
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestSuiteNamespaceManager:129] Test suite `SecurityST` creates these additional namespaces:[security-st]
2022-03-30 20:05:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: security-st
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Namespace security-st
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace security-st -o json
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace security-st -o json
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T20:05:19Z",
        "name": "security-st",
        "resourceVersion": "83795",
        "selfLink": "/api/v1/namespaces/security-st",
        "uid": "41625c6e-4e37-421f-95f0-abd717c3edb3"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[namespace-2], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-1], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: security-st
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=security-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: security-st
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testAutoRenewAllCaCertsTriggeredByAnno-STARTED
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:659] [security.SecurityST - Before Each] - Setup test case environment
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:77] [security.SecurityST] - Adding parallel test: testAutoRenewAllCaCertsTriggeredByAnno
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:81] [security.SecurityST] - Parallel test count: 3
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:230] testAutoRenewAllCaCertsTriggeredByAnno test now can proceed its execution
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba, testSendSimpleMessageTls=my-cluster-ca7325d7, testUpdateUser=my-cluster-6e6fa8fe, testSendMessagesTlsScramSha=my-cluster-84882f67, testReceiveSimpleMessageTls=my-cluster-2e450144, testCruiseControlBasicAPIRequests=my-cluster-8aed4745, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140}
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testAutoRenewAllCaCertsTriggeredByAnno=my-user-1731919654-539989781, testSendSimpleMessageTls=my-user-1309491966-2015599610, testUpdateUser=my-user-1387197422-925410353, testSendMessagesTlsScramSha=my-user-586569987-2105688079, testReceiveSimpleMessageTls=my-user-1669668617-1031881775, testCruiseControlBasicAPIRequests=my-user-1861484466-561878159, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677}
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1672280868-1119644965, testSendSimpleMessageTls=my-topic-435427300-917779110, testUpdateUser=my-topic-1875783706-1763804507, testSendMessagesTlsScramSha=my-topic-255809133-823300208, testReceiveSimpleMessageTls=my-topic-368564676-637091599, testCruiseControlBasicAPIRequests=my-topic-1631570507-1308241024, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206}
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba-kafka-clients, testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testUpdateUser=my-cluster-6e6fa8fe-kafka-clients, testSendMessagesTlsScramSha=my-cluster-84882f67-kafka-clients, testReceiveSimpleMessageTls=my-cluster-2e450144-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-8aed4745-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients}
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-3 for test case:testAutoRenewAllCaCertsTriggeredByAnno
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-3
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-3
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace security-st get Namespace namespace-3 -o json
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace security-st get Namespace namespace-3 -o json
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T20:05:19Z",
        "name": "namespace-3",
        "resourceVersion": "83799",
        "selfLink": "/api/v1/namespaces/namespace-3",
        "uid": "d8b5f9f1-0dfa-4ff4-afc5-abf879f88ccc"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[namespace-2], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-3], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-1], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-3
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-3, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-3
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:596] Creating a cluster
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-cae962ba in namespace namespace-3
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-3
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-cae962ba
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-cae962ba will have desired state: Ready
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-cae962ba will have desired state: Ready
2022-03-30 20:05:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1799997ms till timeout)
2022-03-30 20:05:19 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:05:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648670719969] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-30 20:05:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (572872ms till timeout)
2022-03-30 20:05:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (456204ms till timeout)
2022-03-30 20:05:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1798993ms till timeout)
2022-03-30 20:05:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (455086ms till timeout)
2022-03-30 20:05:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1797990ms till timeout)
2022-03-30 20:05:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (453998ms till timeout)
2022-03-30 20:05:22 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1796986ms till timeout)
2022-03-30 20:05:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-2 get Namespace http-bridge-tls-st -o yaml
2022-03-30 20:05:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 20:05:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 20:05:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "http-bridge-tls-st" not found
2022-03-30 20:05:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 20:05:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[namespace-2], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-3], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-1], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-30 20:05:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:254] HttpBridgeTlsST - Notifies waiting test suites:[HttpBridgeTlsST, CruiseControlApiST, UserST, CruiseControlST, ListenersST] to and randomly select one to start execution
2022-03-30 20:05:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:85] [bridge.HttpBridgeTlsST] - Removing parallel suite: HttpBridgeTlsST
2022-03-30 20:05:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:89] [bridge.HttpBridgeTlsST] - Parallel suites count: 4
[[1;34mINFO[m] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 313.93 s - in io.strimzi.systemtest.bridge.HttpBridgeTlsST
[[1;34mINFO[m] Running io.strimzi.systemtest.rollingupdate.RollingUpdateST
2022-03-30 20:05:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 20:05:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:667] [rollingupdate.RollingUpdateST - Before All] - Setup test suite environment
2022-03-30 20:05:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:69] [rollingupdate.RollingUpdateST] - Adding parallel suite: RollingUpdateST
2022-03-30 20:05:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:73] [rollingupdate.RollingUpdateST] - Parallel suites count: 5
2022-03-30 20:05:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:159] [RollingUpdateST] moved to the WaitZone, because current thread exceed maximum of allowed test suites in parallel. (5/4)
2022-03-30 20:05:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:164] RollingUpdateST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (5/4)
2022-03-30 20:05:23 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1795982ms till timeout)
2022-03-30 20:05:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:05:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 20:05:24 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1794977ms till timeout)
2022-03-30 20:05:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:05:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:05:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648670725268] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-30 20:05:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (567585ms till timeout)
2022-03-30 20:05:25 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 20:05:25 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-03-30 20:05:25 [ForkJoinPool-3-worker-13] [32mINFO [m [ListenersST:377] Checking if generated password has 25 characters
2022-03-30 20:05:25 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 20:05:25 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:675] [kafka.listeners.ListenersST - After Each] - Clean up after test
2022-03-30 20:05:25 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:05:25 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testSendMessagesTlsScramSha
2022-03-30 20:05:25 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-586569987-2105688079 in namespace namespace-2
2022-03-30 20:05:25 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-586569987-2105688079
2022-03-30 20:05:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-586569987-2105688079 not ready, will try again in 10000 ms (179983ms till timeout)
2022-03-30 20:05:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1793967ms till timeout)
2022-03-30 20:05:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1792963ms till timeout)
2022-03-30 20:05:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1791959ms till timeout)
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:184] RollingUpdateST suite now can proceed its execution
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestSuiteNamespaceManager:129] Test suite `RollingUpdateST` creates these additional namespaces:[rolling-update-st]
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: rolling-update-st
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Namespace rolling-update-st
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-3 get Namespace rolling-update-st -o json
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-3 get Namespace rolling-update-st -o json
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T20:05:28Z",
        "name": "rolling-update-st",
        "resourceVersion": "83903",
        "selfLink": "/api/v1/namespaces/rolling-update-st",
        "uid": "17ad4a07-7b4e-4efa-ad34-d779744e8fea"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[namespace-2], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@dce7372d=[rolling-update-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-3], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-1], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: rolling-update-st
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=rolling-update-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: rolling-update-st
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testKafkaAndZookeeperScaleUpScaleDown-STARTED
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:659] [rollingupdate.RollingUpdateST - Before Each] - Setup test case environment
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:77] [rollingupdate.RollingUpdateST] - Adding parallel test: testKafkaAndZookeeperScaleUpScaleDown
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:81] [rollingupdate.RollingUpdateST] - Parallel test count: 4
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:230] testKafkaAndZookeeperScaleUpScaleDown test now can proceed its execution
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba, testSendSimpleMessageTls=my-cluster-ca7325d7, testUpdateUser=my-cluster-6e6fa8fe, testSendMessagesTlsScramSha=my-cluster-84882f67, testReceiveSimpleMessageTls=my-cluster-2e450144, testCruiseControlBasicAPIRequests=my-cluster-8aed4745, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140}
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-838849261-890460734, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1731919654-539989781, testSendSimpleMessageTls=my-user-1309491966-2015599610, testUpdateUser=my-user-1387197422-925410353, testSendMessagesTlsScramSha=my-user-586569987-2105688079, testReceiveSimpleMessageTls=my-user-1669668617-1031881775, testCruiseControlBasicAPIRequests=my-user-1861484466-561878159, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677}
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-2138084746-704381117, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1672280868-1119644965, testSendSimpleMessageTls=my-topic-435427300-917779110, testUpdateUser=my-topic-1875783706-1763804507, testSendMessagesTlsScramSha=my-topic-255809133-823300208, testReceiveSimpleMessageTls=my-topic-368564676-637091599, testCruiseControlBasicAPIRequests=my-topic-1631570507-1308241024, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206}
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba-kafka-clients, testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testUpdateUser=my-cluster-6e6fa8fe-kafka-clients, testSendMessagesTlsScramSha=my-cluster-84882f67-kafka-clients, testReceiveSimpleMessageTls=my-cluster-2e450144-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-8aed4745-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients}
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-4 for test case:testKafkaAndZookeeperScaleUpScaleDown
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-4
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-4
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace rolling-update-st get Namespace namespace-4 -o json
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace rolling-update-st get Namespace namespace-4 -o json
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T20:05:28Z",
        "name": "namespace-4",
        "resourceVersion": "83907",
        "selfLink": "/api/v1/namespaces/namespace-4",
        "uid": "eaf62fb6-3dc2-4ad4-9780-711791ce6906"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[namespace-2], io.strimzi.test.logs.CollectorElement@2c38455d=[namespace-4], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@dce7372d=[rolling-update-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-3], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-1], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-4
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-4, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-4
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-2a79abff in namespace namespace-4
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-2a79abff
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-2a79abff will have desired state: Ready
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-2a79abff will have desired state: Ready
2022-03-30 20:05:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (839997ms till timeout)
2022-03-30 20:05:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1790955ms till timeout)
2022-03-30 20:05:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:05:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 20:05:29 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (838993ms till timeout)
2022-03-30 20:05:29 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1789951ms till timeout)
2022-03-30 20:05:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:05:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:05:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648670730522] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-30 20:05:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (562331ms till timeout)
2022-03-30 20:05:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (837988ms till timeout)
2022-03-30 20:05:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1788947ms till timeout)
2022-03-30 20:05:31 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (836985ms till timeout)
2022-03-30 20:05:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1787944ms till timeout)
2022-03-30 20:05:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (835981ms till timeout)
2022-03-30 20:05:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1786940ms till timeout)
2022-03-30 20:05:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (834978ms till timeout)
2022-03-30 20:05:33 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1785935ms till timeout)
2022-03-30 20:05:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:05:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 20:05:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (833973ms till timeout)
2022-03-30 20:05:34 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1784931ms till timeout)
2022-03-30 20:05:35 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-84882f67-kafka-clients in namespace namespace-2
2022-03-30 20:05:35 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-84882f67-kafka-clients
2022-03-30 20:05:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-84882f67-kafka-clients not ready, will try again in 10000 ms (479984ms till timeout)
2022-03-30 20:05:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:05:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (832960ms till timeout)
2022-03-30 20:05:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1783927ms till timeout)
2022-03-30 20:05:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:05:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There is no window available in range [-1, 1648670735863] (index [1, -1]). Window index (current: 0, oldest: 0).'.

2022-03-30 20:05:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (556992ms till timeout)
2022-03-30 20:05:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (831955ms till timeout)
2022-03-30 20:05:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1782923ms till timeout)
2022-03-30 20:05:37 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (830950ms till timeout)
2022-03-30 20:05:37 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1781920ms till timeout)
2022-03-30 20:05:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (829947ms till timeout)
2022-03-30 20:05:38 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1780916ms till timeout)
2022-03-30 20:05:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:05:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 20:05:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (828943ms till timeout)
2022-03-30 20:05:39 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1779913ms till timeout)
2022-03-30 20:05:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (827934ms till timeout)
2022-03-30 20:05:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1778904ms till timeout)
2022-03-30 20:05:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:05:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:05:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648670741265] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 20:05:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (551592ms till timeout)
2022-03-30 20:05:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (826930ms till timeout)
2022-03-30 20:05:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1777901ms till timeout)
2022-03-30 20:05:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (825926ms till timeout)
2022-03-30 20:05:42 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1776897ms till timeout)
2022-03-30 20:05:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (824923ms till timeout)
2022-03-30 20:05:43 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1775894ms till timeout)
2022-03-30 20:05:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:05:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 20:05:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (823920ms till timeout)
2022-03-30 20:05:44 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1774890ms till timeout)
2022-03-30 20:05:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-84882f67-kafka-clients not ready, will try again in 10000 ms (469973ms till timeout)
2022-03-30 20:05:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (822916ms till timeout)
2022-03-30 20:05:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1773887ms till timeout)
2022-03-30 20:05:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:05:46 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:05:46 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:46 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648670746502] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 20:05:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (546352ms till timeout)
2022-03-30 20:05:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (821913ms till timeout)
2022-03-30 20:05:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1772883ms till timeout)
2022-03-30 20:05:47 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (820909ms till timeout)
2022-03-30 20:05:47 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1771880ms till timeout)
2022-03-30 20:05:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (819906ms till timeout)
2022-03-30 20:05:48 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1770876ms till timeout)
2022-03-30 20:05:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:05:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 20:05:49 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (818902ms till timeout)
2022-03-30 20:05:49 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1769873ms till timeout)
2022-03-30 20:05:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (817898ms till timeout)
2022-03-30 20:05:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1768869ms till timeout)
2022-03-30 20:05:51 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:05:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (816895ms till timeout)
2022-03-30 20:05:51 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:05:51 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:51 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648670751766] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 20:05:51 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (541090ms till timeout)
2022-03-30 20:05:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1767864ms till timeout)
2022-03-30 20:05:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (815891ms till timeout)
2022-03-30 20:05:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1766859ms till timeout)
2022-03-30 20:05:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (814887ms till timeout)
2022-03-30 20:05:53 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1765856ms till timeout)
2022-03-30 20:05:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:05:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 20:05:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (813883ms till timeout)
2022-03-30 20:05:54 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1764852ms till timeout)
2022-03-30 20:05:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-84882f67-kafka-clients not ready, will try again in 10000 ms (459961ms till timeout)
2022-03-30 20:05:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (812879ms till timeout)
2022-03-30 20:05:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1763848ms till timeout)
2022-03-30 20:05:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (811875ms till timeout)
2022-03-30 20:05:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:05:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1762843ms till timeout)
2022-03-30 20:05:57 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:05:57 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:05:57 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648670757051] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 20:05:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (535791ms till timeout)
2022-03-30 20:05:57 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (810870ms till timeout)
2022-03-30 20:05:57 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1761837ms till timeout)
2022-03-30 20:05:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (809865ms till timeout)
2022-03-30 20:05:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1760832ms till timeout)
2022-03-30 20:05:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:05:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 20:05:59 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (808860ms till timeout)
2022-03-30 20:05:59 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1759828ms till timeout)
2022-03-30 20:06:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (807854ms till timeout)
2022-03-30 20:06:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1758823ms till timeout)
2022-03-30 20:06:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (806846ms till timeout)
2022-03-30 20:06:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1757819ms till timeout)
2022-03-30 20:06:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:06:02 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:06:02 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:02 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648670762464] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 20:06:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (530369ms till timeout)
2022-03-30 20:06:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (805836ms till timeout)
2022-03-30 20:06:02 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1756815ms till timeout)
2022-03-30 20:06:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (804832ms till timeout)
2022-03-30 20:06:03 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1755812ms till timeout)
2022-03-30 20:06:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:06:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 20:06:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (803825ms till timeout)
2022-03-30 20:06:04 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1754807ms till timeout)
2022-03-30 20:06:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-84882f67-kafka-clients not ready, will try again in 10000 ms (449950ms till timeout)
2022-03-30 20:06:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (802821ms till timeout)
2022-03-30 20:06:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1753803ms till timeout)
2022-03-30 20:06:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (801817ms till timeout)
2022-03-30 20:06:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1752798ms till timeout)
2022-03-30 20:06:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:06:07 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (800812ms till timeout)
2022-03-30 20:06:07 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:06:07 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:07 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648670767838] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 20:06:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (525012ms till timeout)
2022-03-30 20:06:07 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1751793ms till timeout)
2022-03-30 20:06:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (799806ms till timeout)
2022-03-30 20:06:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1750788ms till timeout)
2022-03-30 20:06:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:06:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 20:06:09 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (798802ms till timeout)
2022-03-30 20:06:09 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1749784ms till timeout)
2022-03-30 20:06:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (797798ms till timeout)
2022-03-30 20:06:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1748781ms till timeout)
2022-03-30 20:06:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (796794ms till timeout)
2022-03-30 20:06:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1747777ms till timeout)
2022-03-30 20:06:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (795790ms till timeout)
2022-03-30 20:06:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:06:12 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1746774ms till timeout)
2022-03-30 20:06:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:06:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648670773117] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 20:06:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (519740ms till timeout)
2022-03-30 20:06:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (794787ms till timeout)
2022-03-30 20:06:13 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1745771ms till timeout)
2022-03-30 20:06:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:06:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 20:06:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (793783ms till timeout)
2022-03-30 20:06:14 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1744767ms till timeout)
2022-03-30 20:06:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-255809133-823300208 in namespace namespace-2
2022-03-30 20:06:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-255809133-823300208
2022-03-30 20:06:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-255809133-823300208 not ready, will try again in 10000 ms (179992ms till timeout)
2022-03-30 20:06:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (792779ms till timeout)
2022-03-30 20:06:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1743764ms till timeout)
2022-03-30 20:06:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (791775ms till timeout)
2022-03-30 20:06:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1742759ms till timeout)
2022-03-30 20:06:17 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (790771ms till timeout)
2022-03-30 20:06:17 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1741755ms till timeout)
2022-03-30 20:06:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:06:18 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:06:18 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:18 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648670778375] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 20:06:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (514474ms till timeout)
2022-03-30 20:06:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (789767ms till timeout)
2022-03-30 20:06:18 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1740751ms till timeout)
2022-03-30 20:06:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:06:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 20:06:19 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (788763ms till timeout)
2022-03-30 20:06:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1739747ms till timeout)
2022-03-30 20:06:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (787760ms till timeout)
2022-03-30 20:06:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1738742ms till timeout)
2022-03-30 20:06:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (786755ms till timeout)
2022-03-30 20:06:22 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1737738ms till timeout)
2022-03-30 20:06:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (785751ms till timeout)
2022-03-30 20:06:23 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1736735ms till timeout)
2022-03-30 20:06:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:06:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:06:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648670783637] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 20:06:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (509218ms till timeout)
2022-03-30 20:06:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (784748ms till timeout)
2022-03-30 20:06:24 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1735731ms till timeout)
2022-03-30 20:06:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:06:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 20:06:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (783744ms till timeout)
2022-03-30 20:06:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1734728ms till timeout)
2022-03-30 20:06:25 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-84882f67 in namespace namespace-2
2022-03-30 20:06:25 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-84882f67
2022-03-30 20:06:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-84882f67 not ready, will try again in 10000 ms (839996ms till timeout)
2022-03-30 20:06:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (782739ms till timeout)
2022-03-30 20:06:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1733725ms till timeout)
2022-03-30 20:06:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (781735ms till timeout)
2022-03-30 20:06:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1732719ms till timeout)
2022-03-30 20:06:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (780732ms till timeout)
2022-03-30 20:06:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1731716ms till timeout)
2022-03-30 20:06:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:06:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (779728ms till timeout)
2022-03-30 20:06:28 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:06:28 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:28 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648670788951] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 20:06:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (503896ms till timeout)
2022-03-30 20:06:29 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1730712ms till timeout)
2022-03-30 20:06:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:06:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 20:06:29 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (778723ms till timeout)
2022-03-30 20:06:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1729708ms till timeout)
2022-03-30 20:06:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (777720ms till timeout)
2022-03-30 20:06:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1728704ms till timeout)
2022-03-30 20:06:31 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (776710ms till timeout)
2022-03-30 20:06:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1727701ms till timeout)
2022-03-30 20:06:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (775704ms till timeout)
2022-03-30 20:06:33 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1726697ms till timeout)
2022-03-30 20:06:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (774699ms till timeout)
2022-03-30 20:06:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:06:34 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1725693ms till timeout)
2022-03-30 20:06:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:06:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response Error processing POST request '/rebalance' due to: 'com.linkedin.kafka.cruisecontrol.exception.KafkaCruiseControlException: com.linkedin.cruisecontrol.exception.NotEnoughValidWindowsException: There are only 0 valid windows when aggregating in range [-1, 1648670794269] for aggregation options (minValidEntityRatio=1.00, minValidEntityGroupRatio=0.00, minValidWindows=1, numEntitiesToInclude=117, granularity=ENTITY)'.

2022-03-30 20:06:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for rebalance endpoint is ready not ready, will try again in 5000 ms (498578ms till timeout)
2022-03-30 20:06:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:06:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 20:06:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (773693ms till timeout)
2022-03-30 20:06:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1724687ms till timeout)
2022-03-30 20:06:35 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:06:35 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-2 for test case:testSendMessagesTlsScramSha
2022-03-30 20:06:35 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-2 removal
2022-03-30 20:06:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:35 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:35 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (479924ms till timeout)
2022-03-30 20:06:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (772689ms till timeout)
2022-03-30 20:06:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1723684ms till timeout)
2022-03-30 20:06:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:36 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:36 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (478849ms till timeout)
2022-03-30 20:06:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (771686ms till timeout)
2022-03-30 20:06:37 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1722680ms till timeout)
2022-03-30 20:06:37 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:37 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:37 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:37 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (477779ms till timeout)
2022-03-30 20:06:37 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (770681ms till timeout)
2022-03-30 20:06:38 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1721677ms till timeout)
2022-03-30 20:06:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:38 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:38 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (476691ms till timeout)
2022-03-30 20:06:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (769676ms till timeout)
2022-03-30 20:06:39 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1720672ms till timeout)
2022-03-30 20:06:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:06:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:06:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 20:06:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:39 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:39 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (475584ms till timeout)
2022-03-30 20:06:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (768670ms till timeout)
2022-03-30 20:06:39 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:06:39 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:39 [ForkJoinPool-3-worker-9] [36mDEBUG[m [CruiseControlUtils:175] API response 

Optimization has 0 inter-broker replica(0 MB) moves, 0 intra-broker replica(0 MB) moves and 5 leadership moves with a cluster model of 1 recent windows and 100.000% of the partitions covered.
Excluded Topics: [].
Excluded Brokers For Leadership: [].
Excluded Brokers For Replica Move: [].
Counts: 3 brokers 287 replicas 6 topics.
On-demand Balancedness Score Before (90.209) After(90.209).
Provision Status: RIGHT_SIZED.

[    25 ms] Stats for RackAwareGoal(NO-ACTION):
AVG:{cpu:       0.020 networkInbound:       0.604 networkOutbound:       0.465 disk:       0.005 potentialNwOut:       1.395 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       0.046 networkInbound:       0.604 networkOutbound:       1.223 disk:       0.005 potentialNwOut:       1.395 replicas:96 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.007 networkInbound:       0.604 networkOutbound:       0.000 disk:       0.005 potentialNwOut:       1.395 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       0.019 networkInbound:       0.000 networkOutbound:       0.541 disk:       0.000 potentialNwOut:       0.000 replicas:0.4714045207910317 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[     1 ms] Stats for MinTopicLeadersPerBrokerGoal(NO-ACTION):
AVG:{cpu:       0.020 networkInbound:       0.604 networkOutbound:       0.465 disk:       0.005 potentialNwOut:       1.395 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       0.046 networkInbound:       0.604 networkOutbound:       1.223 disk:       0.005 potentialNwOut:       1.395 replicas:96 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.007 networkInbound:       0.604 networkOutbound:       0.000 disk:       0.005 potentialNwOut:       1.395 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       0.019 networkInbound:       0.000 networkOutbound:       0.541 disk:       0.000 potentialNwOut:       0.000 replicas:0.4714045207910317 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[     3 ms] Stats for ReplicaCapacityGoal(NO-ACTION):
AVG:{cpu:       0.020 networkInbound:       0.604 networkOutbound:       0.465 disk:       0.005 potentialNwOut:       1.395 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       0.046 networkInbound:       0.604 networkOutbound:       1.223 disk:       0.005 potentialNwOut:       1.395 replicas:96 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.007 networkInbound:       0.604 networkOutbound:       0.000 disk:       0.005 potentialNwOut:       1.395 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       0.019 networkInbound:       0.000 networkOutbound:       0.541 disk:       0.000 potentialNwOut:       0.000 replicas:0.4714045207910317 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[     1 ms] Stats for DiskCapacityGoal(NO-ACTION):
AVG:{cpu:       0.020 networkInbound:       0.604 networkOutbound:       0.465 disk:       0.005 potentialNwOut:       1.395 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       0.046 networkInbound:       0.604 networkOutbound:       1.223 disk:       0.005 potentialNwOut:       1.395 replicas:96 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.007 networkInbound:       0.604 networkOutbound:       0.000 disk:       0.005 potentialNwOut:       1.395 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       0.019 networkInbound:       0.000 networkOutbound:       0.541 disk:       0.000 potentialNwOut:       0.000 replicas:0.4714045207910317 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[     1 ms] Stats for NetworkInboundCapacityGoal(NO-ACTION):
AVG:{cpu:       0.020 networkInbound:       0.604 networkOutbound:       0.465 disk:       0.005 potentialNwOut:       1.395 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       0.046 networkInbound:       0.604 networkOutbound:       1.223 disk:       0.005 potentialNwOut:       1.395 replicas:96 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.007 networkInbound:       0.604 networkOutbound:       0.000 disk:       0.005 potentialNwOut:       1.395 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       0.019 networkInbound:       0.000 networkOutbound:       0.541 disk:       0.000 potentialNwOut:       0.000 replicas:0.4714045207910317 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[     1 ms] Stats for NetworkOutboundCapacityGoal(NO-ACTION):
AVG:{cpu:       0.020 networkInbound:       0.604 networkOutbound:       0.465 disk:       0.005 potentialNwOut:       1.395 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       0.046 networkInbound:       0.604 networkOutbound:       1.223 disk:       0.005 potentialNwOut:       1.395 replicas:96 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.007 networkInbound:       0.604 networkOutbound:       0.000 disk:       0.005 potentialNwOut:       1.395 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       0.019 networkInbound:       0.000 networkOutbound:       0.541 disk:       0.000 potentialNwOut:       0.000 replicas:0.4714045207910317 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[     1 ms] Stats for CpuCapacityGoal(NO-ACTION):
AVG:{cpu:       0.020 networkInbound:       0.604 networkOutbound:       0.465 disk:       0.005 potentialNwOut:       1.395 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       0.046 networkInbound:       0.604 networkOutbound:       1.223 disk:       0.005 potentialNwOut:       1.395 replicas:96 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.007 networkInbound:       0.604 networkOutbound:       0.000 disk:       0.005 potentialNwOut:       1.395 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       0.019 networkInbound:       0.000 networkOutbound:       0.541 disk:       0.000 potentialNwOut:       0.000 replicas:0.4714045207910317 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[     4 ms] Stats for ReplicaDistributionGoal(NO-ACTION):
AVG:{cpu:       0.020 networkInbound:       0.604 networkOutbound:       0.465 disk:       0.005 potentialNwOut:       1.395 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       0.046 networkInbound:       0.604 networkOutbound:       1.223 disk:       0.005 potentialNwOut:       1.395 replicas:96 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.007 networkInbound:       0.604 networkOutbound:       0.000 disk:       0.005 potentialNwOut:       1.395 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       0.019 networkInbound:       0.000 networkOutbound:       0.541 disk:       0.000 potentialNwOut:       0.000 replicas:0.4714045207910317 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[     2 ms] Stats for PotentialNwOutGoal(NO-ACTION):
AVG:{cpu:       0.020 networkInbound:       0.604 networkOutbound:       0.465 disk:       0.005 potentialNwOut:       1.395 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       0.046 networkInbound:       0.604 networkOutbound:       1.223 disk:       0.005 potentialNwOut:       1.395 replicas:96 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.007 networkInbound:       0.604 networkOutbound:       0.000 disk:       0.005 potentialNwOut:       1.395 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       0.019 networkInbound:       0.000 networkOutbound:       0.541 disk:       0.000 potentialNwOut:       0.000 replicas:0.4714045207910317 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[     2 ms] Stats for DiskUsageDistributionGoal(NO-ACTION):
AVG:{cpu:       0.020 networkInbound:       0.604 networkOutbound:       0.465 disk:       0.005 potentialNwOut:       1.395 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       0.046 networkInbound:       0.604 networkOutbound:       1.223 disk:       0.005 potentialNwOut:       1.395 replicas:96 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.007 networkInbound:       0.604 networkOutbound:       0.000 disk:       0.005 potentialNwOut:       1.395 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       0.019 networkInbound:       0.000 networkOutbound:       0.541 disk:       0.000 potentialNwOut:       0.000 replicas:0.4714045207910317 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[     0 ms] Stats for NetworkInboundUsageDistributionGoal(NO-ACTION):
AVG:{cpu:       0.020 networkInbound:       0.604 networkOutbound:       0.465 disk:       0.005 potentialNwOut:       1.395 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       0.046 networkInbound:       0.604 networkOutbound:       1.223 disk:       0.005 potentialNwOut:       1.395 replicas:96 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.007 networkInbound:       0.604 networkOutbound:       0.000 disk:       0.005 potentialNwOut:       1.395 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       0.019 networkInbound:       0.000 networkOutbound:       0.541 disk:       0.000 potentialNwOut:       0.000 replicas:0.4714045207910317 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[    25 ms] Stats for NetworkOutboundUsageDistributionGoal(VIOLATED):
AVG:{cpu:       0.020 networkInbound:       0.604 networkOutbound:       0.465 disk:       0.005 potentialNwOut:       1.395 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       0.046 networkInbound:       0.604 networkOutbound:       1.223 disk:       0.005 potentialNwOut:       1.395 replicas:96 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.007 networkInbound:       0.604 networkOutbound:       0.000 disk:       0.005 potentialNwOut:       1.395 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       0.019 networkInbound:       0.000 networkOutbound:       0.541 disk:       0.000 potentialNwOut:       0.000 replicas:0.4714045207910317 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[     5 ms] Stats for CpuUsageDistributionGoal(VIOLATED):
AVG:{cpu:       0.020 networkInbound:       0.604 networkOutbound:       0.465 disk:       0.005 potentialNwOut:       1.395 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       0.046 networkInbound:       0.604 networkOutbound:       1.223 disk:       0.005 potentialNwOut:       1.395 replicas:96 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.007 networkInbound:       0.604 networkOutbound:       0.000 disk:       0.005 potentialNwOut:       1.395 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       0.019 networkInbound:       0.000 networkOutbound:       0.541 disk:       0.000 potentialNwOut:       0.000 replicas:0.4714045207910317 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[     1 ms] Stats for TopicReplicaDistributionGoal(NO-ACTION):
AVG:{cpu:       0.020 networkInbound:       0.604 networkOutbound:       0.465 disk:       0.005 potentialNwOut:       1.395 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       0.046 networkInbound:       0.604 networkOutbound:       1.223 disk:       0.005 potentialNwOut:       1.395 replicas:96 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.007 networkInbound:       0.604 networkOutbound:       0.000 disk:       0.005 potentialNwOut:       1.395 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       0.019 networkInbound:       0.000 networkOutbound:       0.541 disk:       0.000 potentialNwOut:       0.000 replicas:0.4714045207910317 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[     1 ms] Stats for LeaderReplicaDistributionGoal(NO-ACTION):
AVG:{cpu:       0.020 networkInbound:       0.604 networkOutbound:       0.465 disk:       0.005 potentialNwOut:       1.395 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       0.046 networkInbound:       0.604 networkOutbound:       1.223 disk:       0.005 potentialNwOut:       1.395 replicas:96 leaderReplicas:40 topicReplicas:50}
MIN:{cpu:       0.007 networkInbound:       0.604 networkOutbound:       0.000 disk:       0.005 potentialNwOut:       1.395 replicas:95 leaderReplicas:38 topicReplicas:1}
STD:{cpu:       0.019 networkInbound:       0.000 networkOutbound:       0.541 disk:       0.000 potentialNwOut:       0.000 replicas:0.4714045207910317 leaderReplicas:0.816496580927726 topicReplicas:0.15713484026367722

[    10 ms] Stats for LeaderBytesInDistributionGoal(VIOLATED):
AVG:{cpu:       0.020 networkInbound:       0.604 networkOutbound:       0.465 disk:       0.005 potentialNwOut:       1.395 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       0.046 networkInbound:       0.604 networkOutbound:       1.223 disk:       0.005 potentialNwOut:       1.395 replicas:96 leaderReplicas:43 topicReplicas:50}
MIN:{cpu:       0.007 networkInbound:       0.604 networkOutbound:       0.000 disk:       0.005 potentialNwOut:       1.395 replicas:95 leaderReplicas:35 topicReplicas:1}
STD:{cpu:       0.019 networkInbound:       0.000 networkOutbound:       0.541 disk:       0.000 potentialNwOut:       0.000 replicas:0.4714045207910317 leaderReplicas:3.265986323710904 topicReplicas:0.15713484026367722

[     1 ms] Stats for PreferredLeaderElectionGoal(VIOLATED):
AVG:{cpu:       0.020 networkInbound:       0.604 networkOutbound:       0.465 disk:       0.005 potentialNwOut:       1.395 replicas:95.66666666666667 leaderReplicas:39.0 topicReplicas:15.944444444444443}
MAX:{cpu:       0.046 networkInbound:       0.604 networkOutbound:       1.223 disk:       0.005 potentialNwOut:       1.395 replicas:96 leaderReplicas:43 topicReplicas:50}
MIN:{cpu:       0.007 networkInbound:       0.604 networkOutbound:       0.000 disk:       0.005 potentialNwOut:       1.395 replicas:95 leaderReplicas:35 topicReplicas:1}
STD:{cpu:       0.019 networkInbound:       0.000 networkOutbound:       0.541 disk:       0.000 potentialNwOut:       0.000 replicas:0.4714045207910317 leaderReplicas:3.265986323710904 topicReplicas:0.15713484026367722

Cluster load after rebalance:


                                                                         HOST         BROKER                                                                         RACK         DISK_CAP(MB)            DISK(MB)/_(%)_            CORE_NUM         CPU(%)          NW_IN_CAP(KB/s)       LEADER_NW_IN(KB/s)     FOLLOWER_NW_IN(KB/s)         NW_OUT_CAP(KB/s)        NW_OUT(KB/s)       PNW_OUT(KB/s)    LEADERS/REPLICAS
my-cluster-09b59e48-kafka-0.my-cluster-09b59e48-kafka-brokers.namespace-1.svc,             0,my-cluster-09b59e48-kafka-0.my-cluster-09b59e48-kafka-brokers.namespace-1.svc,          100000.000,              0.005/00.00,                  1,         0.046,               10000.000,                   0.518,                   0.086,               10000.000,              1.223,              1.395,            35/96
my-cluster-09b59e48-kafka-1.my-cluster-09b59e48-kafka-brokers.namespace-1.svc,             1,my-cluster-09b59e48-kafka-1.my-cluster-09b59e48-kafka-brokers.namespace-1.svc,          100000.000,              0.005/00.00,                  1,         0.007,               10000.000,                   0.086,                   0.518,               10000.000,              0.172,              1.395,            39/96
my-cluster-09b59e48-kafka-2.my-cluster-09b59e48-kafka-brokers.namespace-1.svc,             2,my-cluster-09b59e48-kafka-2.my-cluster-09b59e48-kafka-brokers.namespace-1.svc,          100000.000,              0.005/00.00,                  1,         0.007,               10000.000,                   0.000,                   0.604,               10000.000,              0.000,              1.395,            43/95

2022-03-30 20:06:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:06:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1719668ms till timeout)
2022-03-30 20:06:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/rebalance
2022-03-30 20:06:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:40 [ForkJoinPool-3-worker-9] [32mINFO [m [CruiseControlApiST:97] ----> EXECUTION OF STOP PROPOSAL <----
2022-03-30 20:06:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XGET --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/stop_proposal_execution
2022-03-30 20:06:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XGET --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/stop_proposal_execution
2022-03-30 20:06:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/stop_proposal_execution
2022-03-30 20:06:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/stop_proposal_execution
2022-03-30 20:06:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:40 [ForkJoinPool-3-worker-9] [32mINFO [m [CruiseControlApiST:108] ----> USER TASKS <----
2022-03-30 20:06:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/user_tasks
2022-03-30 20:06:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (767666ms till timeout)
2022-03-30 20:06:40 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:40 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (474488ms till timeout)
2022-03-30 20:06:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1718662ms till timeout)
2022-03-30 20:06:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XPOST --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/user_tasks
2022-03-30 20:06:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XGET --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/user_tasks
2022-03-30 20:06:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XGET --cacert /etc/tls-sidecar/cc-certs/cruise-control.crt --user admin:$(cat /opt/cruise-control/api-auth-config/cruise-control.apiAdminPassword)  HTTPS://localhost:9090/kafkacruisecontrol/user_tasks
2022-03-30 20:06:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:41 [ForkJoinPool-3-worker-9] [32mINFO [m [CruiseControlApiST:126] Verifying that Cruise Control REST API doesn't allow HTTP requests
2022-03-30 20:06:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XGET -k  HTTP://localhost:9090/kafkacruisecontrol/state
2022-03-30 20:06:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XGET -k  HTTP://localhost:9090/kafkacruisecontrol/state
2022-03-30 20:06:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:41 [ForkJoinPool-3-worker-9] [32mINFO [m [CruiseControlApiST:132] Verifying that Cruise Control REST API doesn't allow unauthenticated requests
2022-03-30 20:06:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XGET -k  HTTPS://localhost:9090/kafkacruisecontrol/state
2022-03-30 20:06:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-1 exec my-cluster-09b59e48-cruise-control-66c97ff447-4d89b -c cruise-control -- /bin/bash -c curl -XGET -k  HTTPS://localhost:9090/kafkacruisecontrol/state
2022-03-30 20:06:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 20:06:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:675] [cruisecontrol.CruiseControlApiST - After Each] - Clean up after test
2022-03-30 20:06:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:06:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for testCruiseControlBasicAPIRequests
2022-03-30 20:06:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-09b59e48 in namespace namespace-1
2022-03-30 20:06:41 [ForkJoinPool-3-worker-9] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-1, for cruise control Kafka cluster my-cluster-09b59e48
2022-03-30 20:06:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-09b59e48
2022-03-30 20:06:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-09b59e48 not ready, will try again in 10000 ms (839987ms till timeout)
2022-03-30 20:06:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (766660ms till timeout)
2022-03-30 20:06:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:42 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:42 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:42 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (473386ms till timeout)
2022-03-30 20:06:42 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1717659ms till timeout)
2022-03-30 20:06:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (765656ms till timeout)
2022-03-30 20:06:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:43 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1716654ms till timeout)
2022-03-30 20:06:43 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:43 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (472296ms till timeout)
2022-03-30 20:06:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (764652ms till timeout)
2022-03-30 20:06:44 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1715650ms till timeout)
2022-03-30 20:06:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:44 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:44 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (471217ms till timeout)
2022-03-30 20:06:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:06:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 20:06:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (763649ms till timeout)
2022-03-30 20:06:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1714647ms till timeout)
2022-03-30 20:06:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:45 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:45 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (470145ms till timeout)
2022-03-30 20:06:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (762646ms till timeout)
2022-03-30 20:06:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1713644ms till timeout)
2022-03-30 20:06:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:46 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:46 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (469066ms till timeout)
2022-03-30 20:06:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (761643ms till timeout)
2022-03-30 20:06:47 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1712641ms till timeout)
2022-03-30 20:06:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:47 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:47 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (467988ms till timeout)
2022-03-30 20:06:47 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (760639ms till timeout)
2022-03-30 20:06:48 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1711637ms till timeout)
2022-03-30 20:06:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:48 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:48 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (466903ms till timeout)
2022-03-30 20:06:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (759635ms till timeout)
2022-03-30 20:06:49 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1710633ms till timeout)
2022-03-30 20:06:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:06:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 20:06:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:49 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:49 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (465789ms till timeout)
2022-03-30 20:06:49 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (758632ms till timeout)
2022-03-30 20:06:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1709621ms till timeout)
2022-03-30 20:06:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:50 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:50 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (464664ms till timeout)
2022-03-30 20:06:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (757627ms till timeout)
2022-03-30 20:06:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1708616ms till timeout)
2022-03-30 20:06:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:51 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:51 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (463581ms till timeout)
2022-03-30 20:06:51 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:06:51 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-1 for test case:testCruiseControlBasicAPIRequests
2022-03-30 20:06:51 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-1 removal
2022-03-30 20:06:51 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:06:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-2a79abff will have desired state: Ready not ready, will try again in 1000 ms (756621ms till timeout)
2022-03-30 20:06:51 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:06:51 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:51 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (479912ms till timeout)
2022-03-30 20:06:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1707612ms till timeout)
2022-03-30 20:06:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:52 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-2a79abff is in desired state: Ready
2022-03-30 20:06:52 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-838849261-890460734 in namespace namespace-4
2022-03-30 20:06:52 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-03-30 20:06:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (462502ms till timeout)
2022-03-30 20:06:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-838849261-890460734
2022-03-30 20:06:52 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-838849261-890460734 will have desired state: Ready
2022-03-30 20:06:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-838849261-890460734 will have desired state: Ready
2022-03-30 20:06:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-838849261-890460734 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 20:06:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:06:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:06:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (478815ms till timeout)
2022-03-30 20:06:53 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1706609ms till timeout)
2022-03-30 20:06:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:53 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-838849261-890460734 is in desired state: Ready
2022-03-30 20:06:53 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:489] Verifying docker image names
2022-03-30 20:06:53 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:172] strimzi-cluster-operator
2022-03-30 20:06:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get pod -l strimzi.io/name=my-cluster-2a79abff-entity-operator -o jsonpath={range .items[*]}{.metadata.name} 
2022-03-30 20:06:54 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:54 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (461404ms till timeout)
2022-03-30 20:06:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:06:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get pod -l strimzi.io/name=my-cluster-2a79abff-entity-operator -o jsonpath={range .items[*]}{.metadata.name} 
2022-03-30 20:06:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:54 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1705604ms till timeout)
2022-03-30 20:06:54 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:525] Docker images verified
2022-03-30 20:06:54 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateST:292] Running kafkaScaleUpScaleDown my-cluster-2a79abff
2022-03-30 20:06:54 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-2138084746-704381117 in namespace namespace-4
2022-03-30 20:06:54 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-03-30 20:06:54 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:06:54 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (477720ms till timeout)
2022-03-30 20:06:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-2138084746-704381117
2022-03-30 20:06:54 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-2138084746-704381117 will have desired state: Ready
2022-03-30 20:06:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-2138084746-704381117 will have desired state: Ready
2022-03-30 20:06:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-2138084746-704381117 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 20:06:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:06:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 20:06:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1704601ms till timeout)
2022-03-30 20:06:55 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:55 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (460321ms till timeout)
2022-03-30 20:06:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:06:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-2138084746-704381117 is in desired state: Ready
2022-03-30 20:06:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-2a79abff-kafka-clients in namespace namespace-4
2022-03-30 20:06:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-03-30 20:06:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for pod with prefixmy-cluster-2a79abff-kafka-clients is present.
2022-03-30 20:06:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] pod with prefixmy-cluster-2a79abff-kafka-clients is present. not ready, will try again in 10000 ms (299991ms till timeout)
2022-03-30 20:06:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:06:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (476641ms till timeout)
2022-03-30 20:06:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1703597ms till timeout)
2022-03-30 20:06:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (459242ms till timeout)
2022-03-30 20:06:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:06:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:06:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (475563ms till timeout)
2022-03-30 20:06:57 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1702594ms till timeout)
2022-03-30 20:06:57 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:57 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:57 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:57 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (458166ms till timeout)
2022-03-30 20:06:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:06:57 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:06:57 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (474485ms till timeout)
2022-03-30 20:06:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1701590ms till timeout)
2022-03-30 20:06:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:58 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:58 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (457086ms till timeout)
2022-03-30 20:06:58 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:06:58 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:06:58 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:58 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (473404ms till timeout)
2022-03-30 20:06:59 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1700587ms till timeout)
2022-03-30 20:06:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:06:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 20:06:59 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:06:59 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (456018ms till timeout)
2022-03-30 20:06:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:06:59 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:06:59 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:06:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (472328ms till timeout)
2022-03-30 20:07:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1699582ms till timeout)
2022-03-30 20:07:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:00 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:00 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (454944ms till timeout)
2022-03-30 20:07:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:00 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:00 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (471251ms till timeout)
2022-03-30 20:07:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1698579ms till timeout)
2022-03-30 20:07:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (453862ms till timeout)
2022-03-30 20:07:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (470173ms till timeout)
2022-03-30 20:07:02 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1697575ms till timeout)
2022-03-30 20:07:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:02 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:02 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (452780ms till timeout)
2022-03-30 20:07:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:02 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:02 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (469101ms till timeout)
2022-03-30 20:07:03 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1696572ms till timeout)
2022-03-30 20:07:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (451707ms till timeout)
2022-03-30 20:07:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (468026ms till timeout)
2022-03-30 20:07:04 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1695568ms till timeout)
2022-03-30 20:07:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:07:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 20:07:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:04 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:04 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (450635ms till timeout)
2022-03-30 20:07:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (466955ms till timeout)
2022-03-30 20:07:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1694565ms till timeout)
2022-03-30 20:07:05 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-30 20:07:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Producer and consumer will successfully send and receive messages.
2022-03-30 20:07:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2ccd74d1, which are set.
2022-03-30 20:07:05 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@268bf269, messages=[], arguments=[USER=my_user_838849261_890460734, --topic, my-topic-2138084746-704381117, --bootstrap-server, my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-2a79abff-kafka-clients-6bfb76b8f-f4rf2', podNamespace='namespace-4', bootstrapServer='my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093', topicName='my-topic-2138084746-704381117', maxMessages=100, kafkaUsername='my-user-838849261-890460734', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2ccd74d1}
2022-03-30 20:07:05 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093:my-topic-2138084746-704381117 from pod my-cluster-2a79abff-kafka-clients-6bfb76b8f-f4rf2
2022-03-30 20:07:05 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-2a79abff-kafka-clients-6bfb76b8f-f4rf2 -n namespace-4 -- /opt/kafka/producer.sh USER=my_user_838849261_890460734 --topic my-topic-2138084746-704381117 --bootstrap-server my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093 --max-messages 100
2022-03-30 20:07:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-2a79abff-kafka-clients-6bfb76b8f-f4rf2 -n namespace-4 -- /opt/kafka/producer.sh USER=my_user_838849261_890460734 --topic my-topic-2138084746-704381117 --bootstrap-server my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093 --max-messages 100
2022-03-30 20:07:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:05 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:05 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (449566ms till timeout)
2022-03-30 20:07:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:06 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:06 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (465873ms till timeout)
2022-03-30 20:07:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1693562ms till timeout)
2022-03-30 20:07:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:06 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:06 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (448497ms till timeout)
2022-03-30 20:07:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:07 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:07 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (464792ms till timeout)
2022-03-30 20:07:07 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1692558ms till timeout)
2022-03-30 20:07:07 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:08 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:08 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (447422ms till timeout)
2022-03-30 20:07:08 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:08 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:08 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:08 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (463714ms till timeout)
2022-03-30 20:07:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1691555ms till timeout)
2022-03-30 20:07:08 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-30 20:07:08 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-03-30 20:07:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2c44715b, which are set.
2022-03-30 20:07:08 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@6c75fa3e, messages=[], arguments=[USER=my_user_838849261_890460734, --group-id, my-consumer-group-1459233228, --topic, my-topic-2138084746-704381117, --group-instance-id, instance119124148, --bootstrap-server, my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-2a79abff-kafka-clients-6bfb76b8f-f4rf2', podNamespace='namespace-4', bootstrapServer='my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093', topicName='my-topic-2138084746-704381117', maxMessages=100, kafkaUsername='my-user-838849261-890460734', consumerGroupName='my-consumer-group-1459233228', consumerInstanceId='instance119124148', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2c44715b}
2022-03-30 20:07:08 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093:my-topic-2138084746-704381117 from pod my-cluster-2a79abff-kafka-clients-6bfb76b8f-f4rf2
2022-03-30 20:07:08 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-2a79abff-kafka-clients-6bfb76b8f-f4rf2 -n namespace-4 -- /opt/kafka/consumer.sh USER=my_user_838849261_890460734 --group-id my-consumer-group-1459233228 --topic my-topic-2138084746-704381117 --group-instance-id instance119124148 --bootstrap-server my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093 --max-messages 100
2022-03-30 20:07:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-2a79abff-kafka-clients-6bfb76b8f-f4rf2 -n namespace-4 -- /opt/kafka/consumer.sh USER=my_user_838849261_890460734 --group-id my-consumer-group-1459233228 --topic my-topic-2138084746-704381117 --group-instance-id instance119124148 --bootstrap-server my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093 --max-messages 100
2022-03-30 20:07:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:09 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:09 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (446336ms till timeout)
2022-03-30 20:07:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:09 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1690548ms till timeout)
2022-03-30 20:07:09 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:09 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (462636ms till timeout)
2022-03-30 20:07:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:07:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 20:07:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1689545ms till timeout)
2022-03-30 20:07:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (445256ms till timeout)
2022-03-30 20:07:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (461548ms till timeout)
2022-03-30 20:07:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1688541ms till timeout)
2022-03-30 20:07:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (444178ms till timeout)
2022-03-30 20:07:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:11 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:11 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (460472ms till timeout)
2022-03-30 20:07:12 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1687537ms till timeout)
2022-03-30 20:07:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:12 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:12 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (443086ms till timeout)
2022-03-30 20:07:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:12 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:12 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (459393ms till timeout)
2022-03-30 20:07:13 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1686534ms till timeout)
2022-03-30 20:07:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:13 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:13 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (442008ms till timeout)
2022-03-30 20:07:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:13 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (458314ms till timeout)
2022-03-30 20:07:14 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1685531ms till timeout)
2022-03-30 20:07:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:07:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 20:07:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:14 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:14 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (440935ms till timeout)
2022-03-30 20:07:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:14 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:14 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (457223ms till timeout)
2022-03-30 20:07:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1684527ms till timeout)
2022-03-30 20:07:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (439858ms till timeout)
2022-03-30 20:07:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:15 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:15 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (456144ms till timeout)
2022-03-30 20:07:15 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 20:07:15 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-03-30 20:07:15 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateST:317] Scale up Kafka to 7
2022-03-30 20:07:15 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-2a79abff-kafka rolling update
2022-03-30 20:07:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:74] Waiting for rolling update of component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})
2022-03-30 20:07:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for component with name my-cluster-2a79abff-kafka rolling update
2022-03-30 20:07:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=f7756e91-a158-4c67-95c0-4787da0616a0, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:07:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=f7756e91-a158-4c67-95c0-4787da0616a0, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:07:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=f7756e91-a158-4c67-95c0-4787da0616a0, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:07:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-1 hasn't rolled
2022-03-30 20:07:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (1799996ms till timeout)
2022-03-30 20:07:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1683523ms till timeout)
2022-03-30 20:07:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:16 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:16 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (438785ms till timeout)
2022-03-30 20:07:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:16 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:16 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (455066ms till timeout)
2022-03-30 20:07:17 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1682520ms till timeout)
2022-03-30 20:07:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (437708ms till timeout)
2022-03-30 20:07:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (453986ms till timeout)
2022-03-30 20:07:18 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1681516ms till timeout)
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-4 get Namespace namespace-2 -o yaml
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-2" not found
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[namespace-4], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@dce7372d=[rolling-update-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-3], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-1], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:267] testSendMessagesTlsScramSha - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendSimpleMessageTls, testUpdateUser, testCruiseControlBasicAPIRequests, testReceiveSimpleMessageTls, testSendMessagesTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown] to and randomly select one to start execution
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:93] [kafka.listeners.ListenersST] - Removing parallel test: testSendMessagesTlsScramSha
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:97] [kafka.listeners.ListenersST] - Parallel test count: 3
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesTlsScramSha-FINISHED
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesCustomListenerTlsScramSha-STARTED
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:659] [kafka.listeners.ListenersST - Before Each] - Setup test case environment
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:77] [kafka.listeners.ListenersST] - Adding parallel test: testSendMessagesCustomListenerTlsScramSha
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:81] [kafka.listeners.ListenersST] - Parallel test count: 4
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:230] testSendMessagesCustomListenerTlsScramSha test now can proceed its execution
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba, testSendSimpleMessageTls=my-cluster-ca7325d7, testUpdateUser=my-cluster-6e6fa8fe, testSendMessagesTlsScramSha=my-cluster-84882f67, testReceiveSimpleMessageTls=my-cluster-2e450144, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37, testCruiseControlBasicAPIRequests=my-cluster-8aed4745, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140}
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-838849261-890460734, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1731919654-539989781, testSendSimpleMessageTls=my-user-1309491966-2015599610, testUpdateUser=my-user-1387197422-925410353, testSendMessagesTlsScramSha=my-user-586569987-2105688079, testReceiveSimpleMessageTls=my-user-1669668617-1031881775, testSendMessagesCustomListenerTlsScramSha=my-user-1118022021-805525218, testCruiseControlBasicAPIRequests=my-user-1861484466-561878159, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677}
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-2138084746-704381117, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1672280868-1119644965, testSendSimpleMessageTls=my-topic-435427300-917779110, testUpdateUser=my-topic-1875783706-1763804507, testSendMessagesTlsScramSha=my-topic-255809133-823300208, testReceiveSimpleMessageTls=my-topic-368564676-637091599, testSendMessagesCustomListenerTlsScramSha=my-topic-963888555-1712711326, testCruiseControlBasicAPIRequests=my-topic-1631570507-1308241024, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206}
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba-kafka-clients, testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testUpdateUser=my-cluster-6e6fa8fe-kafka-clients, testSendMessagesTlsScramSha=my-cluster-84882f67-kafka-clients, testReceiveSimpleMessageTls=my-cluster-2e450144-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-8aed4745-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients}
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-5 for test case:testSendMessagesCustomListenerTlsScramSha
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-5
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-5
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-5 -o json
2022-03-30 20:07:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-5 -o json
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T20:07:18Z",
        "name": "namespace-5",
        "resourceVersion": "85030",
        "selfLink": "/api/v1/namespaces/namespace-5",
        "uid": "4a373a8b-4888-4993-9940-04d6ed62af7c"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[namespace-5], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[namespace-4], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@dce7372d=[rolling-update-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-3], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[namespace-1], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-5
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-5, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-5
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-d4963f37 in namespace namespace-5
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-03-30 20:07:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-d4963f37
2022-03-30 20:07:19 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-d4963f37 will have desired state: Ready
2022-03-30 20:07:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-d4963f37 will have desired state: Ready
2022-03-30 20:07:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (839996ms till timeout)
2022-03-30 20:07:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (452866ms till timeout)
2022-03-30 20:07:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1680513ms till timeout)
2022-03-30 20:07:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:07:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 20:07:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (838992ms till timeout)
2022-03-30 20:07:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:20 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:20 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (451759ms till timeout)
2022-03-30 20:07:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1679507ms till timeout)
2022-03-30 20:07:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=f7756e91-a158-4c67-95c0-4787da0616a0, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:07:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=f7756e91-a158-4c67-95c0-4787da0616a0, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:07:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=f7756e91-a158-4c67-95c0-4787da0616a0, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:07:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-1 hasn't rolled
2022-03-30 20:07:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (1794989ms till timeout)
2022-03-30 20:07:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (837988ms till timeout)
2022-03-30 20:07:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:21 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:21 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (450688ms till timeout)
2022-03-30 20:07:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1678504ms till timeout)
2022-03-30 20:07:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (836985ms till timeout)
2022-03-30 20:07:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:22 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1677501ms till timeout)
2022-03-30 20:07:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (449604ms till timeout)
2022-03-30 20:07:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (835981ms till timeout)
2022-03-30 20:07:23 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1676497ms till timeout)
2022-03-30 20:07:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (448522ms till timeout)
2022-03-30 20:07:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (834977ms till timeout)
2022-03-30 20:07:24 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1675493ms till timeout)
2022-03-30 20:07:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:07:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 20:07:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (447431ms till timeout)
2022-03-30 20:07:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (833973ms till timeout)
2022-03-30 20:07:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1674490ms till timeout)
2022-03-30 20:07:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (446346ms till timeout)
2022-03-30 20:07:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=f7756e91-a158-4c67-95c0-4787da0616a0, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:07:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=f7756e91-a158-4c67-95c0-4787da0616a0, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:07:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=f7756e91-a158-4c67-95c0-4787da0616a0, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:07:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-1 hasn't rolled
2022-03-30 20:07:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (1789981ms till timeout)
2022-03-30 20:07:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (832969ms till timeout)
2022-03-30 20:07:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1673484ms till timeout)
2022-03-30 20:07:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:26 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:26 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (445253ms till timeout)
2022-03-30 20:07:27 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (831964ms till timeout)
2022-03-30 20:07:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (1672474ms till timeout)
2022-03-30 20:07:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:27 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:27 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (444150ms till timeout)
2022-03-30 20:07:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (830958ms till timeout)
2022-03-30 20:07:28 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-cae962ba is in desired state: Ready
2022-03-30 20:07:28 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1731919654-539989781 in namespace namespace-5
2022-03-30 20:07:28 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-3
2022-03-30 20:07:28 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1731919654-539989781
2022-03-30 20:07:28 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1731919654-539989781 will have desired state: Ready
2022-03-30 20:07:28 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-1731919654-539989781 will have desired state: Ready
2022-03-30 20:07:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-1731919654-539989781 will have desired state: Ready not ready, will try again in 1000 ms (179991ms till timeout)
2022-03-30 20:07:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:28 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:28 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (443022ms till timeout)
2022-03-30 20:07:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (829953ms till timeout)
2022-03-30 20:07:29 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1731919654-539989781 is in desired state: Ready
2022-03-30 20:07:29 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1672280868-1119644965 in namespace namespace-5
2022-03-30 20:07:29 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-3
2022-03-30 20:07:29 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1672280868-1119644965
2022-03-30 20:07:29 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1672280868-1119644965 will have desired state: Ready
2022-03-30 20:07:29 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1672280868-1119644965 will have desired state: Ready
2022-03-30 20:07:29 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1672280868-1119644965 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 20:07:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:07:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 20:07:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:29 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:29 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (441946ms till timeout)
2022-03-30 20:07:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (828949ms till timeout)
2022-03-30 20:07:30 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1672280868-1119644965 is in desired state: Ready
2022-03-30 20:07:30 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-cae962ba-kafka-clients in namespace namespace-5
2022-03-30 20:07:30 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-3
2022-03-30 20:07:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-cae962ba-kafka-clients
2022-03-30 20:07:30 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-cae962ba-kafka-clients will be ready
2022-03-30 20:07:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-cae962ba-kafka-clients will be ready
2022-03-30 20:07:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-kafka-clients will be ready not ready, will try again in 1000 ms (479996ms till timeout)
2022-03-30 20:07:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=f7756e91-a158-4c67-95c0-4787da0616a0, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:07:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:07:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:07:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-1 hasn't rolled
2022-03-30 20:07:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (1784975ms till timeout)
2022-03-30 20:07:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (440864ms till timeout)
2022-03-30 20:07:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (827935ms till timeout)
2022-03-30 20:07:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-kafka-clients will be ready not ready, will try again in 1000 ms (478990ms till timeout)
2022-03-30 20:07:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:32 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (826931ms till timeout)
2022-03-30 20:07:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (439787ms till timeout)
2022-03-30 20:07:32 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-cae962ba-kafka-clients is ready
2022-03-30 20:07:32 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-30 20:07:32 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:269] Checking produced and consumed messages to pod:my-cluster-cae962ba-kafka-clients-59f44d88cb-82mb5
2022-03-30 20:07:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4b62bc0c, which are set.
2022-03-30 20:07:32 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@17073841, messages=[], arguments=[--topic, my-topic-1672280868-1119644965, --bootstrap-server, my-cluster-cae962ba-kafka-bootstrap.namespace-3.svc:9092, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-cae962ba-kafka-clients-59f44d88cb-82mb5', podNamespace='namespace-3', bootstrapServer='my-cluster-cae962ba-kafka-bootstrap.namespace-3.svc:9092', topicName='my-topic-1672280868-1119644965', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4b62bc0c}
2022-03-30 20:07:32 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-cae962ba-kafka-bootstrap.namespace-3.svc:9092:my-topic-1672280868-1119644965 from pod my-cluster-cae962ba-kafka-clients-59f44d88cb-82mb5
2022-03-30 20:07:32 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-cae962ba-kafka-clients-59f44d88cb-82mb5 -n namespace-3 -- /opt/kafka/producer.sh --topic my-topic-1672280868-1119644965 --bootstrap-server my-cluster-cae962ba-kafka-bootstrap.namespace-3.svc:9092 --max-messages 100
2022-03-30 20:07:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-cae962ba-kafka-clients-59f44d88cb-82mb5 -n namespace-3 -- /opt/kafka/producer.sh --topic my-topic-1672280868-1119644965 --bootstrap-server my-cluster-cae962ba-kafka-bootstrap.namespace-3.svc:9092 --max-messages 100
2022-03-30 20:07:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (825928ms till timeout)
2022-03-30 20:07:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (438706ms till timeout)
2022-03-30 20:07:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (824924ms till timeout)
2022-03-30 20:07:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (437618ms till timeout)
2022-03-30 20:07:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:07:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 20:07:34 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-03-30 20:07:34 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-03-30 20:07:34 [ForkJoinPool-3-worker-15] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@13d6b275, which are set.
2022-03-30 20:07:34 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@51d2c719, messages=[], arguments=[--group-id, my-consumer-group-1796633044, --topic, my-topic-1672280868-1119644965, --group-instance-id, instance1805027295, --bootstrap-server, my-cluster-cae962ba-kafka-bootstrap.namespace-3.svc:9092, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-cae962ba-kafka-clients-59f44d88cb-82mb5', podNamespace='namespace-3', bootstrapServer='my-cluster-cae962ba-kafka-bootstrap.namespace-3.svc:9092', topicName='my-topic-1672280868-1119644965', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1796633044', consumerInstanceId='instance1805027295', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@13d6b275}
2022-03-30 20:07:34 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-cae962ba-kafka-bootstrap.namespace-3.svc:9092#my-topic-1672280868-1119644965 from pod my-cluster-cae962ba-kafka-clients-59f44d88cb-82mb5
2022-03-30 20:07:34 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-cae962ba-kafka-clients-59f44d88cb-82mb5 -n namespace-3 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1796633044 --topic my-topic-1672280868-1119644965 --group-instance-id instance1805027295 --bootstrap-server my-cluster-cae962ba-kafka-bootstrap.namespace-3.svc:9092 --max-messages 100
2022-03-30 20:07:34 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-cae962ba-kafka-clients-59f44d88cb-82mb5 -n namespace-3 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1796633044 --topic my-topic-1672280868-1119644965 --group-instance-id instance1805027295 --bootstrap-server my-cluster-cae962ba-kafka-bootstrap.namespace-3.svc:9092 --max-messages 100
2022-03-30 20:07:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (823920ms till timeout)
2022-03-30 20:07:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-4 get Namespace namespace-1 -o yaml
2022-03-30 20:07:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 20:07:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 20:07:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-1" not found
2022-03-30 20:07:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 20:07:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[namespace-5], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[namespace-4], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@dce7372d=[rolling-update-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-3], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[cruise-control-api-st], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-30 20:07:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:267] testCruiseControlBasicAPIRequests - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendSimpleMessageTls, testUpdateUser, testCruiseControlBasicAPIRequests, testReceiveSimpleMessageTls, testSendMessagesTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testSendMessagesCustomListenerTlsScramSha] to and randomly select one to start execution
2022-03-30 20:07:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:93] [cruisecontrol.CruiseControlApiST] - Removing parallel test: testCruiseControlBasicAPIRequests
2022-03-30 20:07:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:97] [cruisecontrol.CruiseControlApiST] - Parallel test count: 3
2022-03-30 20:07:35 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlApiST.testCruiseControlBasicAPIRequests-FINISHED
2022-03-30 20:07:35 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 20:07:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 20:07:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:690] [cruisecontrol.CruiseControlApiST - After All] - Clean up after test suite
2022-03-30 20:07:35 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:07:35 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:346] In context CruiseControlApiST is everything deleted.
2022-03-30 20:07:35 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Running io.strimzi.systemtest.specific.SpecificIsolatedST
2022-03-30 20:07:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 20:07:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:667] [specific.SpecificIsolatedST - Before All] - Setup test suite environment
2022-03-30 20:07:35 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 20:07:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-30 20:07:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Namespace cruise-control-api-st removal
2022-03-30 20:07:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-30 20:07:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-30 20:07:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace cruise-control-api-st removal not ready, will try again in 1000 ms (479913ms till timeout)
2022-03-30 20:07:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=f7756e91-a158-4c67-95c0-4787da0616a0, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:07:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:07:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:07:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-1 hasn't rolled
2022-03-30 20:07:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (1779969ms till timeout)
2022-03-30 20:07:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (822916ms till timeout)
2022-03-30 20:07:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-30 20:07:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-30 20:07:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace cruise-control-api-st removal not ready, will try again in 1000 ms (478782ms till timeout)
2022-03-30 20:07:37 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (821913ms till timeout)
2022-03-30 20:07:37 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-30 20:07:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-30 20:07:37 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:37 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace cruise-control-api-st removal not ready, will try again in 1000 ms (477704ms till timeout)
2022-03-30 20:07:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (820909ms till timeout)
2022-03-30 20:07:38 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-30 20:07:38 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-30 20:07:38 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:38 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace cruise-control-api-st removal not ready, will try again in 1000 ms (476623ms till timeout)
2022-03-30 20:07:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (819906ms till timeout)
2022-03-30 20:07:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:07:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 5
2022-03-30 20:07:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-30 20:07:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-30 20:07:39 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:07:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Namespace cruise-control-api-st removal not ready, will try again in 1000 ms (475545ms till timeout)
2022-03-30 20:07:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (818903ms till timeout)
2022-03-30 20:07:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:07:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 5
2022-03-30 20:07:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-30 20:07:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=f7756e91-a158-4c67-95c0-4787da0616a0, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:07:40 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-30 20:07:40 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-03-30 20:07:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:07:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:07:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-1 hasn't rolled
2022-03-30 20:07:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (1774962ms till timeout)
2022-03-30 20:07:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace infra-namespace get Namespace cruise-control-api-st -o yaml
2022-03-30 20:07:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 20:07:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 20:07:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "cruise-control-api-st" not found
2022-03-30 20:07:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 20:07:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[namespace-5], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[namespace-4], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@dce7372d=[rolling-update-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-3], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-30 20:07:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:254] CruiseControlApiST - Notifies waiting test suites:[HttpBridgeTlsST, CruiseControlApiST, UserST, CruiseControlST, ListenersST] to and randomly select one to start execution
2022-03-30 20:07:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:85] [cruisecontrol.CruiseControlApiST] - Removing parallel suite: CruiseControlApiST
2022-03-30 20:07:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:89] [cruisecontrol.CruiseControlApiST] - Parallel suites count: 4
[[1;34mINFO[m] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 451.535 s - in io.strimzi.systemtest.cruisecontrol.CruiseControlApiST
[[1;34mINFO[m] Running io.strimzi.systemtest.watcher.AllNamespaceIsolatedST
2022-03-30 20:07:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 20:07:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:667] [watcher.AllNamespaceIsolatedST - Before All] - Setup test suite environment
2022-03-30 20:07:40 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 20:07:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:07:40 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:283] Triggering CA cert renewal by adding the annotation
2022-03-30 20:07:40 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:295] Patching secret my-cluster-cae962ba-cluster-ca-cert with strimzi.io/force-renew
2022-03-30 20:07:40 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:295] Patching secret my-cluster-cae962ba-clients-ca-cert with strimzi.io/force-renew
2022-03-30 20:07:40 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:300] Wait for zk to rolling restart ...
2022-03-30 20:07:40 [ForkJoinPool-3-worker-15] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-cae962ba-zookeeper rolling update
2022-03-30 20:07:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:74] Waiting for rolling update of component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})
2022-03-30 20:07:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for component with name my-cluster-cae962ba-zookeeper rolling update
2022-03-30 20:07:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:07:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:07:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:07:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-zookeeper-1 hasn't rolled
2022-03-30 20:07:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-zookeeper rolling update not ready, will try again in 5000 ms (1799995ms till timeout)
2022-03-30 20:07:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (817899ms till timeout)
2022-03-30 20:07:42 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (816895ms till timeout)
2022-03-30 20:07:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (815892ms till timeout)
2022-03-30 20:07:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (814888ms till timeout)
2022-03-30 20:07:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:07:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:07:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (813884ms till timeout)
2022-03-30 20:07:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:07:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:07:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=f7756e91-a158-4c67-95c0-4787da0616a0, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:07:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:07:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:07:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-1 hasn't rolled
2022-03-30 20:07:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (1769956ms till timeout)
2022-03-30 20:07:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:07:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:07:45 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:07:45 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:07:45 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:07:45 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-zookeeper-1 hasn't rolled
2022-03-30 20:07:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-zookeeper rolling update not ready, will try again in 5000 ms (1794990ms till timeout)
2022-03-30 20:07:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (812879ms till timeout)
2022-03-30 20:07:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (811875ms till timeout)
2022-03-30 20:07:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (810872ms till timeout)
2022-03-30 20:07:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (809868ms till timeout)
2022-03-30 20:07:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:07:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:07:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (808864ms till timeout)
2022-03-30 20:07:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:07:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:07:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=f7756e91-a158-4c67-95c0-4787da0616a0, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:07:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:07:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:07:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:07:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:07:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-1 hasn't rolled
2022-03-30 20:07:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (1764950ms till timeout)
2022-03-30 20:07:50 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:07:50 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:07:50 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:07:50 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-zookeeper-1 hasn't rolled
2022-03-30 20:07:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-zookeeper rolling update not ready, will try again in 5000 ms (1789984ms till timeout)
2022-03-30 20:07:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (807859ms till timeout)
2022-03-30 20:07:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (806856ms till timeout)
2022-03-30 20:07:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (805849ms till timeout)
2022-03-30 20:07:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (804845ms till timeout)
2022-03-30 20:07:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:07:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:07:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (803841ms till timeout)
2022-03-30 20:07:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:07:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:07:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:07:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:07:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=f7756e91-a158-4c67-95c0-4787da0616a0, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:07:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:07:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:07:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-1 hasn't rolled
2022-03-30 20:07:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (1759944ms till timeout)
2022-03-30 20:07:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:07:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:07:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:07:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-zookeeper-1 hasn't rolled
2022-03-30 20:07:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-zookeeper rolling update not ready, will try again in 5000 ms (1784977ms till timeout)
2022-03-30 20:07:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (802837ms till timeout)
2022-03-30 20:07:57 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (801833ms till timeout)
2022-03-30 20:07:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (800830ms till timeout)
2022-03-30 20:07:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (799827ms till timeout)
2022-03-30 20:07:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:07:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (798823ms till timeout)
2022-03-30 20:08:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=f7756e91-a158-4c67-95c0-4787da0616a0, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:08:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:08:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:08:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-1 hasn't rolled
2022-03-30 20:08:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (1754937ms till timeout)
2022-03-30 20:08:00 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:00 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:00 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:00 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-zookeeper-1 hasn't rolled
2022-03-30 20:08:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-zookeeper rolling update not ready, will try again in 5000 ms (1779971ms till timeout)
2022-03-30 20:08:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (797819ms till timeout)
2022-03-30 20:08:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (796816ms till timeout)
2022-03-30 20:08:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (795812ms till timeout)
2022-03-30 20:08:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (794808ms till timeout)
2022-03-30 20:08:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (793805ms till timeout)
2022-03-30 20:08:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=f7756e91-a158-4c67-95c0-4787da0616a0, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:08:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:08:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:08:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-1 hasn't rolled
2022-03-30 20:08:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (1749932ms till timeout)
2022-03-30 20:08:05 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-zookeeper-1 hasn't rolled
2022-03-30 20:08:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-zookeeper rolling update not ready, will try again in 5000 ms (1774965ms till timeout)
2022-03-30 20:08:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (792801ms till timeout)
2022-03-30 20:08:07 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (791797ms till timeout)
2022-03-30 20:08:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (790794ms till timeout)
2022-03-30 20:08:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (789789ms till timeout)
2022-03-30 20:08:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (788785ms till timeout)
2022-03-30 20:08:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=f7756e91-a158-4c67-95c0-4787da0616a0, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:08:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:08:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:08:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-1 hasn't rolled
2022-03-30 20:08:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (1744926ms till timeout)
2022-03-30 20:08:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-zookeeper-1 hasn't rolled
2022-03-30 20:08:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-zookeeper rolling update not ready, will try again in 5000 ms (1769958ms till timeout)
2022-03-30 20:08:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (787781ms till timeout)
2022-03-30 20:08:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (786776ms till timeout)
2022-03-30 20:08:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (785772ms till timeout)
2022-03-30 20:08:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (784769ms till timeout)
2022-03-30 20:08:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (783764ms till timeout)
2022-03-30 20:08:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=f7756e91-a158-4c67-95c0-4787da0616a0, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:08:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:08:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:08:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-1 hasn't rolled
2022-03-30 20:08:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (1739920ms till timeout)
2022-03-30 20:08:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-zookeeper-1 hasn't rolled
2022-03-30 20:08:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-zookeeper rolling update not ready, will try again in 5000 ms (1764951ms till timeout)
2022-03-30 20:08:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (782760ms till timeout)
2022-03-30 20:08:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (781755ms till timeout)
2022-03-30 20:08:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (780751ms till timeout)
2022-03-30 20:08:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (779748ms till timeout)
2022-03-30 20:08:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (778745ms till timeout)
2022-03-30 20:08:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=f7756e91-a158-4c67-95c0-4787da0616a0, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:08:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:08:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:08:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-1 hasn't rolled
2022-03-30 20:08:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (1734915ms till timeout)
2022-03-30 20:08:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-zookeeper-1 hasn't rolled
2022-03-30 20:08:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-zookeeper rolling update not ready, will try again in 5000 ms (1759945ms till timeout)
2022-03-30 20:08:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (777742ms till timeout)
2022-03-30 20:08:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (776738ms till timeout)
2022-03-30 20:08:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (775735ms till timeout)
2022-03-30 20:08:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (774731ms till timeout)
2022-03-30 20:08:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (773728ms till timeout)
2022-03-30 20:08:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=f7756e91-a158-4c67-95c0-4787da0616a0, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:08:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:08:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:08:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-1 hasn't rolled
2022-03-30 20:08:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (1729909ms till timeout)
2022-03-30 20:08:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-zookeeper-1 hasn't rolled
2022-03-30 20:08:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-zookeeper rolling update not ready, will try again in 5000 ms (1754940ms till timeout)
2022-03-30 20:08:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (772725ms till timeout)
2022-03-30 20:08:27 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (771721ms till timeout)
2022-03-30 20:08:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (770718ms till timeout)
2022-03-30 20:08:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (769714ms till timeout)
2022-03-30 20:08:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (768711ms till timeout)
2022-03-30 20:08:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=f7756e91-a158-4c67-95c0-4787da0616a0, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:08:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:08:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:08:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-1 hasn't rolled
2022-03-30 20:08:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (1724903ms till timeout)
2022-03-30 20:08:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-zookeeper-1 hasn't rolled
2022-03-30 20:08:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-zookeeper rolling update not ready, will try again in 5000 ms (1749932ms till timeout)
2022-03-30 20:08:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (767707ms till timeout)
2022-03-30 20:08:32 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (766704ms till timeout)
2022-03-30 20:08:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (765700ms till timeout)
2022-03-30 20:08:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (764696ms till timeout)
2022-03-30 20:08:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (763692ms till timeout)
2022-03-30 20:08:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=f7756e91-a158-4c67-95c0-4787da0616a0, my-cluster-2a79abff-kafka-1=3ecce6b8-0d30-42ba-bb37-603b704ebf23, my-cluster-2a79abff-kafka-2=c27b9be2-dd01-46c8-a7e5-6fcf05f23417}
2022-03-30 20:08:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:08:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:08:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:55] All pods seem to have rolled
2022-03-30 20:08:35 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-2a79abff-kafka has been successfully rolled
2022-03-30 20:08:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:87] Component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) successfully rolled
2022-03-30 20:08:35 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:127] Waiting for 7 Pod(s) of my-cluster-2a79abff-kafka to be ready
2022-03-30 20:08:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-30 20:08:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:08:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4199996ms till timeout)
2022-03-30 20:08:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-zookeeper-1 hasn't rolled
2022-03-30 20:08:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-zookeeper rolling update not ready, will try again in 5000 ms (1744926ms till timeout)
2022-03-30 20:08:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-d4963f37 will have desired state: Ready not ready, will try again in 1000 ms (762689ms till timeout)
2022-03-30 20:08:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:08:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4198991ms till timeout)
2022-03-30 20:08:37 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-d4963f37 is in desired state: Ready
2022-03-30 20:08:37 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-963888555-1712711326 in namespace infra-namespace
2022-03-30 20:08:37 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-03-30 20:08:37 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-963888555-1712711326
2022-03-30 20:08:37 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-963888555-1712711326 will have desired state: Ready
2022-03-30 20:08:37 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-963888555-1712711326 will have desired state: Ready
2022-03-30 20:08:37 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-963888555-1712711326 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 20:08:37 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:08:37 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4197985ms till timeout)
2022-03-30 20:08:38 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-963888555-1712711326 is in desired state: Ready
2022-03-30 20:08:38 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1118022021-805525218 in namespace infra-namespace
2022-03-30 20:08:38 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-03-30 20:08:38 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1118022021-805525218
2022-03-30 20:08:38 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1118022021-805525218 will have desired state: Ready
2022-03-30 20:08:38 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-1118022021-805525218 will have desired state: Ready
2022-03-30 20:08:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-1118022021-805525218 will have desired state: Ready not ready, will try again in 1000 ms (179997ms till timeout)
2022-03-30 20:08:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:08:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4196980ms till timeout)
2022-03-30 20:08:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1118022021-805525218 is in desired state: Ready
2022-03-30 20:08:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-d4963f37-kafka-clients in namespace namespace-5
2022-03-30 20:08:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-03-30 20:08:39 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-d4963f37-kafka-clients
2022-03-30 20:08:39 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-d4963f37-kafka-clients will be ready
2022-03-30 20:08:39 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-d4963f37-kafka-clients will be ready
2022-03-30 20:08:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-d4963f37-kafka-clients will be ready not ready, will try again in 1000 ms (479993ms till timeout)
2022-03-30 20:08:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:08:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4195974ms till timeout)
2022-03-30 20:08:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-d4963f37-kafka-clients will be ready not ready, will try again in 1000 ms (478990ms till timeout)
2022-03-30 20:08:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:41 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:08:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4194969ms till timeout)
2022-03-30 20:08:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-zookeeper-1 hasn't rolled
2022-03-30 20:08:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-zookeeper rolling update not ready, will try again in 5000 ms (1739920ms till timeout)
2022-03-30 20:08:41 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-d4963f37-kafka-clients is ready
2022-03-30 20:08:41 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-30 20:08:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ListenersST:442] Checking produced and consumed messages to pod:my-cluster-d4963f37-kafka-clients-7d79dfc677-5gwvq
2022-03-30 20:08:41 [ForkJoinPool-3-worker-13] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1b38291, which are set.
2022-03-30 20:08:41 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@5f9d32af, messages=[], arguments=[USER=my_user_1118022021_805525218, --topic, my-topic-963888555-1712711326, --bootstrap-server, my-cluster-d4963f37-kafka-bootstrap.namespace-5.svc:9122, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-d4963f37-kafka-clients-7d79dfc677-5gwvq', podNamespace='namespace-5', bootstrapServer='my-cluster-d4963f37-kafka-bootstrap.namespace-5.svc:9122', topicName='my-topic-963888555-1712711326', maxMessages=100, kafkaUsername='my-user-1118022021-805525218', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1b38291}
2022-03-30 20:08:41 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-d4963f37-kafka-bootstrap.namespace-5.svc:9122:my-topic-963888555-1712711326 from pod my-cluster-d4963f37-kafka-clients-7d79dfc677-5gwvq
2022-03-30 20:08:41 [ForkJoinPool-3-worker-13] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-d4963f37-kafka-clients-7d79dfc677-5gwvq -n namespace-5 -- /opt/kafka/producer.sh USER=my_user_1118022021_805525218 --topic my-topic-963888555-1712711326 --bootstrap-server my-cluster-d4963f37-kafka-bootstrap.namespace-5.svc:9122 --max-messages 100
2022-03-30 20:08:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-d4963f37-kafka-clients-7d79dfc677-5gwvq -n namespace-5 -- /opt/kafka/producer.sh USER=my_user_1118022021_805525218 --topic my-topic-963888555-1712711326 --bootstrap-server my-cluster-d4963f37-kafka-bootstrap.namespace-5.svc:9122 --max-messages 100
2022-03-30 20:08:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:08:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4193964ms till timeout)
2022-03-30 20:08:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:08:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4192959ms till timeout)
2022-03-30 20:08:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:08:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4191953ms till timeout)
2022-03-30 20:08:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:08:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4190948ms till timeout)
2022-03-30 20:08:45 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-30 20:08:45 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-03-30 20:08:45 [ForkJoinPool-3-worker-13] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@66206c51, which are set.
2022-03-30 20:08:45 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@7b9add0e, messages=[], arguments=[USER=my_user_1118022021_805525218, --group-id, my-consumer-group-120394736, --topic, my-topic-963888555-1712711326, --group-instance-id, instance380088984, --bootstrap-server, my-cluster-d4963f37-kafka-bootstrap.namespace-5.svc:9122, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-d4963f37-kafka-clients-7d79dfc677-5gwvq', podNamespace='namespace-5', bootstrapServer='my-cluster-d4963f37-kafka-bootstrap.namespace-5.svc:9122', topicName='my-topic-963888555-1712711326', maxMessages=100, kafkaUsername='my-user-1118022021-805525218', consumerGroupName='my-consumer-group-120394736', consumerInstanceId='instance380088984', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@66206c51}
2022-03-30 20:08:45 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-d4963f37-kafka-bootstrap.namespace-5.svc:9122:my-topic-963888555-1712711326 from pod my-cluster-d4963f37-kafka-clients-7d79dfc677-5gwvq
2022-03-30 20:08:45 [ForkJoinPool-3-worker-13] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-d4963f37-kafka-clients-7d79dfc677-5gwvq -n namespace-5 -- /opt/kafka/consumer.sh USER=my_user_1118022021_805525218 --group-id my-consumer-group-120394736 --topic my-topic-963888555-1712711326 --group-instance-id instance380088984 --bootstrap-server my-cluster-d4963f37-kafka-bootstrap.namespace-5.svc:9122 --max-messages 100
2022-03-30 20:08:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-d4963f37-kafka-clients-7d79dfc677-5gwvq -n namespace-5 -- /opt/kafka/consumer.sh USER=my_user_1118022021_805525218 --group-id my-consumer-group-120394736 --topic my-topic-963888555-1712711326 --group-instance-id instance380088984 --bootstrap-server my-cluster-d4963f37-kafka-bootstrap.namespace-5.svc:9122 --max-messages 100
2022-03-30 20:08:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:08:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4189943ms till timeout)
2022-03-30 20:08:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-zookeeper-1 hasn't rolled
2022-03-30 20:08:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-zookeeper rolling update not ready, will try again in 5000 ms (1734915ms till timeout)
2022-03-30 20:08:47 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:08:47 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4188937ms till timeout)
2022-03-30 20:08:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:08:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4187932ms till timeout)
2022-03-30 20:08:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:08:49 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4186927ms till timeout)
2022-03-30 20:08:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:08:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4185922ms till timeout)
2022-03-30 20:08:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:08:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4184917ms till timeout)
2022-03-30 20:08:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-zookeeper-1 hasn't rolled
2022-03-30 20:08:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-zookeeper rolling update not ready, will try again in 5000 ms (1729910ms till timeout)
2022-03-30 20:08:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:08:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4183911ms till timeout)
2022-03-30 20:08:52 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 20:08:52 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-03-30 20:08:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 20:08:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:675] [kafka.listeners.ListenersST - After Each] - Clean up after test
2022-03-30 20:08:52 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:08:52 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testSendMessagesCustomListenerTlsScramSha
2022-03-30 20:08:52 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1118022021-805525218 in namespace namespace-5
2022-03-30 20:08:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1118022021-805525218
2022-03-30 20:08:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1118022021-805525218 not ready, will try again in 10000 ms (179992ms till timeout)
2022-03-30 20:08:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:08:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4182906ms till timeout)
2022-03-30 20:08:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:08:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4181901ms till timeout)
2022-03-30 20:08:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:08:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4180896ms till timeout)
2022-03-30 20:08:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:08:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:08:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-zookeeper-1 hasn't rolled
2022-03-30 20:08:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-zookeeper rolling update not ready, will try again in 5000 ms (1724905ms till timeout)
2022-03-30 20:08:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:08:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:08:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:08:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-3)
2022-03-30 20:08:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4179890ms till timeout)
2022-03-30 20:08:57 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:08:57 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:08:57 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:08:57 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-3)
2022-03-30 20:08:57 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4178885ms till timeout)
2022-03-30 20:08:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:08:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:08:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:08:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-3)
2022-03-30 20:08:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4177879ms till timeout)
2022-03-30 20:08:59 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:08:59 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:08:59 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:08:59 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-3)
2022-03-30 20:08:59 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4176872ms till timeout)
2022-03-30 20:08:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:08:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:09:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:09:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:09:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-3)
2022-03-30 20:09:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4175864ms till timeout)
2022-03-30 20:09:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:09:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=b486eb43-b9c3-4022-abdb-47a9bca85543, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:09:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=b486eb43-b9c3-4022-abdb-47a9bca85543, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:09:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-zookeeper-2 hasn't rolled
2022-03-30 20:09:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-zookeeper rolling update not ready, will try again in 5000 ms (1719894ms till timeout)
2022-03-30 20:09:01 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:09:01 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:09:01 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:09:01 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-3)
2022-03-30 20:09:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4174857ms till timeout)
2022-03-30 20:09:02 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:09:02 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:09:02 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:09:02 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-3)
2022-03-30 20:09:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4173850ms till timeout)
2022-03-30 20:09:02 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-d4963f37-kafka-clients in namespace namespace-5
2022-03-30 20:09:02 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-d4963f37-kafka-clients
2022-03-30 20:09:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-d4963f37-kafka-clients not ready, will try again in 10000 ms (479976ms till timeout)
2022-03-30 20:09:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:09:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:09:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:09:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-3)
2022-03-30 20:09:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4172839ms till timeout)
2022-03-30 20:09:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:09:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:09:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:09:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-3)
2022-03-30 20:09:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4171829ms till timeout)
2022-03-30 20:09:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:09:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:09:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:09:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-3)
2022-03-30 20:09:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4170822ms till timeout)
2022-03-30 20:09:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:09:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=b486eb43-b9c3-4022-abdb-47a9bca85543, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:09:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=b486eb43-b9c3-4022-abdb-47a9bca85543, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:09:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-zookeeper-2 hasn't rolled
2022-03-30 20:09:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-zookeeper rolling update not ready, will try again in 5000 ms (1714888ms till timeout)
2022-03-30 20:09:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:09:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:09:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:09:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-3)
2022-03-30 20:09:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4169814ms till timeout)
2022-03-30 20:09:07 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:09:07 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:09:07 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:09:07 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-3)
2022-03-30 20:09:07 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4168807ms till timeout)
2022-03-30 20:09:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:09:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:09:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:09:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-3)
2022-03-30 20:09:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4167797ms till timeout)
2022-03-30 20:09:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:09:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:09:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:09:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-3)
2022-03-30 20:09:09 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4166790ms till timeout)
2022-03-30 20:09:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:09:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:09:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:09:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-3)
2022-03-30 20:09:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4165782ms till timeout)
2022-03-30 20:09:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:09:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=b486eb43-b9c3-4022-abdb-47a9bca85543, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:09:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=b486eb43-b9c3-4022-abdb-47a9bca85543, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:09:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-zookeeper-2 hasn't rolled
2022-03-30 20:09:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-zookeeper rolling update not ready, will try again in 5000 ms (1709882ms till timeout)
2022-03-30 20:09:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:09:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:09:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:09:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-3)
2022-03-30 20:09:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4164776ms till timeout)
2022-03-30 20:09:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:09:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:09:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:09:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-3)
2022-03-30 20:09:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4163771ms till timeout)
2022-03-30 20:09:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-d4963f37-kafka-clients not ready, will try again in 10000 ms (469966ms till timeout)
2022-03-30 20:09:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:09:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:09:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:09:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-3)
2022-03-30 20:09:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4162764ms till timeout)
2022-03-30 20:09:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:09:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:09:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:09:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-3)
2022-03-30 20:09:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4161758ms till timeout)
2022-03-30 20:09:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:09:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:09:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:09:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-3 not ready: kafka)
2022-03-30 20:09:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-4 not ready: kafka)
2022-03-30 20:09:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-5)
2022-03-30 20:09:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4160751ms till timeout)
2022-03-30 20:09:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:09:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=b486eb43-b9c3-4022-abdb-47a9bca85543, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:09:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=b486eb43-b9c3-4022-abdb-47a9bca85543, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:09:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-zookeeper-2 hasn't rolled
2022-03-30 20:09:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-zookeeper rolling update not ready, will try again in 5000 ms (1704877ms till timeout)
2022-03-30 20:09:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:09:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:09:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:09:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-3 not ready: kafka)
2022-03-30 20:09:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-4 not ready: kafka)
2022-03-30 20:09:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-5)
2022-03-30 20:09:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4159745ms till timeout)
2022-03-30 20:09:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:09:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:09:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:09:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-3 not ready: kafka)
2022-03-30 20:09:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-4 not ready: kafka)
2022-03-30 20:09:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-5 not ready: kafka)
2022-03-30 20:09:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-6)
2022-03-30 20:09:17 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4158739ms till timeout)
2022-03-30 20:09:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:09:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:09:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:09:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-3 not ready: kafka)
2022-03-30 20:09:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-4 not ready: kafka)
2022-03-30 20:09:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-5 not ready: kafka)
2022-03-30 20:09:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-6 not ready: kafka)
2022-03-30 20:09:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2a79abff-kafka-0, my-cluster-2a79abff-kafka-1, my-cluster-2a79abff-kafka-2, my-cluster-2a79abff-kafka-3, my-cluster-2a79abff-kafka-4, my-cluster-2a79abff-kafka-5, my-cluster-2a79abff-kafka-6 are ready
2022-03-30 20:09:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4157732ms till timeout)
2022-03-30 20:09:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:09:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:09:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:09:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-3 not ready: kafka)
2022-03-30 20:09:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-4 not ready: kafka)
2022-03-30 20:09:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-5 not ready: kafka)
2022-03-30 20:09:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-6 not ready: kafka)
2022-03-30 20:09:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2a79abff-kafka-0, my-cluster-2a79abff-kafka-1, my-cluster-2a79abff-kafka-2, my-cluster-2a79abff-kafka-3, my-cluster-2a79abff-kafka-4, my-cluster-2a79abff-kafka-5, my-cluster-2a79abff-kafka-6 are ready
2022-03-30 20:09:19 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4156726ms till timeout)
2022-03-30 20:09:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:09:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:09:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:09:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-3 not ready: kafka)
2022-03-30 20:09:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-4 not ready: kafka)
2022-03-30 20:09:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-5 not ready: kafka)
2022-03-30 20:09:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-6 not ready: kafka)
2022-03-30 20:09:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2a79abff-kafka-0, my-cluster-2a79abff-kafka-1, my-cluster-2a79abff-kafka-2, my-cluster-2a79abff-kafka-3, my-cluster-2a79abff-kafka-4, my-cluster-2a79abff-kafka-5, my-cluster-2a79abff-kafka-6 are ready
2022-03-30 20:09:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4155719ms till timeout)
2022-03-30 20:09:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:09:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=b486eb43-b9c3-4022-abdb-47a9bca85543, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:09:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=b486eb43-b9c3-4022-abdb-47a9bca85543, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:09:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-zookeeper-2 hasn't rolled
2022-03-30 20:09:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-zookeeper rolling update not ready, will try again in 5000 ms (1699871ms till timeout)
2022-03-30 20:09:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:09:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:09:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:09:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-3 not ready: kafka)
2022-03-30 20:09:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-4 not ready: kafka)
2022-03-30 20:09:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-5 not ready: kafka)
2022-03-30 20:09:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-6 not ready: kafka)
2022-03-30 20:09:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2a79abff-kafka-0, my-cluster-2a79abff-kafka-1, my-cluster-2a79abff-kafka-2, my-cluster-2a79abff-kafka-3, my-cluster-2a79abff-kafka-4, my-cluster-2a79abff-kafka-5, my-cluster-2a79abff-kafka-6 are ready
2022-03-30 20:09:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4154711ms till timeout)
2022-03-30 20:09:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:09:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:09:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:09:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-3 not ready: kafka)
2022-03-30 20:09:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-4 not ready: kafka)
2022-03-30 20:09:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-5 not ready: kafka)
2022-03-30 20:09:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-6 not ready: kafka)
2022-03-30 20:09:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2a79abff-kafka-0, my-cluster-2a79abff-kafka-1, my-cluster-2a79abff-kafka-2, my-cluster-2a79abff-kafka-3, my-cluster-2a79abff-kafka-4, my-cluster-2a79abff-kafka-5, my-cluster-2a79abff-kafka-6 are ready
2022-03-30 20:09:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4153698ms till timeout)
2022-03-30 20:09:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-d4963f37-kafka-clients not ready, will try again in 10000 ms (459956ms till timeout)
2022-03-30 20:09:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:09:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:09:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:09:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-3 not ready: kafka)
2022-03-30 20:09:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-4 not ready: kafka)
2022-03-30 20:09:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-5 not ready: kafka)
2022-03-30 20:09:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-6 not ready: kafka)
2022-03-30 20:09:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2a79abff-kafka-0, my-cluster-2a79abff-kafka-1, my-cluster-2a79abff-kafka-2, my-cluster-2a79abff-kafka-3, my-cluster-2a79abff-kafka-4, my-cluster-2a79abff-kafka-5, my-cluster-2a79abff-kafka-6 are ready
2022-03-30 20:09:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4152692ms till timeout)
2022-03-30 20:09:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:09:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:09:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:09:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-3 not ready: kafka)
2022-03-30 20:09:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-4 not ready: kafka)
2022-03-30 20:09:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-5 not ready: kafka)
2022-03-30 20:09:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-6 not ready: kafka)
2022-03-30 20:09:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2a79abff-kafka-0, my-cluster-2a79abff-kafka-1, my-cluster-2a79abff-kafka-2, my-cluster-2a79abff-kafka-3, my-cluster-2a79abff-kafka-4, my-cluster-2a79abff-kafka-5, my-cluster-2a79abff-kafka-6 are ready
2022-03-30 20:09:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4151684ms till timeout)
2022-03-30 20:09:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:09:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:09:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:09:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-3 not ready: kafka)
2022-03-30 20:09:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-4 not ready: kafka)
2022-03-30 20:09:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-5 not ready: kafka)
2022-03-30 20:09:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-6 not ready: kafka)
2022-03-30 20:09:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2a79abff-kafka-0, my-cluster-2a79abff-kafka-1, my-cluster-2a79abff-kafka-2, my-cluster-2a79abff-kafka-3, my-cluster-2a79abff-kafka-4, my-cluster-2a79abff-kafka-5, my-cluster-2a79abff-kafka-6 are ready
2022-03-30 20:09:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4150677ms till timeout)
2022-03-30 20:09:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:09:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=b486eb43-b9c3-4022-abdb-47a9bca85543, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:09:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=b486eb43-b9c3-4022-abdb-47a9bca85543, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:09:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-zookeeper-2 hasn't rolled
2022-03-30 20:09:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-zookeeper rolling update not ready, will try again in 5000 ms (1694865ms till timeout)
2022-03-30 20:09:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:09:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:09:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:09:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-3 not ready: kafka)
2022-03-30 20:09:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-4 not ready: kafka)
2022-03-30 20:09:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-5 not ready: kafka)
2022-03-30 20:09:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-6 not ready: kafka)
2022-03-30 20:09:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2a79abff-kafka-0, my-cluster-2a79abff-kafka-1, my-cluster-2a79abff-kafka-2, my-cluster-2a79abff-kafka-3, my-cluster-2a79abff-kafka-4, my-cluster-2a79abff-kafka-5, my-cluster-2a79abff-kafka-6 are ready
2022-03-30 20:09:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4149671ms till timeout)
2022-03-30 20:09:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:09:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:09:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:09:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-3 not ready: kafka)
2022-03-30 20:09:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-4 not ready: kafka)
2022-03-30 20:09:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-5 not ready: kafka)
2022-03-30 20:09:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-6 not ready: kafka)
2022-03-30 20:09:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2a79abff-kafka-0, my-cluster-2a79abff-kafka-1, my-cluster-2a79abff-kafka-2, my-cluster-2a79abff-kafka-3, my-cluster-2a79abff-kafka-4, my-cluster-2a79abff-kafka-5, my-cluster-2a79abff-kafka-6 are ready
2022-03-30 20:09:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (4148665ms till timeout)
2022-03-30 20:09:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:09:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:09:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:09:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-3 not ready: kafka)
2022-03-30 20:09:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-4 not ready: kafka)
2022-03-30 20:09:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-5 not ready: kafka)
2022-03-30 20:09:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-6 not ready: kafka)
2022-03-30 20:09:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2a79abff-kafka-0, my-cluster-2a79abff-kafka-1, my-cluster-2a79abff-kafka-2, my-cluster-2a79abff-kafka-3, my-cluster-2a79abff-kafka-4, my-cluster-2a79abff-kafka-5, my-cluster-2a79abff-kafka-6 are ready
2022-03-30 20:09:28 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-2a79abff will have desired state: Ready
2022-03-30 20:09:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-2a79abff will have desired state: Ready
2022-03-30 20:09:28 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-2a79abff is in desired state: Ready
2022-03-30 20:09:28 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-2a79abff is ready
2022-03-30 20:09:28 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateST:327] Kafka scale up to 7 finished
2022-03-30 20:09:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Consumer will successfully receive messages.
2022-03-30 20:09:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@55234e9c, which are set.
2022-03-30 20:09:28 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@26677f25, messages=[], arguments=[USER=my_user_838849261_890460734, --group-id, my-consumer-group-261941106, --topic, my-topic-2138084746-704381117, --group-instance-id, instance1580069275, --bootstrap-server, my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-2a79abff-kafka-clients-6bfb76b8f-f4rf2', podNamespace='namespace-4', bootstrapServer='my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093', topicName='my-topic-2138084746-704381117', maxMessages=100, kafkaUsername='my-user-838849261-890460734', consumerGroupName='my-consumer-group-261941106', consumerInstanceId='instance1580069275', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@55234e9c}
2022-03-30 20:09:28 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093:my-topic-2138084746-704381117 from pod my-cluster-2a79abff-kafka-clients-6bfb76b8f-f4rf2
2022-03-30 20:09:28 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-2a79abff-kafka-clients-6bfb76b8f-f4rf2 -n namespace-4 -- /opt/kafka/consumer.sh USER=my_user_838849261_890460734 --group-id my-consumer-group-261941106 --topic my-topic-2138084746-704381117 --group-instance-id instance1580069275 --bootstrap-server my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093 --max-messages 100
2022-03-30 20:09:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-2a79abff-kafka-clients-6bfb76b8f-f4rf2 -n namespace-4 -- /opt/kafka/consumer.sh USER=my_user_838849261_890460734 --group-id my-consumer-group-261941106 --topic my-topic-2138084746-704381117 --group-instance-id instance1580069275 --bootstrap-server my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093 --max-messages 100
2022-03-30 20:09:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:09:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=b486eb43-b9c3-4022-abdb-47a9bca85543, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:09:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=b486eb43-b9c3-4022-abdb-47a9bca85543, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:09:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-zookeeper-2 hasn't rolled
2022-03-30 20:09:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-zookeeper rolling update not ready, will try again in 5000 ms (1689859ms till timeout)
2022-03-30 20:09:32 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-d4963f37-kafka-clients not ready, will try again in 10000 ms (449946ms till timeout)
2022-03-30 20:09:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:35 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 20:09:35 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-03-30 20:09:35 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateST:339] Scale up Zookeeper to 5
2022-03-30 20:09:35 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:127] Waiting for 5 Pod(s) of my-cluster-2a79abff-zookeeper to be ready
2022-03-30 20:09:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-30 20:09:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:09:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2999996ms till timeout)
2022-03-30 20:09:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:09:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=b486eb43-b9c3-4022-abdb-47a9bca85543, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:09:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=b486eb43-b9c3-4022-abdb-47a9bca85543, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:09:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-zookeeper-2 hasn't rolled
2022-03-30 20:09:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-zookeeper rolling update not ready, will try again in 5000 ms (1684854ms till timeout)
2022-03-30 20:09:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:09:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2998991ms till timeout)
2022-03-30 20:09:37 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:09:37 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2997985ms till timeout)
2022-03-30 20:09:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:09:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2996979ms till timeout)
2022-03-30 20:09:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:09:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2995974ms till timeout)
2022-03-30 20:09:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:09:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2994969ms till timeout)
2022-03-30 20:09:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:09:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=b486eb43-b9c3-4022-abdb-47a9bca85543, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:09:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=b486eb43-b9c3-4022-abdb-47a9bca85543, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:09:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-zookeeper-2 hasn't rolled
2022-03-30 20:09:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-zookeeper rolling update not ready, will try again in 5000 ms (1679848ms till timeout)
2022-03-30 20:09:41 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:09:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2993963ms till timeout)
2022-03-30 20:09:42 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-963888555-1712711326 in namespace namespace-5
2022-03-30 20:09:42 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-963888555-1712711326
2022-03-30 20:09:42 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-963888555-1712711326 not ready, will try again in 10000 ms (179989ms till timeout)
2022-03-30 20:09:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:09:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2992956ms till timeout)
2022-03-30 20:09:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:09:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2991950ms till timeout)
2022-03-30 20:09:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:09:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2990945ms till timeout)
2022-03-30 20:09:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:09:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2989939ms till timeout)
2022-03-30 20:09:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-zookeeper-0=1cb62efa-0d6a-4b1f-b525-a301808ce33d, my-cluster-cae962ba-zookeeper-1=0189c884-cb2c-47c7-9d6b-84e8ee6a24e1, my-cluster-cae962ba-zookeeper-2=81570d10-7720-4055-b54b-9f960f3c50fd}
2022-03-30 20:09:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=b486eb43-b9c3-4022-abdb-47a9bca85543, my-cluster-cae962ba-zookeeper-2=0efed69d-7436-4845-b4b5-4b8eae901e33}
2022-03-30 20:09:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-zookeeper-0=ae3da2f9-da5b-41aa-9098-e8f0ce9b0231, my-cluster-cae962ba-zookeeper-1=b486eb43-b9c3-4022-abdb-47a9bca85543, my-cluster-cae962ba-zookeeper-2=0efed69d-7436-4845-b4b5-4b8eae901e33}
2022-03-30 20:09:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:55] All pods seem to have rolled
2022-03-30 20:09:46 [ForkJoinPool-3-worker-15] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-cae962ba-zookeeper has been successfully rolled
2022-03-30 20:09:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:87] Component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={}) successfully rolled
2022-03-30 20:09:46 [ForkJoinPool-3-worker-15] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-cae962ba-zookeeper to be ready
2022-03-30 20:09:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-30 20:09:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:09:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:09:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:09:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1799996ms till timeout)
2022-03-30 20:09:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:09:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2988934ms till timeout)
2022-03-30 20:09:47 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:09:47 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:09:47 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:09:47 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1798990ms till timeout)
2022-03-30 20:09:47 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:09:47 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2987929ms till timeout)
2022-03-30 20:09:48 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:09:48 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:09:48 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:09:48 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1797985ms till timeout)
2022-03-30 20:09:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:09:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2986923ms till timeout)
2022-03-30 20:09:49 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:09:49 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:09:49 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:09:49 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1796979ms till timeout)
2022-03-30 20:09:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:09:49 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2985918ms till timeout)
2022-03-30 20:09:50 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:09:50 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:09:50 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:09:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1795973ms till timeout)
2022-03-30 20:09:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:09:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2984911ms till timeout)
2022-03-30 20:09:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:09:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:09:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:09:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1794968ms till timeout)
2022-03-30 20:09:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:09:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2983905ms till timeout)
2022-03-30 20:09:52 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:09:52 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:09:52 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:09:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1793962ms till timeout)
2022-03-30 20:09:52 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-d4963f37 in namespace namespace-5
2022-03-30 20:09:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-d4963f37
2022-03-30 20:09:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-d4963f37 not ready, will try again in 10000 ms (839986ms till timeout)
2022-03-30 20:09:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:09:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2982900ms till timeout)
2022-03-30 20:09:53 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:09:53 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:09:53 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:09:53 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1792956ms till timeout)
2022-03-30 20:09:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:09:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2981895ms till timeout)
2022-03-30 20:09:54 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:09:54 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:09:54 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:09:54 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1791951ms till timeout)
2022-03-30 20:09:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:09:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2980889ms till timeout)
2022-03-30 20:09:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:09:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:09:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:09:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1790946ms till timeout)
2022-03-30 20:09:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:09:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2979881ms till timeout)
2022-03-30 20:09:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:09:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:09:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:09:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1789940ms till timeout)
2022-03-30 20:09:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:09:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2978876ms till timeout)
2022-03-30 20:09:57 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:09:57 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:09:57 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:09:57 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1788935ms till timeout)
2022-03-30 20:09:57 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:09:57 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2977871ms till timeout)
2022-03-30 20:09:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:09:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:09:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:09:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1787930ms till timeout)
2022-03-30 20:09:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:09:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2976866ms till timeout)
2022-03-30 20:09:59 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:09:59 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:09:59 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:09:59 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1786925ms till timeout)
2022-03-30 20:09:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:09:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:09:59 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:09:59 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2975861ms till timeout)
2022-03-30 20:10:00 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:00 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:00 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1785919ms till timeout)
2022-03-30 20:10:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:10:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2974856ms till timeout)
2022-03-30 20:10:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:10:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1784913ms till timeout)
2022-03-30 20:10:01 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2973850ms till timeout)
2022-03-30 20:10:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:02 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1783908ms till timeout)
2022-03-30 20:10:02 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:10:02 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-5 for test case:testSendMessagesCustomListenerTlsScramSha
2022-03-30 20:10:02 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-5 removal
2022-03-30 20:10:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:02 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:02 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:10:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (479911ms till timeout)
2022-03-30 20:10:02 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2972845ms till timeout)
2022-03-30 20:10:03 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:03 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:03 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:03 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1782902ms till timeout)
2022-03-30 20:10:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:03 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:10:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (478833ms till timeout)
2022-03-30 20:10:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2971840ms till timeout)
2022-03-30 20:10:04 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:04 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:04 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:04 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1781897ms till timeout)
2022-03-30 20:10:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:10:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:04 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:04 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:10:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (477759ms till timeout)
2022-03-30 20:10:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2970835ms till timeout)
2022-03-30 20:10:05 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:05 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:05 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1780892ms till timeout)
2022-03-30 20:10:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:10:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:05 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:05 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:10:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (476668ms till timeout)
2022-03-30 20:10:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2969829ms till timeout)
2022-03-30 20:10:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:10:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1779886ms till timeout)
2022-03-30 20:10:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2968824ms till timeout)
2022-03-30 20:10:06 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:06 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:10:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (475587ms till timeout)
2022-03-30 20:10:07 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:07 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:07 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:07 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1778881ms till timeout)
2022-03-30 20:10:07 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:07 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2967819ms till timeout)
2022-03-30 20:10:07 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:07 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:07 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:10:07 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (474516ms till timeout)
2022-03-30 20:10:08 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:08 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:08 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1777875ms till timeout)
2022-03-30 20:10:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2966813ms till timeout)
2022-03-30 20:10:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:09 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:09 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:10:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (473432ms till timeout)
2022-03-30 20:10:09 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:09 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:09 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:09 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1776870ms till timeout)
2022-03-30 20:10:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:10:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:09 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2965808ms till timeout)
2022-03-30 20:10:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:10:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (472357ms till timeout)
2022-03-30 20:10:10 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:10 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:10 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1775865ms till timeout)
2022-03-30 20:10:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:10:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2964803ms till timeout)
2022-03-30 20:10:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:10:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:10:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (471272ms till timeout)
2022-03-30 20:10:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1774859ms till timeout)
2022-03-30 20:10:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2963797ms till timeout)
2022-03-30 20:10:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:12 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1773852ms till timeout)
2022-03-30 20:10:12 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:12 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:10:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (470024ms till timeout)
2022-03-30 20:10:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2962793ms till timeout)
2022-03-30 20:10:13 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:13 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:13 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:13 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1772847ms till timeout)
2022-03-30 20:10:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:13 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:13 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:10:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (468946ms till timeout)
2022-03-30 20:10:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2961787ms till timeout)
2022-03-30 20:10:14 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:14 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:14 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:14 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1771841ms till timeout)
2022-03-30 20:10:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:10:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:14 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:14 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:10:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (467875ms till timeout)
2022-03-30 20:10:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2960781ms till timeout)
2022-03-30 20:10:15 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:15 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:15 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1770836ms till timeout)
2022-03-30 20:10:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:10:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:10:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (466787ms till timeout)
2022-03-30 20:10:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2959776ms till timeout)
2022-03-30 20:10:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:10:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1769830ms till timeout)
2022-03-30 20:10:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:16 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:16 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:10:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (465714ms till timeout)
2022-03-30 20:10:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2958769ms till timeout)
2022-03-30 20:10:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:17 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1768825ms till timeout)
2022-03-30 20:10:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:10:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (464636ms till timeout)
2022-03-30 20:10:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:17 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2957764ms till timeout)
2022-03-30 20:10:18 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:18 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:18 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:18 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1767820ms till timeout)
2022-03-30 20:10:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2956759ms till timeout)
2022-03-30 20:10:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:10:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (463557ms till timeout)
2022-03-30 20:10:19 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:19 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:19 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1766814ms till timeout)
2022-03-30 20:10:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:10:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:19 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2955754ms till timeout)
2022-03-30 20:10:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:10:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (462484ms till timeout)
2022-03-30 20:10:20 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:20 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:20 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1765809ms till timeout)
2022-03-30 20:10:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:10:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2954749ms till timeout)
2022-03-30 20:10:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:10:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:21 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:21 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:10:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (461401ms till timeout)
2022-03-30 20:10:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1764804ms till timeout)
2022-03-30 20:10:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2953743ms till timeout)
2022-03-30 20:10:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:10:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (460322ms till timeout)
2022-03-30 20:10:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:22 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1763799ms till timeout)
2022-03-30 20:10:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2952738ms till timeout)
2022-03-30 20:10:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:23 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:23 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:10:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (459239ms till timeout)
2022-03-30 20:10:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:23 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1762794ms till timeout)
2022-03-30 20:10:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2951730ms till timeout)
2022-03-30 20:10:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:24 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:24 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:10:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (458144ms till timeout)
2022-03-30 20:10:24 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:24 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:24 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:24 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1761788ms till timeout)
2022-03-30 20:10:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:10:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2950724ms till timeout)
2022-03-30 20:10:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:25 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:25 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:25 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1760783ms till timeout)
2022-03-30 20:10:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:10:25 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:25 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:10:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (457066ms till timeout)
2022-03-30 20:10:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2949719ms till timeout)
2022-03-30 20:10:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:10:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1759778ms till timeout)
2022-03-30 20:10:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:10:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (455991ms till timeout)
2022-03-30 20:10:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2948714ms till timeout)
2022-03-30 20:10:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1758772ms till timeout)
2022-03-30 20:10:27 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:27 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:27 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:10:27 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (454909ms till timeout)
2022-03-30 20:10:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2947709ms till timeout)
2022-03-30 20:10:28 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:28 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:28 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1757767ms till timeout)
2022-03-30 20:10:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:28 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:28 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:10:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (453825ms till timeout)
2022-03-30 20:10:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2946704ms till timeout)
2022-03-30 20:10:29 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:29 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:29 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:29 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1756762ms till timeout)
2022-03-30 20:10:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:10:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:29 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace infra-namespace get Namespace namespace-5 -o yaml
2022-03-30 20:10:29 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 20:10:29 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 20:10:29 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-5" not found
2022-03-30 20:10:29 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 20:10:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[namespace-4], io.strimzi.test.logs.CollectorElement@3d7a59f3=[listeners-st], io.strimzi.test.logs.CollectorElement@dce7372d=[rolling-update-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-3], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-30 20:10:29 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:267] testSendMessagesCustomListenerTlsScramSha - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendSimpleMessageTls, testUpdateUser, testCruiseControlBasicAPIRequests, testReceiveSimpleMessageTls, testSendMessagesTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testSendMessagesCustomListenerTlsScramSha] to and randomly select one to start execution
2022-03-30 20:10:29 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:93] [kafka.listeners.ListenersST] - Removing parallel test: testSendMessagesCustomListenerTlsScramSha
2022-03-30 20:10:29 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:97] [kafka.listeners.ListenersST] - Parallel test count: 2
2022-03-30 20:10:29 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesCustomListenerTlsScramSha-FINISHED
2022-03-30 20:10:29 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 20:10:29 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 20:10:29 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:690] [kafka.listeners.ListenersST - After All] - Clean up after test suite
2022-03-30 20:10:29 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:10:29 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:346] In context ListenersST is everything deleted.
2022-03-30 20:10:29 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:10:29 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Namespace listeners-st removal
2022-03-30 20:10:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-30 20:10:29 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-30 20:10:29 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:10:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace listeners-st removal not ready, will try again in 1000 ms (479920ms till timeout)
2022-03-30 20:10:29 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:29 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2945699ms till timeout)
2022-03-30 20:10:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1755757ms till timeout)
2022-03-30 20:10:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 4
2022-03-30 20:10:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-30 20:10:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-30 20:10:30 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:10:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace listeners-st removal not ready, will try again in 1000 ms (478851ms till timeout)
2022-03-30 20:10:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 4
2022-03-30 20:10:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2944694ms till timeout)
2022-03-30 20:10:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1754751ms till timeout)
2022-03-30 20:10:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-30 20:10:31 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-30 20:10:31 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:10:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace listeners-st removal not ready, will try again in 1000 ms (477778ms till timeout)
2022-03-30 20:10:31 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:31 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2943689ms till timeout)
2022-03-30 20:10:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1753746ms till timeout)
2022-03-30 20:10:32 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-30 20:10:32 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2942684ms till timeout)
2022-03-30 20:10:32 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-30 20:10:32 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:10:32 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace listeners-st removal not ready, will try again in 1000 ms (476705ms till timeout)
2022-03-30 20:10:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:33 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1752741ms till timeout)
2022-03-30 20:10:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2941678ms till timeout)
2022-03-30 20:10:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-30 20:10:34 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-30 20:10:34 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:10:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace listeners-st removal not ready, will try again in 1000 ms (475631ms till timeout)
2022-03-30 20:10:34 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:34 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:34 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:34 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1751736ms till timeout)
2022-03-30 20:10:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 4
2022-03-30 20:10:34 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2940672ms till timeout)
2022-03-30 20:10:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-30 20:10:35 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace infra-namespace get Namespace listeners-st -o yaml
2022-03-30 20:10:35 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 20:10:35 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 20:10:35 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "listeners-st" not found
2022-03-30 20:10:35 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 20:10:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[namespace-4], io.strimzi.test.logs.CollectorElement@3d7a59f3=[], io.strimzi.test.logs.CollectorElement@dce7372d=[rolling-update-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-3], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-30 20:10:35 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:254] ListenersST - Notifies waiting test suites:[HttpBridgeTlsST, CruiseControlApiST, UserST, CruiseControlST, ListenersST] to and randomly select one to start execution
2022-03-30 20:10:35 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:85] [kafka.listeners.ListenersST] - Removing parallel suite: ListenersST
2022-03-30 20:10:35 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:89] [kafka.listeners.ListenersST] - Parallel suites count: 3
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 406.062 s - in io.strimzi.systemtest.kafka.listeners.ListenersST
[[1;34mINFO[m] Running io.strimzi.systemtest.connect.ConnectIsolatedST
2022-03-30 20:10:35 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 20:10:35 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:667] [connect.ConnectIsolatedST - Before All] - Setup test suite environment
2022-03-30 20:10:35 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 20:10:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:10:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:10:35 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:35 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:35 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-zookeeper-2)
2022-03-30 20:10:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1750731ms till timeout)
2022-03-30 20:10:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:10:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2939667ms till timeout)
2022-03-30 20:10:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-2 not ready: zookeeper)
2022-03-30 20:10:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-zookeeper-0, my-cluster-cae962ba-zookeeper-1, my-cluster-cae962ba-zookeeper-2 are ready
2022-03-30 20:10:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1749726ms till timeout)
2022-03-30 20:10:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2938662ms till timeout)
2022-03-30 20:10:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-2 not ready: zookeeper)
2022-03-30 20:10:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-zookeeper-0, my-cluster-cae962ba-zookeeper-1, my-cluster-cae962ba-zookeeper-2 are ready
2022-03-30 20:10:37 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1748721ms till timeout)
2022-03-30 20:10:37 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:37 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2937657ms till timeout)
2022-03-30 20:10:38 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:38 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:38 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-2 not ready: zookeeper)
2022-03-30 20:10:38 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-zookeeper-0, my-cluster-cae962ba-zookeeper-1, my-cluster-cae962ba-zookeeper-2 are ready
2022-03-30 20:10:38 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1747715ms till timeout)
2022-03-30 20:10:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2936651ms till timeout)
2022-03-30 20:10:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:10:39 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:39 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:39 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-2 not ready: zookeeper)
2022-03-30 20:10:39 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-zookeeper-0, my-cluster-cae962ba-zookeeper-1, my-cluster-cae962ba-zookeeper-2 are ready
2022-03-30 20:10:39 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1746710ms till timeout)
2022-03-30 20:10:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2935646ms till timeout)
2022-03-30 20:10:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:10:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:10:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-2 not ready: zookeeper)
2022-03-30 20:10:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-zookeeper-0, my-cluster-cae962ba-zookeeper-1, my-cluster-cae962ba-zookeeper-2 are ready
2022-03-30 20:10:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1745705ms till timeout)
2022-03-30 20:10:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:10:41 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2934640ms till timeout)
2022-03-30 20:10:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-2 not ready: zookeeper)
2022-03-30 20:10:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-zookeeper-0, my-cluster-cae962ba-zookeeper-1, my-cluster-cae962ba-zookeeper-2 are ready
2022-03-30 20:10:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1744700ms till timeout)
2022-03-30 20:10:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2933635ms till timeout)
2022-03-30 20:10:42 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:42 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:42 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-2 not ready: zookeeper)
2022-03-30 20:10:42 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-zookeeper-0, my-cluster-cae962ba-zookeeper-1, my-cluster-cae962ba-zookeeper-2 are ready
2022-03-30 20:10:42 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1743695ms till timeout)
2022-03-30 20:10:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2932630ms till timeout)
2022-03-30 20:10:43 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:43 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:43 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-2 not ready: zookeeper)
2022-03-30 20:10:43 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-zookeeper-0, my-cluster-cae962ba-zookeeper-1, my-cluster-cae962ba-zookeeper-2 are ready
2022-03-30 20:10:43 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1742690ms till timeout)
2022-03-30 20:10:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-30 20:10:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2931625ms till timeout)
2022-03-30 20:10:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:10:44 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:44 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:44 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-2 not ready: zookeeper)
2022-03-30 20:10:44 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-zookeeper-0, my-cluster-cae962ba-zookeeper-1, my-cluster-cae962ba-zookeeper-2 are ready
2022-03-30 20:10:44 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1741685ms till timeout)
2022-03-30 20:10:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:10:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:10:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-zookeeper-4)
2022-03-30 20:10:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2930620ms till timeout)
2022-03-30 20:10:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:10:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:10:45 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:45 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:45 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-2 not ready: zookeeper)
2022-03-30 20:10:45 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-zookeeper-0, my-cluster-cae962ba-zookeeper-1, my-cluster-cae962ba-zookeeper-2 are ready
2022-03-30 20:10:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-zookeeper, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1740679ms till timeout)
2022-03-30 20:10:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:10:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:10:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:10:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-zookeeper-4)
2022-03-30 20:10:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2929615ms till timeout)
2022-03-30 20:10:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-zookeeper-2 not ready: zookeeper)
2022-03-30 20:10:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-zookeeper-0, my-cluster-cae962ba-zookeeper-1, my-cluster-cae962ba-zookeeper-2 are ready
2022-03-30 20:10:46 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:304] Wait for kafka to rolling restart ...
2022-03-30 20:10:46 [ForkJoinPool-3-worker-15] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-cae962ba-kafka rolling update
2022-03-30 20:10:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:74] Waiting for rolling update of component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})
2022-03-30 20:10:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for component with name my-cluster-cae962ba-kafka rolling update
2022-03-30 20:10:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:10:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:10:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:10:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-kafka-2 hasn't rolled
2022-03-30 20:10:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-kafka rolling update not ready, will try again in 5000 ms (1799995ms till timeout)
2022-03-30 20:10:47 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:47 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:47 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:10:47 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:10:47 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-zookeeper-4)
2022-03-30 20:10:47 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2928609ms till timeout)
2022-03-30 20:10:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:10:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:10:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-zookeeper-4)
2022-03-30 20:10:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2927601ms till timeout)
2022-03-30 20:10:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:10:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:10:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-zookeeper-4)
2022-03-30 20:10:49 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2926595ms till timeout)
2022-03-30 20:10:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:10:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:10:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:10:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-zookeeper-4)
2022-03-30 20:10:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2925588ms till timeout)
2022-03-30 20:10:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:10:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:10:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:10:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:10:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:10:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-zookeeper-4)
2022-03-30 20:10:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2924583ms till timeout)
2022-03-30 20:10:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:10:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:10:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:10:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-kafka-2 hasn't rolled
2022-03-30 20:10:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-kafka rolling update not ready, will try again in 5000 ms (1794990ms till timeout)
2022-03-30 20:10:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:10:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:10:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-zookeeper-4)
2022-03-30 20:10:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2923577ms till timeout)
2022-03-30 20:10:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:10:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:10:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-zookeeper-4)
2022-03-30 20:10:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2922570ms till timeout)
2022-03-30 20:10:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:10:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:10:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-zookeeper-4)
2022-03-30 20:10:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2921565ms till timeout)
2022-03-30 20:10:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:10:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:10:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:10:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-zookeeper-4)
2022-03-30 20:10:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2920558ms till timeout)
2022-03-30 20:10:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:10:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:10:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:10:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:10:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:10:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-zookeeper-4)
2022-03-30 20:10:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2919553ms till timeout)
2022-03-30 20:10:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:10:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:10:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:10:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-kafka-2 hasn't rolled
2022-03-30 20:10:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-kafka rolling update not ready, will try again in 5000 ms (1789984ms till timeout)
2022-03-30 20:10:57 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:57 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:57 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:10:57 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:10:57 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-zookeeper-4)
2022-03-30 20:10:57 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2918547ms till timeout)
2022-03-30 20:10:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:10:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:10:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-zookeeper-4)
2022-03-30 20:10:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2917541ms till timeout)
2022-03-30 20:10:59 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:10:59 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:10:59 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:10:59 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:10:59 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-zookeeper-4)
2022-03-30 20:10:59 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2916533ms till timeout)
2022-03-30 20:10:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:10:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:11:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:11:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:11:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:11:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-zookeeper-4)
2022-03-30 20:11:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2915527ms till timeout)
2022-03-30 20:11:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:01 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:11:01 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:11:01 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:11:01 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:11:01 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-zookeeper-4)
2022-03-30 20:11:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2914521ms till timeout)
2022-03-30 20:11:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-kafka-2 hasn't rolled
2022-03-30 20:11:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-kafka rolling update not ready, will try again in 5000 ms (1784978ms till timeout)
2022-03-30 20:11:02 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:11:02 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:11:02 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:11:02 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:11:02 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-zookeeper-4)
2022-03-30 20:11:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2913515ms till timeout)
2022-03-30 20:11:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:11:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:11:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:11:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:11:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-zookeeper-4)
2022-03-30 20:11:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2912509ms till timeout)
2022-03-30 20:11:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:11:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:11:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:11:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:11:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-zookeeper-4)
2022-03-30 20:11:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2911503ms till timeout)
2022-03-30 20:11:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:11:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:11:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:11:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:11:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-zookeeper-4)
2022-03-30 20:11:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2910498ms till timeout)
2022-03-30 20:11:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:11:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:11:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:11:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:11:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-zookeeper-4)
2022-03-30 20:11:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2909492ms till timeout)
2022-03-30 20:11:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-kafka-2 hasn't rolled
2022-03-30 20:11:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-kafka rolling update not ready, will try again in 5000 ms (1779972ms till timeout)
2022-03-30 20:11:07 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:11:07 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:11:07 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:11:07 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:11:07 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-zookeeper-4)
2022-03-30 20:11:07 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2908486ms till timeout)
2022-03-30 20:11:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:11:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:11:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:11:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:11:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-zookeeper-4)
2022-03-30 20:11:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2907481ms till timeout)
2022-03-30 20:11:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:11:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:11:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:11:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:11:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-zookeeper-4)
2022-03-30 20:11:09 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2906475ms till timeout)
2022-03-30 20:11:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:11:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:11:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:11:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:11:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-zookeeper-4)
2022-03-30 20:11:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2905469ms till timeout)
2022-03-30 20:11:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:11:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:11:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:11:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:11:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-zookeeper-4)
2022-03-30 20:11:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2904463ms till timeout)
2022-03-30 20:11:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-kafka-2 hasn't rolled
2022-03-30 20:11:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-kafka rolling update not ready, will try again in 5000 ms (1774967ms till timeout)
2022-03-30 20:11:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:11:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:11:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:11:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:11:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-zookeeper-4)
2022-03-30 20:11:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2903457ms till timeout)
2022-03-30 20:11:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:11:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:11:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:11:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:11:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-zookeeper-4)
2022-03-30 20:11:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2902452ms till timeout)
2022-03-30 20:11:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:11:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:11:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:11:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:11:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-zookeeper-4)
2022-03-30 20:11:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2901446ms till timeout)
2022-03-30 20:11:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:11:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:11:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:11:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:11:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-zookeeper-4)
2022-03-30 20:11:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2900440ms till timeout)
2022-03-30 20:11:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:11:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:11:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:11:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:11:16 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-zookeeper-4)
2022-03-30 20:11:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2899435ms till timeout)
2022-03-30 20:11:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-kafka-2 hasn't rolled
2022-03-30 20:11:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-kafka rolling update not ready, will try again in 5000 ms (1769961ms till timeout)
2022-03-30 20:11:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:11:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:11:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:11:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:11:17 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-zookeeper-4)
2022-03-30 20:11:17 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2898429ms till timeout)
2022-03-30 20:11:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:11:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:11:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:11:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:11:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-4 not ready: zookeeper)
2022-03-30 20:11:18 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2a79abff-zookeeper-0, my-cluster-2a79abff-zookeeper-1, my-cluster-2a79abff-zookeeper-2, my-cluster-2a79abff-zookeeper-3, my-cluster-2a79abff-zookeeper-4 are ready
2022-03-30 20:11:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2897423ms till timeout)
2022-03-30 20:11:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:11:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:11:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:11:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:11:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-4 not ready: zookeeper)
2022-03-30 20:11:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2a79abff-zookeeper-0, my-cluster-2a79abff-zookeeper-1, my-cluster-2a79abff-zookeeper-2, my-cluster-2a79abff-zookeeper-3, my-cluster-2a79abff-zookeeper-4 are ready
2022-03-30 20:11:19 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2896417ms till timeout)
2022-03-30 20:11:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:11:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:11:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:11:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:11:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-4 not ready: zookeeper)
2022-03-30 20:11:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2a79abff-zookeeper-0, my-cluster-2a79abff-zookeeper-1, my-cluster-2a79abff-zookeeper-2, my-cluster-2a79abff-zookeeper-3, my-cluster-2a79abff-zookeeper-4 are ready
2022-03-30 20:11:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2895412ms till timeout)
2022-03-30 20:11:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:11:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:11:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:11:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:11:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-4 not ready: zookeeper)
2022-03-30 20:11:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2a79abff-zookeeper-0, my-cluster-2a79abff-zookeeper-1, my-cluster-2a79abff-zookeeper-2, my-cluster-2a79abff-zookeeper-3, my-cluster-2a79abff-zookeeper-4 are ready
2022-03-30 20:11:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2894406ms till timeout)
2022-03-30 20:11:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-kafka-2 hasn't rolled
2022-03-30 20:11:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-kafka rolling update not ready, will try again in 5000 ms (1764956ms till timeout)
2022-03-30 20:11:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:11:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:11:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:11:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:11:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-4 not ready: zookeeper)
2022-03-30 20:11:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2a79abff-zookeeper-0, my-cluster-2a79abff-zookeeper-1, my-cluster-2a79abff-zookeeper-2, my-cluster-2a79abff-zookeeper-3, my-cluster-2a79abff-zookeeper-4 are ready
2022-03-30 20:11:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2893400ms till timeout)
2022-03-30 20:11:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:11:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:11:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:11:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:11:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-4 not ready: zookeeper)
2022-03-30 20:11:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2a79abff-zookeeper-0, my-cluster-2a79abff-zookeeper-1, my-cluster-2a79abff-zookeeper-2, my-cluster-2a79abff-zookeeper-3, my-cluster-2a79abff-zookeeper-4 are ready
2022-03-30 20:11:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2892395ms till timeout)
2022-03-30 20:11:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:11:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:11:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:11:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:11:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-4 not ready: zookeeper)
2022-03-30 20:11:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2a79abff-zookeeper-0, my-cluster-2a79abff-zookeeper-1, my-cluster-2a79abff-zookeeper-2, my-cluster-2a79abff-zookeeper-3, my-cluster-2a79abff-zookeeper-4 are ready
2022-03-30 20:11:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2891389ms till timeout)
2022-03-30 20:11:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:11:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:11:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:11:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:11:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-4 not ready: zookeeper)
2022-03-30 20:11:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2a79abff-zookeeper-0, my-cluster-2a79abff-zookeeper-1, my-cluster-2a79abff-zookeeper-2, my-cluster-2a79abff-zookeeper-3, my-cluster-2a79abff-zookeeper-4 are ready
2022-03-30 20:11:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2890383ms till timeout)
2022-03-30 20:11:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:11:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:11:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:11:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:11:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-4 not ready: zookeeper)
2022-03-30 20:11:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2a79abff-zookeeper-0, my-cluster-2a79abff-zookeeper-1, my-cluster-2a79abff-zookeeper-2, my-cluster-2a79abff-zookeeper-3, my-cluster-2a79abff-zookeeper-4 are ready
2022-03-30 20:11:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2889378ms till timeout)
2022-03-30 20:11:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-kafka-2 hasn't rolled
2022-03-30 20:11:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-kafka rolling update not ready, will try again in 5000 ms (1759950ms till timeout)
2022-03-30 20:11:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:11:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:11:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:11:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:11:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-4 not ready: zookeeper)
2022-03-30 20:11:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2a79abff-zookeeper-0, my-cluster-2a79abff-zookeeper-1, my-cluster-2a79abff-zookeeper-2, my-cluster-2a79abff-zookeeper-3, my-cluster-2a79abff-zookeeper-4 are ready
2022-03-30 20:11:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-zookeeper, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2888372ms till timeout)
2022-03-30 20:11:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-0 not ready: zookeeper)
2022-03-30 20:11:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-1 not ready: zookeeper)
2022-03-30 20:11:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-2 not ready: zookeeper)
2022-03-30 20:11:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-3 not ready: zookeeper)
2022-03-30 20:11:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-zookeeper-4 not ready: zookeeper)
2022-03-30 20:11:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2a79abff-zookeeper-0, my-cluster-2a79abff-zookeeper-1, my-cluster-2a79abff-zookeeper-2, my-cluster-2a79abff-zookeeper-3, my-cluster-2a79abff-zookeeper-4 are ready
2022-03-30 20:11:28 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-2a79abff will have desired state: Ready
2022-03-30 20:11:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-2a79abff will have desired state: Ready
2022-03-30 20:11:28 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-2a79abff is in desired state: Ready
2022-03-30 20:11:28 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-2a79abff is ready
2022-03-30 20:11:28 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateST:342] Kafka scale up to 5 finished
2022-03-30 20:11:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Consumer will successfully receive messages.
2022-03-30 20:11:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@154627c5, which are set.
2022-03-30 20:11:28 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@b7dcdc8, messages=[], arguments=[USER=my_user_838849261_890460734, --group-id, my-consumer-group-1764335140, --topic, my-topic-2138084746-704381117, --group-instance-id, instance1682539672, --bootstrap-server, my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-2a79abff-kafka-clients-6bfb76b8f-f4rf2', podNamespace='namespace-4', bootstrapServer='my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093', topicName='my-topic-2138084746-704381117', maxMessages=100, kafkaUsername='my-user-838849261-890460734', consumerGroupName='my-consumer-group-1764335140', consumerInstanceId='instance1682539672', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@154627c5}
2022-03-30 20:11:28 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093:my-topic-2138084746-704381117 from pod my-cluster-2a79abff-kafka-clients-6bfb76b8f-f4rf2
2022-03-30 20:11:28 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-2a79abff-kafka-clients-6bfb76b8f-f4rf2 -n namespace-4 -- /opt/kafka/consumer.sh USER=my_user_838849261_890460734 --group-id my-consumer-group-1764335140 --topic my-topic-2138084746-704381117 --group-instance-id instance1682539672 --bootstrap-server my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093 --max-messages 100
2022-03-30 20:11:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-2a79abff-kafka-clients-6bfb76b8f-f4rf2 -n namespace-4 -- /opt/kafka/consumer.sh USER=my_user_838849261_890460734 --group-id my-consumer-group-1764335140 --topic my-topic-2138084746-704381117 --group-instance-id instance1682539672 --bootstrap-server my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093 --max-messages 100
2022-03-30 20:11:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-kafka-2 hasn't rolled
2022-03-30 20:11:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-kafka rolling update not ready, will try again in 5000 ms (1754944ms till timeout)
2022-03-30 20:11:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:35 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 20:11:35 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-03-30 20:11:35 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateST:351] Scale down Kafka to 3
2022-03-30 20:11:35 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-2a79abff-kafka rolling update
2022-03-30 20:11:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:74] Waiting for rolling update of component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})
2022-03-30 20:11:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for component with name my-cluster-2a79abff-kafka rolling update
2022-03-30 20:11:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b, my-cluster-2a79abff-kafka-5=32cf6bff-71f0-4f25-b01a-9e315821920f, my-cluster-2a79abff-kafka-6=8fff144e-67f6-4908-a5a9-c2a55cf13928}
2022-03-30 20:11:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b, my-cluster-2a79abff-kafka-5=32cf6bff-71f0-4f25-b01a-9e315821920f, my-cluster-2a79abff-kafka-6=8fff144e-67f6-4908-a5a9-c2a55cf13928}
2022-03-30 20:11:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b, my-cluster-2a79abff-kafka-5=32cf6bff-71f0-4f25-b01a-9e315821920f, my-cluster-2a79abff-kafka-6=8fff144e-67f6-4908-a5a9-c2a55cf13928}
2022-03-30 20:11:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-1 hasn't rolled
2022-03-30 20:11:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (4199995ms till timeout)
2022-03-30 20:11:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-kafka-2 hasn't rolled
2022-03-30 20:11:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-kafka rolling update not ready, will try again in 5000 ms (1749939ms till timeout)
2022-03-30 20:11:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b, my-cluster-2a79abff-kafka-5=32cf6bff-71f0-4f25-b01a-9e315821920f, my-cluster-2a79abff-kafka-6=8fff144e-67f6-4908-a5a9-c2a55cf13928}
2022-03-30 20:11:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b, my-cluster-2a79abff-kafka-5=32cf6bff-71f0-4f25-b01a-9e315821920f, my-cluster-2a79abff-kafka-6=8fff144e-67f6-4908-a5a9-c2a55cf13928}
2022-03-30 20:11:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b, my-cluster-2a79abff-kafka-5=32cf6bff-71f0-4f25-b01a-9e315821920f, my-cluster-2a79abff-kafka-6=8fff144e-67f6-4908-a5a9-c2a55cf13928}
2022-03-30 20:11:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-1 hasn't rolled
2022-03-30 20:11:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (4194988ms till timeout)
2022-03-30 20:11:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-kafka-2 hasn't rolled
2022-03-30 20:11:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-kafka rolling update not ready, will try again in 5000 ms (1744934ms till timeout)
2022-03-30 20:11:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b, my-cluster-2a79abff-kafka-5=32cf6bff-71f0-4f25-b01a-9e315821920f, my-cluster-2a79abff-kafka-6=8fff144e-67f6-4908-a5a9-c2a55cf13928}
2022-03-30 20:11:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b, my-cluster-2a79abff-kafka-5=32cf6bff-71f0-4f25-b01a-9e315821920f}
2022-03-30 20:11:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b, my-cluster-2a79abff-kafka-5=32cf6bff-71f0-4f25-b01a-9e315821920f}
2022-03-30 20:11:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-1 hasn't rolled
2022-03-30 20:11:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (4189982ms till timeout)
2022-03-30 20:11:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-kafka-2 hasn't rolled
2022-03-30 20:11:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-kafka rolling update not ready, will try again in 5000 ms (1739926ms till timeout)
2022-03-30 20:11:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b, my-cluster-2a79abff-kafka-5=32cf6bff-71f0-4f25-b01a-9e315821920f, my-cluster-2a79abff-kafka-6=8fff144e-67f6-4908-a5a9-c2a55cf13928}
2022-03-30 20:11:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b}
2022-03-30 20:11:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b}
2022-03-30 20:11:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-1 hasn't rolled
2022-03-30 20:11:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (4184976ms till timeout)
2022-03-30 20:11:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-kafka-2 hasn't rolled
2022-03-30 20:11:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-kafka rolling update not ready, will try again in 5000 ms (1734921ms till timeout)
2022-03-30 20:11:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b, my-cluster-2a79abff-kafka-5=32cf6bff-71f0-4f25-b01a-9e315821920f, my-cluster-2a79abff-kafka-6=8fff144e-67f6-4908-a5a9-c2a55cf13928}
2022-03-30 20:11:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b}
2022-03-30 20:11:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b}
2022-03-30 20:11:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-1 hasn't rolled
2022-03-30 20:11:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (4179970ms till timeout)
2022-03-30 20:11:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:11:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:11:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:11:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:11:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-kafka-1 hasn't rolled
2022-03-30 20:11:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-kafka rolling update not ready, will try again in 5000 ms (1729916ms till timeout)
2022-03-30 20:11:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:11:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b, my-cluster-2a79abff-kafka-5=32cf6bff-71f0-4f25-b01a-9e315821920f, my-cluster-2a79abff-kafka-6=8fff144e-67f6-4908-a5a9-c2a55cf13928}
2022-03-30 20:12:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b}
2022-03-30 20:12:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b}
2022-03-30 20:12:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-1 hasn't rolled
2022-03-30 20:12:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (4174964ms till timeout)
2022-03-30 20:12:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:12:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:12:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:12:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-kafka-1 hasn't rolled
2022-03-30 20:12:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-kafka rolling update not ready, will try again in 5000 ms (1724910ms till timeout)
2022-03-30 20:12:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b, my-cluster-2a79abff-kafka-5=32cf6bff-71f0-4f25-b01a-9e315821920f, my-cluster-2a79abff-kafka-6=8fff144e-67f6-4908-a5a9-c2a55cf13928}
2022-03-30 20:12:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898}
2022-03-30 20:12:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898}
2022-03-30 20:12:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-1 hasn't rolled
2022-03-30 20:12:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (4169958ms till timeout)
2022-03-30 20:12:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:12:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:12:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:12:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-kafka-1 hasn't rolled
2022-03-30 20:12:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-kafka rolling update not ready, will try again in 5000 ms (1719905ms till timeout)
2022-03-30 20:12:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b, my-cluster-2a79abff-kafka-5=32cf6bff-71f0-4f25-b01a-9e315821920f, my-cluster-2a79abff-kafka-6=8fff144e-67f6-4908-a5a9-c2a55cf13928}
2022-03-30 20:12:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:12:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:12:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-1 hasn't rolled
2022-03-30 20:12:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (4164943ms till timeout)
2022-03-30 20:12:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:12:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:12:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:12:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-kafka-1 hasn't rolled
2022-03-30 20:12:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-kafka rolling update not ready, will try again in 5000 ms (1714899ms till timeout)
2022-03-30 20:12:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b, my-cluster-2a79abff-kafka-5=32cf6bff-71f0-4f25-b01a-9e315821920f, my-cluster-2a79abff-kafka-6=8fff144e-67f6-4908-a5a9-c2a55cf13928}
2022-03-30 20:12:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:12:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:12:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-0 hasn't rolled
2022-03-30 20:12:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (4159938ms till timeout)
2022-03-30 20:12:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:12:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:12:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:12:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-kafka-1 hasn't rolled
2022-03-30 20:12:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-kafka rolling update not ready, will try again in 5000 ms (1709894ms till timeout)
2022-03-30 20:12:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b, my-cluster-2a79abff-kafka-5=32cf6bff-71f0-4f25-b01a-9e315821920f, my-cluster-2a79abff-kafka-6=8fff144e-67f6-4908-a5a9-c2a55cf13928}
2022-03-30 20:12:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:12:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:12:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-0 hasn't rolled
2022-03-30 20:12:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (4154932ms till timeout)
2022-03-30 20:12:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:12:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:12:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:12:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-kafka-1 hasn't rolled
2022-03-30 20:12:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-kafka rolling update not ready, will try again in 5000 ms (1704888ms till timeout)
2022-03-30 20:12:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b, my-cluster-2a79abff-kafka-5=32cf6bff-71f0-4f25-b01a-9e315821920f, my-cluster-2a79abff-kafka-6=8fff144e-67f6-4908-a5a9-c2a55cf13928}
2022-03-30 20:12:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:12:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:12:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-0 hasn't rolled
2022-03-30 20:12:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (4149927ms till timeout)
2022-03-30 20:12:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:12:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:12:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:12:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-kafka-1 hasn't rolled
2022-03-30 20:12:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-kafka rolling update not ready, will try again in 5000 ms (1699883ms till timeout)
2022-03-30 20:12:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b, my-cluster-2a79abff-kafka-5=32cf6bff-71f0-4f25-b01a-9e315821920f, my-cluster-2a79abff-kafka-6=8fff144e-67f6-4908-a5a9-c2a55cf13928}
2022-03-30 20:12:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:12:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:12:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-0 hasn't rolled
2022-03-30 20:12:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (4144921ms till timeout)
2022-03-30 20:12:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:12:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:12:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:12:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-kafka-1 hasn't rolled
2022-03-30 20:12:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-kafka rolling update not ready, will try again in 5000 ms (1694877ms till timeout)
2022-03-30 20:12:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b, my-cluster-2a79abff-kafka-5=32cf6bff-71f0-4f25-b01a-9e315821920f, my-cluster-2a79abff-kafka-6=8fff144e-67f6-4908-a5a9-c2a55cf13928}
2022-03-30 20:12:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:12:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:12:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-0 hasn't rolled
2022-03-30 20:12:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (4139916ms till timeout)
2022-03-30 20:12:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:12:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:12:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:12:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-kafka-1 hasn't rolled
2022-03-30 20:12:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-kafka rolling update not ready, will try again in 5000 ms (1689872ms till timeout)
2022-03-30 20:12:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b, my-cluster-2a79abff-kafka-5=32cf6bff-71f0-4f25-b01a-9e315821920f, my-cluster-2a79abff-kafka-6=8fff144e-67f6-4908-a5a9-c2a55cf13928}
2022-03-30 20:12:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:12:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:12:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-0 hasn't rolled
2022-03-30 20:12:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (4134910ms till timeout)
2022-03-30 20:12:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:12:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:12:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:12:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-kafka-1 hasn't rolled
2022-03-30 20:12:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-kafka rolling update not ready, will try again in 5000 ms (1684866ms till timeout)
2022-03-30 20:12:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b, my-cluster-2a79abff-kafka-5=32cf6bff-71f0-4f25-b01a-9e315821920f, my-cluster-2a79abff-kafka-6=8fff144e-67f6-4908-a5a9-c2a55cf13928}
2022-03-30 20:12:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:12:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:12:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-0 hasn't rolled
2022-03-30 20:12:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (4129904ms till timeout)
2022-03-30 20:12:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:12:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:12:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:12:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-kafka-1 hasn't rolled
2022-03-30 20:12:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-kafka rolling update not ready, will try again in 5000 ms (1679861ms till timeout)
2022-03-30 20:12:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b, my-cluster-2a79abff-kafka-5=32cf6bff-71f0-4f25-b01a-9e315821920f, my-cluster-2a79abff-kafka-6=8fff144e-67f6-4908-a5a9-c2a55cf13928}
2022-03-30 20:12:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:12:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:12:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-0 hasn't rolled
2022-03-30 20:12:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (4124899ms till timeout)
2022-03-30 20:12:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:12:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:12:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:12:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-kafka-1 hasn't rolled
2022-03-30 20:12:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-kafka rolling update not ready, will try again in 5000 ms (1674856ms till timeout)
2022-03-30 20:12:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b, my-cluster-2a79abff-kafka-5=32cf6bff-71f0-4f25-b01a-9e315821920f, my-cluster-2a79abff-kafka-6=8fff144e-67f6-4908-a5a9-c2a55cf13928}
2022-03-30 20:12:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:12:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:12:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-0 hasn't rolled
2022-03-30 20:12:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (4119893ms till timeout)
2022-03-30 20:12:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:12:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:12:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:12:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:12:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-kafka-1 hasn't rolled
2022-03-30 20:12:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-kafka rolling update not ready, will try again in 5000 ms (1669851ms till timeout)
2022-03-30 20:12:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:12:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b, my-cluster-2a79abff-kafka-5=32cf6bff-71f0-4f25-b01a-9e315821920f, my-cluster-2a79abff-kafka-6=8fff144e-67f6-4908-a5a9-c2a55cf13928}
2022-03-30 20:13:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:13:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:13:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-0 hasn't rolled
2022-03-30 20:13:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (4114888ms till timeout)
2022-03-30 20:13:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:13:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:13:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:13:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-kafka-1 hasn't rolled
2022-03-30 20:13:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-kafka rolling update not ready, will try again in 5000 ms (1664846ms till timeout)
2022-03-30 20:13:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b, my-cluster-2a79abff-kafka-5=32cf6bff-71f0-4f25-b01a-9e315821920f, my-cluster-2a79abff-kafka-6=8fff144e-67f6-4908-a5a9-c2a55cf13928}
2022-03-30 20:13:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:13:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:13:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-0 hasn't rolled
2022-03-30 20:13:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (4109883ms till timeout)
2022-03-30 20:13:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:13:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:13:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:13:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-kafka-1 hasn't rolled
2022-03-30 20:13:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-kafka rolling update not ready, will try again in 5000 ms (1659841ms till timeout)
2022-03-30 20:13:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b, my-cluster-2a79abff-kafka-5=32cf6bff-71f0-4f25-b01a-9e315821920f, my-cluster-2a79abff-kafka-6=8fff144e-67f6-4908-a5a9-c2a55cf13928}
2022-03-30 20:13:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:13:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1}
2022-03-30 20:13:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-0 hasn't rolled
2022-03-30 20:13:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (4104877ms till timeout)
2022-03-30 20:13:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:13:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:13:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:13:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-kafka-1 hasn't rolled
2022-03-30 20:13:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-kafka rolling update not ready, will try again in 5000 ms (1654835ms till timeout)
2022-03-30 20:13:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b, my-cluster-2a79abff-kafka-5=32cf6bff-71f0-4f25-b01a-9e315821920f, my-cluster-2a79abff-kafka-6=8fff144e-67f6-4908-a5a9-c2a55cf13928}
2022-03-30 20:13:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=633ff4b5-f458-45ca-936e-fa5bc7dfb85b}
2022-03-30 20:13:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=633ff4b5-f458-45ca-936e-fa5bc7dfb85b}
2022-03-30 20:13:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-0 hasn't rolled
2022-03-30 20:13:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (4099872ms till timeout)
2022-03-30 20:13:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:13:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:13:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:13:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-kafka-1 hasn't rolled
2022-03-30 20:13:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-kafka rolling update not ready, will try again in 5000 ms (1649830ms till timeout)
2022-03-30 20:13:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b, my-cluster-2a79abff-kafka-5=32cf6bff-71f0-4f25-b01a-9e315821920f, my-cluster-2a79abff-kafka-6=8fff144e-67f6-4908-a5a9-c2a55cf13928}
2022-03-30 20:13:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=633ff4b5-f458-45ca-936e-fa5bc7dfb85b}
2022-03-30 20:13:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=633ff4b5-f458-45ca-936e-fa5bc7dfb85b}
2022-03-30 20:13:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-0 hasn't rolled
2022-03-30 20:13:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (4094866ms till timeout)
2022-03-30 20:13:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:13:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:13:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:13:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-kafka-1 hasn't rolled
2022-03-30 20:13:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-kafka rolling update not ready, will try again in 5000 ms (1644821ms till timeout)
2022-03-30 20:13:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b, my-cluster-2a79abff-kafka-5=32cf6bff-71f0-4f25-b01a-9e315821920f, my-cluster-2a79abff-kafka-6=8fff144e-67f6-4908-a5a9-c2a55cf13928}
2022-03-30 20:13:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=633ff4b5-f458-45ca-936e-fa5bc7dfb85b}
2022-03-30 20:13:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=633ff4b5-f458-45ca-936e-fa5bc7dfb85b}
2022-03-30 20:13:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-0 hasn't rolled
2022-03-30 20:13:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (4089861ms till timeout)
2022-03-30 20:13:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:13:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:13:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:13:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-cae962ba-kafka-1 hasn't rolled
2022-03-30 20:13:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] component with name my-cluster-cae962ba-kafka rolling update not ready, will try again in 5000 ms (1639816ms till timeout)
2022-03-30 20:13:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b, my-cluster-2a79abff-kafka-5=32cf6bff-71f0-4f25-b01a-9e315821920f, my-cluster-2a79abff-kafka-6=8fff144e-67f6-4908-a5a9-c2a55cf13928}
2022-03-30 20:13:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=633ff4b5-f458-45ca-936e-fa5bc7dfb85b}
2022-03-30 20:13:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=633ff4b5-f458-45ca-936e-fa5bc7dfb85b}
2022-03-30 20:13:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-0 hasn't rolled
2022-03-30 20:13:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (4084856ms till timeout)
2022-03-30 20:13:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-cae962ba-kafka-0=b12b40e5-e59e-4e51-98c3-74550e22a38e, my-cluster-cae962ba-kafka-1=de2612e4-d9b5-4048-bb43-76046892aaa8, my-cluster-cae962ba-kafka-2=4522c0d0-5e70-4f2e-b07b-780b0ba33285}
2022-03-30 20:13:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=ac57bf51-bae6-497d-9dc2-d682fee26fc6, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:13:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-cae962ba-kafka-0=7a1d59a5-7e90-4f40-8997-42fae8e66c48, my-cluster-cae962ba-kafka-1=ac57bf51-bae6-497d-9dc2-d682fee26fc6, my-cluster-cae962ba-kafka-2=64aa5da6-7f61-48a5-b797-dba79dec35d6}
2022-03-30 20:13:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:55] All pods seem to have rolled
2022-03-30 20:13:31 [ForkJoinPool-3-worker-15] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-cae962ba-kafka has been successfully rolled
2022-03-30 20:13:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [RollingUpdateUtils:87] Component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={}) successfully rolled
2022-03-30 20:13:31 [ForkJoinPool-3-worker-15] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-cae962ba-kafka to be ready
2022-03-30 20:13:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-30 20:13:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:13:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-kafka-1)
2022-03-30 20:13:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1799996ms till timeout)
2022-03-30 20:13:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:13:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-kafka-1)
2022-03-30 20:13:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1798991ms till timeout)
2022-03-30 20:13:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:13:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-kafka-1)
2022-03-30 20:13:33 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1797986ms till timeout)
2022-03-30 20:13:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:34 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:13:34 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-kafka-1)
2022-03-30 20:13:34 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1796981ms till timeout)
2022-03-30 20:13:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b, my-cluster-2a79abff-kafka-5=32cf6bff-71f0-4f25-b01a-9e315821920f, my-cluster-2a79abff-kafka-6=8fff144e-67f6-4908-a5a9-c2a55cf13928}
2022-03-30 20:13:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=633ff4b5-f458-45ca-936e-fa5bc7dfb85b}
2022-03-30 20:13:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=633ff4b5-f458-45ca-936e-fa5bc7dfb85b}
2022-03-30 20:13:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-0 hasn't rolled
2022-03-30 20:13:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (4079851ms till timeout)
2022-03-30 20:13:35 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:13:35 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-kafka-1)
2022-03-30 20:13:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1795976ms till timeout)
2022-03-30 20:13:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:13:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-kafka-1)
2022-03-30 20:13:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1794970ms till timeout)
2022-03-30 20:13:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:13:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-kafka-1)
2022-03-30 20:13:37 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1793965ms till timeout)
2022-03-30 20:13:38 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:13:38 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-kafka-1)
2022-03-30 20:13:38 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1792960ms till timeout)
2022-03-30 20:13:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:39 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:13:39 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-kafka-1)
2022-03-30 20:13:39 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1791955ms till timeout)
2022-03-30 20:13:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b, my-cluster-2a79abff-kafka-5=32cf6bff-71f0-4f25-b01a-9e315821920f, my-cluster-2a79abff-kafka-6=8fff144e-67f6-4908-a5a9-c2a55cf13928}
2022-03-30 20:13:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=633ff4b5-f458-45ca-936e-fa5bc7dfb85b}
2022-03-30 20:13:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=633ff4b5-f458-45ca-936e-fa5bc7dfb85b}
2022-03-30 20:13:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:50] At least my-cluster-2a79abff-kafka-0 hasn't rolled
2022-03-30 20:13:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] component with name my-cluster-2a79abff-kafka rolling update not ready, will try again in 5000 ms (4074845ms till timeout)
2022-03-30 20:13:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:13:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-kafka-1)
2022-03-30 20:13:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1790949ms till timeout)
2022-03-30 20:13:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:13:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-kafka-1)
2022-03-30 20:13:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1789944ms till timeout)
2022-03-30 20:13:42 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:13:42 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-kafka-1)
2022-03-30 20:13:42 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1788939ms till timeout)
2022-03-30 20:13:43 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:13:43 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-kafka-1)
2022-03-30 20:13:43 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1787934ms till timeout)
2022-03-30 20:13:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:44 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:13:44 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-kafka-1)
2022-03-30 20:13:44 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1786929ms till timeout)
2022-03-30 20:13:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:35] Existing snapshot: {my-cluster-2a79abff-kafka-0=cea0c855-9df4-4c39-9329-10f9f30b08f3, my-cluster-2a79abff-kafka-1=4c18918d-11ca-49b9-9fb6-3a93767e6dd3, my-cluster-2a79abff-kafka-2=4160a2da-1c60-4c52-9fa3-7d0db59475c1, my-cluster-2a79abff-kafka-3=46b6b4a5-d087-4067-8d0b-067427848898, my-cluster-2a79abff-kafka-4=8e0865cc-1d4f-41b1-ba19-8ee26b0e855b, my-cluster-2a79abff-kafka-5=32cf6bff-71f0-4f25-b01a-9e315821920f, my-cluster-2a79abff-kafka-6=8fff144e-67f6-4908-a5a9-c2a55cf13928}
2022-03-30 20:13:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:39] Current snapshot: {my-cluster-2a79abff-kafka-0=cd9346f4-594f-4913-a297-c87e22fd88ea, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=633ff4b5-f458-45ca-936e-fa5bc7dfb85b}
2022-03-30 20:13:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:44] Pods in common: {my-cluster-2a79abff-kafka-0=cd9346f4-594f-4913-a297-c87e22fd88ea, my-cluster-2a79abff-kafka-1=7cf84ce5-3f8f-4cb5-9d8f-00faa94ca29c, my-cluster-2a79abff-kafka-2=633ff4b5-f458-45ca-936e-fa5bc7dfb85b}
2022-03-30 20:13:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:55] All pods seem to have rolled
2022-03-30 20:13:45 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-2a79abff-kafka has been successfully rolled
2022-03-30 20:13:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [RollingUpdateUtils:87] Component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={}) successfully rolled
2022-03-30 20:13:45 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-2a79abff-kafka to be ready
2022-03-30 20:13:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-30 20:13:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-0)
2022-03-30 20:13:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1799997ms till timeout)
2022-03-30 20:13:45 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:13:45 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-kafka-1)
2022-03-30 20:13:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1785924ms till timeout)
2022-03-30 20:13:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-0)
2022-03-30 20:13:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1798992ms till timeout)
2022-03-30 20:13:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:13:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-kafka-1)
2022-03-30 20:13:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1784919ms till timeout)
2022-03-30 20:13:47 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-0)
2022-03-30 20:13:47 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1797987ms till timeout)
2022-03-30 20:13:47 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:13:47 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-kafka-1)
2022-03-30 20:13:47 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1783913ms till timeout)
2022-03-30 20:13:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-0)
2022-03-30 20:13:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1796982ms till timeout)
2022-03-30 20:13:48 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:13:48 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-kafka-1)
2022-03-30 20:13:48 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1782908ms till timeout)
2022-03-30 20:13:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-0)
2022-03-30 20:13:49 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1795977ms till timeout)
2022-03-30 20:13:49 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:13:49 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-kafka-1)
2022-03-30 20:13:49 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1781897ms till timeout)
2022-03-30 20:13:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-0)
2022-03-30 20:13:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1794973ms till timeout)
2022-03-30 20:13:50 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:13:50 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-kafka-1)
2022-03-30 20:13:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1780892ms till timeout)
2022-03-30 20:13:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-0)
2022-03-30 20:13:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1793968ms till timeout)
2022-03-30 20:13:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:13:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-kafka-1)
2022-03-30 20:13:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1779887ms till timeout)
2022-03-30 20:13:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-0)
2022-03-30 20:13:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1792964ms till timeout)
2022-03-30 20:13:52 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:13:52 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-kafka-1)
2022-03-30 20:13:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1778882ms till timeout)
2022-03-30 20:13:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-0)
2022-03-30 20:13:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1791959ms till timeout)
2022-03-30 20:13:53 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:13:53 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-cae962ba-kafka-1)
2022-03-30 20:13:53 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1777877ms till timeout)
2022-03-30 20:13:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-0)
2022-03-30 20:13:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1790954ms till timeout)
2022-03-30 20:13:54 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:13:54 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-1 not ready: kafka)
2022-03-30 20:13:54 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-2 not ready: kafka)
2022-03-30 20:13:54 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-kafka-0, my-cluster-cae962ba-kafka-1, my-cluster-cae962ba-kafka-2 are ready
2022-03-30 20:13:54 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1776872ms till timeout)
2022-03-30 20:13:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-0)
2022-03-30 20:13:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1789949ms till timeout)
2022-03-30 20:13:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:13:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-1 not ready: kafka)
2022-03-30 20:13:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-2 not ready: kafka)
2022-03-30 20:13:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-kafka-0, my-cluster-cae962ba-kafka-1, my-cluster-cae962ba-kafka-2 are ready
2022-03-30 20:13:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1775863ms till timeout)
2022-03-30 20:13:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-0)
2022-03-30 20:13:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1788945ms till timeout)
2022-03-30 20:13:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:13:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-1 not ready: kafka)
2022-03-30 20:13:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-2 not ready: kafka)
2022-03-30 20:13:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-kafka-0, my-cluster-cae962ba-kafka-1, my-cluster-cae962ba-kafka-2 are ready
2022-03-30 20:13:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1774858ms till timeout)
2022-03-30 20:13:57 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-0)
2022-03-30 20:13:57 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1787940ms till timeout)
2022-03-30 20:13:57 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:13:57 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-1 not ready: kafka)
2022-03-30 20:13:57 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-2 not ready: kafka)
2022-03-30 20:13:57 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-kafka-0, my-cluster-cae962ba-kafka-1, my-cluster-cae962ba-kafka-2 are ready
2022-03-30 20:13:57 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1773853ms till timeout)
2022-03-30 20:13:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-0)
2022-03-30 20:13:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1786936ms till timeout)
2022-03-30 20:13:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:13:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-1 not ready: kafka)
2022-03-30 20:13:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-2 not ready: kafka)
2022-03-30 20:13:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-kafka-0, my-cluster-cae962ba-kafka-1, my-cluster-cae962ba-kafka-2 are ready
2022-03-30 20:13:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1772849ms till timeout)
2022-03-30 20:13:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:13:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:13:59 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-0)
2022-03-30 20:13:59 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1785931ms till timeout)
2022-03-30 20:13:59 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:13:59 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-1 not ready: kafka)
2022-03-30 20:13:59 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-2 not ready: kafka)
2022-03-30 20:13:59 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-kafka-0, my-cluster-cae962ba-kafka-1, my-cluster-cae962ba-kafka-2 are ready
2022-03-30 20:13:59 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1771844ms till timeout)
2022-03-30 20:14:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-0)
2022-03-30 20:14:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1784926ms till timeout)
2022-03-30 20:14:00 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:14:00 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-1 not ready: kafka)
2022-03-30 20:14:00 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-2 not ready: kafka)
2022-03-30 20:14:00 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-kafka-0, my-cluster-cae962ba-kafka-1, my-cluster-cae962ba-kafka-2 are ready
2022-03-30 20:14:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1770840ms till timeout)
2022-03-30 20:14:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:01 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-0)
2022-03-30 20:14:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1783921ms till timeout)
2022-03-30 20:14:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:14:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-1 not ready: kafka)
2022-03-30 20:14:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-2 not ready: kafka)
2022-03-30 20:14:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-kafka-0, my-cluster-cae962ba-kafka-1, my-cluster-cae962ba-kafka-2 are ready
2022-03-30 20:14:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1769835ms till timeout)
2022-03-30 20:14:02 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-2a79abff-kafka-0)
2022-03-30 20:14:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1782916ms till timeout)
2022-03-30 20:14:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:14:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-1 not ready: kafka)
2022-03-30 20:14:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-2 not ready: kafka)
2022-03-30 20:14:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-kafka-0, my-cluster-cae962ba-kafka-1, my-cluster-cae962ba-kafka-2 are ready
2022-03-30 20:14:02 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1768831ms till timeout)
2022-03-30 20:14:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:14:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:14:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:14:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2a79abff-kafka-0, my-cluster-2a79abff-kafka-1, my-cluster-2a79abff-kafka-2 are ready
2022-03-30 20:14:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1781909ms till timeout)
2022-03-30 20:14:03 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:14:03 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-1 not ready: kafka)
2022-03-30 20:14:03 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-2 not ready: kafka)
2022-03-30 20:14:03 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-kafka-0, my-cluster-cae962ba-kafka-1, my-cluster-cae962ba-kafka-2 are ready
2022-03-30 20:14:03 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-cae962ba-kafka, strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1767826ms till timeout)
2022-03-30 20:14:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:14:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:14:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:14:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2a79abff-kafka-0, my-cluster-2a79abff-kafka-1, my-cluster-2a79abff-kafka-2 are ready
2022-03-30 20:14:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1780904ms till timeout)
2022-03-30 20:14:04 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-0 not ready: kafka)
2022-03-30 20:14:04 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-1 not ready: kafka)
2022-03-30 20:14:04 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-2 not ready: kafka)
2022-03-30 20:14:04 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-kafka-0, my-cluster-cae962ba-kafka-1, my-cluster-cae962ba-kafka-2 are ready
2022-03-30 20:14:04 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:308] Wait for EO to rolling restart ...
2022-03-30 20:14:04 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-cae962ba-entity-operator rolling update
2022-03-30 20:14:04 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Deployment my-cluster-cae962ba-entity-operator rolling update in namespace:namespace-3
2022-03-30 20:14:04 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-cae962ba-entity-operator-849d5ffc88-2xmjp=c0a920d0-7a29-480e-8d9e-9d1c1d1dad6f}
2022-03-30 20:14:04 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj=3268b144-5a92-40d2-b570-eadaaac473ea}
2022-03-30 20:14:04 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:119] All pods seem to have rolled
2022-03-30 20:14:04 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready
2022-03-30 20:14:04 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready
2022-03-30 20:14:04 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (479998ms till timeout)
2022-03-30 20:14:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:14:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:14:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:14:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2a79abff-kafka-0, my-cluster-2a79abff-kafka-1, my-cluster-2a79abff-kafka-2 are ready
2022-03-30 20:14:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1779900ms till timeout)
2022-03-30 20:14:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (478994ms till timeout)
2022-03-30 20:14:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:14:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:14:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:14:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2a79abff-kafka-0, my-cluster-2a79abff-kafka-1, my-cluster-2a79abff-kafka-2 are ready
2022-03-30 20:14:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1778894ms till timeout)
2022-03-30 20:14:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (477991ms till timeout)
2022-03-30 20:14:07 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:14:07 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:14:07 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:14:07 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2a79abff-kafka-0, my-cluster-2a79abff-kafka-1, my-cluster-2a79abff-kafka-2 are ready
2022-03-30 20:14:07 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1777889ms till timeout)
2022-03-30 20:14:07 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (476988ms till timeout)
2022-03-30 20:14:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:14:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:14:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:14:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2a79abff-kafka-0, my-cluster-2a79abff-kafka-1, my-cluster-2a79abff-kafka-2 are ready
2022-03-30 20:14:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1776885ms till timeout)
2022-03-30 20:14:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (475985ms till timeout)
2022-03-30 20:14:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:14:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:14:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:14:09 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2a79abff-kafka-0, my-cluster-2a79abff-kafka-1, my-cluster-2a79abff-kafka-2 are ready
2022-03-30 20:14:09 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1775879ms till timeout)
2022-03-30 20:14:09 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (474981ms till timeout)
2022-03-30 20:14:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:14:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:14:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:14:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2a79abff-kafka-0, my-cluster-2a79abff-kafka-1, my-cluster-2a79abff-kafka-2 are ready
2022-03-30 20:14:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1774874ms till timeout)
2022-03-30 20:14:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (473977ms till timeout)
2022-03-30 20:14:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:14:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:14:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:14:11 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2a79abff-kafka-0, my-cluster-2a79abff-kafka-1, my-cluster-2a79abff-kafka-2 are ready
2022-03-30 20:14:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1773870ms till timeout)
2022-03-30 20:14:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (472974ms till timeout)
2022-03-30 20:14:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:14:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:14:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:14:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2a79abff-kafka-0, my-cluster-2a79abff-kafka-1, my-cluster-2a79abff-kafka-2 are ready
2022-03-30 20:14:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-2a79abff-kafka, strimzi.io/cluster=my-cluster-2a79abff, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1772865ms till timeout)
2022-03-30 20:14:12 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (471970ms till timeout)
2022-03-30 20:14:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-0 not ready: kafka)
2022-03-30 20:14:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-1 not ready: kafka)
2022-03-30 20:14:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-2a79abff-kafka-2 not ready: kafka)
2022-03-30 20:14:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods my-cluster-2a79abff-kafka-0, my-cluster-2a79abff-kafka-1, my-cluster-2a79abff-kafka-2 are ready
2022-03-30 20:14:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-2a79abff will have desired state: Ready
2022-03-30 20:14:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-2a79abff will have desired state: Ready
2022-03-30 20:14:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-2a79abff is in desired state: Ready
2022-03-30 20:14:13 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-2a79abff is ready
2022-03-30 20:14:13 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateST:356] Kafka scale down to 3 finished
2022-03-30 20:14:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Consumer will successfully receive messages.
2022-03-30 20:14:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@49f844fb, which are set.
2022-03-30 20:14:13 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@73f7b707, messages=[], arguments=[USER=my_user_838849261_890460734, --group-id, my-consumer-group-659158444, --topic, my-topic-2138084746-704381117, --group-instance-id, instance491008179, --bootstrap-server, my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-2a79abff-kafka-clients-6bfb76b8f-f4rf2', podNamespace='namespace-4', bootstrapServer='my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093', topicName='my-topic-2138084746-704381117', maxMessages=100, kafkaUsername='my-user-838849261-890460734', consumerGroupName='my-consumer-group-659158444', consumerInstanceId='instance491008179', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@49f844fb}
2022-03-30 20:14:13 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093:my-topic-2138084746-704381117 from pod my-cluster-2a79abff-kafka-clients-6bfb76b8f-f4rf2
2022-03-30 20:14:13 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-2a79abff-kafka-clients-6bfb76b8f-f4rf2 -n namespace-4 -- /opt/kafka/consumer.sh USER=my_user_838849261_890460734 --group-id my-consumer-group-659158444 --topic my-topic-2138084746-704381117 --group-instance-id instance491008179 --bootstrap-server my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093 --max-messages 100
2022-03-30 20:14:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-2a79abff-kafka-clients-6bfb76b8f-f4rf2 -n namespace-4 -- /opt/kafka/consumer.sh USER=my_user_838849261_890460734 --group-id my-consumer-group-659158444 --topic my-topic-2138084746-704381117 --group-instance-id instance491008179 --bootstrap-server my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093 --max-messages 100
2022-03-30 20:14:13 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (470967ms till timeout)
2022-03-30 20:14:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:14 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (469963ms till timeout)
2022-03-30 20:14:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (468960ms till timeout)
2022-03-30 20:14:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (467956ms till timeout)
2022-03-30 20:14:17 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (466952ms till timeout)
2022-03-30 20:14:18 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (465949ms till timeout)
2022-03-30 20:14:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (464945ms till timeout)
2022-03-30 20:14:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:20 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 20:14:20 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-03-30 20:14:20 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-2138084746-704381117-new in namespace infra-namespace
2022-03-30 20:14:20 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-03-30 20:14:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-2138084746-704381117-new
2022-03-30 20:14:20 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-2138084746-704381117-new will have desired state: Ready
2022-03-30 20:14:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-2138084746-704381117-new will have desired state: Ready
2022-03-30 20:14:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-2138084746-704381117-new will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 20:14:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (463942ms till timeout)
2022-03-30 20:14:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:21 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-2138084746-704381117-new is in desired state: Ready
2022-03-30 20:14:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Producer and consumer will successfully send and receive messages.
2022-03-30 20:14:21 [ForkJoinPool-3-worker-7] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@634fd736, which are set.
2022-03-30 20:14:21 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@331ea689, messages=[], arguments=[USER=my_user_838849261_890460734, --topic, my-topic-2138084746-704381117-new, --bootstrap-server, my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-2a79abff-kafka-clients-6bfb76b8f-f4rf2', podNamespace='namespace-4', bootstrapServer='my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093', topicName='my-topic-2138084746-704381117-new', maxMessages=100, kafkaUsername='my-user-838849261-890460734', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@634fd736}
2022-03-30 20:14:21 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093:my-topic-2138084746-704381117-new from pod my-cluster-2a79abff-kafka-clients-6bfb76b8f-f4rf2
2022-03-30 20:14:21 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-2a79abff-kafka-clients-6bfb76b8f-f4rf2 -n namespace-4 -- /opt/kafka/producer.sh USER=my_user_838849261_890460734 --topic my-topic-2138084746-704381117-new --bootstrap-server my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093 --max-messages 100
2022-03-30 20:14:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-2a79abff-kafka-clients-6bfb76b8f-f4rf2 -n namespace-4 -- /opt/kafka/producer.sh USER=my_user_838849261_890460734 --topic my-topic-2138084746-704381117-new --bootstrap-server my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093 --max-messages 100
2022-03-30 20:14:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (462939ms till timeout)
2022-03-30 20:14:22 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (461935ms till timeout)
2022-03-30 20:14:23 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (460932ms till timeout)
2022-03-30 20:14:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:24 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (459927ms till timeout)
2022-03-30 20:14:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:25 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-30 20:14:25 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-03-30 20:14:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5e40097a, which are set.
2022-03-30 20:14:25 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@789337da, messages=[], arguments=[USER=my_user_838849261_890460734, --group-id, my-consumer-group-458269451, --topic, my-topic-2138084746-704381117-new, --group-instance-id, instance606213825, --bootstrap-server, my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-2a79abff-kafka-clients-6bfb76b8f-f4rf2', podNamespace='namespace-4', bootstrapServer='my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093', topicName='my-topic-2138084746-704381117-new', maxMessages=100, kafkaUsername='my-user-838849261-890460734', consumerGroupName='my-consumer-group-458269451', consumerInstanceId='instance606213825', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5e40097a}
2022-03-30 20:14:25 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093:my-topic-2138084746-704381117-new from pod my-cluster-2a79abff-kafka-clients-6bfb76b8f-f4rf2
2022-03-30 20:14:25 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-2a79abff-kafka-clients-6bfb76b8f-f4rf2 -n namespace-4 -- /opt/kafka/consumer.sh USER=my_user_838849261_890460734 --group-id my-consumer-group-458269451 --topic my-topic-2138084746-704381117-new --group-instance-id instance606213825 --bootstrap-server my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093 --max-messages 100
2022-03-30 20:14:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-2a79abff-kafka-clients-6bfb76b8f-f4rf2 -n namespace-4 -- /opt/kafka/consumer.sh USER=my_user_838849261_890460734 --group-id my-consumer-group-458269451 --topic my-topic-2138084746-704381117-new --group-instance-id instance606213825 --bootstrap-server my-cluster-2a79abff-kafka-bootstrap.namespace-4.svc:9093 --max-messages 100
2022-03-30 20:14:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (458923ms till timeout)
2022-03-30 20:14:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (457919ms till timeout)
2022-03-30 20:14:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (456916ms till timeout)
2022-03-30 20:14:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (455912ms till timeout)
2022-03-30 20:14:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:29 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (454908ms till timeout)
2022-03-30 20:14:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (453905ms till timeout)
2022-03-30 20:14:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (452901ms till timeout)
2022-03-30 20:14:32 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 20:14:32 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-03-30 20:14:32 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 20:14:32 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:675] [rollingupdate.RollingUpdateST - After Each] - Clean up after test
2022-03-30 20:14:32 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:14:32 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaAndZookeeperScaleUpScaleDown
2022-03-30 20:14:32 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-2138084746-704381117 in namespace namespace-4
2022-03-30 20:14:32 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-2138084746-704381117
2022-03-30 20:14:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-2138084746-704381117 not ready, will try again in 10000 ms (179994ms till timeout)
2022-03-30 20:14:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (451897ms till timeout)
2022-03-30 20:14:33 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (450894ms till timeout)
2022-03-30 20:14:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:34 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (449891ms till timeout)
2022-03-30 20:14:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (448887ms till timeout)
2022-03-30 20:14:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (447883ms till timeout)
2022-03-30 20:14:37 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (446880ms till timeout)
2022-03-30 20:14:38 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (445876ms till timeout)
2022-03-30 20:14:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:39 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (444873ms till timeout)
2022-03-30 20:14:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (443869ms till timeout)
2022-03-30 20:14:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (442866ms till timeout)
2022-03-30 20:14:42 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-2138084746-704381117-new in namespace namespace-4
2022-03-30 20:14:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-2138084746-704381117-new
2022-03-30 20:14:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-2138084746-704381117-new not ready, will try again in 10000 ms (179995ms till timeout)
2022-03-30 20:14:42 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (441862ms till timeout)
2022-03-30 20:14:43 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (440859ms till timeout)
2022-03-30 20:14:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:44 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (439855ms till timeout)
2022-03-30 20:14:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (438852ms till timeout)
2022-03-30 20:14:47 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (437848ms till timeout)
2022-03-30 20:14:48 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (436844ms till timeout)
2022-03-30 20:14:49 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (435841ms till timeout)
2022-03-30 20:14:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (434837ms till timeout)
2022-03-30 20:14:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (433834ms till timeout)
2022-03-30 20:14:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (432831ms till timeout)
2022-03-30 20:14:52 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-2a79abff-kafka-clients in namespace namespace-4
2022-03-30 20:14:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-2a79abff-kafka-clients
2022-03-30 20:14:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-2a79abff-kafka-clients not ready, will try again in 10000 ms (479951ms till timeout)
2022-03-30 20:14:53 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (431827ms till timeout)
2022-03-30 20:14:54 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (430823ms till timeout)
2022-03-30 20:14:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (429820ms till timeout)
2022-03-30 20:14:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:14:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (428816ms till timeout)
2022-03-30 20:14:57 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (427813ms till timeout)
2022-03-30 20:14:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (426809ms till timeout)
2022-03-30 20:14:59 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (425806ms till timeout)
2022-03-30 20:14:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:14:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (424802ms till timeout)
2022-03-30 20:15:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (423799ms till timeout)
2022-03-30 20:15:02 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (422795ms till timeout)
2022-03-30 20:15:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-2a79abff-kafka-clients not ready, will try again in 10000 ms (469942ms till timeout)
2022-03-30 20:15:03 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (421792ms till timeout)
2022-03-30 20:15:04 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (420788ms till timeout)
2022-03-30 20:15:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (419785ms till timeout)
2022-03-30 20:15:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (418781ms till timeout)
2022-03-30 20:15:07 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (417778ms till timeout)
2022-03-30 20:15:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (416774ms till timeout)
2022-03-30 20:15:09 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (415771ms till timeout)
2022-03-30 20:15:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (414767ms till timeout)
2022-03-30 20:15:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (413763ms till timeout)
2022-03-30 20:15:12 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (412760ms till timeout)
2022-03-30 20:15:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-2a79abff-kafka-clients not ready, will try again in 10000 ms (459934ms till timeout)
2022-03-30 20:15:13 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (411756ms till timeout)
2022-03-30 20:15:14 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (410753ms till timeout)
2022-03-30 20:15:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (409749ms till timeout)
2022-03-30 20:15:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (408746ms till timeout)
2022-03-30 20:15:17 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (407742ms till timeout)
2022-03-30 20:15:18 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (406739ms till timeout)
2022-03-30 20:15:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (405735ms till timeout)
2022-03-30 20:15:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (404731ms till timeout)
2022-03-30 20:15:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (403728ms till timeout)
2022-03-30 20:15:22 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (402725ms till timeout)
2022-03-30 20:15:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-2a79abff-kafka-clients not ready, will try again in 10000 ms (449924ms till timeout)
2022-03-30 20:15:23 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (401721ms till timeout)
2022-03-30 20:15:24 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (400718ms till timeout)
2022-03-30 20:15:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (399715ms till timeout)
2022-03-30 20:15:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-entity-operator will be ready not ready, will try again in 1000 ms (398711ms till timeout)
2022-03-30 20:15:27 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-cae962ba-entity-operator is ready
2022-03-30 20:15:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-cae962ba-entity-operator}, additionalProperties={})to be ready
2022-03-30 20:15:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj not ready: tls-sidecar)
2022-03-30 20:15:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj not ready: topic-operator)
2022-03-30 20:15:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj not ready: user-operator)
2022-03-30 20:15:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj are ready
2022-03-30 20:15:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-cae962ba-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-30 20:15:28 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj not ready: tls-sidecar)
2022-03-30 20:15:28 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj not ready: topic-operator)
2022-03-30 20:15:28 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj not ready: user-operator)
2022-03-30 20:15:28 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj are ready
2022-03-30 20:15:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-cae962ba-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598991ms till timeout)
2022-03-30 20:15:29 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj not ready: tls-sidecar)
2022-03-30 20:15:29 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj not ready: topic-operator)
2022-03-30 20:15:29 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj not ready: user-operator)
2022-03-30 20:15:29 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj are ready
2022-03-30 20:15:29 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-cae962ba-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597986ms till timeout)
2022-03-30 20:15:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj not ready: tls-sidecar)
2022-03-30 20:15:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj not ready: topic-operator)
2022-03-30 20:15:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj not ready: user-operator)
2022-03-30 20:15:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj are ready
2022-03-30 20:15:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-cae962ba-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596981ms till timeout)
2022-03-30 20:15:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj not ready: tls-sidecar)
2022-03-30 20:15:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj not ready: topic-operator)
2022-03-30 20:15:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj not ready: user-operator)
2022-03-30 20:15:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj are ready
2022-03-30 20:15:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-cae962ba-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595975ms till timeout)
2022-03-30 20:15:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj not ready: tls-sidecar)
2022-03-30 20:15:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj not ready: topic-operator)
2022-03-30 20:15:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj not ready: user-operator)
2022-03-30 20:15:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj are ready
2022-03-30 20:15:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-cae962ba-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594970ms till timeout)
2022-03-30 20:15:32 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-838849261-890460734 in namespace namespace-4
2022-03-30 20:15:32 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-838849261-890460734
2022-03-30 20:15:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-838849261-890460734 not ready, will try again in 10000 ms (179994ms till timeout)
2022-03-30 20:15:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj not ready: tls-sidecar)
2022-03-30 20:15:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj not ready: topic-operator)
2022-03-30 20:15:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj not ready: user-operator)
2022-03-30 20:15:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj are ready
2022-03-30 20:15:33 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-cae962ba-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593966ms till timeout)
2022-03-30 20:15:34 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj not ready: tls-sidecar)
2022-03-30 20:15:34 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj not ready: topic-operator)
2022-03-30 20:15:34 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj not ready: user-operator)
2022-03-30 20:15:34 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj are ready
2022-03-30 20:15:34 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-cae962ba-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592961ms till timeout)
2022-03-30 20:15:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:35 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj not ready: tls-sidecar)
2022-03-30 20:15:35 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj not ready: topic-operator)
2022-03-30 20:15:35 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj not ready: user-operator)
2022-03-30 20:15:35 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj are ready
2022-03-30 20:15:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-cae962ba-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591956ms till timeout)
2022-03-30 20:15:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj not ready: tls-sidecar)
2022-03-30 20:15:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj not ready: topic-operator)
2022-03-30 20:15:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj not ready: user-operator)
2022-03-30 20:15:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj are ready
2022-03-30 20:15:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-cae962ba-entity-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590951ms till timeout)
2022-03-30 20:15:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj not ready: tls-sidecar)
2022-03-30 20:15:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj not ready: topic-operator)
2022-03-30 20:15:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj not ready: user-operator)
2022-03-30 20:15:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-entity-operator-56f65c7b6f-t76sj are ready
2022-03-30 20:15:37 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-cae962ba-entity-operator rolling update finished
2022-03-30 20:15:37 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:312] Wait for CC and KE to rolling restart ...
2022-03-30 20:15:37 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-cae962ba-kafka-exporter rolling update
2022-03-30 20:15:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Deployment my-cluster-cae962ba-kafka-exporter rolling update in namespace:namespace-3
2022-03-30 20:15:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-cae962ba-kafka-exporter-7f7b884685-z58x2=3f47c89e-54ea-4d0f-baef-e0524f136ecc}
2022-03-30 20:15:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-cae962ba-kafka-exporter-7f7b884685-z58x2=3f47c89e-54ea-4d0f-baef-e0524f136ecc}
2022-03-30 20:15:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-cae962ba-kafka-exporter-7f7b884685-z58x2=3f47c89e-54ea-4d0f-baef-e0524f136ecc}
2022-03-30 20:15:37 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Deployment my-cluster-cae962ba-kafka-exporter rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (599994ms till timeout)
2022-03-30 20:15:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:42 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-cae962ba-kafka-exporter-7f7b884685-z58x2=3f47c89e-54ea-4d0f-baef-e0524f136ecc}
2022-03-30 20:15:42 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-cae962ba-kafka-exporter-7f7b884685-z58x2=3f47c89e-54ea-4d0f-baef-e0524f136ecc}
2022-03-30 20:15:42 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-cae962ba-kafka-exporter-7f7b884685-z58x2=3f47c89e-54ea-4d0f-baef-e0524f136ecc}
2022-03-30 20:15:42 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Deployment my-cluster-cae962ba-kafka-exporter rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (594987ms till timeout)
2022-03-30 20:15:42 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-2a79abff in namespace namespace-4
2022-03-30 20:15:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-2a79abff
2022-03-30 20:15:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-2a79abff not ready, will try again in 10000 ms (839992ms till timeout)
2022-03-30 20:15:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:47 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-cae962ba-kafka-exporter-7f7b884685-z58x2=3f47c89e-54ea-4d0f-baef-e0524f136ecc}
2022-03-30 20:15:47 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-cae962ba-kafka-exporter-7f7b884685-z58x2=3f47c89e-54ea-4d0f-baef-e0524f136ecc}
2022-03-30 20:15:47 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-cae962ba-kafka-exporter-7f7b884685-z58x2=3f47c89e-54ea-4d0f-baef-e0524f136ecc}
2022-03-30 20:15:47 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Deployment my-cluster-cae962ba-kafka-exporter rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (589980ms till timeout)
2022-03-30 20:15:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:52 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-cae962ba-kafka-exporter-7f7b884685-z58x2=3f47c89e-54ea-4d0f-baef-e0524f136ecc}
2022-03-30 20:15:52 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-cae962ba-kafka-exporter-7f7b884685-z58x2=3f47c89e-54ea-4d0f-baef-e0524f136ecc}
2022-03-30 20:15:52 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-cae962ba-kafka-exporter-7f7b884685-z58x2=3f47c89e-54ea-4d0f-baef-e0524f136ecc}
2022-03-30 20:15:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Deployment my-cluster-cae962ba-kafka-exporter rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (584974ms till timeout)
2022-03-30 20:15:52 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:15:52 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-4 for test case:testKafkaAndZookeeperScaleUpScaleDown
2022-03-30 20:15:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-4 removal
2022-03-30 20:15:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-30 20:15:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-30 20:15:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:15:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (479932ms till timeout)
2022-03-30 20:15:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-30 20:15:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-30 20:15:53 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:15:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (478861ms till timeout)
2022-03-30 20:15:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-30 20:15:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-30 20:15:54 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:15:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (477749ms till timeout)
2022-03-30 20:15:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-30 20:15:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-30 20:15:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:15:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (476668ms till timeout)
2022-03-30 20:15:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-30 20:15:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-30 20:15:56 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:15:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (475589ms till timeout)
2022-03-30 20:15:57 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-cae962ba-kafka-exporter-7f7b884685-z58x2=3f47c89e-54ea-4d0f-baef-e0524f136ecc}
2022-03-30 20:15:57 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-cae962ba-kafka-exporter-7f7b884685-z58x2=3f47c89e-54ea-4d0f-baef-e0524f136ecc, my-cluster-cae962ba-kafka-exporter-8585666bcd-lr6gn=b38199a1-d119-4cf4-be0f-50fbe757b292}
2022-03-30 20:15:57 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-cae962ba-kafka-exporter-7f7b884685-z58x2=3f47c89e-54ea-4d0f-baef-e0524f136ecc}
2022-03-30 20:15:57 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Deployment my-cluster-cae962ba-kafka-exporter rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (579966ms till timeout)
2022-03-30 20:15:57 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-30 20:15:57 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-30 20:15:57 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:15:57 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (474514ms till timeout)
2022-03-30 20:15:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-30 20:15:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-30 20:15:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:15:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (473435ms till timeout)
2022-03-30 20:15:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:15:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:15:59 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-30 20:15:59 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-30 20:15:59 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:15:59 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (472355ms till timeout)
2022-03-30 20:16:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:16:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:16:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-30 20:16:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:16:01 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-30 20:16:01 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:16:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (471278ms till timeout)
2022-03-30 20:16:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-30 20:16:02 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-30 20:16:02 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:16:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (470201ms till timeout)
2022-03-30 20:16:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-cae962ba-kafka-exporter-7f7b884685-z58x2=3f47c89e-54ea-4d0f-baef-e0524f136ecc}
2022-03-30 20:16:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-cae962ba-kafka-exporter-7f7b884685-z58x2=3f47c89e-54ea-4d0f-baef-e0524f136ecc, my-cluster-cae962ba-kafka-exporter-8585666bcd-lr6gn=b38199a1-d119-4cf4-be0f-50fbe757b292}
2022-03-30 20:16:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-cae962ba-kafka-exporter-7f7b884685-z58x2=3f47c89e-54ea-4d0f-baef-e0524f136ecc}
2022-03-30 20:16:02 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Deployment my-cluster-cae962ba-kafka-exporter rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (574959ms till timeout)
2022-03-30 20:16:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-30 20:16:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace infra-namespace get Namespace namespace-4 -o yaml
2022-03-30 20:16:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 20:16:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 20:16:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-4" not found
2022-03-30 20:16:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 20:16:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[], io.strimzi.test.logs.CollectorElement@3d7a59f3=[], io.strimzi.test.logs.CollectorElement@dce7372d=[rolling-update-st], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-3], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-30 20:16:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:267] testKafkaAndZookeeperScaleUpScaleDown - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendSimpleMessageTls, testUpdateUser, testCruiseControlBasicAPIRequests, testReceiveSimpleMessageTls, testSendMessagesTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testSendMessagesCustomListenerTlsScramSha] to and randomly select one to start execution
2022-03-30 20:16:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:93] [rollingupdate.RollingUpdateST] - Removing parallel test: testKafkaAndZookeeperScaleUpScaleDown
2022-03-30 20:16:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:97] [rollingupdate.RollingUpdateST] - Parallel test count: 1
2022-03-30 20:16:03 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testKafkaAndZookeeperScaleUpScaleDown-FINISHED
2022-03-30 20:16:03 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 20:16:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 20:16:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:690] [rollingupdate.RollingUpdateST - After All] - Clean up after test suite
2022-03-30 20:16:03 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:16:03 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:346] In context RollingUpdateST is everything deleted.
2022-03-30 20:16:03 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:16:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Namespace rolling-update-st removal
2022-03-30 20:16:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-30 20:16:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-30 20:16:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:16:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace rolling-update-st removal not ready, will try again in 1000 ms (479921ms till timeout)
2022-03-30 20:16:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-30 20:16:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-30 20:16:04 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:16:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace rolling-update-st removal not ready, will try again in 1000 ms (478841ms till timeout)
2022-03-30 20:16:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 3
2022-03-30 20:16:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 3
2022-03-30 20:16:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-30 20:16:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 3
2022-03-30 20:16:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-30 20:16:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:16:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace rolling-update-st removal not ready, will try again in 1000 ms (477767ms till timeout)
2022-03-30 20:16:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 3
2022-03-30 20:16:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-30 20:16:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-30 20:16:06 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:16:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace rolling-update-st removal not ready, will try again in 1000 ms (476696ms till timeout)
2022-03-30 20:16:07 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-cae962ba-kafka-exporter-7f7b884685-z58x2=3f47c89e-54ea-4d0f-baef-e0524f136ecc}
2022-03-30 20:16:07 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-cae962ba-kafka-exporter-7f7b884685-z58x2=3f47c89e-54ea-4d0f-baef-e0524f136ecc, my-cluster-cae962ba-kafka-exporter-8585666bcd-lr6gn=b38199a1-d119-4cf4-be0f-50fbe757b292}
2022-03-30 20:16:07 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-cae962ba-kafka-exporter-7f7b884685-z58x2=3f47c89e-54ea-4d0f-baef-e0524f136ecc}
2022-03-30 20:16:07 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Deployment my-cluster-cae962ba-kafka-exporter rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (569952ms till timeout)
2022-03-30 20:16:07 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-30 20:16:07 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-30 20:16:07 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:16:07 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace rolling-update-st removal not ready, will try again in 1000 ms (475618ms till timeout)
2022-03-30 20:16:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-30 20:16:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace infra-namespace get Namespace rolling-update-st -o yaml
2022-03-30 20:16:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 20:16:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 20:16:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "rolling-update-st" not found
2022-03-30 20:16:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 20:16:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[], io.strimzi.test.logs.CollectorElement@3d7a59f3=[], io.strimzi.test.logs.CollectorElement@dce7372d=[], io.strimzi.test.logs.CollectorElement@7386938d=[namespace-3], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-30 20:16:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:254] RollingUpdateST - Notifies waiting test suites:[HttpBridgeTlsST, CruiseControlApiST, UserST, CruiseControlST, ListenersST] to and randomly select one to start execution
2022-03-30 20:16:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:85] [rollingupdate.RollingUpdateST] - Removing parallel suite: RollingUpdateST
2022-03-30 20:16:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:89] [rollingupdate.RollingUpdateST] - Parallel suites count: 2
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 645.34 s - in io.strimzi.systemtest.rollingupdate.RollingUpdateST
[[1;34mINFO[m] Running io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST
2022-03-30 20:16:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 20:16:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:667] [mirrormaker.MirrorMaker2IsolatedST - Before All] - Setup test suite environment
2022-03-30 20:16:08 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 20:16:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:16:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-cae962ba-kafka-exporter-7f7b884685-z58x2=3f47c89e-54ea-4d0f-baef-e0524f136ecc}
2022-03-30 20:16:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-cae962ba-kafka-exporter-7f7b884685-z58x2=3f47c89e-54ea-4d0f-baef-e0524f136ecc, my-cluster-cae962ba-kafka-exporter-8585666bcd-lr6gn=b38199a1-d119-4cf4-be0f-50fbe757b292}
2022-03-30 20:16:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {my-cluster-cae962ba-kafka-exporter-7f7b884685-z58x2=3f47c89e-54ea-4d0f-baef-e0524f136ecc}
2022-03-30 20:16:12 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Deployment my-cluster-cae962ba-kafka-exporter rolling update in namespace:namespace-3 not ready, will try again in 5000 ms (564945ms till timeout)
2022-03-30 20:16:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:16:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {my-cluster-cae962ba-kafka-exporter-7f7b884685-z58x2=3f47c89e-54ea-4d0f-baef-e0524f136ecc}
2022-03-30 20:16:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {my-cluster-cae962ba-kafka-exporter-8585666bcd-lr6gn=b38199a1-d119-4cf4-be0f-50fbe757b292}
2022-03-30 20:16:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DeploymentUtils:119] All pods seem to have rolled
2022-03-30 20:16:17 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-cae962ba-kafka-exporter will be ready
2022-03-30 20:16:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-cae962ba-kafka-exporter will be ready
2022-03-30 20:16:17 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-cae962ba-kafka-exporter is ready
2022-03-30 20:16:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-cae962ba-kafka-exporter}, additionalProperties={})to be ready
2022-03-30 20:16:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-exporter-8585666bcd-lr6gn not ready: my-cluster-cae962ba-kafka-exporter)
2022-03-30 20:16:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-kafka-exporter-8585666bcd-lr6gn are ready
2022-03-30 20:16:17 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-cae962ba-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-30 20:16:18 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-exporter-8585666bcd-lr6gn not ready: my-cluster-cae962ba-kafka-exporter)
2022-03-30 20:16:18 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-kafka-exporter-8585666bcd-lr6gn are ready
2022-03-30 20:16:18 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-cae962ba-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598992ms till timeout)
2022-03-30 20:16:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:16:19 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-exporter-8585666bcd-lr6gn not ready: my-cluster-cae962ba-kafka-exporter)
2022-03-30 20:16:19 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-kafka-exporter-8585666bcd-lr6gn are ready
2022-03-30 20:16:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-cae962ba-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597989ms till timeout)
2022-03-30 20:16:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:20 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-exporter-8585666bcd-lr6gn not ready: my-cluster-cae962ba-kafka-exporter)
2022-03-30 20:16:20 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-kafka-exporter-8585666bcd-lr6gn are ready
2022-03-30 20:16:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-cae962ba-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596984ms till timeout)
2022-03-30 20:16:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-exporter-8585666bcd-lr6gn not ready: my-cluster-cae962ba-kafka-exporter)
2022-03-30 20:16:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-kafka-exporter-8585666bcd-lr6gn are ready
2022-03-30 20:16:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-cae962ba-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595979ms till timeout)
2022-03-30 20:16:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-exporter-8585666bcd-lr6gn not ready: my-cluster-cae962ba-kafka-exporter)
2022-03-30 20:16:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-kafka-exporter-8585666bcd-lr6gn are ready
2022-03-30 20:16:22 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-cae962ba-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594975ms till timeout)
2022-03-30 20:16:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-exporter-8585666bcd-lr6gn not ready: my-cluster-cae962ba-kafka-exporter)
2022-03-30 20:16:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-kafka-exporter-8585666bcd-lr6gn are ready
2022-03-30 20:16:23 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-cae962ba-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593970ms till timeout)
2022-03-30 20:16:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:16:24 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-exporter-8585666bcd-lr6gn not ready: my-cluster-cae962ba-kafka-exporter)
2022-03-30 20:16:24 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-kafka-exporter-8585666bcd-lr6gn are ready
2022-03-30 20:16:24 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-cae962ba-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592966ms till timeout)
2022-03-30 20:16:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:25 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-exporter-8585666bcd-lr6gn not ready: my-cluster-cae962ba-kafka-exporter)
2022-03-30 20:16:25 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-kafka-exporter-8585666bcd-lr6gn are ready
2022-03-30 20:16:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-cae962ba-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591961ms till timeout)
2022-03-30 20:16:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-exporter-8585666bcd-lr6gn not ready: my-cluster-cae962ba-kafka-exporter)
2022-03-30 20:16:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-kafka-exporter-8585666bcd-lr6gn are ready
2022-03-30 20:16:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-cae962ba, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-cae962ba-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590956ms till timeout)
2022-03-30 20:16:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-cae962ba-kafka-exporter-8585666bcd-lr6gn not ready: my-cluster-cae962ba-kafka-exporter)
2022-03-30 20:16:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods my-cluster-cae962ba-kafka-exporter-8585666bcd-lr6gn are ready
2022-03-30 20:16:27 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-cae962ba-kafka-exporter rolling update finished
2022-03-30 20:16:27 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:317] Checking the certificates have been replaced
2022-03-30 20:16:27 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:331] Checking consumed messages to pod:my-cluster-cae962ba-kafka-clients-59f44d88cb-82mb5
2022-03-30 20:16:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4fcb6f9, which are set.
2022-03-30 20:16:27 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@52df279d, messages=[], arguments=[--group-id, my-consumer-group-615746865, --topic, my-topic-1672280868-1119644965, --group-instance-id, instance381411698, --bootstrap-server, my-cluster-cae962ba-kafka-bootstrap.namespace-3.svc:9092, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-cae962ba-kafka-clients-59f44d88cb-82mb5', podNamespace='namespace-3', bootstrapServer='my-cluster-cae962ba-kafka-bootstrap.namespace-3.svc:9092', topicName='my-topic-1672280868-1119644965', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-615746865', consumerInstanceId='instance381411698', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4fcb6f9}
2022-03-30 20:16:27 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-cae962ba-kafka-bootstrap.namespace-3.svc:9092#my-topic-1672280868-1119644965 from pod my-cluster-cae962ba-kafka-clients-59f44d88cb-82mb5
2022-03-30 20:16:27 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-cae962ba-kafka-clients-59f44d88cb-82mb5 -n namespace-3 -- /opt/kafka/consumer.sh --group-id my-consumer-group-615746865 --topic my-topic-1672280868-1119644965 --group-instance-id instance381411698 --bootstrap-server my-cluster-cae962ba-kafka-bootstrap.namespace-3.svc:9092 --max-messages 100
2022-03-30 20:16:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-cae962ba-kafka-clients-59f44d88cb-82mb5 -n namespace-3 -- /opt/kafka/consumer.sh --group-id my-consumer-group-615746865 --topic my-topic-1672280868-1119644965 --group-instance-id instance381411698 --bootstrap-server my-cluster-cae962ba-kafka-bootstrap.namespace-3.svc:9092 --max-messages 100
2022-03-30 20:16:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:16:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:33 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-30 20:16:33 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-03-30 20:16:33 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser bob-my-cluster-cae962ba in namespace infra-namespace
2022-03-30 20:16:33 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-3
2022-03-30 20:16:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:bob-my-cluster-cae962ba
2022-03-30 20:16:33 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: bob-my-cluster-cae962ba will have desired state: Ready
2022-03-30 20:16:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: bob-my-cluster-cae962ba will have desired state: Ready
2022-03-30 20:16:33 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaUser: bob-my-cluster-cae962ba will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 20:16:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:16:34 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaUser: bob-my-cluster-cae962ba is in desired state: Ready
2022-03-30 20:16:34 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-cae962ba-kafka-clients-tls in namespace namespace-3
2022-03-30 20:16:34 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-3
2022-03-30 20:16:34 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-cae962ba-kafka-clients-tls
2022-03-30 20:16:34 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-cae962ba-kafka-clients-tls will be ready
2022-03-30 20:16:34 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-cae962ba-kafka-clients-tls will be ready
2022-03-30 20:16:34 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-cae962ba-kafka-clients-tls will be ready not ready, will try again in 1000 ms (479996ms till timeout)
2022-03-30 20:16:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:35 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-cae962ba-kafka-clients-tls is ready
2022-03-30 20:16:35 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:355] Checking consumed messages to pod:my-cluster-cae962ba-kafka-clients-tls-549fc5fd8f-cm7n8
2022-03-30 20:16:35 [ForkJoinPool-3-worker-15] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1ba46c41, which are set.
2022-03-30 20:16:35 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@33bf8569, messages=[], arguments=[USER=bob_my_cluster_cae962ba, --group-id, my-consumer-group-339944825, --topic, my-topic-1672280868-1119644965, --group-instance-id, instance919479402, --bootstrap-server, my-cluster-cae962ba-kafka-bootstrap.namespace-3.svc:9093, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-cae962ba-kafka-clients-tls-549fc5fd8f-cm7n8', podNamespace='namespace-3', bootstrapServer='my-cluster-cae962ba-kafka-bootstrap.namespace-3.svc:9093', topicName='my-topic-1672280868-1119644965', maxMessages=100, kafkaUsername='bob-my-cluster-cae962ba', consumerGroupName='my-consumer-group-339944825', consumerInstanceId='instance919479402', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1ba46c41}
2022-03-30 20:16:35 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-cae962ba-kafka-bootstrap.namespace-3.svc:9093#my-topic-1672280868-1119644965 from pod my-cluster-cae962ba-kafka-clients-tls-549fc5fd8f-cm7n8
2022-03-30 20:16:35 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-cae962ba-kafka-clients-tls-549fc5fd8f-cm7n8 -n namespace-3 -- /opt/kafka/consumer.sh USER=bob_my_cluster_cae962ba --group-id my-consumer-group-339944825 --topic my-topic-1672280868-1119644965 --group-instance-id instance919479402 --bootstrap-server my-cluster-cae962ba-kafka-bootstrap.namespace-3.svc:9093 --max-messages 100
2022-03-30 20:16:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-cae962ba-kafka-clients-tls-549fc5fd8f-cm7n8 -n namespace-3 -- /opt/kafka/consumer.sh USER=bob_my_cluster_cae962ba --group-id my-consumer-group-339944825 --topic my-topic-1672280868-1119644965 --group-instance-id instance919479402 --bootstrap-server my-cluster-cae962ba-kafka-bootstrap.namespace-3.svc:9093 --max-messages 100
2022-03-30 20:16:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:16:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:42 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-30 20:16:42 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-03-30 20:16:42 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 20:16:42 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:675] [security.SecurityST - After Each] - Clean up after test
2022-03-30 20:16:42 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:16:42 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testAutoRenewAllCaCertsTriggeredByAnno
2022-03-30 20:16:42 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-cae962ba-kafka-clients in namespace namespace-3
2022-03-30 20:16:42 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-cae962ba-kafka-clients
2022-03-30 20:16:42 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-cae962ba-kafka-clients not ready, will try again in 10000 ms (479992ms till timeout)
2022-03-30 20:16:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:16:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:16:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-cae962ba-kafka-clients not ready, will try again in 10000 ms (469982ms till timeout)
2022-03-30 20:16:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:16:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:16:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:16:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:16:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:02 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-cae962ba-kafka-clients not ready, will try again in 10000 ms (459972ms till timeout)
2022-03-30 20:17:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:17:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:17:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:12 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-cae962ba-kafka-clients not ready, will try again in 10000 ms (449962ms till timeout)
2022-03-30 20:17:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:17:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:17:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:22 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-cae962ba-kafka-clients not ready, will try again in 10000 ms (439953ms till timeout)
2022-03-30 20:17:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:17:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:17:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-cae962ba-kafka-clients not ready, will try again in 10000 ms (429943ms till timeout)
2022-03-30 20:17:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:17:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:17:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:42 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-cae962ba-kafka-clients not ready, will try again in 10000 ms (419934ms till timeout)
2022-03-30 20:17:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:17:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:17:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-cae962ba-kafka-clients not ready, will try again in 10000 ms (409924ms till timeout)
2022-03-30 20:17:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:17:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:17:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:17:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:17:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:02 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-cae962ba-kafka-clients-tls in namespace namespace-3
2022-03-30 20:18:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-cae962ba-kafka-clients-tls
2022-03-30 20:18:02 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaUser bob-my-cluster-cae962ba in namespace namespace-3
2022-03-30 20:18:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:bob-my-cluster-cae962ba
2022-03-30 20:18:02 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:bob-my-cluster-cae962ba not ready, will try again in 10000 ms (179993ms till timeout)
2022-03-30 20:18:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:18:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:18:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1731919654-539989781 in namespace namespace-3
2022-03-30 20:18:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1731919654-539989781
2022-03-30 20:18:12 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1731919654-539989781 not ready, will try again in 10000 ms (179995ms till timeout)
2022-03-30 20:18:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:18:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:18:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:22 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1672280868-1119644965 in namespace namespace-3
2022-03-30 20:18:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1672280868-1119644965
2022-03-30 20:18:22 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1672280868-1119644965 not ready, will try again in 10000 ms (179994ms till timeout)
2022-03-30 20:18:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:18:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:18:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:32 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-cae962ba in namespace namespace-3
2022-03-30 20:18:32 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-3, for cruise control Kafka cluster my-cluster-cae962ba
2022-03-30 20:18:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-cae962ba
2022-03-30 20:18:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-cae962ba not ready, will try again in 10000 ms (839957ms till timeout)
2022-03-30 20:18:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:18:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:18:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:42 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:18:42 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-3 for test case:testAutoRenewAllCaCertsTriggeredByAnno
2022-03-30 20:18:42 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-3 removal
2022-03-30 20:18:42 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:42 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:42 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:18:42 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (479915ms till timeout)
2022-03-30 20:18:43 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:43 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:43 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:18:43 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (478848ms till timeout)
2022-03-30 20:18:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:18:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:44 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:44 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:44 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:18:44 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (477761ms till timeout)
2022-03-30 20:18:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:45 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:45 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:18:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (476686ms till timeout)
2022-03-30 20:18:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:18:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (475614ms till timeout)
2022-03-30 20:18:47 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:47 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:47 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:18:47 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (474546ms till timeout)
2022-03-30 20:18:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:18:48 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:48 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:48 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:18:48 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (473465ms till timeout)
2022-03-30 20:18:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:49 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:49 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:49 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:18:49 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (472391ms till timeout)
2022-03-30 20:18:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:18:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (471315ms till timeout)
2022-03-30 20:18:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:52 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:52 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:18:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (470238ms till timeout)
2022-03-30 20:18:53 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:53 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:53 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:18:53 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (469167ms till timeout)
2022-03-30 20:18:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:18:54 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:54 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:54 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:18:54 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (468088ms till timeout)
2022-03-30 20:18:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:18:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (467015ms till timeout)
2022-03-30 20:18:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:56 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:18:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (465938ms till timeout)
2022-03-30 20:18:57 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:57 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:57 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:18:57 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (464857ms till timeout)
2022-03-30 20:18:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:18:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (463786ms till timeout)
2022-03-30 20:18:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:18:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:18:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:18:59 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:59 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:18:59 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:18:59 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (462712ms till timeout)
2022-03-30 20:19:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:19:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:19:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:00 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:00 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:19:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (461641ms till timeout)
2022-03-30 20:19:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:19:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:19:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (460573ms till timeout)
2022-03-30 20:19:02 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:02 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:19:02 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (459495ms till timeout)
2022-03-30 20:19:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:19:03 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:03 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:03 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:19:03 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (458426ms till timeout)
2022-03-30 20:19:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:19:04 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:04 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:04 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:19:04 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (457353ms till timeout)
2022-03-30 20:19:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:19:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:19:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:19:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:06 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:19:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (456281ms till timeout)
2022-03-30 20:19:07 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:07 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:07 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:19:07 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (455199ms till timeout)
2022-03-30 20:19:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:08 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:08 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:19:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (454123ms till timeout)
2022-03-30 20:19:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:19:09 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:09 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:09 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:19:09 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (453050ms till timeout)
2022-03-30 20:19:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:19:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:19:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:10 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:10 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:19:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (451976ms till timeout)
2022-03-30 20:19:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:19:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:19:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:11 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:19:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (450904ms till timeout)
2022-03-30 20:19:12 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:12 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:19:12 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (449830ms till timeout)
2022-03-30 20:19:13 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:13 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:13 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:19:13 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (448755ms till timeout)
2022-03-30 20:19:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:19:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:19:14 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:14 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:14 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:19:14 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (447680ms till timeout)
2022-03-30 20:19:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:19:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:19:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:15 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:15 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:19:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (446604ms till timeout)
2022-03-30 20:19:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:19:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:19:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (445527ms till timeout)
2022-03-30 20:19:17 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:17 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:19:17 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (444455ms till timeout)
2022-03-30 20:19:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:19:18 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:18 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:18 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:19:18 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (443379ms till timeout)
2022-03-30 20:19:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:19:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:20 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:20 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:19:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (442306ms till timeout)
2022-03-30 20:19:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:19:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:19:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:19:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:19:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (441225ms till timeout)
2022-03-30 20:19:22 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:19:22 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (440153ms till timeout)
2022-03-30 20:19:23 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:19:23 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (439086ms till timeout)
2022-03-30 20:19:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:19:24 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:24 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:24 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:19:24 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (438005ms till timeout)
2022-03-30 20:19:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:19:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:19:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:25 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace infra-namespace get Namespace namespace-3 -o yaml
2022-03-30 20:19:25 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 20:19:25 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 20:19:25 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-3" not found
2022-03-30 20:19:25 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 20:19:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[], io.strimzi.test.logs.CollectorElement@3d7a59f3=[], io.strimzi.test.logs.CollectorElement@dce7372d=[], io.strimzi.test.logs.CollectorElement@7386938d=[], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@bfb2174d=[security-st]}
2022-03-30 20:19:25 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:267] testAutoRenewAllCaCertsTriggeredByAnno - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendSimpleMessageTls, testUpdateUser, testCruiseControlBasicAPIRequests, testReceiveSimpleMessageTls, testSendMessagesTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testSendMessagesCustomListenerTlsScramSha] to and randomly select one to start execution
2022-03-30 20:19:25 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:93] [security.SecurityST] - Removing parallel test: testAutoRenewAllCaCertsTriggeredByAnno
2022-03-30 20:19:25 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:97] [security.SecurityST] - Parallel test count: 0
2022-03-30 20:19:25 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testAutoRenewAllCaCertsTriggeredByAnno-FINISHED
2022-03-30 20:19:25 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 20:19:25 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 20:19:25 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:690] [security.SecurityST - After All] - Clean up after test suite
2022-03-30 20:19:25 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:19:25 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:346] In context SecurityST is everything deleted.
2022-03-30 20:19:25 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:19:25 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 20:19:25 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlWithRebalanceResourceAndRefreshAnnotation-STARTED
2022-03-30 20:19:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 20:19:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:659] [cruisecontrol.CruiseControlST - Before Each] - Setup test case environment
2022-03-30 20:19:25 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 20:19:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba, testSendSimpleMessageTls=my-cluster-ca7325d7, testUpdateUser=my-cluster-6e6fa8fe, testSendMessagesTlsScramSha=my-cluster-84882f67, testReceiveSimpleMessageTls=my-cluster-2e450144, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37, testCruiseControlBasicAPIRequests=my-cluster-8aed4745, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140}
2022-03-30 20:19:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-838849261-890460734, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1731919654-539989781, testSendSimpleMessageTls=my-user-1309491966-2015599610, testUpdateUser=my-user-1387197422-925410353, testSendMessagesTlsScramSha=my-user-586569987-2105688079, testReceiveSimpleMessageTls=my-user-1669668617-1031881775, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1886945581-244087591, testSendMessagesCustomListenerTlsScramSha=my-user-1118022021-805525218, testCruiseControlBasicAPIRequests=my-user-1861484466-561878159, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677}
2022-03-30 20:19:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-2138084746-704381117, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1672280868-1119644965, testSendSimpleMessageTls=my-topic-435427300-917779110, testUpdateUser=my-topic-1875783706-1763804507, testSendMessagesTlsScramSha=my-topic-255809133-823300208, testReceiveSimpleMessageTls=my-topic-368564676-637091599, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-306860904-106838350, testSendMessagesCustomListenerTlsScramSha=my-topic-963888555-1712711326, testCruiseControlBasicAPIRequests=my-topic-1631570507-1308241024, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206}
2022-03-30 20:19:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba-kafka-clients, testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testUpdateUser=my-cluster-6e6fa8fe-kafka-clients, testSendMessagesTlsScramSha=my-cluster-84882f67-kafka-clients, testReceiveSimpleMessageTls=my-cluster-2e450144-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-8aed4745-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients}
2022-03-30 20:19:25 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Namespace security-st removal
2022-03-30 20:19:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-30 20:19:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:19:25 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-f141387d in namespace cruise-control-st
2022-03-30 20:19:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-f141387d
2022-03-30 20:19:25 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-f141387d will have desired state: Ready
2022-03-30 20:19:25 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-f141387d will have desired state: Ready
2022-03-30 20:19:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1319998ms till timeout)
2022-03-30 20:19:25 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-30 20:19:25 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:19:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace security-st removal not ready, will try again in 1000 ms (479912ms till timeout)
2022-03-30 20:19:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 2
2022-03-30 20:19:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1318995ms till timeout)
2022-03-30 20:19:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-30 20:19:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-30 20:19:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:19:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace security-st removal not ready, will try again in 1000 ms (478836ms till timeout)
2022-03-30 20:19:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1317991ms till timeout)
2022-03-30 20:19:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-30 20:19:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-30 20:19:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:19:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace security-st removal not ready, will try again in 1000 ms (477761ms till timeout)
2022-03-30 20:19:28 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1316989ms till timeout)
2022-03-30 20:19:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-30 20:19:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 2
2022-03-30 20:19:28 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-30 20:19:28 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:19:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace security-st removal not ready, will try again in 1000 ms (476691ms till timeout)
2022-03-30 20:19:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 2
2022-03-30 20:19:29 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1315986ms till timeout)
2022-03-30 20:19:29 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-30 20:19:29 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-30 20:19:29 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:19:29 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace security-st removal not ready, will try again in 1000 ms (475621ms till timeout)
2022-03-30 20:19:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 2
2022-03-30 20:19:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 2
2022-03-30 20:19:30 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1314983ms till timeout)
2022-03-30 20:19:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-30 20:19:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace infra-namespace get Namespace security-st -o yaml
2022-03-30 20:19:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 20:19:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 20:19:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "security-st" not found
2022-03-30 20:19:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 20:19:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[], io.strimzi.test.logs.CollectorElement@3d7a59f3=[], io.strimzi.test.logs.CollectorElement@dce7372d=[], io.strimzi.test.logs.CollectorElement@7386938d=[], io.strimzi.test.logs.CollectorElement@afd54539=[cruise-control-st], io.strimzi.test.logs.CollectorElement@cf7c9d53=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@bfb2174d=[]}
2022-03-30 20:19:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:254] SecurityST - Notifies waiting test suites:[HttpBridgeTlsST, CruiseControlApiST, UserST, CruiseControlST, ListenersST] to and randomly select one to start execution
2022-03-30 20:19:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:85] [security.SecurityST] - Removing parallel suite: SecurityST
2022-03-30 20:19:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:89] [security.SecurityST] - Parallel suites count: 1
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 856.412 s - in io.strimzi.systemtest.security.SecurityST
[[1;34mINFO[m] Running io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST
2022-03-30 20:19:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:666] ============================================================================
2022-03-30 20:19:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:667] [mirrormaker.MirrorMakerIsolatedST - Before All] - Setup test suite environment
2022-03-30 20:19:30 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 20:19:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:19:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:19:31 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1313979ms till timeout)
2022-03-30 20:19:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1312976ms till timeout)
2022-03-30 20:19:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1311972ms till timeout)
2022-03-30 20:19:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:19:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:19:34 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1310968ms till timeout)
2022-03-30 20:19:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:19:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:19:35 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1309965ms till timeout)
2022-03-30 20:19:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:19:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:19:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1308962ms till timeout)
2022-03-30 20:19:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1307959ms till timeout)
2022-03-30 20:19:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1306956ms till timeout)
2022-03-30 20:19:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:19:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:19:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1305953ms till timeout)
2022-03-30 20:19:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:19:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:19:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1304950ms till timeout)
2022-03-30 20:19:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:19:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:19:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1303946ms till timeout)
2022-03-30 20:19:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1302943ms till timeout)
2022-03-30 20:19:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1301940ms till timeout)
2022-03-30 20:19:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:19:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:19:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1300937ms till timeout)
2022-03-30 20:19:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:19:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:19:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1299934ms till timeout)
2022-03-30 20:19:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:19:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:19:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1298931ms till timeout)
2022-03-30 20:19:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1297928ms till timeout)
2022-03-30 20:19:48 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1296925ms till timeout)
2022-03-30 20:19:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:19:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:19:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1295922ms till timeout)
2022-03-30 20:19:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:19:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:19:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1294917ms till timeout)
2022-03-30 20:19:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:19:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:19:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1293914ms till timeout)
2022-03-30 20:19:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1292911ms till timeout)
2022-03-30 20:19:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1291908ms till timeout)
2022-03-30 20:19:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:19:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:19:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1290905ms till timeout)
2022-03-30 20:19:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:19:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:19:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1289901ms till timeout)
2022-03-30 20:19:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:19:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:19:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1288899ms till timeout)
2022-03-30 20:19:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1287895ms till timeout)
2022-03-30 20:19:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1286892ms till timeout)
2022-03-30 20:19:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:19:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:19:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:19:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1285889ms till timeout)
2022-03-30 20:20:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1284885ms till timeout)
2022-03-30 20:20:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1283882ms till timeout)
2022-03-30 20:20:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1282878ms till timeout)
2022-03-30 20:20:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1281870ms till timeout)
2022-03-30 20:20:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:20:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1280865ms till timeout)
2022-03-30 20:20:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1279862ms till timeout)
2022-03-30 20:20:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1278859ms till timeout)
2022-03-30 20:20:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1277856ms till timeout)
2022-03-30 20:20:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1276853ms till timeout)
2022-03-30 20:20:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:20:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1275850ms till timeout)
2022-03-30 20:20:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1274847ms till timeout)
2022-03-30 20:20:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1273843ms till timeout)
2022-03-30 20:20:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1272840ms till timeout)
2022-03-30 20:20:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1271837ms till timeout)
2022-03-30 20:20:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:20:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1270834ms till timeout)
2022-03-30 20:20:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1269831ms till timeout)
2022-03-30 20:20:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1268828ms till timeout)
2022-03-30 20:20:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1267825ms till timeout)
2022-03-30 20:20:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1266822ms till timeout)
2022-03-30 20:20:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:20:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1265818ms till timeout)
2022-03-30 20:20:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1264815ms till timeout)
2022-03-30 20:20:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1263812ms till timeout)
2022-03-30 20:20:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1262808ms till timeout)
2022-03-30 20:20:23 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1261805ms till timeout)
2022-03-30 20:20:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:20:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1260802ms till timeout)
2022-03-30 20:20:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1259798ms till timeout)
2022-03-30 20:20:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1258794ms till timeout)
2022-03-30 20:20:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1257791ms till timeout)
2022-03-30 20:20:28 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1256788ms till timeout)
2022-03-30 20:20:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:20:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:29 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1255784ms till timeout)
2022-03-30 20:20:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:30 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1254781ms till timeout)
2022-03-30 20:20:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:31 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1253777ms till timeout)
2022-03-30 20:20:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1252774ms till timeout)
2022-03-30 20:20:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:20:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1251771ms till timeout)
2022-03-30 20:20:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:34 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1250768ms till timeout)
2022-03-30 20:20:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:35 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1249764ms till timeout)
2022-03-30 20:20:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1248761ms till timeout)
2022-03-30 20:20:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1247757ms till timeout)
2022-03-30 20:20:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:20:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1246754ms till timeout)
2022-03-30 20:20:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1245751ms till timeout)
2022-03-30 20:20:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1244748ms till timeout)
2022-03-30 20:20:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1243744ms till timeout)
2022-03-30 20:20:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1242741ms till timeout)
2022-03-30 20:20:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:20:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1241738ms till timeout)
2022-03-30 20:20:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1240735ms till timeout)
2022-03-30 20:20:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1239731ms till timeout)
2022-03-30 20:20:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1238728ms till timeout)
2022-03-30 20:20:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1237724ms till timeout)
2022-03-30 20:20:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:20:48 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1236721ms till timeout)
2022-03-30 20:20:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1235717ms till timeout)
2022-03-30 20:20:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1234714ms till timeout)
2022-03-30 20:20:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1233711ms till timeout)
2022-03-30 20:20:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1232706ms till timeout)
2022-03-30 20:20:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:20:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1231703ms till timeout)
2022-03-30 20:20:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1230700ms till timeout)
2022-03-30 20:20:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1229696ms till timeout)
2022-03-30 20:20:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1228693ms till timeout)
2022-03-30 20:20:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1227690ms till timeout)
2022-03-30 20:20:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:20:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1226687ms till timeout)
2022-03-30 20:20:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:20:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:20:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1225683ms till timeout)
2022-03-30 20:21:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1224680ms till timeout)
2022-03-30 20:21:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1223676ms till timeout)
2022-03-30 20:21:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1222673ms till timeout)
2022-03-30 20:21:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:21:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1221670ms till timeout)
2022-03-30 20:21:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1220667ms till timeout)
2022-03-30 20:21:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1219663ms till timeout)
2022-03-30 20:21:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (1218660ms till timeout)
2022-03-30 20:21:07 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-f141387d is in desired state: Ready
2022-03-30 20:21:07 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaRebalance my-cluster-f141387d in namespace cruise-control-st
2022-03-30 20:21:07 [ForkJoinPool-3-worker-5] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkarebalances' with unstable version 'v1beta2'
2022-03-30 20:21:07 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaRebalance:my-cluster-f141387d
2022-03-30 20:21:07 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-f141387d will have desired state: PendingProposal
2022-03-30 20:21:07 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for KafkaRebalance: my-cluster-f141387d will have desired state: PendingProposal
2022-03-30 20:21:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: PendingProposal not ready, will try again in 1000 ms (359998ms till timeout)
2022-03-30 20:21:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:21:08 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-f141387d is in desired state: PendingProposal
2022-03-30 20:21:08 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaRebalanceUtils:75] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-f141387d): ============================================================================
2022-03-30 20:21:08 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaRebalanceUtils:76] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-f141387d): PendingProposal
2022-03-30 20:21:08 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaRebalanceUtils:77] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-f141387d): ============================================================================
2022-03-30 20:21:08 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaRebalanceUtils:81] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-f141387d): Verifying that KafkaRebalance resource is in PendingProposal state
2022-03-30 20:21:08 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-f141387d will have desired state: PendingProposal
2022-03-30 20:21:08 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for KafkaRebalance: my-cluster-f141387d will have desired state: PendingProposal
2022-03-30 20:21:08 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-f141387d is in desired state: PendingProposal
2022-03-30 20:21:08 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaRebalanceUtils:85] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-f141387d): Verifying that KafkaRebalance resource is in ProposalReady state
2022-03-30 20:21:08 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady
2022-03-30 20:21:08 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady
2022-03-30 20:21:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 20:21:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-30 20:21:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (597991ms till timeout)
2022-03-30 20:21:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (596987ms till timeout)
2022-03-30 20:21:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (595983ms till timeout)
2022-03-30 20:21:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:21:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (594979ms till timeout)
2022-03-30 20:21:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (593976ms till timeout)
2022-03-30 20:21:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (592973ms till timeout)
2022-03-30 20:21:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (591970ms till timeout)
2022-03-30 20:21:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (590966ms till timeout)
2022-03-30 20:21:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:21:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (589963ms till timeout)
2022-03-30 20:21:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (588960ms till timeout)
2022-03-30 20:21:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (587957ms till timeout)
2022-03-30 20:21:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (586954ms till timeout)
2022-03-30 20:21:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (585951ms till timeout)
2022-03-30 20:21:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:21:23 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (584948ms till timeout)
2022-03-30 20:21:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (583946ms till timeout)
2022-03-30 20:21:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (582942ms till timeout)
2022-03-30 20:21:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (581939ms till timeout)
2022-03-30 20:21:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (580936ms till timeout)
2022-03-30 20:21:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:21:28 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (579933ms till timeout)
2022-03-30 20:21:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:29 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (578930ms till timeout)
2022-03-30 20:21:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:30 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (577927ms till timeout)
2022-03-30 20:21:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:31 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (576924ms till timeout)
2022-03-30 20:21:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (575921ms till timeout)
2022-03-30 20:21:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:21:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (574917ms till timeout)
2022-03-30 20:21:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:34 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (573914ms till timeout)
2022-03-30 20:21:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:35 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (572911ms till timeout)
2022-03-30 20:21:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (571908ms till timeout)
2022-03-30 20:21:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (570905ms till timeout)
2022-03-30 20:21:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:21:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (569902ms till timeout)
2022-03-30 20:21:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (568899ms till timeout)
2022-03-30 20:21:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (567896ms till timeout)
2022-03-30 20:21:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (566893ms till timeout)
2022-03-30 20:21:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (565890ms till timeout)
2022-03-30 20:21:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:21:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (564887ms till timeout)
2022-03-30 20:21:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (563884ms till timeout)
2022-03-30 20:21:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (562880ms till timeout)
2022-03-30 20:21:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (561877ms till timeout)
2022-03-30 20:21:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (560874ms till timeout)
2022-03-30 20:21:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:21:48 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (559871ms till timeout)
2022-03-30 20:21:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (558868ms till timeout)
2022-03-30 20:21:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (557865ms till timeout)
2022-03-30 20:21:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (556862ms till timeout)
2022-03-30 20:21:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (555859ms till timeout)
2022-03-30 20:21:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:21:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (554855ms till timeout)
2022-03-30 20:21:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (553852ms till timeout)
2022-03-30 20:21:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:21:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (552849ms till timeout)
2022-03-30 20:21:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (551845ms till timeout)
2022-03-30 20:21:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (550842ms till timeout)
2022-03-30 20:21:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:21:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (549839ms till timeout)
2022-03-30 20:21:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:21:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (548836ms till timeout)
2022-03-30 20:22:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (547833ms till timeout)
2022-03-30 20:22:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (546830ms till timeout)
2022-03-30 20:22:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (545827ms till timeout)
2022-03-30 20:22:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:22:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (544823ms till timeout)
2022-03-30 20:22:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (543820ms till timeout)
2022-03-30 20:22:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (542817ms till timeout)
2022-03-30 20:22:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (541814ms till timeout)
2022-03-30 20:22:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (540811ms till timeout)
2022-03-30 20:22:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:22:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (539808ms till timeout)
2022-03-30 20:22:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (538804ms till timeout)
2022-03-30 20:22:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (537801ms till timeout)
2022-03-30 20:22:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (536796ms till timeout)
2022-03-30 20:22:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (535794ms till timeout)
2022-03-30 20:22:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:22:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (534790ms till timeout)
2022-03-30 20:22:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (533787ms till timeout)
2022-03-30 20:22:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (532784ms till timeout)
2022-03-30 20:22:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (531781ms till timeout)
2022-03-30 20:22:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (530778ms till timeout)
2022-03-30 20:22:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:22:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (529775ms till timeout)
2022-03-30 20:22:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (528771ms till timeout)
2022-03-30 20:22:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (527769ms till timeout)
2022-03-30 20:22:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (526765ms till timeout)
2022-03-30 20:22:23 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (525762ms till timeout)
2022-03-30 20:22:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:22:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (524759ms till timeout)
2022-03-30 20:22:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (523756ms till timeout)
2022-03-30 20:22:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (522753ms till timeout)
2022-03-30 20:22:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (521750ms till timeout)
2022-03-30 20:22:28 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (520747ms till timeout)
2022-03-30 20:22:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:22:29 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (519744ms till timeout)
2022-03-30 20:22:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:30 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (518740ms till timeout)
2022-03-30 20:22:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:31 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (517737ms till timeout)
2022-03-30 20:22:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (516733ms till timeout)
2022-03-30 20:22:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (515730ms till timeout)
2022-03-30 20:22:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:22:34 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (514726ms till timeout)
2022-03-30 20:22:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:35 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (513723ms till timeout)
2022-03-30 20:22:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (512720ms till timeout)
2022-03-30 20:22:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (511717ms till timeout)
2022-03-30 20:22:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (510714ms till timeout)
2022-03-30 20:22:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:22:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (509711ms till timeout)
2022-03-30 20:22:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (508708ms till timeout)
2022-03-30 20:22:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (507705ms till timeout)
2022-03-30 20:22:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (506701ms till timeout)
2022-03-30 20:22:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (505698ms till timeout)
2022-03-30 20:22:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:22:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (504695ms till timeout)
2022-03-30 20:22:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (503692ms till timeout)
2022-03-30 20:22:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (502689ms till timeout)
2022-03-30 20:22:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (501685ms till timeout)
2022-03-30 20:22:48 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (500682ms till timeout)
2022-03-30 20:22:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:22:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (499679ms till timeout)
2022-03-30 20:22:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (498676ms till timeout)
2022-03-30 20:22:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (497673ms till timeout)
2022-03-30 20:22:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (496670ms till timeout)
2022-03-30 20:22:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (495667ms till timeout)
2022-03-30 20:22:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:22:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (494664ms till timeout)
2022-03-30 20:22:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (493660ms till timeout)
2022-03-30 20:22:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:22:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (492657ms till timeout)
2022-03-30 20:22:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (491654ms till timeout)
2022-03-30 20:22:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (490651ms till timeout)
2022-03-30 20:22:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:22:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (489648ms till timeout)
2022-03-30 20:22:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:22:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (488645ms till timeout)
2022-03-30 20:23:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (487642ms till timeout)
2022-03-30 20:23:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (486639ms till timeout)
2022-03-30 20:23:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (485636ms till timeout)
2022-03-30 20:23:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:23:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (484632ms till timeout)
2022-03-30 20:23:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (483629ms till timeout)
2022-03-30 20:23:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (482626ms till timeout)
2022-03-30 20:23:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (481623ms till timeout)
2022-03-30 20:23:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (480620ms till timeout)
2022-03-30 20:23:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:23:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (479616ms till timeout)
2022-03-30 20:23:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (478613ms till timeout)
2022-03-30 20:23:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (477610ms till timeout)
2022-03-30 20:23:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (476607ms till timeout)
2022-03-30 20:23:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (475604ms till timeout)
2022-03-30 20:23:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:23:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (474601ms till timeout)
2022-03-30 20:23:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (473598ms till timeout)
2022-03-30 20:23:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (472595ms till timeout)
2022-03-30 20:23:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (471592ms till timeout)
2022-03-30 20:23:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (470589ms till timeout)
2022-03-30 20:23:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:23:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (469586ms till timeout)
2022-03-30 20:23:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (468583ms till timeout)
2022-03-30 20:23:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (467580ms till timeout)
2022-03-30 20:23:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (466577ms till timeout)
2022-03-30 20:23:23 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (465574ms till timeout)
2022-03-30 20:23:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:23:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (464571ms till timeout)
2022-03-30 20:23:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (463567ms till timeout)
2022-03-30 20:23:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (462564ms till timeout)
2022-03-30 20:23:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (461561ms till timeout)
2022-03-30 20:23:28 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (460558ms till timeout)
2022-03-30 20:23:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:23:29 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (459554ms till timeout)
2022-03-30 20:23:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:30 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (458551ms till timeout)
2022-03-30 20:23:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:31 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (457548ms till timeout)
2022-03-30 20:23:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (456545ms till timeout)
2022-03-30 20:23:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (455542ms till timeout)
2022-03-30 20:23:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:23:34 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (454539ms till timeout)
2022-03-30 20:23:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:35 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (453536ms till timeout)
2022-03-30 20:23:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (452533ms till timeout)
2022-03-30 20:23:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (451530ms till timeout)
2022-03-30 20:23:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (450527ms till timeout)
2022-03-30 20:23:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:23:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (449523ms till timeout)
2022-03-30 20:23:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (448520ms till timeout)
2022-03-30 20:23:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (447517ms till timeout)
2022-03-30 20:23:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (446514ms till timeout)
2022-03-30 20:23:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (445511ms till timeout)
2022-03-30 20:23:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:23:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (444507ms till timeout)
2022-03-30 20:23:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (443504ms till timeout)
2022-03-30 20:23:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (442501ms till timeout)
2022-03-30 20:23:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (441498ms till timeout)
2022-03-30 20:23:48 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (440495ms till timeout)
2022-03-30 20:23:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:23:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (439492ms till timeout)
2022-03-30 20:23:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (438489ms till timeout)
2022-03-30 20:23:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (437485ms till timeout)
2022-03-30 20:23:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (436482ms till timeout)
2022-03-30 20:23:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (435479ms till timeout)
2022-03-30 20:23:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:23:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (434476ms till timeout)
2022-03-30 20:23:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (433473ms till timeout)
2022-03-30 20:23:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:23:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (432470ms till timeout)
2022-03-30 20:23:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (431466ms till timeout)
2022-03-30 20:23:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (430463ms till timeout)
2022-03-30 20:23:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:23:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (429460ms till timeout)
2022-03-30 20:23:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:23:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (428457ms till timeout)
2022-03-30 20:24:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (427454ms till timeout)
2022-03-30 20:24:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (426451ms till timeout)
2022-03-30 20:24:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (425448ms till timeout)
2022-03-30 20:24:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:24:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (424444ms till timeout)
2022-03-30 20:24:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (423441ms till timeout)
2022-03-30 20:24:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (422438ms till timeout)
2022-03-30 20:24:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (421435ms till timeout)
2022-03-30 20:24:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (420432ms till timeout)
2022-03-30 20:24:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:24:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (419429ms till timeout)
2022-03-30 20:24:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (418426ms till timeout)
2022-03-30 20:24:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (417422ms till timeout)
2022-03-30 20:24:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (416419ms till timeout)
2022-03-30 20:24:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (415416ms till timeout)
2022-03-30 20:24:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:24:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (414413ms till timeout)
2022-03-30 20:24:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (413410ms till timeout)
2022-03-30 20:24:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (412407ms till timeout)
2022-03-30 20:24:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (411404ms till timeout)
2022-03-30 20:24:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (410401ms till timeout)
2022-03-30 20:24:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:24:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (409398ms till timeout)
2022-03-30 20:24:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (408395ms till timeout)
2022-03-30 20:24:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (407392ms till timeout)
2022-03-30 20:24:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (406389ms till timeout)
2022-03-30 20:24:23 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (405386ms till timeout)
2022-03-30 20:24:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:24:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (404383ms till timeout)
2022-03-30 20:24:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (403380ms till timeout)
2022-03-30 20:24:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (402377ms till timeout)
2022-03-30 20:24:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (401374ms till timeout)
2022-03-30 20:24:28 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (400370ms till timeout)
2022-03-30 20:24:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:24:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:29 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (399367ms till timeout)
2022-03-30 20:24:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:30 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (398364ms till timeout)
2022-03-30 20:24:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:31 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (397361ms till timeout)
2022-03-30 20:24:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (396358ms till timeout)
2022-03-30 20:24:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (395355ms till timeout)
2022-03-30 20:24:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:24:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:34 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (394352ms till timeout)
2022-03-30 20:24:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:35 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (393349ms till timeout)
2022-03-30 20:24:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (392346ms till timeout)
2022-03-30 20:24:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (391343ms till timeout)
2022-03-30 20:24:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (390340ms till timeout)
2022-03-30 20:24:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:24:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (389337ms till timeout)
2022-03-30 20:24:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (388334ms till timeout)
2022-03-30 20:24:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (387330ms till timeout)
2022-03-30 20:24:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (386327ms till timeout)
2022-03-30 20:24:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (385324ms till timeout)
2022-03-30 20:24:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:24:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (384321ms till timeout)
2022-03-30 20:24:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (383318ms till timeout)
2022-03-30 20:24:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (382315ms till timeout)
2022-03-30 20:24:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (381312ms till timeout)
2022-03-30 20:24:48 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (380309ms till timeout)
2022-03-30 20:24:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:24:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (379306ms till timeout)
2022-03-30 20:24:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (378304ms till timeout)
2022-03-30 20:24:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (377301ms till timeout)
2022-03-30 20:24:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (376298ms till timeout)
2022-03-30 20:24:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (375295ms till timeout)
2022-03-30 20:24:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:24:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (374291ms till timeout)
2022-03-30 20:24:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (373288ms till timeout)
2022-03-30 20:24:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (372285ms till timeout)
2022-03-30 20:24:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (371282ms till timeout)
2022-03-30 20:24:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (370279ms till timeout)
2022-03-30 20:24:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:24:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:24:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:24:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (369275ms till timeout)
2022-03-30 20:25:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (368272ms till timeout)
2022-03-30 20:25:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (367269ms till timeout)
2022-03-30 20:25:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (366266ms till timeout)
2022-03-30 20:25:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (365263ms till timeout)
2022-03-30 20:25:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:25:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (364260ms till timeout)
2022-03-30 20:25:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (363257ms till timeout)
2022-03-30 20:25:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (362253ms till timeout)
2022-03-30 20:25:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (361250ms till timeout)
2022-03-30 20:25:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (360247ms till timeout)
2022-03-30 20:25:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:25:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (359244ms till timeout)
2022-03-30 20:25:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (358240ms till timeout)
2022-03-30 20:25:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (357237ms till timeout)
2022-03-30 20:25:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (356234ms till timeout)
2022-03-30 20:25:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (355231ms till timeout)
2022-03-30 20:25:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:25:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (354228ms till timeout)
2022-03-30 20:25:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (353225ms till timeout)
2022-03-30 20:25:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (352222ms till timeout)
2022-03-30 20:25:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (351219ms till timeout)
2022-03-30 20:25:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (350216ms till timeout)
2022-03-30 20:25:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:25:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (349212ms till timeout)
2022-03-30 20:25:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (348210ms till timeout)
2022-03-30 20:25:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (347206ms till timeout)
2022-03-30 20:25:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (346203ms till timeout)
2022-03-30 20:25:23 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (345200ms till timeout)
2022-03-30 20:25:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:25:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (344197ms till timeout)
2022-03-30 20:25:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (343194ms till timeout)
2022-03-30 20:25:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (342191ms till timeout)
2022-03-30 20:25:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (341187ms till timeout)
2022-03-30 20:25:28 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (340184ms till timeout)
2022-03-30 20:25:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:25:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:29 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (339181ms till timeout)
2022-03-30 20:25:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:30 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (338178ms till timeout)
2022-03-30 20:25:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:31 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (337174ms till timeout)
2022-03-30 20:25:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (336171ms till timeout)
2022-03-30 20:25:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (335168ms till timeout)
2022-03-30 20:25:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:25:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:34 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (334165ms till timeout)
2022-03-30 20:25:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:35 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (333162ms till timeout)
2022-03-30 20:25:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (332159ms till timeout)
2022-03-30 20:25:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (331156ms till timeout)
2022-03-30 20:25:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:25:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (330153ms till timeout)
2022-03-30 20:25:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (329150ms till timeout)
2022-03-30 20:25:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (328147ms till timeout)
2022-03-30 20:25:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (327143ms till timeout)
2022-03-30 20:25:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (326140ms till timeout)
2022-03-30 20:25:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:25:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (325137ms till timeout)
2022-03-30 20:25:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (324134ms till timeout)
2022-03-30 20:25:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (323131ms till timeout)
2022-03-30 20:25:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (322127ms till timeout)
2022-03-30 20:25:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (321125ms till timeout)
2022-03-30 20:25:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:25:48 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (320121ms till timeout)
2022-03-30 20:25:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (319118ms till timeout)
2022-03-30 20:25:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (318116ms till timeout)
2022-03-30 20:25:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (317113ms till timeout)
2022-03-30 20:25:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (316109ms till timeout)
2022-03-30 20:25:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:25:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (315106ms till timeout)
2022-03-30 20:25:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (314103ms till timeout)
2022-03-30 20:25:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (313100ms till timeout)
2022-03-30 20:25:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (312097ms till timeout)
2022-03-30 20:25:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (311094ms till timeout)
2022-03-30 20:25:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:25:58 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-f141387d is in desired state: ProposalReady
2022-03-30 20:25:58 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaRebalanceUtils:90] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-f141387d): ============================================================================
2022-03-30 20:25:58 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaRebalanceUtils:91] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-f141387d): ProposalReady
2022-03-30 20:25:58 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaRebalanceUtils:92] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-f141387d): ============================================================================
2022-03-30 20:25:58 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaRebalanceUtils:94] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-f141387d): Triggering the rebalance with annotation strimzi.io/rebalance=approve of KafkaRebalance resource
2022-03-30 20:25:58 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaRebalanceUtils:63] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-f141387d): Annotating KafkaRebalance:my-cluster-f141387d with annotation approve
2022-03-30 20:25:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st annotate kafkarebalance my-cluster-f141387d strimzi.io/rebalance=approve
2022-03-30 20:25:58 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st annotate kafkarebalance my-cluster-f141387d strimzi.io/rebalance=approve
2022-03-30 20:25:58 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:25:58 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaRebalanceUtils:98] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-f141387d): Response from the annotation process kafkarebalance.kafka.strimzi.io/my-cluster-f141387d annotated
2022-03-30 20:25:58 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaRebalanceUtils:100] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-f141387d): Verifying that annotation triggers the Rebalancing state
2022-03-30 20:25:58 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-f141387d will have desired state: Rebalancing
2022-03-30 20:25:58 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for KafkaRebalance: my-cluster-f141387d will have desired state: Rebalancing
2022-03-30 20:25:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Rebalancing not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 20:25:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:25:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:25:59 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-f141387d is in desired state: Rebalancing
2022-03-30 20:25:59 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaRebalanceUtils:104] Reconciliation #1(test) KafkaRebalance(cruise-control-st/my-cluster-f141387d): Verifying that KafkaRebalance is in the Ready state
2022-03-30 20:25:59 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-f141387d will have desired state: Ready
2022-03-30 20:25:59 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for KafkaRebalance: my-cluster-f141387d will have desired state: Ready
2022-03-30 20:26:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 20:26:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-30 20:26:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (597991ms till timeout)
2022-03-30 20:26:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (596987ms till timeout)
2022-03-30 20:26:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:26:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (595984ms till timeout)
2022-03-30 20:26:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (594981ms till timeout)
2022-03-30 20:26:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (593977ms till timeout)
2022-03-30 20:26:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (592974ms till timeout)
2022-03-30 20:26:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (591971ms till timeout)
2022-03-30 20:26:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:26:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (590967ms till timeout)
2022-03-30 20:26:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (589964ms till timeout)
2022-03-30 20:26:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (588961ms till timeout)
2022-03-30 20:26:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (587957ms till timeout)
2022-03-30 20:26:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (586954ms till timeout)
2022-03-30 20:26:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:26:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (585951ms till timeout)
2022-03-30 20:26:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (584947ms till timeout)
2022-03-30 20:26:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (583944ms till timeout)
2022-03-30 20:26:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (582940ms till timeout)
2022-03-30 20:26:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (581937ms till timeout)
2022-03-30 20:26:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:26:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (580934ms till timeout)
2022-03-30 20:26:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (579930ms till timeout)
2022-03-30 20:26:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (578927ms till timeout)
2022-03-30 20:26:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (577924ms till timeout)
2022-03-30 20:26:23 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (576921ms till timeout)
2022-03-30 20:26:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:26:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (575918ms till timeout)
2022-03-30 20:26:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:25 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (574914ms till timeout)
2022-03-30 20:26:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:26 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (573911ms till timeout)
2022-03-30 20:26:27 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (572908ms till timeout)
2022-03-30 20:26:28 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (571905ms till timeout)
2022-03-30 20:26:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:26:29 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (570901ms till timeout)
2022-03-30 20:26:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:30 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (569898ms till timeout)
2022-03-30 20:26:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:31 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (568895ms till timeout)
2022-03-30 20:26:32 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (567891ms till timeout)
2022-03-30 20:26:33 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (566888ms till timeout)
2022-03-30 20:26:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:26:34 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (565885ms till timeout)
2022-03-30 20:26:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:35 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (564882ms till timeout)
2022-03-30 20:26:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:36 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (563878ms till timeout)
2022-03-30 20:26:37 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (562875ms till timeout)
2022-03-30 20:26:38 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (561871ms till timeout)
2022-03-30 20:26:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:26:39 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (560868ms till timeout)
2022-03-30 20:26:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:40 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (559865ms till timeout)
2022-03-30 20:26:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (558862ms till timeout)
2022-03-30 20:26:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (557859ms till timeout)
2022-03-30 20:26:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (556856ms till timeout)
2022-03-30 20:26:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:26:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (555852ms till timeout)
2022-03-30 20:26:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (554849ms till timeout)
2022-03-30 20:26:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (553846ms till timeout)
2022-03-30 20:26:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (552842ms till timeout)
2022-03-30 20:26:48 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (551839ms till timeout)
2022-03-30 20:26:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:26:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (550836ms till timeout)
2022-03-30 20:26:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (549832ms till timeout)
2022-03-30 20:26:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (548829ms till timeout)
2022-03-30 20:26:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (547826ms till timeout)
2022-03-30 20:26:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (546822ms till timeout)
2022-03-30 20:26:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:26:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (545819ms till timeout)
2022-03-30 20:26:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (544816ms till timeout)
2022-03-30 20:26:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:26:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (543812ms till timeout)
2022-03-30 20:26:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (542809ms till timeout)
2022-03-30 20:26:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (541806ms till timeout)
2022-03-30 20:26:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:26:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (540803ms till timeout)
2022-03-30 20:26:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:26:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (539799ms till timeout)
2022-03-30 20:27:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (538796ms till timeout)
2022-03-30 20:27:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (537793ms till timeout)
2022-03-30 20:27:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (536789ms till timeout)
2022-03-30 20:27:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:27:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (535786ms till timeout)
2022-03-30 20:27:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (534783ms till timeout)
2022-03-30 20:27:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (533779ms till timeout)
2022-03-30 20:27:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (532776ms till timeout)
2022-03-30 20:27:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (531773ms till timeout)
2022-03-30 20:27:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:27:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (530769ms till timeout)
2022-03-30 20:27:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (529766ms till timeout)
2022-03-30 20:27:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (528763ms till timeout)
2022-03-30 20:27:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (527759ms till timeout)
2022-03-30 20:27:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (526756ms till timeout)
2022-03-30 20:27:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:27:14 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-f141387d is in desired state: Ready
2022-03-30 20:27:14 [ForkJoinPool-3-worker-5] [32mINFO [m [CruiseControlST:152] Annotating KafkaRebalance: my-cluster-f141387d with 'refresh' anno
2022-03-30 20:27:14 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaRebalanceUtils:63] Reconciliation #2(test) KafkaRebalance(cruise-control-st/my-cluster-f141387d): Annotating KafkaRebalance:my-cluster-f141387d with annotation refresh
2022-03-30 20:27:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st annotate kafkarebalance my-cluster-f141387d strimzi.io/rebalance=refresh
2022-03-30 20:27:14 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st annotate kafkarebalance my-cluster-f141387d strimzi.io/rebalance=refresh
2022-03-30 20:27:14 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:27:14 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady
2022-03-30 20:27:14 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady
2022-03-30 20:27:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: ProposalReady not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 20:27:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-f141387d is in desired state: ProposalReady
2022-03-30 20:27:15 [ForkJoinPool-3-worker-5] [32mINFO [m [CruiseControlST:156] Trying rebalancing process again
2022-03-30 20:27:15 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaRebalanceUtils:75] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-f141387d): ============================================================================
2022-03-30 20:27:15 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaRebalanceUtils:76] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-f141387d): ProposalReady
2022-03-30 20:27:15 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaRebalanceUtils:77] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-f141387d): ============================================================================
2022-03-30 20:27:15 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaRebalanceUtils:90] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-f141387d): ============================================================================
2022-03-30 20:27:15 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaRebalanceUtils:91] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-f141387d): ProposalReady
2022-03-30 20:27:15 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaRebalanceUtils:92] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-f141387d): ============================================================================
2022-03-30 20:27:15 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaRebalanceUtils:94] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-f141387d): Triggering the rebalance with annotation strimzi.io/rebalance=approve of KafkaRebalance resource
2022-03-30 20:27:15 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaRebalanceUtils:63] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-f141387d): Annotating KafkaRebalance:my-cluster-f141387d with annotation approve
2022-03-30 20:27:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace cruise-control-st annotate kafkarebalance my-cluster-f141387d strimzi.io/rebalance=approve
2022-03-30 20:27:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace cruise-control-st annotate kafkarebalance my-cluster-f141387d strimzi.io/rebalance=approve
2022-03-30 20:27:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:27:15 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaRebalanceUtils:98] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-f141387d): Response from the annotation process kafkarebalance.kafka.strimzi.io/my-cluster-f141387d annotated
2022-03-30 20:27:15 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaRebalanceUtils:100] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-f141387d): Verifying that annotation triggers the Rebalancing state
2022-03-30 20:27:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-f141387d will have desired state: Rebalancing
2022-03-30 20:27:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for KafkaRebalance: my-cluster-f141387d will have desired state: Rebalancing
2022-03-30 20:27:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Rebalancing not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 20:27:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:16 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-f141387d is in desired state: Rebalancing
2022-03-30 20:27:16 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaRebalanceUtils:104] Reconciliation #3(test) KafkaRebalance(cruise-control-st/my-cluster-f141387d): Verifying that KafkaRebalance is in the Ready state
2022-03-30 20:27:16 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-f141387d will have desired state: Ready
2022-03-30 20:27:16 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for KafkaRebalance: my-cluster-f141387d will have desired state: Ready
2022-03-30 20:27:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 20:27:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (598995ms till timeout)
2022-03-30 20:27:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (597992ms till timeout)
2022-03-30 20:27:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:27:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (596989ms till timeout)
2022-03-30 20:27:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] KafkaRebalance: my-cluster-f141387d will have desired state: Ready not ready, will try again in 1000 ms (595986ms till timeout)
2022-03-30 20:27:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:21 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-f141387d is in desired state: Ready
2022-03-30 20:27:21 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 20:27:21 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:675] [cruisecontrol.CruiseControlST - After Each] - Clean up after test
2022-03-30 20:27:21 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:27:21 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testCruiseControlWithRebalanceResourceAndRefreshAnnotation
2022-03-30 20:27:21 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaRebalance my-cluster-f141387d in namespace cruise-control-st
2022-03-30 20:27:21 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaRebalance:my-cluster-f141387d
2022-03-30 20:27:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaRebalance:my-cluster-f141387d not ready, will try again in 10000 ms (179994ms till timeout)
2022-03-30 20:27:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:27:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:27:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-f141387d in namespace cruise-control-st
2022-03-30 20:27:31 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace cruise-control-st, for cruise control Kafka cluster my-cluster-f141387d
2022-03-30 20:27:31 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-f141387d
2022-03-30 20:27:31 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-f141387d not ready, will try again in 10000 ms (839997ms till timeout)
2022-03-30 20:27:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:27:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:27:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:41 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:27:41 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlWithRebalanceResourceAndRefreshAnnotation-FINISHED
2022-03-30 20:27:41 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 20:27:41 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 20:27:41 [ForkJoinPool-3-worker-5] [36mDEBUG[m [AbstractST:690] [cruisecontrol.CruiseControlST - After All] - Clean up after test suite
2022-03-30 20:27:41 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:27:41 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:346] In context CruiseControlST is everything deleted.
2022-03-30 20:27:41 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:27:41 [ForkJoinPool-3-worker-5] [36mDEBUG[m [TestUtils:125] Waiting for Namespace cruise-control-st removal
2022-03-30 20:27:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:41 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:41 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:27:41 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (479922ms till timeout)
2022-03-30 20:27:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:42 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:42 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:27:42 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (478846ms till timeout)
2022-03-30 20:27:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:27:43 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:43 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:27:43 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (477773ms till timeout)
2022-03-30 20:27:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:44 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:44 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:27:44 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (476700ms till timeout)
2022-03-30 20:27:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:45 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:45 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:27:45 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (475628ms till timeout)
2022-03-30 20:27:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:46 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:46 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:27:46 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (474552ms till timeout)
2022-03-30 20:27:47 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:48 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:48 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:27:48 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (473476ms till timeout)
2022-03-30 20:27:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:27:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:49 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:49 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:27:49 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (472396ms till timeout)
2022-03-30 20:27:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:50 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:50 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:27:50 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (471324ms till timeout)
2022-03-30 20:27:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:51 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:51 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:27:51 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (470247ms till timeout)
2022-03-30 20:27:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:52 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:27:52 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (469170ms till timeout)
2022-03-30 20:27:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:53 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:53 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:27:53 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (468097ms till timeout)
2022-03-30 20:27:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:27:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:54 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:27:54 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (467025ms till timeout)
2022-03-30 20:27:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:55 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:55 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:27:55 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (465945ms till timeout)
2022-03-30 20:27:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:56 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:56 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:27:56 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (464877ms till timeout)
2022-03-30 20:27:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:57 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:57 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:27:57 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (463802ms till timeout)
2022-03-30 20:27:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:27:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:58 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:58 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:27:58 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (462730ms till timeout)
2022-03-30 20:27:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:27:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:27:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:59 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:27:59 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:27:59 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (461652ms till timeout)
2022-03-30 20:28:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:28:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:28:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:28:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:28:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:28:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:28:00 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:00 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:28:00 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (460578ms till timeout)
2022-03-30 20:28:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:28:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:28:01 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:02 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:02 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:28:02 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (459496ms till timeout)
2022-03-30 20:28:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:03 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:28:03 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (458417ms till timeout)
2022-03-30 20:28:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:28:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:28:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:04 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:04 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:28:04 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (457341ms till timeout)
2022-03-30 20:28:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:28:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:28:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:28:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:28:05 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:05 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:28:05 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (456263ms till timeout)
2022-03-30 20:28:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:28:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:28:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:28:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:28:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:28:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:28:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:06 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:06 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:28:06 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (455185ms till timeout)
2022-03-30 20:28:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:07 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:07 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:28:07 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (454109ms till timeout)
2022-03-30 20:28:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:08 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:08 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:28:08 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (453035ms till timeout)
2022-03-30 20:28:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:28:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:28:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:28:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:28:09 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:09 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:28:09 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (451961ms till timeout)
2022-03-30 20:28:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:28:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:28:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:28:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:28:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:10 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:10 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:28:10 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (450885ms till timeout)
2022-03-30 20:28:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:28:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:28:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:28:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:28:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:11 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:11 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:28:11 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (449817ms till timeout)
2022-03-30 20:28:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:12 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:12 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:28:12 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (448738ms till timeout)
2022-03-30 20:28:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:28:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:28:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:13 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:28:13 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (447663ms till timeout)
2022-03-30 20:28:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:28:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:28:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:14 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:14 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:28:14 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (446590ms till timeout)
2022-03-30 20:28:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:28:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:28:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:28:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:28:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:28:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:28:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:15 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:28:15 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (445515ms till timeout)
2022-03-30 20:28:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:28:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:28:16 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:17 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:17 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:28:17 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (444442ms till timeout)
2022-03-30 20:28:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:18 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:18 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:28:18 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (443364ms till timeout)
2022-03-30 20:28:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:28:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:28:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:19 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:28:19 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (442287ms till timeout)
2022-03-30 20:28:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:28:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:28:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:28:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 1
2022-03-30 20:28:20 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:20 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:28:20 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (441206ms till timeout)
2022-03-30 20:28:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:28:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 1
2022-03-30 20:28:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:28:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 1
2022-03-30 20:28:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:28:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 1
2022-03-30 20:28:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:21 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:21 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:28:21 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (440133ms till timeout)
2022-03-30 20:28:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:22 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:22 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:28:22 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (439057ms till timeout)
2022-03-30 20:28:23 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:23 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:23 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:28:23 [ForkJoinPool-3-worker-5] [30mTRACE[m [TestUtils:176] Namespace cruise-control-st removal not ready, will try again in 1000 ms (437984ms till timeout)
2022-03-30 20:28:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:28:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 1
2022-03-30 20:28:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 0
2022-03-30 20:28:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 1
2022-03-30 20:28:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace infra-namespace get Namespace cruise-control-st -o yaml
2022-03-30 20:28:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 20:28:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 20:28:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "cruise-control-st" not found
2022-03-30 20:28:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 20:28:24 [ForkJoinPool-3-worker-5] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@8804981=[], io.strimzi.test.logs.CollectorElement@6e85bf86=[], io.strimzi.test.logs.CollectorElement@2c38455d=[], io.strimzi.test.logs.CollectorElement@3d7a59f3=[], io.strimzi.test.logs.CollectorElement@dce7372d=[], io.strimzi.test.logs.CollectorElement@7386938d=[], io.strimzi.test.logs.CollectorElement@afd54539=[], io.strimzi.test.logs.CollectorElement@cf7c9d53=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@12282135=[], io.strimzi.test.logs.CollectorElement@c4d202ed=[], io.strimzi.test.logs.CollectorElement@bfb2174d=[]}
2022-03-30 20:28:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:254] CruiseControlST - Notifies waiting test suites:[HttpBridgeTlsST, CruiseControlApiST, UserST, CruiseControlST, ListenersST] to and randomly select one to start execution
2022-03-30 20:28:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:85] [cruisecontrol.CruiseControlST] - Removing parallel suite: CruiseControlST
2022-03-30 20:28:24 [ForkJoinPool-3-worker-5] [36mDEBUG[m [SuiteThreadController:89] [cruisecontrol.CruiseControlST] - Parallel suites count: 0
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,511.477 s - in io.strimzi.systemtest.cruisecontrol.CruiseControlST
2022-03-30 20:28:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 1
2022-03-30 20:28:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 0
2022-03-30 20:28:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 1
2022-03-30 20:28:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 0
2022-03-30 20:28:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 1
2022-03-30 20:28:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 0
2022-03-30 20:28:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 1
2022-03-30 20:28:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 0
2022-03-30 20:28:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 1
2022-03-30 20:28:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 0
2022-03-30 20:28:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 1
2022-03-30 20:28:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 0
2022-03-30 20:28:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 2
2022-03-30 20:28:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 0
2022-03-30 20:28:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 2
2022-03-30 20:28:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 0
2022-03-30 20:28:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 2
2022-03-30 20:28:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 0
2022-03-30 20:28:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 2
2022-03-30 20:28:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 0
2022-03-30 20:28:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 2
2022-03-30 20:28:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 0
2022-03-30 20:28:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 2
2022-03-30 20:28:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 0
2022-03-30 20:28:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 3
2022-03-30 20:28:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 0
2022-03-30 20:28:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 3
2022-03-30 20:28:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 0
2022-03-30 20:28:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 3
2022-03-30 20:28:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 0
2022-03-30 20:28:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 3
2022-03-30 20:28:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 0
2022-03-30 20:28:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 3
2022-03-30 20:28:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 0
2022-03-30 20:28:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 3
2022-03-30 20:28:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 0
2022-03-30 20:28:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 4
2022-03-30 20:28:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:107] [connect.ConnectIsolatedST] - Parallel suites count: 0
2022-03-30 20:28:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 4
2022-03-30 20:28:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:107] [specific.SpecificIsolatedST] - Parallel suites count: 0
2022-03-30 20:28:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 4
2022-03-30 20:28:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMakerIsolatedST] - Parallel suites count: 0
2022-03-30 20:28:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 4
2022-03-30 20:28:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:107] [watcher.AllNamespaceIsolatedST] - Parallel suites count: 0
2022-03-30 20:28:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 4
2022-03-30 20:28:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:107] [mirrormaker.MirrorMaker2IsolatedST] - Parallel suites count: 0
2022-03-30 20:28:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 4
2022-03-30 20:28:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:107] [metrics.MetricsIsolatedST] - Parallel suites count: 0
2022-03-30 20:28:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [SuiteThreadController:116] [connect.ConnectIsolatedST] - isZeroParallelSuitesCounter counter is: 5
2022-03-30 20:28:45 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:136] Suite connect.ConnectIsolatedST has locked the @IsolatedSuite and other @IsolatedSuites must wait until lock is released.
2022-03-30 20:28:45 [ForkJoinPool-3-worker-13] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-30 20:28:45 [ForkJoinPool-3-worker-13] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-30 20:28:45 [ForkJoinPool-3-worker-13] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-30 20:28:45 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:28:45 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-30 20:28:45 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 20:28:45 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 20:28:45 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 20:28:45 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 20:28:45 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:28:45 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 20:28:45 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 20:28:45 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 20:28:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io not ready, will try again in 10000 ms (179993ms till timeout)
2022-03-30 20:28:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [SuiteThreadController:116] [specific.SpecificIsolatedST] - isZeroParallelSuitesCounter counter is: 5
2022-03-30 20:28:45 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:28:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMakerIsolatedST] - isZeroParallelSuitesCounter counter is: 5
2022-03-30 20:28:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [SuiteThreadController:116] [watcher.AllNamespaceIsolatedST] - isZeroParallelSuitesCounter counter is: 5
2022-03-30 20:28:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [SuiteThreadController:116] [mirrormaker.MirrorMaker2IsolatedST] - isZeroParallelSuitesCounter counter is: 5
2022-03-30 20:28:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [SuiteThreadController:116] [metrics.MetricsIsolatedST] - isZeroParallelSuitesCounter counter is: 5
2022-03-30 20:28:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:28:55 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 20:28:55 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 20:28:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179947ms till timeout)
2022-03-30 20:28:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:29:00 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:29:05 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 20:29:05 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 20:29:05 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 20:29:05 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 20:29:05 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 20:29:05 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:29:05 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 20:29:05 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:29:05 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 20:29:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator not ready, will try again in 10000 ms (479951ms till timeout)
2022-03-30 20:29:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:29:15 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:29:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 20:29:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 20:29:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 20:29:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 20:29:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:29:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 20:29:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 20:29:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 20:29:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 20:29:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 20:29:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179919ms till timeout)
2022-03-30 20:29:20 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:29:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:29:25 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 20:29:25 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 20:29:25 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 20:29:25 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 20:29:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179817ms till timeout)
2022-03-30 20:29:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:29:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:29:35 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 20:29:35 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 20:29:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io not ready, will try again in 10000 ms (179988ms till timeout)
2022-03-30 20:29:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:29:45 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:29:45 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 20:29:45 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 20:29:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io not ready, will try again in 10000 ms (179933ms till timeout)
2022-03-30 20:29:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:29:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:29:56 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 20:29:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 20:29:56 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 20:29:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 20:29:56 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 20:29:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 20:29:56 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 20:29:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 20:29:56 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 20:29:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 20:29:56 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 20:29:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 20:29:56 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:29:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 20:29:56 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:29:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-30 20:29:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 20:29:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 20:29:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v88844
2022-03-30 20:29:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v88844
2022-03-30 20:29:56 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=88844&allowWatchBookmarks=true&watch=true...
2022-03-30 20:29:56 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 20:29:56 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 88845
2022-03-30 20:30:00 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:30:01 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 88858
2022-03-30 20:30:01 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 88859
2022-03-30 20:30:01 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v88858 in namespace default
2022-03-30 20:30:01 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@7bbe6a0a
2022-03-30 20:30:01 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 20:30:01 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@6f4950f
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5a34beae
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@5314833a, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5a34beae, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-30 20:30:01 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@6f4950f
2022-03-30 20:30:01 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@6f4950f
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-30 20:30:01 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 20:30:01 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T20:30:01Z",
        "name": "infra-namespace",
        "resourceVersion": "88860",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "c7f728f7-0622-47b8-89e4-595436ebf158"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 20:30:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 20:30:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479994ms till timeout)
2022-03-30 20:30:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478991ms till timeout)
2022-03-30 20:30:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477984ms till timeout)
2022-03-30 20:30:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476980ms till timeout)
2022-03-30 20:30:05 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:30:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475977ms till timeout)
2022-03-30 20:30:07 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474973ms till timeout)
2022-03-30 20:30:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473970ms till timeout)
2022-03-30 20:30:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472967ms till timeout)
2022-03-30 20:30:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471964ms till timeout)
2022-03-30 20:30:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:30:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470960ms till timeout)
2022-03-30 20:30:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469957ms till timeout)
2022-03-30 20:30:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468954ms till timeout)
2022-03-30 20:30:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467951ms till timeout)
2022-03-30 20:30:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466947ms till timeout)
2022-03-30 20:30:15 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:30:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465944ms till timeout)
2022-03-30 20:30:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464941ms till timeout)
2022-03-30 20:30:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463938ms till timeout)
2022-03-30 20:30:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462935ms till timeout)
2022-03-30 20:30:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461932ms till timeout)
2022-03-30 20:30:20 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:30:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (460928ms till timeout)
2022-03-30 20:30:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (459925ms till timeout)
2022-03-30 20:30:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (458922ms till timeout)
2022-03-30 20:30:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (457919ms till timeout)
2022-03-30 20:30:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (456916ms till timeout)
2022-03-30 20:30:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:30:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (455913ms till timeout)
2022-03-30 20:30:27 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (454909ms till timeout)
2022-03-30 20:30:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (453906ms till timeout)
2022-03-30 20:30:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (452903ms till timeout)
2022-03-30 20:30:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (451898ms till timeout)
2022-03-30 20:30:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:30:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (450895ms till timeout)
2022-03-30 20:30:32 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (449892ms till timeout)
2022-03-30 20:30:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (448889ms till timeout)
2022-03-30 20:30:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (447886ms till timeout)
2022-03-30 20:30:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (446882ms till timeout)
2022-03-30 20:30:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:30:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (445879ms till timeout)
2022-03-30 20:30:37 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (444876ms till timeout)
2022-03-30 20:30:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (443873ms till timeout)
2022-03-30 20:30:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (442870ms till timeout)
2022-03-30 20:30:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (441866ms till timeout)
2022-03-30 20:30:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:30:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (440863ms till timeout)
2022-03-30 20:30:42 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (439859ms till timeout)
2022-03-30 20:30:43 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-30 20:30:43 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-30 20:30:43 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-30 20:30:43 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-h74bv not ready: strimzi-cluster-operator)
2022-03-30 20:30:43 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-h74bv are ready
2022-03-30 20:30:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-30 20:30:44 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-h74bv not ready: strimzi-cluster-operator)
2022-03-30 20:30:44 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-h74bv are ready
2022-03-30 20:30:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-30 20:30:45 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-h74bv not ready: strimzi-cluster-operator)
2022-03-30 20:30:45 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-h74bv are ready
2022-03-30 20:30:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597990ms till timeout)
2022-03-30 20:30:45 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:30:46 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-h74bv not ready: strimzi-cluster-operator)
2022-03-30 20:30:46 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-h74bv are ready
2022-03-30 20:30:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596986ms till timeout)
2022-03-30 20:30:47 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-h74bv not ready: strimzi-cluster-operator)
2022-03-30 20:30:47 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-h74bv are ready
2022-03-30 20:30:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595982ms till timeout)
2022-03-30 20:30:48 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-h74bv not ready: strimzi-cluster-operator)
2022-03-30 20:30:48 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-h74bv are ready
2022-03-30 20:30:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594979ms till timeout)
2022-03-30 20:30:49 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-h74bv not ready: strimzi-cluster-operator)
2022-03-30 20:30:49 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-h74bv are ready
2022-03-30 20:30:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593975ms till timeout)
2022-03-30 20:30:50 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-h74bv not ready: strimzi-cluster-operator)
2022-03-30 20:30:50 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-h74bv are ready
2022-03-30 20:30:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592971ms till timeout)
2022-03-30 20:30:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:30:51 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-h74bv not ready: strimzi-cluster-operator)
2022-03-30 20:30:51 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-h74bv are ready
2022-03-30 20:30:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591967ms till timeout)
2022-03-30 20:30:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-h74bv not ready: strimzi-cluster-operator)
2022-03-30 20:30:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-h74bv are ready
2022-03-30 20:30:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590963ms till timeout)
2022-03-30 20:30:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-h74bv not ready: strimzi-cluster-operator)
2022-03-30 20:30:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-h74bv are ready
2022-03-30 20:30:53 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-30 20:30:53 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 20:30:53 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testMultiNodeKafkaConnectWithConnectorCreation-STARTED
2022-03-30 20:30:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 20:30:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:659] [connect.ConnectIsolatedST - Before Each] - Setup test case environment
2022-03-30 20:30:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:77] [connect.ConnectIsolatedST] - Adding parallel test: testMultiNodeKafkaConnectWithConnectorCreation
2022-03-30 20:30:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:81] [connect.ConnectIsolatedST] - Parallel test count: 1
2022-03-30 20:30:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:230] testMultiNodeKafkaConnectWithConnectorCreation test now can proceed its execution
2022-03-30 20:30:53 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 20:30:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba, testSendSimpleMessageTls=my-cluster-ca7325d7, testUpdateUser=my-cluster-6e6fa8fe, testSendMessagesTlsScramSha=my-cluster-84882f67, testReceiveSimpleMessageTls=my-cluster-2e450144, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37, testCruiseControlBasicAPIRequests=my-cluster-8aed4745, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140}
2022-03-30 20:30:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testMultiNodeKafkaConnectWithConnectorCreation=my-user-1851888062-673454065, testKafkaAndZookeeperScaleUpScaleDown=my-user-838849261-890460734, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1731919654-539989781, testSendSimpleMessageTls=my-user-1309491966-2015599610, testUpdateUser=my-user-1387197422-925410353, testSendMessagesTlsScramSha=my-user-586569987-2105688079, testReceiveSimpleMessageTls=my-user-1669668617-1031881775, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1886945581-244087591, testSendMessagesCustomListenerTlsScramSha=my-user-1118022021-805525218, testCruiseControlBasicAPIRequests=my-user-1861484466-561878159, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677}
2022-03-30 20:30:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testMultiNodeKafkaConnectWithConnectorCreation=my-topic-307985263-926887950, testKafkaAndZookeeperScaleUpScaleDown=my-topic-2138084746-704381117, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1672280868-1119644965, testSendSimpleMessageTls=my-topic-435427300-917779110, testUpdateUser=my-topic-1875783706-1763804507, testSendMessagesTlsScramSha=my-topic-255809133-823300208, testReceiveSimpleMessageTls=my-topic-368564676-637091599, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-306860904-106838350, testSendMessagesCustomListenerTlsScramSha=my-topic-963888555-1712711326, testCruiseControlBasicAPIRequests=my-topic-1631570507-1308241024, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206}
2022-03-30 20:30:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba-kafka-clients, testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testUpdateUser=my-cluster-6e6fa8fe-kafka-clients, testSendMessagesTlsScramSha=my-cluster-84882f67-kafka-clients, testReceiveSimpleMessageTls=my-cluster-2e450144-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-8aed4745-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients}
2022-03-30 20:30:53 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-6 for test case:testMultiNodeKafkaConnectWithConnectorCreation
2022-03-30 20:30:53 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-6
2022-03-30 20:30:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-6
2022-03-30 20:30:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-6 -o json
2022-03-30 20:30:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-6 -o json
2022-03-30 20:30:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:30:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T20:30:53Z",
        "name": "namespace-6",
        "resourceVersion": "88973",
        "selfLink": "/api/v1/namespaces/namespace-6",
        "uid": "cc2241e2-723f-4651-b771-0b1ad6faf76b"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 20:30:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@c27758f5=[namespace-6]}
2022-03-30 20:30:53 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-6
2022-03-30 20:30:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-6, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 20:30:53 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-6
2022-03-30 20:30:53 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-70aa1aeb in namespace namespace-6
2022-03-30 20:30:53 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-03-30 20:30:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-70aa1aeb
2022-03-30 20:30:53 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-70aa1aeb will have desired state: Ready
2022-03-30 20:30:53 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-70aa1aeb will have desired state: Ready
2022-03-30 20:30:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (839998ms till timeout)
2022-03-30 20:30:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (838995ms till timeout)
2022-03-30 20:30:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:30:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (837991ms till timeout)
2022-03-30 20:30:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (836988ms till timeout)
2022-03-30 20:30:57 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (835985ms till timeout)
2022-03-30 20:30:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (834982ms till timeout)
2022-03-30 20:30:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (833978ms till timeout)
2022-03-30 20:31:00 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:31:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (832975ms till timeout)
2022-03-30 20:31:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (831970ms till timeout)
2022-03-30 20:31:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (830966ms till timeout)
2022-03-30 20:31:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (829963ms till timeout)
2022-03-30 20:31:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (828960ms till timeout)
2022-03-30 20:31:05 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:31:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (827957ms till timeout)
2022-03-30 20:31:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (826953ms till timeout)
2022-03-30 20:31:07 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (825950ms till timeout)
2022-03-30 20:31:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (824947ms till timeout)
2022-03-30 20:31:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (823943ms till timeout)
2022-03-30 20:31:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:31:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (822940ms till timeout)
2022-03-30 20:31:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (821936ms till timeout)
2022-03-30 20:31:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (820933ms till timeout)
2022-03-30 20:31:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (819930ms till timeout)
2022-03-30 20:31:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (818927ms till timeout)
2022-03-30 20:31:15 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:31:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (817924ms till timeout)
2022-03-30 20:31:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (816920ms till timeout)
2022-03-30 20:31:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (815917ms till timeout)
2022-03-30 20:31:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (814914ms till timeout)
2022-03-30 20:31:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (813911ms till timeout)
2022-03-30 20:31:20 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:31:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (812907ms till timeout)
2022-03-30 20:31:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (811903ms till timeout)
2022-03-30 20:31:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (810900ms till timeout)
2022-03-30 20:31:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (809897ms till timeout)
2022-03-30 20:31:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (808894ms till timeout)
2022-03-30 20:31:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:31:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (807889ms till timeout)
2022-03-30 20:31:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (806886ms till timeout)
2022-03-30 20:31:27 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (805882ms till timeout)
2022-03-30 20:31:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (804879ms till timeout)
2022-03-30 20:31:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (803875ms till timeout)
2022-03-30 20:31:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:31:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (802870ms till timeout)
2022-03-30 20:31:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (801866ms till timeout)
2022-03-30 20:31:32 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (800864ms till timeout)
2022-03-30 20:31:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (799860ms till timeout)
2022-03-30 20:31:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (798857ms till timeout)
2022-03-30 20:31:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:31:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (797853ms till timeout)
2022-03-30 20:31:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (796850ms till timeout)
2022-03-30 20:31:37 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (795846ms till timeout)
2022-03-30 20:31:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (794843ms till timeout)
2022-03-30 20:31:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (793840ms till timeout)
2022-03-30 20:31:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:31:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (792836ms till timeout)
2022-03-30 20:31:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (791833ms till timeout)
2022-03-30 20:31:42 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (790830ms till timeout)
2022-03-30 20:31:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (789826ms till timeout)
2022-03-30 20:31:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (788823ms till timeout)
2022-03-30 20:31:45 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:31:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (787820ms till timeout)
2022-03-30 20:31:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (786816ms till timeout)
2022-03-30 20:31:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (785813ms till timeout)
2022-03-30 20:31:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (784810ms till timeout)
2022-03-30 20:31:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (783806ms till timeout)
2022-03-30 20:31:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:31:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (782803ms till timeout)
2022-03-30 20:31:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (781800ms till timeout)
2022-03-30 20:31:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (780796ms till timeout)
2022-03-30 20:31:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (779793ms till timeout)
2022-03-30 20:31:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (778790ms till timeout)
2022-03-30 20:31:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:31:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (777786ms till timeout)
2022-03-30 20:31:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (776783ms till timeout)
2022-03-30 20:31:57 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (775780ms till timeout)
2022-03-30 20:31:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (774776ms till timeout)
2022-03-30 20:31:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (773772ms till timeout)
2022-03-30 20:32:00 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:32:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (772769ms till timeout)
2022-03-30 20:32:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (771766ms till timeout)
2022-03-30 20:32:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (770762ms till timeout)
2022-03-30 20:32:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (769759ms till timeout)
2022-03-30 20:32:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (768756ms till timeout)
2022-03-30 20:32:05 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:32:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (767753ms till timeout)
2022-03-30 20:32:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (766749ms till timeout)
2022-03-30 20:32:07 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (765746ms till timeout)
2022-03-30 20:32:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (764743ms till timeout)
2022-03-30 20:32:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (763740ms till timeout)
2022-03-30 20:32:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:32:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-70aa1aeb is in desired state: Ready
2022-03-30 20:32:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-70aa1aeb-scraper in namespace namespace-6
2022-03-30 20:32:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-03-30 20:32:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-70aa1aeb-scraper
2022-03-30 20:32:10 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-70aa1aeb-scraper will be ready
2022-03-30 20:32:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-70aa1aeb-scraper will be ready
2022-03-30 20:32:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-70aa1aeb-scraper will be ready not ready, will try again in 1000 ms (479998ms till timeout)
2022-03-30 20:32:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-70aa1aeb-scraper will be ready not ready, will try again in 1000 ms (478996ms till timeout)
2022-03-30 20:32:12 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-70aa1aeb-scraper is ready
2022-03-30 20:32:12 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-70aa1aeb-scraper to be ready
2022-03-30 20:32:12 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-70aa1aeb-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready
2022-03-30 20:32:12 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-70aa1aeb-scraper-79d9fcbdb4-gt6kk not ready: my-cluster-70aa1aeb-scraper)
2022-03-30 20:32:12 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods my-cluster-70aa1aeb-scraper-79d9fcbdb4-gt6kk are ready
2022-03-30 20:32:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-70aa1aeb-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599996ms till timeout)
2022-03-30 20:32:13 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-70aa1aeb-scraper-79d9fcbdb4-gt6kk not ready: my-cluster-70aa1aeb-scraper)
2022-03-30 20:32:13 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods my-cluster-70aa1aeb-scraper-79d9fcbdb4-gt6kk are ready
2022-03-30 20:32:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-70aa1aeb-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598992ms till timeout)
2022-03-30 20:32:14 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-70aa1aeb-scraper-79d9fcbdb4-gt6kk not ready: my-cluster-70aa1aeb-scraper)
2022-03-30 20:32:14 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods my-cluster-70aa1aeb-scraper-79d9fcbdb4-gt6kk are ready
2022-03-30 20:32:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-70aa1aeb-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597988ms till timeout)
2022-03-30 20:32:15 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:32:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-70aa1aeb-scraper-79d9fcbdb4-gt6kk not ready: my-cluster-70aa1aeb-scraper)
2022-03-30 20:32:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods my-cluster-70aa1aeb-scraper-79d9fcbdb4-gt6kk are ready
2022-03-30 20:32:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-70aa1aeb-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596984ms till timeout)
2022-03-30 20:32:16 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-70aa1aeb-scraper-79d9fcbdb4-gt6kk not ready: my-cluster-70aa1aeb-scraper)
2022-03-30 20:32:16 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods my-cluster-70aa1aeb-scraper-79d9fcbdb4-gt6kk are ready
2022-03-30 20:32:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-70aa1aeb-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595980ms till timeout)
2022-03-30 20:32:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-70aa1aeb-scraper-79d9fcbdb4-gt6kk not ready: my-cluster-70aa1aeb-scraper)
2022-03-30 20:32:17 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods my-cluster-70aa1aeb-scraper-79d9fcbdb4-gt6kk are ready
2022-03-30 20:32:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-70aa1aeb-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594976ms till timeout)
2022-03-30 20:32:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-70aa1aeb-scraper-79d9fcbdb4-gt6kk not ready: my-cluster-70aa1aeb-scraper)
2022-03-30 20:32:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods my-cluster-70aa1aeb-scraper-79d9fcbdb4-gt6kk are ready
2022-03-30 20:32:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-70aa1aeb-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593972ms till timeout)
2022-03-30 20:32:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-70aa1aeb-scraper-79d9fcbdb4-gt6kk not ready: my-cluster-70aa1aeb-scraper)
2022-03-30 20:32:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods my-cluster-70aa1aeb-scraper-79d9fcbdb4-gt6kk are ready
2022-03-30 20:32:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-70aa1aeb-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592967ms till timeout)
2022-03-30 20:32:20 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:32:20 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-70aa1aeb-scraper-79d9fcbdb4-gt6kk not ready: my-cluster-70aa1aeb-scraper)
2022-03-30 20:32:20 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods my-cluster-70aa1aeb-scraper-79d9fcbdb4-gt6kk are ready
2022-03-30 20:32:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-70aa1aeb-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591963ms till timeout)
2022-03-30 20:32:21 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-70aa1aeb-scraper-79d9fcbdb4-gt6kk not ready: my-cluster-70aa1aeb-scraper)
2022-03-30 20:32:21 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods my-cluster-70aa1aeb-scraper-79d9fcbdb4-gt6kk are ready
2022-03-30 20:32:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-70aa1aeb-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590958ms till timeout)
2022-03-30 20:32:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-70aa1aeb-scraper-79d9fcbdb4-gt6kk not ready: my-cluster-70aa1aeb-scraper)
2022-03-30 20:32:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [PodUtils:106] Pods my-cluster-70aa1aeb-scraper-79d9fcbdb4-gt6kk are ready
2022-03-30 20:32:22 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-70aa1aeb-scraper is ready
2022-03-30 20:32:22 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-70aa1aeb-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-03-30 20:32:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [NetworkPolicyResource:227] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=my-cluster-70aa1aeb-allow, namespace=namespace-6, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8083, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9404, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8080, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9999, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-70aa1aeb, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-70aa1aeb-connect}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 20:32:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-70aa1aeb-allow in namespace namespace-6
2022-03-30 20:32:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-03-30 20:32:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:my-cluster-70aa1aeb-allow
2022-03-30 20:32:22 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-03-30 20:32:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-70aa1aeb in namespace namespace-6
2022-03-30 20:32:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-03-30 20:32:22 [ForkJoinPool-3-worker-13] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkaconnects' with unstable version 'v1beta2'
2022-03-30 20:32:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaConnect:my-cluster-70aa1aeb
2022-03-30 20:32:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready
2022-03-30 20:32:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready
2022-03-30 20:32:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 20:32:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (598995ms till timeout)
2022-03-30 20:32:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (597992ms till timeout)
2022-03-30 20:32:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:32:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (596985ms till timeout)
2022-03-30 20:32:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (595981ms till timeout)
2022-03-30 20:32:27 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (594978ms till timeout)
2022-03-30 20:32:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (593974ms till timeout)
2022-03-30 20:32:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (592970ms till timeout)
2022-03-30 20:32:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:32:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (591964ms till timeout)
2022-03-30 20:32:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (590957ms till timeout)
2022-03-30 20:32:32 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (589951ms till timeout)
2022-03-30 20:32:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (588940ms till timeout)
2022-03-30 20:32:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (587937ms till timeout)
2022-03-30 20:32:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:32:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (586933ms till timeout)
2022-03-30 20:32:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (585930ms till timeout)
2022-03-30 20:32:37 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (584925ms till timeout)
2022-03-30 20:32:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (583921ms till timeout)
2022-03-30 20:32:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (582919ms till timeout)
2022-03-30 20:32:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:32:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (581915ms till timeout)
2022-03-30 20:32:42 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (580912ms till timeout)
2022-03-30 20:32:43 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (579908ms till timeout)
2022-03-30 20:32:44 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (578905ms till timeout)
2022-03-30 20:32:45 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (577901ms till timeout)
2022-03-30 20:32:45 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:32:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (576897ms till timeout)
2022-03-30 20:32:47 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (575894ms till timeout)
2022-03-30 20:32:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (574890ms till timeout)
2022-03-30 20:32:49 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (573887ms till timeout)
2022-03-30 20:32:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (572883ms till timeout)
2022-03-30 20:32:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:32:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (571880ms till timeout)
2022-03-30 20:32:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (570877ms till timeout)
2022-03-30 20:32:53 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (569873ms till timeout)
2022-03-30 20:32:54 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (568869ms till timeout)
2022-03-30 20:32:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (567866ms till timeout)
2022-03-30 20:32:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:32:56 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (566862ms till timeout)
2022-03-30 20:32:57 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (565859ms till timeout)
2022-03-30 20:32:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (564856ms till timeout)
2022-03-30 20:32:59 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (563852ms till timeout)
2022-03-30 20:33:00 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (562849ms till timeout)
2022-03-30 20:33:00 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:33:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (561845ms till timeout)
2022-03-30 20:33:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (560842ms till timeout)
2022-03-30 20:33:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (559838ms till timeout)
2022-03-30 20:33:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (558835ms till timeout)
2022-03-30 20:33:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (557831ms till timeout)
2022-03-30 20:33:05 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:33:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (556827ms till timeout)
2022-03-30 20:33:07 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (555824ms till timeout)
2022-03-30 20:33:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (554821ms till timeout)
2022-03-30 20:33:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (553818ms till timeout)
2022-03-30 20:33:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (552814ms till timeout)
2022-03-30 20:33:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:33:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (551811ms till timeout)
2022-03-30 20:33:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (550807ms till timeout)
2022-03-30 20:33:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (549804ms till timeout)
2022-03-30 20:33:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (548801ms till timeout)
2022-03-30 20:33:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (547797ms till timeout)
2022-03-30 20:33:15 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:33:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (546794ms till timeout)
2022-03-30 20:33:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (545790ms till timeout)
2022-03-30 20:33:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (544787ms till timeout)
2022-03-30 20:33:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (543784ms till timeout)
2022-03-30 20:33:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (542781ms till timeout)
2022-03-30 20:33:20 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:33:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (541778ms till timeout)
2022-03-30 20:33:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (540774ms till timeout)
2022-03-30 20:33:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (539771ms till timeout)
2022-03-30 20:33:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (538768ms till timeout)
2022-03-30 20:33:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (537765ms till timeout)
2022-03-30 20:33:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:33:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (536761ms till timeout)
2022-03-30 20:33:27 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (535758ms till timeout)
2022-03-30 20:33:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (534755ms till timeout)
2022-03-30 20:33:29 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (533752ms till timeout)
2022-03-30 20:33:30 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (532749ms till timeout)
2022-03-30 20:33:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:33:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (531745ms till timeout)
2022-03-30 20:33:32 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (530742ms till timeout)
2022-03-30 20:33:33 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (529739ms till timeout)
2022-03-30 20:33:34 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-70aa1aeb is in desired state: Ready
2022-03-30 20:33:34 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector my-cluster-70aa1aeb in namespace namespace-6
2022-03-30 20:33:34 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-03-30 20:33:34 [ForkJoinPool-3-worker-13] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkaconnectors' with unstable version 'v1beta2'
2022-03-30 20:33:34 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaConnector:my-cluster-70aa1aeb
2022-03-30 20:33:34 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-70aa1aeb will have desired state: Ready
2022-03-30 20:33:34 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for KafkaConnector: my-cluster-70aa1aeb will have desired state: Ready
2022-03-30 20:33:34 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] KafkaConnector: my-cluster-70aa1aeb will have desired state: Ready not ready, will try again in 1000 ms (239998ms till timeout)
2022-03-30 20:33:35 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-70aa1aeb is in desired state: Ready
2022-03-30 20:33:35 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-30 20:33:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 exec my-cluster-70aa1aeb-connect-84c47f467b-t4hc2 -- curl -X GET http://localhost:8083/connectors/my-cluster-70aa1aeb/status
2022-03-30 20:33:35 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-6 exec my-cluster-70aa1aeb-connect-84c47f467b-t4hc2 -- curl -X GET http://localhost:8083/connectors/my-cluster-70aa1aeb/status
2022-03-30 20:33:35 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-03-30 20:33:35 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-70aa1aeb-hello-world-producer in namespace namespace-6
2022-03-30 20:33:35 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-03-30 20:33:35 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-70aa1aeb-hello-world-consumer in namespace namespace-6
2022-03-30 20:33:35 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-03-30 20:33:35 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:my-cluster-70aa1aeb-hello-world-producer
2022-03-30 20:33:35 [ForkJoinPool-3-worker-13] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-70aa1aeb-hello-world-producer will be in active state
2022-03-30 20:33:35 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for job active
2022-03-30 20:33:35 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:my-cluster-70aa1aeb-hello-world-consumer
2022-03-30 20:33:35 [ForkJoinPool-3-worker-13] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-70aa1aeb-hello-world-consumer will be in active state
2022-03-30 20:33:35 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for job active
2022-03-30 20:33:35 [ForkJoinPool-3-worker-13] [32mINFO [m [ClientUtils:61] Waiting till producer my-cluster-70aa1aeb-hello-world-producer and consumer my-cluster-70aa1aeb-hello-world-consumer finish
2022-03-30 20:33:35 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for clients finished
2022-03-30 20:33:35 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (219997ms till timeout)
2022-03-30 20:33:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:33:36 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (218994ms till timeout)
2022-03-30 20:33:37 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (217991ms till timeout)
2022-03-30 20:33:38 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (216988ms till timeout)
2022-03-30 20:33:39 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (215980ms till timeout)
2022-03-30 20:33:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:33:40 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] clients finished not ready, will try again in 1000 ms (214976ms till timeout)
2022-03-30 20:33:41 [ForkJoinPool-3-worker-13] [36mDEBUG[m [JobUtils:37] Waiting for ReplicaSet of Deployment my-cluster-70aa1aeb-hello-world-producer deletion
2022-03-30 20:33:41 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for ReplicaSet my-cluster-70aa1aeb-hello-world-producer to be deleted
2022-03-30 20:33:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] ReplicaSet my-cluster-70aa1aeb-hello-world-producer to be deleted not ready, will try again in 5000 ms (179997ms till timeout)
2022-03-30 20:33:45 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:33:46 [ForkJoinPool-3-worker-13] [36mDEBUG[m [JobUtils:40] Job my-cluster-70aa1aeb-hello-world-producer was deleted
2022-03-30 20:33:46 [ForkJoinPool-3-worker-13] [36mDEBUG[m [JobUtils:37] Waiting for ReplicaSet of Deployment my-cluster-70aa1aeb-hello-world-consumer deletion
2022-03-30 20:33:46 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for ReplicaSet my-cluster-70aa1aeb-hello-world-consumer to be deleted
2022-03-30 20:33:46 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] ReplicaSet my-cluster-70aa1aeb-hello-world-consumer to be deleted not ready, will try again in 5000 ms (179997ms till timeout)
2022-03-30 20:33:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:33:51 [ForkJoinPool-3-worker-13] [36mDEBUG[m [JobUtils:40] Job my-cluster-70aa1aeb-hello-world-consumer was deleted
2022-03-30 20:33:51 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-70aa1aeb-connect-84c47f467b-wv9p7
2022-03-30 20:33:51 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for messages in file sink
2022-03-30 20:33:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 exec my-cluster-70aa1aeb-connect-84c47f467b-wv9p7 -- /bin/bash -c cat /tmp/test-file-sink.txt
2022-03-30 20:33:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:421] Command: kubectl --namespace namespace-6 exec my-cluster-70aa1aeb-connect-84c47f467b-wv9p7 -- /bin/bash -c cat /tmp/test-file-sink.txt
2022-03-30 20:33:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:421] Return code: 0
2022-03-30 20:33:51 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectUtils:77] Expected messages are in file sink on my-cluster-70aa1aeb-connect-84c47f467b-wv9p7
2022-03-30 20:33:51 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 20:33:51 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:675] [connect.ConnectIsolatedST - After Each] - Clean up after test
2022-03-30 20:33:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:33:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testMultiNodeKafkaConnectWithConnectorCreation
2022-03-30 20:33:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector my-cluster-70aa1aeb in namespace namespace-6
2022-03-30 20:33:51 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaConnector:my-cluster-70aa1aeb
2022-03-30 20:33:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaConnector:my-cluster-70aa1aeb not ready, will try again in 10000 ms (239983ms till timeout)
2022-03-30 20:33:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:34:00 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:34:01 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-70aa1aeb in namespace namespace-6
2022-03-30 20:34:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaConnect:my-cluster-70aa1aeb
2022-03-30 20:34:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaConnect:my-cluster-70aa1aeb not ready, will try again in 10000 ms (599994ms till timeout)
2022-03-30 20:34:05 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:34:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:34:11 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-70aa1aeb-hello-world-consumer in namespace namespace-6
2022-03-30 20:34:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:my-cluster-70aa1aeb-hello-world-consumer
2022-03-30 20:34:11 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-70aa1aeb-hello-world-producer in namespace namespace-6
2022-03-30 20:34:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:my-cluster-70aa1aeb-hello-world-producer
2022-03-30 20:34:11 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-70aa1aeb-scraper in namespace namespace-6
2022-03-30 20:34:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-70aa1aeb-scraper
2022-03-30 20:34:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-70aa1aeb-scraper not ready, will try again in 10000 ms (479995ms till timeout)
2022-03-30 20:34:15 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:34:20 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:34:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-70aa1aeb-scraper not ready, will try again in 10000 ms (469989ms till timeout)
2022-03-30 20:34:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:34:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:34:31 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-70aa1aeb-scraper not ready, will try again in 10000 ms (459982ms till timeout)
2022-03-30 20:34:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:34:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:34:41 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-70aa1aeb-scraper not ready, will try again in 10000 ms (449976ms till timeout)
2022-03-30 20:34:45 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:34:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:34:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-70aa1aeb-allow in namespace namespace-6
2022-03-30 20:34:51 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:my-cluster-70aa1aeb-allow
2022-03-30 20:34:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-70aa1aeb in namespace namespace-6
2022-03-30 20:34:51 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-70aa1aeb
2022-03-30 20:34:51 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-70aa1aeb not ready, will try again in 10000 ms (839997ms till timeout)
2022-03-30 20:34:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:35:00 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:35:01 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:35:01 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-6 for test case:testMultiNodeKafkaConnectWithConnectorCreation
2022-03-30 20:35:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-6 removal
2022-03-30 20:35:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:01 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:35:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (479927ms till timeout)
2022-03-30 20:35:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:02 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:02 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:35:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (478841ms till timeout)
2022-03-30 20:35:03 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:04 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:04 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:35:04 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (477766ms till timeout)
2022-03-30 20:35:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:05 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:05 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:35:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (476692ms till timeout)
2022-03-30 20:35:05 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:35:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:06 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:06 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:35:06 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (475617ms till timeout)
2022-03-30 20:35:07 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:07 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:07 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:35:07 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (474549ms till timeout)
2022-03-30 20:35:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:08 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:08 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:35:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (473476ms till timeout)
2022-03-30 20:35:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:09 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:09 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:35:09 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (472400ms till timeout)
2022-03-30 20:35:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:35:10 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (471326ms till timeout)
2022-03-30 20:35:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:35:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:11 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:35:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (470259ms till timeout)
2022-03-30 20:35:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:12 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:12 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:35:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (469181ms till timeout)
2022-03-30 20:35:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:13 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:13 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:35:13 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (468106ms till timeout)
2022-03-30 20:35:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:14 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:14 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:35:14 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (467027ms till timeout)
2022-03-30 20:35:15 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:35:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:15 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:35:15 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (465951ms till timeout)
2022-03-30 20:35:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:16 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:16 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:35:16 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (464880ms till timeout)
2022-03-30 20:35:17 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:18 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:35:18 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (463812ms till timeout)
2022-03-30 20:35:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:19 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:35:19 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (462732ms till timeout)
2022-03-30 20:35:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:20 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:20 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:35:20 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (461660ms till timeout)
2022-03-30 20:35:20 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:35:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:21 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:21 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:35:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (460575ms till timeout)
2022-03-30 20:35:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:22 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:35:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (459503ms till timeout)
2022-03-30 20:35:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:23 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:23 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:35:23 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (458424ms till timeout)
2022-03-30 20:35:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:24 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:24 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:35:24 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (457344ms till timeout)
2022-03-30 20:35:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:127] Suite specific.SpecificIsolatedST is waiting to lock to be released.
2022-03-30 20:35:25 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:25 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:35:25 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (456261ms till timeout)
2022-03-30 20:35:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:35:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (455181ms till timeout)
2022-03-30 20:35:27 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:27 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:27 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:35:27 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (454111ms till timeout)
2022-03-30 20:35:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:28 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-6 get Namespace namespace-6 -o yaml
2022-03-30 20:35:28 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 20:35:28 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 20:35:28 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-6" not found
2022-03-30 20:35:28 [ForkJoinPool-3-worker-13] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 20:35:28 [ForkJoinPool-3-worker-13] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@c27758f5=[]}
2022-03-30 20:35:28 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:267] testMultiNodeKafkaConnectWithConnectorCreation - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendSimpleMessageTls, testUpdateUser, testCruiseControlBasicAPIRequests, testReceiveSimpleMessageTls, testSendMessagesTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation] to and randomly select one to start execution
2022-03-30 20:35:28 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:93] [connect.ConnectIsolatedST] - Removing parallel test: testMultiNodeKafkaConnectWithConnectorCreation
2022-03-30 20:35:28 [ForkJoinPool-3-worker-13] [36mDEBUG[m [SuiteThreadController:97] [connect.ConnectIsolatedST] - Parallel test count: 0
2022-03-30 20:35:28 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testMultiNodeKafkaConnectWithConnectorCreation-FINISHED
2022-03-30 20:35:28 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 20:35:28 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 20:35:28 [ForkJoinPool-3-worker-13] [36mDEBUG[m [AbstractST:690] [connect.ConnectIsolatedST - After All] - Clean up after test suite
2022-03-30 20:35:28 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:35:28 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:346] In context ConnectIsolatedST is everything deleted.
2022-03-30 20:35:28 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,493.617 s - in io.strimzi.systemtest.connect.ConnectIsolatedST
2022-03-30 20:35:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:136] Suite specific.SpecificIsolatedST has locked the @IsolatedSuite and other @IsolatedSuites must wait until lock is released.
2022-03-30 20:35:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:35:30 [ForkJoinPool-3-worker-9] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-30 20:35:30 [ForkJoinPool-3-worker-9] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-30 20:35:30 [ForkJoinPool-3-worker-9] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-30 20:35:30 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:35:30 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-30 20:35:30 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 20:35:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 20:35:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io not ready, will try again in 10000 ms (179991ms till timeout)
2022-03-30 20:35:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:35:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:35:40 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 20:35:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 20:35:40 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:35:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 20:35:40 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 20:35:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 20:35:40 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 20:35:40 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 20:35:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179948ms till timeout)
2022-03-30 20:35:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:35:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:35:50 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 20:35:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 20:35:50 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 20:35:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 20:35:50 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 20:35:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 20:35:50 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:35:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 20:35:50 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 20:35:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 20:35:50 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 20:35:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 20:35:50 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:35:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 20:35:50 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 20:35:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 20:35:50 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 20:35:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 20:35:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179948ms till timeout)
2022-03-30 20:35:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:36:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:36:00 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 20:36:00 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 20:36:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io not ready, will try again in 10000 ms (179928ms till timeout)
2022-03-30 20:36:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:36:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:36:10 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 20:36:11 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 20:36:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179819ms till timeout)
2022-03-30 20:36:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:36:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:36:21 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 20:36:21 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 20:36:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io not ready, will try again in 10000 ms (179984ms till timeout)
2022-03-30 20:36:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:36:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:36:31 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 20:36:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 20:36:31 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 20:36:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 20:36:31 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 20:36:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 20:36:31 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 20:36:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 20:36:31 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 20:36:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 20:36:31 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 20:36:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 20:36:31 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 20:36:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 20:36:31 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:36:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 20:36:31 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:36:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-30 20:36:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 20:36:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 20:36:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v89870
2022-03-30 20:36:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v89870
2022-03-30 20:36:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=89870&allowWatchBookmarks=true&watch=true...
2022-03-30 20:36:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 20:36:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 89871
2022-03-30 20:36:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:36:36 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 89891
2022-03-30 20:36:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:36:41 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 89899
2022-03-30 20:36:41 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v89891 in namespace default
2022-03-30 20:36:41 [ForkJoinPool-3-worker-9] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5a34beae
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-30 20:36:41 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@9d1b907
2022-03-30 20:36:41 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 20:36:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@5314833a, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5a34beae, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-30 20:36:41 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@2d4b1bfd
2022-03-30 20:36:41 [ForkJoinPool-3-worker-9] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-30 20:36:41 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@2d4b1bfd
2022-03-30 20:36:41 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@2d4b1bfd
2022-03-30 20:36:41 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 20:36:41 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 20:36:41 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-30 20:36:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-30 20:36:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-6 get Namespace infra-namespace -o json
2022-03-30 20:36:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-6 get Namespace infra-namespace -o json
2022-03-30 20:36:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:36:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T20:36:41Z",
        "name": "infra-namespace",
        "resourceVersion": "89900",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "9991fca7-2b28-491d-8d1f-8a65b65d8871"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 20:36:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-30 20:36:41 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 20:36:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-30 20:36:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:36:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 20:36:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 20:36:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 20:36:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 20:36:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 20:36:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 20:36:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 20:36:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 20:36:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 20:36:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 20:36:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 20:36:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 20:36:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 20:36:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 20:36:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 20:36:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 20:36:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:36:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 20:36:41 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 20:36:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479997ms till timeout)
2022-03-30 20:36:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478993ms till timeout)
2022-03-30 20:36:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477990ms till timeout)
2022-03-30 20:36:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476987ms till timeout)
2022-03-30 20:36:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:36:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475984ms till timeout)
2022-03-30 20:36:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474980ms till timeout)
2022-03-30 20:36:48 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473976ms till timeout)
2022-03-30 20:36:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472973ms till timeout)
2022-03-30 20:36:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471969ms till timeout)
2022-03-30 20:36:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:36:51 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470966ms till timeout)
2022-03-30 20:36:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469963ms till timeout)
2022-03-30 20:36:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468960ms till timeout)
2022-03-30 20:36:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467957ms till timeout)
2022-03-30 20:36:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466953ms till timeout)
2022-03-30 20:36:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:36:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465950ms till timeout)
2022-03-30 20:36:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464947ms till timeout)
2022-03-30 20:36:58 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463944ms till timeout)
2022-03-30 20:36:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462941ms till timeout)
2022-03-30 20:37:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:37:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461937ms till timeout)
2022-03-30 20:37:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (460934ms till timeout)
2022-03-30 20:37:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (459931ms till timeout)
2022-03-30 20:37:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (458928ms till timeout)
2022-03-30 20:37:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (457924ms till timeout)
2022-03-30 20:37:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:37:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (456921ms till timeout)
2022-03-30 20:37:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (455918ms till timeout)
2022-03-30 20:37:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (454915ms till timeout)
2022-03-30 20:37:08 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (453912ms till timeout)
2022-03-30 20:37:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (452909ms till timeout)
2022-03-30 20:37:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:37:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (451906ms till timeout)
2022-03-30 20:37:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (450903ms till timeout)
2022-03-30 20:37:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (449899ms till timeout)
2022-03-30 20:37:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (448896ms till timeout)
2022-03-30 20:37:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (447893ms till timeout)
2022-03-30 20:37:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:37:15 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-30 20:37:15 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-30 20:37:15 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-30 20:37:15 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-nb28j not ready: strimzi-cluster-operator)
2022-03-30 20:37:15 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-nb28j are ready
2022-03-30 20:37:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-30 20:37:16 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-nb28j not ready: strimzi-cluster-operator)
2022-03-30 20:37:16 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-nb28j are ready
2022-03-30 20:37:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598993ms till timeout)
2022-03-30 20:37:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-nb28j not ready: strimzi-cluster-operator)
2022-03-30 20:37:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-nb28j are ready
2022-03-30 20:37:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597990ms till timeout)
2022-03-30 20:37:18 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-nb28j not ready: strimzi-cluster-operator)
2022-03-30 20:37:18 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-nb28j are ready
2022-03-30 20:37:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596986ms till timeout)
2022-03-30 20:37:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-nb28j not ready: strimzi-cluster-operator)
2022-03-30 20:37:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-nb28j are ready
2022-03-30 20:37:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595983ms till timeout)
2022-03-30 20:37:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:37:20 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-nb28j not ready: strimzi-cluster-operator)
2022-03-30 20:37:20 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-nb28j are ready
2022-03-30 20:37:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594979ms till timeout)
2022-03-30 20:37:21 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-nb28j not ready: strimzi-cluster-operator)
2022-03-30 20:37:21 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-nb28j are ready
2022-03-30 20:37:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593975ms till timeout)
2022-03-30 20:37:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-nb28j not ready: strimzi-cluster-operator)
2022-03-30 20:37:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-nb28j are ready
2022-03-30 20:37:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592971ms till timeout)
2022-03-30 20:37:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-nb28j not ready: strimzi-cluster-operator)
2022-03-30 20:37:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-nb28j are ready
2022-03-30 20:37:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591968ms till timeout)
2022-03-30 20:37:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-nb28j not ready: strimzi-cluster-operator)
2022-03-30 20:37:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-nb28j are ready
2022-03-30 20:37:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590964ms till timeout)
2022-03-30 20:37:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:37:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-nb28j not ready: strimzi-cluster-operator)
2022-03-30 20:37:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-nb28j are ready
2022-03-30 20:37:25 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-30 20:37:25 [ForkJoinPool-3-worker-9] [32mINFO [m [SpecificIsolatedST:503] 0.21.4
2022-03-30 20:37:25 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 20:37:25 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.specific.SpecificIsolatedST.testRackAwareConnectCorrectDeployment-STARTED
2022-03-30 20:37:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 20:37:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:659] [specific.SpecificIsolatedST - Before Each] - Setup test case environment
2022-03-30 20:37:25 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 20:37:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba, testSendSimpleMessageTls=my-cluster-ca7325d7, testUpdateUser=my-cluster-6e6fa8fe, testSendMessagesTlsScramSha=my-cluster-84882f67, testReceiveSimpleMessageTls=my-cluster-2e450144, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37, testCruiseControlBasicAPIRequests=my-cluster-8aed4745, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273}
2022-03-30 20:37:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testMultiNodeKafkaConnectWithConnectorCreation=my-user-1851888062-673454065, testKafkaAndZookeeperScaleUpScaleDown=my-user-838849261-890460734, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1731919654-539989781, testSendSimpleMessageTls=my-user-1309491966-2015599610, testUpdateUser=my-user-1387197422-925410353, testSendMessagesTlsScramSha=my-user-586569987-2105688079, testReceiveSimpleMessageTls=my-user-1669668617-1031881775, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1886945581-244087591, testSendMessagesCustomListenerTlsScramSha=my-user-1118022021-805525218, testCruiseControlBasicAPIRequests=my-user-1861484466-561878159, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677, testRackAwareConnectCorrectDeployment=my-user-1051420592-1240796798}
2022-03-30 20:37:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testMultiNodeKafkaConnectWithConnectorCreation=my-topic-307985263-926887950, testKafkaAndZookeeperScaleUpScaleDown=my-topic-2138084746-704381117, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1672280868-1119644965, testSendSimpleMessageTls=my-topic-435427300-917779110, testUpdateUser=my-topic-1875783706-1763804507, testSendMessagesTlsScramSha=my-topic-255809133-823300208, testReceiveSimpleMessageTls=my-topic-368564676-637091599, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-306860904-106838350, testSendMessagesCustomListenerTlsScramSha=my-topic-963888555-1712711326, testCruiseControlBasicAPIRequests=my-topic-1631570507-1308241024, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206, testRackAwareConnectCorrectDeployment=my-topic-189499239-153088857}
2022-03-30 20:37:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba-kafka-clients, testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testUpdateUser=my-cluster-6e6fa8fe-kafka-clients, testSendMessagesTlsScramSha=my-cluster-84882f67-kafka-clients, testReceiveSimpleMessageTls=my-cluster-2e450144-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-8aed4745-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273-kafka-clients}
2022-03-30 20:37:25 [ForkJoinPool-3-worker-9] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-30 20:37:25 [ForkJoinPool-3-worker-9] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-30 20:37:25 [ForkJoinPool-3-worker-9] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-30 20:37:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:37:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-30 20:37:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 20:37:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 20:37:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 20:37:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 20:37:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:37:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 20:37:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 20:37:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 20:37:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io not ready, will try again in 10000 ms (179993ms till timeout)
2022-03-30 20:37:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:37:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:37:35 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 20:37:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 20:37:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179922ms till timeout)
2022-03-30 20:37:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:37:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:37:45 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 20:37:45 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 20:37:45 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 20:37:45 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 20:37:45 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 20:37:45 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 20:37:45 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:37:45 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 20:37:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator not ready, will try again in 10000 ms (479952ms till timeout)
2022-03-30 20:37:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:37:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:37:55 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 20:37:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 20:37:55 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 20:37:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 20:37:55 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:37:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 20:37:55 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 20:37:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 20:37:56 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 20:37:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 20:37:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179949ms till timeout)
2022-03-30 20:38:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:38:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:38:06 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 20:38:06 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 20:38:06 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 20:38:06 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 20:38:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179829ms till timeout)
2022-03-30 20:38:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:38:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:38:16 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 20:38:16 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 20:38:16 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 20:38:16 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 20:38:16 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 20:38:16 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 20:38:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io not ready, will try again in 10000 ms (179993ms till timeout)
2022-03-30 20:38:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:38:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:38:26 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 20:38:26 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 20:38:26 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 20:38:26 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 20:38:26 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 20:38:26 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 20:38:26 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 20:38:26 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 20:38:26 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 20:38:26 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 20:38:26 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:38:26 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 20:38:26 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:38:26 [ForkJoinPool-3-worker-9] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-30 20:38:26 [ForkJoinPool-3-worker-9] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 20:38:26 [ForkJoinPool-3-worker-9] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 20:38:26 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v90116
2022-03-30 20:38:26 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v90116
2022-03-30 20:38:26 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=90116&allowWatchBookmarks=true&watch=true...
2022-03-30 20:38:26 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 20:38:26 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 90117
2022-03-30 20:38:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:38:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 90132
2022-03-30 20:38:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 90133
2022-03-30 20:38:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v90132 in namespace default
2022-03-30 20:38:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@2fe514d
2022-03-30 20:38:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 20:38:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@6c1d5ca
2022-03-30 20:38:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@6c1d5ca
2022-03-30 20:38:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@6c1d5ca
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [32mINFO [m [SetupClusterOperator:223] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@5314833a, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5a34beae, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=30000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-30 20:38:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 20:38:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T20:38:31Z",
        "name": "infra-namespace",
        "resourceVersion": "90134",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "0b04b8b4-7697-49db-9069-4dd227dec319"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:38:31 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 20:38:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479998ms till timeout)
2022-03-30 20:38:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478995ms till timeout)
2022-03-30 20:38:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477992ms till timeout)
2022-03-30 20:38:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476988ms till timeout)
2022-03-30 20:38:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:38:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475985ms till timeout)
2022-03-30 20:38:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474981ms till timeout)
2022-03-30 20:38:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473978ms till timeout)
2022-03-30 20:38:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472976ms till timeout)
2022-03-30 20:38:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471972ms till timeout)
2022-03-30 20:38:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:38:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470969ms till timeout)
2022-03-30 20:38:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469965ms till timeout)
2022-03-30 20:38:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468962ms till timeout)
2022-03-30 20:38:44 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-30 20:38:44 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-30 20:38:44 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-30 20:38:44 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-2j5b9 not ready: strimzi-cluster-operator)
2022-03-30 20:38:44 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-2j5b9 are ready
2022-03-30 20:38:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-30 20:38:45 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-2j5b9 not ready: strimzi-cluster-operator)
2022-03-30 20:38:45 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-2j5b9 are ready
2022-03-30 20:38:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598993ms till timeout)
2022-03-30 20:38:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:38:46 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-2j5b9 not ready: strimzi-cluster-operator)
2022-03-30 20:38:46 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-2j5b9 are ready
2022-03-30 20:38:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597989ms till timeout)
2022-03-30 20:38:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-2j5b9 not ready: strimzi-cluster-operator)
2022-03-30 20:38:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-2j5b9 are ready
2022-03-30 20:38:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596985ms till timeout)
2022-03-30 20:38:48 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-2j5b9 not ready: strimzi-cluster-operator)
2022-03-30 20:38:48 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-2j5b9 are ready
2022-03-30 20:38:48 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595981ms till timeout)
2022-03-30 20:38:49 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-2j5b9 not ready: strimzi-cluster-operator)
2022-03-30 20:38:49 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-2j5b9 are ready
2022-03-30 20:38:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594978ms till timeout)
2022-03-30 20:38:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-2j5b9 not ready: strimzi-cluster-operator)
2022-03-30 20:38:50 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-2j5b9 are ready
2022-03-30 20:38:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593974ms till timeout)
2022-03-30 20:38:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:38:51 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-2j5b9 not ready: strimzi-cluster-operator)
2022-03-30 20:38:51 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-2j5b9 are ready
2022-03-30 20:38:51 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592970ms till timeout)
2022-03-30 20:38:52 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-2j5b9 not ready: strimzi-cluster-operator)
2022-03-30 20:38:52 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-2j5b9 are ready
2022-03-30 20:38:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591967ms till timeout)
2022-03-30 20:38:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-2j5b9 not ready: strimzi-cluster-operator)
2022-03-30 20:38:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-2j5b9 are ready
2022-03-30 20:38:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590962ms till timeout)
2022-03-30 20:38:54 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-2j5b9 not ready: strimzi-cluster-operator)
2022-03-30 20:38:54 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-2j5b9 are ready
2022-03-30 20:38:54 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-30 20:38:54 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment strimzi-cluster-operator rolling update
2022-03-30 20:38:54 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Deployment strimzi-cluster-operator rolling update in namespace:infra-namespace
2022-03-30 20:38:54 [ForkJoinPool-3-worker-9] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {strimzi-cluster-operator-78689684d4-nb28j=fed35458-7812-4b7a-a78f-d2953882c579}
2022-03-30 20:38:54 [ForkJoinPool-3-worker-9] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {strimzi-cluster-operator-77554ffdfb-2j5b9=c6a9f41a-c6d4-4a22-85af-1b372962b043}
2022-03-30 20:38:54 [ForkJoinPool-3-worker-9] [36mDEBUG[m [DeploymentUtils:119] All pods seem to have rolled
2022-03-30 20:38:54 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 20:38:54 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 20:38:54 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-30 20:38:54 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-30 20:38:54 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-2j5b9 not ready: strimzi-cluster-operator)
2022-03-30 20:38:54 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-2j5b9 are ready
2022-03-30 20:38:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 20:38:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-2j5b9 not ready: strimzi-cluster-operator)
2022-03-30 20:38:55 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-2j5b9 are ready
2022-03-30 20:38:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598992ms till timeout)
2022-03-30 20:38:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:38:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-2j5b9 not ready: strimzi-cluster-operator)
2022-03-30 20:38:56 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-2j5b9 are ready
2022-03-30 20:38:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597988ms till timeout)
2022-03-30 20:38:57 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-2j5b9 not ready: strimzi-cluster-operator)
2022-03-30 20:38:57 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-2j5b9 are ready
2022-03-30 20:38:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596984ms till timeout)
2022-03-30 20:38:58 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-2j5b9 not ready: strimzi-cluster-operator)
2022-03-30 20:38:58 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-2j5b9 are ready
2022-03-30 20:38:58 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595980ms till timeout)
2022-03-30 20:38:59 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-2j5b9 not ready: strimzi-cluster-operator)
2022-03-30 20:38:59 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-2j5b9 are ready
2022-03-30 20:38:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594976ms till timeout)
2022-03-30 20:39:00 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-2j5b9 not ready: strimzi-cluster-operator)
2022-03-30 20:39:00 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-2j5b9 are ready
2022-03-30 20:39:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593972ms till timeout)
2022-03-30 20:39:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:39:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-2j5b9 not ready: strimzi-cluster-operator)
2022-03-30 20:39:01 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-2j5b9 are ready
2022-03-30 20:39:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592968ms till timeout)
2022-03-30 20:39:02 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-2j5b9 not ready: strimzi-cluster-operator)
2022-03-30 20:39:02 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-2j5b9 are ready
2022-03-30 20:39:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591964ms till timeout)
2022-03-30 20:39:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-2j5b9 not ready: strimzi-cluster-operator)
2022-03-30 20:39:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-2j5b9 are ready
2022-03-30 20:39:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590960ms till timeout)
2022-03-30 20:39:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-2j5b9 not ready: strimzi-cluster-operator)
2022-03-30 20:39:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-2j5b9 are ready
2022-03-30 20:39:04 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:141] Deployment strimzi-cluster-operator rolling update finished
2022-03-30 20:39:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-6fad9273 in namespace infra-namespace
2022-03-30 20:39:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-6fad9273
2022-03-30 20:39:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-6fad9273 will have desired state: Ready
2022-03-30 20:39:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-6fad9273 will have desired state: Ready
2022-03-30 20:39:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (839997ms till timeout)
2022-03-30 20:39:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (838994ms till timeout)
2022-03-30 20:39:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:39:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (837991ms till timeout)
2022-03-30 20:39:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (836988ms till timeout)
2022-03-30 20:39:08 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (835985ms till timeout)
2022-03-30 20:39:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (834979ms till timeout)
2022-03-30 20:39:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (833976ms till timeout)
2022-03-30 20:39:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:39:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (832973ms till timeout)
2022-03-30 20:39:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (831967ms till timeout)
2022-03-30 20:39:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (830964ms till timeout)
2022-03-30 20:39:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (829960ms till timeout)
2022-03-30 20:39:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:39:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (828957ms till timeout)
2022-03-30 20:39:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (827954ms till timeout)
2022-03-30 20:39:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (826951ms till timeout)
2022-03-30 20:39:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (825947ms till timeout)
2022-03-30 20:39:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (824944ms till timeout)
2022-03-30 20:39:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:39:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (823941ms till timeout)
2022-03-30 20:39:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (822938ms till timeout)
2022-03-30 20:39:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (821935ms till timeout)
2022-03-30 20:39:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (820932ms till timeout)
2022-03-30 20:39:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (819929ms till timeout)
2022-03-30 20:39:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:39:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (818926ms till timeout)
2022-03-30 20:39:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (817923ms till timeout)
2022-03-30 20:39:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (816920ms till timeout)
2022-03-30 20:39:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (815917ms till timeout)
2022-03-30 20:39:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (814913ms till timeout)
2022-03-30 20:39:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:39:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (813910ms till timeout)
2022-03-30 20:39:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (812907ms till timeout)
2022-03-30 20:39:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (811903ms till timeout)
2022-03-30 20:39:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (810900ms till timeout)
2022-03-30 20:39:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (809897ms till timeout)
2022-03-30 20:39:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:39:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (808890ms till timeout)
2022-03-30 20:39:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (807883ms till timeout)
2022-03-30 20:39:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (806879ms till timeout)
2022-03-30 20:39:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (805875ms till timeout)
2022-03-30 20:39:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (804872ms till timeout)
2022-03-30 20:39:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:39:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (803866ms till timeout)
2022-03-30 20:39:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (802862ms till timeout)
2022-03-30 20:39:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (801859ms till timeout)
2022-03-30 20:39:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (800855ms till timeout)
2022-03-30 20:39:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (799852ms till timeout)
2022-03-30 20:39:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:39:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (798849ms till timeout)
2022-03-30 20:39:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (797846ms till timeout)
2022-03-30 20:39:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (796842ms till timeout)
2022-03-30 20:39:48 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (795839ms till timeout)
2022-03-30 20:39:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (794836ms till timeout)
2022-03-30 20:39:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:39:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (793833ms till timeout)
2022-03-30 20:39:51 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (792830ms till timeout)
2022-03-30 20:39:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (791827ms till timeout)
2022-03-30 20:39:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (790824ms till timeout)
2022-03-30 20:39:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (789821ms till timeout)
2022-03-30 20:39:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:39:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (788818ms till timeout)
2022-03-30 20:39:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (787815ms till timeout)
2022-03-30 20:39:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (786811ms till timeout)
2022-03-30 20:39:58 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (785808ms till timeout)
2022-03-30 20:39:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (784805ms till timeout)
2022-03-30 20:40:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:40:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (783802ms till timeout)
2022-03-30 20:40:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (782798ms till timeout)
2022-03-30 20:40:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (781795ms till timeout)
2022-03-30 20:40:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (780792ms till timeout)
2022-03-30 20:40:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (779788ms till timeout)
2022-03-30 20:40:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:40:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (778785ms till timeout)
2022-03-30 20:40:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (777780ms till timeout)
2022-03-30 20:40:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (776777ms till timeout)
2022-03-30 20:40:08 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (775771ms till timeout)
2022-03-30 20:40:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (774768ms till timeout)
2022-03-30 20:40:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:40:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (773764ms till timeout)
2022-03-30 20:40:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (772761ms till timeout)
2022-03-30 20:40:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (771757ms till timeout)
2022-03-30 20:40:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (770754ms till timeout)
2022-03-30 20:40:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (769750ms till timeout)
2022-03-30 20:40:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:40:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (768747ms till timeout)
2022-03-30 20:40:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (767744ms till timeout)
2022-03-30 20:40:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (766740ms till timeout)
2022-03-30 20:40:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (765737ms till timeout)
2022-03-30 20:40:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (764733ms till timeout)
2022-03-30 20:40:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:40:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (763730ms till timeout)
2022-03-30 20:40:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (762727ms till timeout)
2022-03-30 20:40:22 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-6fad9273 is in desired state: Ready
2022-03-30 20:40:22 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic rw-my-topic-1577676118-1039249209 in namespace infra-namespace
2022-03-30 20:40:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:rw-my-topic-1577676118-1039249209
2022-03-30 20:40:22 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: rw-my-topic-1577676118-1039249209 will have desired state: Ready
2022-03-30 20:40:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: rw-my-topic-1577676118-1039249209 will have desired state: Ready
2022-03-30 20:40:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaTopic: rw-my-topic-1577676118-1039249209 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 20:40:23 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaTopic: rw-my-topic-1577676118-1039249209 is in desired state: Ready
2022-03-30 20:40:23 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-6fad9273-kafka-clients in namespace infra-namespace
2022-03-30 20:40:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-6fad9273-kafka-clients
2022-03-30 20:40:23 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-6fad9273-kafka-clients will be ready
2022-03-30 20:40:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-6fad9273-kafka-clients will be ready
2022-03-30 20:40:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-6fad9273-kafka-clients will be ready not ready, will try again in 1000 ms (479997ms till timeout)
2022-03-30 20:40:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-6fad9273-kafka-clients will be ready not ready, will try again in 1000 ms (478994ms till timeout)
2022-03-30 20:40:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:40:25 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-6fad9273-kafka-clients is ready
2022-03-30 20:40:25 [ForkJoinPool-3-worker-9] [32mINFO [m [SpecificIsolatedST:308] Deploy KafkaConnect with correct rack-aware topology key: rack-key
2022-03-30 20:40:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-6fad9273-scraper in namespace infra-namespace
2022-03-30 20:40:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-6fad9273-scraper
2022-03-30 20:40:25 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-6fad9273-scraper will be ready
2022-03-30 20:40:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-6fad9273-scraper will be ready
2022-03-30 20:40:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-6fad9273-scraper will be ready not ready, will try again in 1000 ms (479998ms till timeout)
2022-03-30 20:40:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-6fad9273-scraper will be ready not ready, will try again in 1000 ms (478994ms till timeout)
2022-03-30 20:40:27 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-6fad9273-scraper is ready
2022-03-30 20:40:27 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-6fad9273-scraper to be ready
2022-03-30 20:40:27 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-6fad9273-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready
2022-03-30 20:40:27 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-6fad9273-scraper-66cc4745f9-4s4np not ready: my-cluster-6fad9273-scraper)
2022-03-30 20:40:27 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods my-cluster-6fad9273-scraper-66cc4745f9-4s4np are ready
2022-03-30 20:40:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-6fad9273-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599996ms till timeout)
2022-03-30 20:40:28 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-6fad9273-scraper-66cc4745f9-4s4np not ready: my-cluster-6fad9273-scraper)
2022-03-30 20:40:28 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods my-cluster-6fad9273-scraper-66cc4745f9-4s4np are ready
2022-03-30 20:40:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-6fad9273-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598992ms till timeout)
2022-03-30 20:40:29 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-6fad9273-scraper-66cc4745f9-4s4np not ready: my-cluster-6fad9273-scraper)
2022-03-30 20:40:29 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods my-cluster-6fad9273-scraper-66cc4745f9-4s4np are ready
2022-03-30 20:40:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-6fad9273-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597987ms till timeout)
2022-03-30 20:40:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:40:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-6fad9273-scraper-66cc4745f9-4s4np not ready: my-cluster-6fad9273-scraper)
2022-03-30 20:40:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods my-cluster-6fad9273-scraper-66cc4745f9-4s4np are ready
2022-03-30 20:40:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-6fad9273-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596983ms till timeout)
2022-03-30 20:40:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-6fad9273-scraper-66cc4745f9-4s4np not ready: my-cluster-6fad9273-scraper)
2022-03-30 20:40:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods my-cluster-6fad9273-scraper-66cc4745f9-4s4np are ready
2022-03-30 20:40:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-6fad9273-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595979ms till timeout)
2022-03-30 20:40:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-6fad9273-scraper-66cc4745f9-4s4np not ready: my-cluster-6fad9273-scraper)
2022-03-30 20:40:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods my-cluster-6fad9273-scraper-66cc4745f9-4s4np are ready
2022-03-30 20:40:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-6fad9273-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594974ms till timeout)
2022-03-30 20:40:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-6fad9273-scraper-66cc4745f9-4s4np not ready: my-cluster-6fad9273-scraper)
2022-03-30 20:40:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods my-cluster-6fad9273-scraper-66cc4745f9-4s4np are ready
2022-03-30 20:40:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-6fad9273-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593970ms till timeout)
2022-03-30 20:40:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-6fad9273-scraper-66cc4745f9-4s4np not ready: my-cluster-6fad9273-scraper)
2022-03-30 20:40:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods my-cluster-6fad9273-scraper-66cc4745f9-4s4np are ready
2022-03-30 20:40:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-6fad9273-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592965ms till timeout)
2022-03-30 20:40:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:40:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-6fad9273-scraper-66cc4745f9-4s4np not ready: my-cluster-6fad9273-scraper)
2022-03-30 20:40:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods my-cluster-6fad9273-scraper-66cc4745f9-4s4np are ready
2022-03-30 20:40:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-6fad9273-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591961ms till timeout)
2022-03-30 20:40:36 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-6fad9273-scraper-66cc4745f9-4s4np not ready: my-cluster-6fad9273-scraper)
2022-03-30 20:40:36 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods my-cluster-6fad9273-scraper-66cc4745f9-4s4np are ready
2022-03-30 20:40:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-6fad9273-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590956ms till timeout)
2022-03-30 20:40:37 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod my-cluster-6fad9273-scraper-66cc4745f9-4s4np not ready: my-cluster-6fad9273-scraper)
2022-03-30 20:40:37 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods my-cluster-6fad9273-scraper-66cc4745f9-4s4np are ready
2022-03-30 20:40:37 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-6fad9273-scraper is ready
2022-03-30 20:40:37 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-6fad9273-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-03-30 20:40:37 [ForkJoinPool-3-worker-9] [36mDEBUG[m [NetworkPolicyResource:227] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=my-cluster-6fad9273-allow, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8083, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9404, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8080, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9999, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-6fad9273, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-6fad9273-connect}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 20:40:37 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-6fad9273-allow in namespace infra-namespace
2022-03-30 20:40:37 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:my-cluster-6fad9273-allow
2022-03-30 20:40:37 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-03-30 20:40:37 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-6fad9273 in namespace infra-namespace
2022-03-30 20:40:37 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaConnect:my-cluster-6fad9273
2022-03-30 20:40:37 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-6fad9273 will have desired state: Ready
2022-03-30 20:40:37 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for KafkaConnect: my-cluster-6fad9273 will have desired state: Ready
2022-03-30 20:40:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 20:40:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (598995ms till timeout)
2022-03-30 20:40:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (597991ms till timeout)
2022-03-30 20:40:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:40:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (596989ms till timeout)
2022-03-30 20:40:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (595985ms till timeout)
2022-03-30 20:40:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (594982ms till timeout)
2022-03-30 20:40:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (593979ms till timeout)
2022-03-30 20:40:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (592975ms till timeout)
2022-03-30 20:40:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:40:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (591972ms till timeout)
2022-03-30 20:40:46 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (590969ms till timeout)
2022-03-30 20:40:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (589965ms till timeout)
2022-03-30 20:40:48 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (588962ms till timeout)
2022-03-30 20:40:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (587958ms till timeout)
2022-03-30 20:40:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:40:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (586955ms till timeout)
2022-03-30 20:40:51 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (585951ms till timeout)
2022-03-30 20:40:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (584948ms till timeout)
2022-03-30 20:40:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (583945ms till timeout)
2022-03-30 20:40:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (582942ms till timeout)
2022-03-30 20:40:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:40:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (581938ms till timeout)
2022-03-30 20:40:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (580935ms till timeout)
2022-03-30 20:40:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (579932ms till timeout)
2022-03-30 20:40:58 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (578928ms till timeout)
2022-03-30 20:40:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (577925ms till timeout)
2022-03-30 20:41:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:41:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (576922ms till timeout)
2022-03-30 20:41:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (575918ms till timeout)
2022-03-30 20:41:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (574915ms till timeout)
2022-03-30 20:41:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (573911ms till timeout)
2022-03-30 20:41:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (572908ms till timeout)
2022-03-30 20:41:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:41:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (571904ms till timeout)
2022-03-30 20:41:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (570901ms till timeout)
2022-03-30 20:41:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (569898ms till timeout)
2022-03-30 20:41:08 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (568894ms till timeout)
2022-03-30 20:41:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (567891ms till timeout)
2022-03-30 20:41:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:41:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (566888ms till timeout)
2022-03-30 20:41:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (565885ms till timeout)
2022-03-30 20:41:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (564881ms till timeout)
2022-03-30 20:41:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (563878ms till timeout)
2022-03-30 20:41:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (562875ms till timeout)
2022-03-30 20:41:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:41:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (561871ms till timeout)
2022-03-30 20:41:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (560868ms till timeout)
2022-03-30 20:41:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (559865ms till timeout)
2022-03-30 20:41:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (558861ms till timeout)
2022-03-30 20:41:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (557858ms till timeout)
2022-03-30 20:41:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:41:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (556854ms till timeout)
2022-03-30 20:41:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (555850ms till timeout)
2022-03-30 20:41:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (554846ms till timeout)
2022-03-30 20:41:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (553843ms till timeout)
2022-03-30 20:41:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (552840ms till timeout)
2022-03-30 20:41:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:41:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (551836ms till timeout)
2022-03-30 20:41:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (550833ms till timeout)
2022-03-30 20:41:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (549830ms till timeout)
2022-03-30 20:41:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (548826ms till timeout)
2022-03-30 20:41:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (547823ms till timeout)
2022-03-30 20:41:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:41:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (546820ms till timeout)
2022-03-30 20:41:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (545816ms till timeout)
2022-03-30 20:41:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (544813ms till timeout)
2022-03-30 20:41:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (543810ms till timeout)
2022-03-30 20:41:35 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (542806ms till timeout)
2022-03-30 20:41:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:41:36 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (541803ms till timeout)
2022-03-30 20:41:37 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (540799ms till timeout)
2022-03-30 20:41:38 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (539796ms till timeout)
2022-03-30 20:41:39 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (538793ms till timeout)
2022-03-30 20:41:40 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (537790ms till timeout)
2022-03-30 20:41:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:41:41 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (536786ms till timeout)
2022-03-30 20:41:42 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (535783ms till timeout)
2022-03-30 20:41:43 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (534780ms till timeout)
2022-03-30 20:41:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] KafkaConnect: my-cluster-6fad9273 will have desired state: Ready not ready, will try again in 1000 ms (533776ms till timeout)
2022-03-30 20:41:45 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-6fad9273 is in desired state: Ready
2022-03-30 20:41:45 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-6fad9273-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-03-30 20:41:45 [ForkJoinPool-3-worker-9] [36mDEBUG[m [NetworkPolicyResource:227] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=my-cluster-6fad9273-allow, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8083, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9404, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8080, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9999, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-6fad9273, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-6fad9273-connect}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 20:41:45 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-6fad9273-allow in namespace infra-namespace
2022-03-30 20:41:45 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:my-cluster-6fad9273-allow
2022-03-30 20:41:45 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-03-30 20:41:45 [ForkJoinPool-3-worker-9] [32mINFO [m [SpecificIsolatedST:333] PodName: my-cluster-6fad9273-connect-649ccdd8cf-whkh2
NodeAffinity: NodeAffinity(preferredDuringSchedulingIgnoredDuringExecution=[], requiredDuringSchedulingIgnoredDuringExecution=NodeSelector(nodeSelectorTerms=[NodeSelectorTerm(matchExpressions=[NodeSelectorRequirement(key=rack-key, operator=Exists, values=[], additionalProperties={})], matchFields=[], additionalProperties={})], additionalProperties={}), additionalProperties={})
2022-03-30 20:41:45 [ForkJoinPool-3-worker-9] [32mINFO [m [KafkaConnectUtils:154] Send and receive messages through KafkaConnect
2022-03-30 20:41:45 [ForkJoinPool-3-worker-9] [32mINFO [m [KafkaConnectUtils:63] Waiting until KafkaConnect API is available
2022-03-30 20:41:45 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Waiting until KafkaConnect API is available
2022-03-30 20:41:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace exec my-cluster-6fad9273-connect-649ccdd8cf-whkh2 -- /bin/bash -c curl -I http://localhost:8083/connectors
2022-03-30 20:41:45 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-6fad9273-connect-649ccdd8cf-whkh2 -- /bin/bash -c curl -I http://localhost:8083/connectors
2022-03-30 20:41:45 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Return code: 0
2022-03-30 20:41:45 [ForkJoinPool-3-worker-9] [32mINFO [m [KafkaConnectUtils:66] KafkaConnect API is available
2022-03-30 20:41:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace exec my-cluster-6fad9273-kafka-clients-54db94cdbd-2jnfm -- /bin/bash -c curl -X POST -H "Content-Type: application/json" --data '{ "name": "sink-test", "config": { "connector.class": "FileStreamSink", "tasks.max": "1", "topics": "rw-my-topic-1577676118-1039249209", "file": "/tmp/test-file-sink.txt" } }' http://my-cluster-6fad9273-connect-api.infra-namespace.svc:8083/connectors
2022-03-30 20:41:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:41:45 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-6fad9273-kafka-clients-54db94cdbd-2jnfm -- /bin/bash -c curl -X POST -H "Content-Type: application/json" --data '{ "name": "sink-test", "config": { "connector.class": "FileStreamSink", "tasks.max": "1", "topics": "rw-my-topic-1577676118-1039249209", "file": "/tmp/test-file-sink.txt" } }' http://my-cluster-6fad9273-connect-api.infra-namespace.svc:8083/connectors
2022-03-30 20:41:45 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Return code: 0
2022-03-30 20:41:45 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-30 20:41:45 [ForkJoinPool-3-worker-9] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7fa62910, which are set.
2022-03-30 20:41:45 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@6243c773, messages=[], arguments=[--topic, rw-my-topic-1577676118-1039249209, --bootstrap-server, my-cluster-6fad9273-kafka-bootstrap.infra-namespace.svc:9092, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-6fad9273-kafka-clients-54db94cdbd-2jnfm', podNamespace='infra-namespace', bootstrapServer='my-cluster-6fad9273-kafka-bootstrap.infra-namespace.svc:9092', topicName='rw-my-topic-1577676118-1039249209', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7fa62910}
2022-03-30 20:41:45 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-6fad9273-kafka-bootstrap.infra-namespace.svc:9092:rw-my-topic-1577676118-1039249209 from pod my-cluster-6fad9273-kafka-clients-54db94cdbd-2jnfm
2022-03-30 20:41:45 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-6fad9273-kafka-clients-54db94cdbd-2jnfm -n infra-namespace -- /opt/kafka/producer.sh --topic rw-my-topic-1577676118-1039249209 --bootstrap-server my-cluster-6fad9273-kafka-bootstrap.infra-namespace.svc:9092 --max-messages 100
2022-03-30 20:41:45 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-6fad9273-kafka-clients-54db94cdbd-2jnfm -n infra-namespace -- /opt/kafka/producer.sh --topic rw-my-topic-1577676118-1039249209 --bootstrap-server my-cluster-6fad9273-kafka-bootstrap.infra-namespace.svc:9092 --max-messages 100
2022-03-30 20:41:48 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-03-30 20:41:48 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-03-30 20:41:48 [ForkJoinPool-3-worker-9] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@55f78d21, which are set.
2022-03-30 20:41:48 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@2e75f6ad, messages=[], arguments=[--group-id, my-consumer-group-1585499714, --topic, rw-my-topic-1577676118-1039249209, --group-instance-id, instance144399461, --bootstrap-server, my-cluster-6fad9273-kafka-bootstrap.infra-namespace.svc:9092, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-6fad9273-kafka-clients-54db94cdbd-2jnfm', podNamespace='infra-namespace', bootstrapServer='my-cluster-6fad9273-kafka-bootstrap.infra-namespace.svc:9092', topicName='rw-my-topic-1577676118-1039249209', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1585499714', consumerInstanceId='instance144399461', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@55f78d21}
2022-03-30 20:41:48 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-6fad9273-kafka-bootstrap.infra-namespace.svc:9092#rw-my-topic-1577676118-1039249209 from pod my-cluster-6fad9273-kafka-clients-54db94cdbd-2jnfm
2022-03-30 20:41:48 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-6fad9273-kafka-clients-54db94cdbd-2jnfm -n infra-namespace -- /opt/kafka/consumer.sh --group-id my-consumer-group-1585499714 --topic rw-my-topic-1577676118-1039249209 --group-instance-id instance144399461 --bootstrap-server my-cluster-6fad9273-kafka-bootstrap.infra-namespace.svc:9092 --max-messages 100
2022-03-30 20:41:48 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-6fad9273-kafka-clients-54db94cdbd-2jnfm -n infra-namespace -- /opt/kafka/consumer.sh --group-id my-consumer-group-1585499714 --topic rw-my-topic-1577676118-1039249209 --group-instance-id instance144399461 --bootstrap-server my-cluster-6fad9273-kafka-bootstrap.infra-namespace.svc:9092 --max-messages 100
2022-03-30 20:41:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:41:53 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-30 20:41:53 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-03-30 20:41:53 [ForkJoinPool-3-worker-9] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-6fad9273-connect-649ccdd8cf-whkh2
2022-03-30 20:41:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for messages in file sink
2022-03-30 20:41:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace exec my-cluster-6fad9273-connect-649ccdd8cf-whkh2 -- /bin/bash -c cat /tmp/test-file-sink.txt
2022-03-30 20:41:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:421] Command: kubectl --namespace infra-namespace exec my-cluster-6fad9273-connect-649ccdd8cf-whkh2 -- /bin/bash -c cat /tmp/test-file-sink.txt
2022-03-30 20:41:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:421] Return code: 0
2022-03-30 20:41:53 [ForkJoinPool-3-worker-9] [32mINFO [m [KafkaConnectUtils:77] Expected messages are in file sink on my-cluster-6fad9273-connect-649ccdd8cf-whkh2
2022-03-30 20:41:53 [ForkJoinPool-3-worker-9] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-30 20:41:53 [ForkJoinPool-3-worker-9] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-30 20:41:53 [ForkJoinPool-3-worker-9] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-30 20:41:53 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:41:53 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-30 20:41:53 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 20:41:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 20:41:53 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 20:41:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 20:41:53 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:41:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 20:41:53 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 20:41:53 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 20:41:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io not ready, will try again in 10000 ms (179993ms till timeout)
2022-03-30 20:41:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:42:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:42:03 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 20:42:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 20:42:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179947ms till timeout)
2022-03-30 20:42:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:42:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:42:14 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 20:42:14 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 20:42:14 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 20:42:14 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 20:42:14 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 20:42:14 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 20:42:14 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:42:14 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 20:42:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator not ready, will try again in 10000 ms (479951ms till timeout)
2022-03-30 20:42:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:42:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:42:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 20:42:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 20:42:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 20:42:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 20:42:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:42:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 20:42:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 20:42:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 20:42:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 20:42:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 20:42:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179944ms till timeout)
2022-03-30 20:42:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:42:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:42:34 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 20:42:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 20:42:34 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 20:42:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 20:42:34 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179877ms till timeout)
2022-03-30 20:42:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:42:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:42:44 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 20:42:44 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 20:42:44 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io not ready, will try again in 10000 ms (179949ms till timeout)
2022-03-30 20:42:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:42:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:42:54 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 20:42:54 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 20:42:54 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 20:42:54 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 20:42:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io not ready, will try again in 10000 ms (179947ms till timeout)
2022-03-30 20:42:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:43:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:43:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 20:43:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 20:43:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 20:43:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 20:43:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 20:43:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 20:43:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 20:43:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 20:43:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 20:43:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 20:43:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:43:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 20:43:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:43:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-30 20:43:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 20:43:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 20:43:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v90883
2022-03-30 20:43:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v90883
2022-03-30 20:43:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=90883&allowWatchBookmarks=true&watch=true...
2022-03-30 20:43:04 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 20:43:04 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 90884
2022-03-30 20:43:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:43:10 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 90997
2022-03-30 20:43:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:43:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:43:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:43:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:43:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:43:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:43:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:43:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:43:47 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 91033
2022-03-30 20:43:47 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v90997 in namespace default
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5a34beae
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-30 20:43:47 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@54ab3ffc
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@5314833a, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5a34beae, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-30 20:43:47 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-30 20:43:47 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@5094a1b1
2022-03-30 20:43:47 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@5094a1b1
2022-03-30 20:43:47 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@5094a1b1
2022-03-30 20:43:47 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 20:43:47 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T20:43:47Z",
        "name": "infra-namespace",
        "resourceVersion": "91034",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "7bb232f9-0c9e-4e6e-9c05-62b27cda1ef8"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 20:43:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:43:48 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 20:43:48 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 20:43:48 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 20:43:48 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479993ms till timeout)
2022-03-30 20:43:49 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478990ms till timeout)
2022-03-30 20:43:50 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477986ms till timeout)
2022-03-30 20:43:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:43:51 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476983ms till timeout)
2022-03-30 20:43:52 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475979ms till timeout)
2022-03-30 20:43:53 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474976ms till timeout)
2022-03-30 20:43:54 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473973ms till timeout)
2022-03-30 20:43:55 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472970ms till timeout)
2022-03-30 20:43:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:43:56 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471967ms till timeout)
2022-03-30 20:43:57 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470964ms till timeout)
2022-03-30 20:43:58 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469960ms till timeout)
2022-03-30 20:43:59 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468957ms till timeout)
2022-03-30 20:44:00 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467954ms till timeout)
2022-03-30 20:44:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:44:01 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466951ms till timeout)
2022-03-30 20:44:02 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465948ms till timeout)
2022-03-30 20:44:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464944ms till timeout)
2022-03-30 20:44:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463941ms till timeout)
2022-03-30 20:44:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462938ms till timeout)
2022-03-30 20:44:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:44:06 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461935ms till timeout)
2022-03-30 20:44:07 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (460931ms till timeout)
2022-03-30 20:44:08 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (459928ms till timeout)
2022-03-30 20:44:09 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (458925ms till timeout)
2022-03-30 20:44:10 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (457921ms till timeout)
2022-03-30 20:44:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:44:11 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (456918ms till timeout)
2022-03-30 20:44:12 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (455915ms till timeout)
2022-03-30 20:44:13 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (454911ms till timeout)
2022-03-30 20:44:14 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-30 20:44:14 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-30 20:44:14 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-30 20:44:14 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-5wkkm not ready: strimzi-cluster-operator)
2022-03-30 20:44:14 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-5wkkm are ready
2022-03-30 20:44:14 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-30 20:44:15 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-5wkkm not ready: strimzi-cluster-operator)
2022-03-30 20:44:15 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-5wkkm are ready
2022-03-30 20:44:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598993ms till timeout)
2022-03-30 20:44:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:44:16 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-5wkkm not ready: strimzi-cluster-operator)
2022-03-30 20:44:16 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-5wkkm are ready
2022-03-30 20:44:16 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597989ms till timeout)
2022-03-30 20:44:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-5wkkm not ready: strimzi-cluster-operator)
2022-03-30 20:44:17 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-5wkkm are ready
2022-03-30 20:44:17 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596985ms till timeout)
2022-03-30 20:44:18 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-5wkkm not ready: strimzi-cluster-operator)
2022-03-30 20:44:18 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-5wkkm are ready
2022-03-30 20:44:18 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595981ms till timeout)
2022-03-30 20:44:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-5wkkm not ready: strimzi-cluster-operator)
2022-03-30 20:44:19 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-5wkkm are ready
2022-03-30 20:44:19 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594978ms till timeout)
2022-03-30 20:44:20 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-5wkkm not ready: strimzi-cluster-operator)
2022-03-30 20:44:20 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-5wkkm are ready
2022-03-30 20:44:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593974ms till timeout)
2022-03-30 20:44:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:44:21 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-5wkkm not ready: strimzi-cluster-operator)
2022-03-30 20:44:21 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-5wkkm are ready
2022-03-30 20:44:21 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592970ms till timeout)
2022-03-30 20:44:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-5wkkm not ready: strimzi-cluster-operator)
2022-03-30 20:44:22 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-5wkkm are ready
2022-03-30 20:44:22 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591966ms till timeout)
2022-03-30 20:44:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-5wkkm not ready: strimzi-cluster-operator)
2022-03-30 20:44:23 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-5wkkm are ready
2022-03-30 20:44:23 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590962ms till timeout)
2022-03-30 20:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-5wkkm not ready: strimzi-cluster-operator)
2022-03-30 20:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-5wkkm are ready
2022-03-30 20:44:24 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-30 20:44:24 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment strimzi-cluster-operator rolling update
2022-03-30 20:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Deployment strimzi-cluster-operator rolling update in namespace:infra-namespace
2022-03-30 20:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {strimzi-cluster-operator-77554ffdfb-2j5b9=c6a9f41a-c6d4-4a22-85af-1b372962b043}
2022-03-30 20:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {strimzi-cluster-operator-78689684d4-5wkkm=2472e3ec-61c0-431f-a14c-d051e80659c8}
2022-03-30 20:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [DeploymentUtils:119] All pods seem to have rolled
2022-03-30 20:44:24 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 20:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 20:44:24 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-30 20:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-30 20:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-5wkkm not ready: strimzi-cluster-operator)
2022-03-30 20:44:24 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-5wkkm are ready
2022-03-30 20:44:24 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 20:44:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-5wkkm not ready: strimzi-cluster-operator)
2022-03-30 20:44:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-5wkkm are ready
2022-03-30 20:44:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-30 20:44:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:44:26 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-5wkkm not ready: strimzi-cluster-operator)
2022-03-30 20:44:26 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-5wkkm are ready
2022-03-30 20:44:26 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597990ms till timeout)
2022-03-30 20:44:27 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-5wkkm not ready: strimzi-cluster-operator)
2022-03-30 20:44:27 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-5wkkm are ready
2022-03-30 20:44:27 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596986ms till timeout)
2022-03-30 20:44:28 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-5wkkm not ready: strimzi-cluster-operator)
2022-03-30 20:44:28 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-5wkkm are ready
2022-03-30 20:44:28 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595982ms till timeout)
2022-03-30 20:44:29 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-5wkkm not ready: strimzi-cluster-operator)
2022-03-30 20:44:29 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-5wkkm are ready
2022-03-30 20:44:29 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594979ms till timeout)
2022-03-30 20:44:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-5wkkm not ready: strimzi-cluster-operator)
2022-03-30 20:44:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-5wkkm are ready
2022-03-30 20:44:30 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593975ms till timeout)
2022-03-30 20:44:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:127] Suite metrics.MetricsIsolatedST is waiting to lock to be released.
2022-03-30 20:44:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-5wkkm not ready: strimzi-cluster-operator)
2022-03-30 20:44:31 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-5wkkm are ready
2022-03-30 20:44:31 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592971ms till timeout)
2022-03-30 20:44:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-5wkkm not ready: strimzi-cluster-operator)
2022-03-30 20:44:32 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-5wkkm are ready
2022-03-30 20:44:32 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591967ms till timeout)
2022-03-30 20:44:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-5wkkm not ready: strimzi-cluster-operator)
2022-03-30 20:44:33 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-5wkkm are ready
2022-03-30 20:44:33 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590963ms till timeout)
2022-03-30 20:44:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-5wkkm not ready: strimzi-cluster-operator)
2022-03-30 20:44:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-5wkkm are ready
2022-03-30 20:44:34 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:141] Deployment strimzi-cluster-operator rolling update finished
2022-03-30 20:44:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 20:44:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:675] [specific.SpecificIsolatedST - After Each] - Clean up after test
2022-03-30 20:44:34 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:44:34 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for testRackAwareConnectCorrectDeployment
2022-03-30 20:44:34 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-6fad9273-allow in namespace infra-namespace
2022-03-30 20:44:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:my-cluster-6fad9273-allow
2022-03-30 20:44:34 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-6fad9273-scraper in namespace infra-namespace
2022-03-30 20:44:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-6fad9273-scraper
2022-03-30 20:44:34 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-6fad9273-allow in namespace infra-namespace
2022-03-30 20:44:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:my-cluster-6fad9273-allow
2022-03-30 20:44:34 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-6fad9273 in namespace infra-namespace
2022-03-30 20:44:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaConnect:my-cluster-6fad9273
2022-03-30 20:44:34 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic rw-my-topic-1577676118-1039249209 in namespace infra-namespace
2022-03-30 20:44:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:rw-my-topic-1577676118-1039249209
2022-03-30 20:44:34 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-6fad9273-kafka-clients in namespace infra-namespace
2022-03-30 20:44:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-6fad9273-kafka-clients
2022-03-30 20:44:34 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-6fad9273 in namespace infra-namespace
2022-03-30 20:44:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-6fad9273
2022-03-30 20:44:34 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:44:34 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.specific.SpecificIsolatedST.testRackAwareConnectCorrectDeployment-FINISHED
2022-03-30 20:44:34 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 20:44:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 20:44:34 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:690] [specific.SpecificIsolatedST - After All] - Clean up after test suite
2022-03-30 20:44:34 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:44:34 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:346] In context SpecificIsolatedST is everything deleted.
2022-03-30 20:44:34 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2,218.909 s - in io.strimzi.systemtest.specific.SpecificIsolatedST
2022-03-30 20:44:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:136] Suite metrics.MetricsIsolatedST has locked the @IsolatedSuite and other @IsolatedSuites must wait until lock is released.
2022-03-30 20:44:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:44:35 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-30 20:44:35 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-30 20:44:35 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-30 20:44:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:44:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-30 20:44:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 20:44:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 20:44:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 20:44:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 20:44:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:44:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 20:44:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 20:44:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 20:44:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 20:44:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 20:44:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179947ms till timeout)
2022-03-30 20:44:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:44:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:44:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 20:44:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 20:44:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 20:44:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 20:44:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 20:44:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 20:44:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:44:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 20:44:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator not ready, will try again in 10000 ms (479952ms till timeout)
2022-03-30 20:44:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:44:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:44:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 20:44:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 20:44:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 20:44:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 20:44:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:44:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 20:44:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 20:44:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 20:44:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 20:44:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 20:44:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179950ms till timeout)
2022-03-30 20:45:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:45:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:45:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 20:45:06 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 20:45:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io not ready, will try again in 10000 ms (179992ms till timeout)
2022-03-30 20:45:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:45:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:45:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 20:45:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 20:45:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179820ms till timeout)
2022-03-30 20:45:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:45:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:45:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 20:45:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 20:45:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 20:45:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 20:45:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 20:45:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 20:45:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 20:45:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 20:45:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 20:45:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 20:45:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 20:45:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 20:45:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 20:45:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 20:45:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 20:45:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 20:45:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:45:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 20:45:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:45:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-30 20:45:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 20:45:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 20:45:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v91245
2022-03-30 20:45:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v91245
2022-03-30 20:45:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=91245&allowWatchBookmarks=true&watch=true...
2022-03-30 20:45:26 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 20:45:26 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 91246
2022-03-30 20:45:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:45:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 91260
2022-03-30 20:45:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 91261
2022-03-30 20:45:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v91260 in namespace default
2022-03-30 20:45:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@2817a94c
2022-03-30 20:45:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 20:45:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@2c283896
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5a34beae
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace,second-metrics-cluster-test
bindingsNamespaces=[infra-namespace, second-metrics-cluster-test]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-30 20:45:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@2c283896
2022-03-30 20:45:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@2c283896
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@5314833a, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5a34beae, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='infra-namespace,second-metrics-cluster-test', bindingsNamespaces=[infra-namespace, second-metrics-cluster-test], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-30 20:45:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 20:45:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T20:45:31Z",
        "name": "infra-namespace",
        "resourceVersion": "91262",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "f79d8d20-6302-42b2-b988-e153818e5e9d"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: second-metrics-cluster-test
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Namespace second-metrics-cluster-test
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace second-metrics-cluster-test -o json
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace second-metrics-cluster-test -o json
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T20:45:31Z",
        "name": "second-metrics-cluster-test",
        "resourceVersion": "91266",
        "selfLink": "/api/v1/namespaces/second-metrics-cluster-test",
        "uid": "c7827d87-4487-4cdc-aa45-32087d34ab15"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace, second-metrics-cluster-test]}
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:45:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=second-metrics-cluster-test, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: second-metrics-cluster-test
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace second-metrics-cluster-test
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace second-metrics-cluster-test
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 20:45:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479959ms till timeout)
2022-03-30 20:45:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478955ms till timeout)
2022-03-30 20:45:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477952ms till timeout)
2022-03-30 20:45:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476949ms till timeout)
2022-03-30 20:45:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:45:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475945ms till timeout)
2022-03-30 20:45:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474942ms till timeout)
2022-03-30 20:45:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473938ms till timeout)
2022-03-30 20:45:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472935ms till timeout)
2022-03-30 20:45:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471932ms till timeout)
2022-03-30 20:45:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:45:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470928ms till timeout)
2022-03-30 20:45:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469925ms till timeout)
2022-03-30 20:45:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468922ms till timeout)
2022-03-30 20:45:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467919ms till timeout)
2022-03-30 20:45:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466916ms till timeout)
2022-03-30 20:45:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:45:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465912ms till timeout)
2022-03-30 20:45:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464909ms till timeout)
2022-03-30 20:45:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463906ms till timeout)
2022-03-30 20:45:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462903ms till timeout)
2022-03-30 20:45:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461900ms till timeout)
2022-03-30 20:45:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:45:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (460897ms till timeout)
2022-03-30 20:45:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (459894ms till timeout)
2022-03-30 20:45:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (458890ms till timeout)
2022-03-30 20:45:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (457887ms till timeout)
2022-03-30 20:45:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (456884ms till timeout)
2022-03-30 20:45:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:45:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (455881ms till timeout)
2022-03-30 20:45:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (454877ms till timeout)
2022-03-30 20:45:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (453874ms till timeout)
2022-03-30 20:45:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (452871ms till timeout)
2022-03-30 20:46:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (451868ms till timeout)
2022-03-30 20:46:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:46:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (450865ms till timeout)
2022-03-30 20:46:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (449861ms till timeout)
2022-03-30 20:46:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (448858ms till timeout)
2022-03-30 20:46:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (447855ms till timeout)
2022-03-30 20:46:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:46:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (446852ms till timeout)
2022-03-30 20:46:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (445848ms till timeout)
2022-03-30 20:46:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (444845ms till timeout)
2022-03-30 20:46:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (443842ms till timeout)
2022-03-30 20:46:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (442839ms till timeout)
2022-03-30 20:46:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:46:10 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-30 20:46:10 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-30 20:46:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-30 20:46:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-css98 not ready: strimzi-cluster-operator)
2022-03-30 20:46:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-css98 are ready
2022-03-30 20:46:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-30 20:46:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-css98 not ready: strimzi-cluster-operator)
2022-03-30 20:46:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-css98 are ready
2022-03-30 20:46:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598993ms till timeout)
2022-03-30 20:46:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-css98 not ready: strimzi-cluster-operator)
2022-03-30 20:46:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-css98 are ready
2022-03-30 20:46:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597990ms till timeout)
2022-03-30 20:46:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-css98 not ready: strimzi-cluster-operator)
2022-03-30 20:46:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-css98 are ready
2022-03-30 20:46:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596986ms till timeout)
2022-03-30 20:46:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-css98 not ready: strimzi-cluster-operator)
2022-03-30 20:46:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-css98 are ready
2022-03-30 20:46:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595982ms till timeout)
2022-03-30 20:46:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:46:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-css98 not ready: strimzi-cluster-operator)
2022-03-30 20:46:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-css98 are ready
2022-03-30 20:46:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594978ms till timeout)
2022-03-30 20:46:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-css98 not ready: strimzi-cluster-operator)
2022-03-30 20:46:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-css98 are ready
2022-03-30 20:46:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593974ms till timeout)
2022-03-30 20:46:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-css98 not ready: strimzi-cluster-operator)
2022-03-30 20:46:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-css98 are ready
2022-03-30 20:46:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592971ms till timeout)
2022-03-30 20:46:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-css98 not ready: strimzi-cluster-operator)
2022-03-30 20:46:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-css98 are ready
2022-03-30 20:46:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591967ms till timeout)
2022-03-30 20:46:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-css98 not ready: strimzi-cluster-operator)
2022-03-30 20:46:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-css98 are ready
2022-03-30 20:46:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590964ms till timeout)
2022-03-30 20:46:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:46:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-6884c86dcb-css98 not ready: strimzi-cluster-operator)
2022-03-30 20:46:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-6884c86dcb-css98 are ready
2022-03-30 20:46:20 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-30 20:46:20 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 20:46:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka metrics-cluster-name in namespace infra-namespace
2022-03-30 20:46:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka second-kafka-cluster in namespace second-metrics-cluster-test
2022-03-30 20:46:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment infra-namespace-kafka-clients in namespace infra-namespace
2022-03-30 20:46:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment second-metrics-cluster-test-kafka-clients in namespace second-metrics-cluster-test
2022-03-30 20:46:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:metrics-cluster-name
2022-03-30 20:46:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: metrics-cluster-name will have desired state: Ready
2022-03-30 20:46:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: metrics-cluster-name will have desired state: Ready
2022-03-30 20:46:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1799993ms till timeout)
2022-03-30 20:46:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1798990ms till timeout)
2022-03-30 20:46:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1797986ms till timeout)
2022-03-30 20:46:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1796982ms till timeout)
2022-03-30 20:46:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1795979ms till timeout)
2022-03-30 20:46:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:46:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1794975ms till timeout)
2022-03-30 20:46:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1793971ms till timeout)
2022-03-30 20:46:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1792968ms till timeout)
2022-03-30 20:46:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1791963ms till timeout)
2022-03-30 20:46:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1790959ms till timeout)
2022-03-30 20:46:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:46:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1789955ms till timeout)
2022-03-30 20:46:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1788952ms till timeout)
2022-03-30 20:46:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1787946ms till timeout)
2022-03-30 20:46:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1786942ms till timeout)
2022-03-30 20:46:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1785938ms till timeout)
2022-03-30 20:46:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:46:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1784935ms till timeout)
2022-03-30 20:46:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1783932ms till timeout)
2022-03-30 20:46:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1782928ms till timeout)
2022-03-30 20:46:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1781925ms till timeout)
2022-03-30 20:46:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1780921ms till timeout)
2022-03-30 20:46:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:46:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1779917ms till timeout)
2022-03-30 20:46:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1778914ms till timeout)
2022-03-30 20:46:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1777910ms till timeout)
2022-03-30 20:46:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1776906ms till timeout)
2022-03-30 20:46:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1775902ms till timeout)
2022-03-30 20:46:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:46:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1774899ms till timeout)
2022-03-30 20:46:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1773895ms till timeout)
2022-03-30 20:46:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1772892ms till timeout)
2022-03-30 20:46:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1771889ms till timeout)
2022-03-30 20:46:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1770886ms till timeout)
2022-03-30 20:46:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:46:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1769882ms till timeout)
2022-03-30 20:46:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1768879ms till timeout)
2022-03-30 20:46:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1767875ms till timeout)
2022-03-30 20:46:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1766872ms till timeout)
2022-03-30 20:46:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1765869ms till timeout)
2022-03-30 20:46:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:46:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1764862ms till timeout)
2022-03-30 20:46:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1763859ms till timeout)
2022-03-30 20:46:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1762856ms till timeout)
2022-03-30 20:46:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1761852ms till timeout)
2022-03-30 20:46:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1760849ms till timeout)
2022-03-30 20:47:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:47:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1759845ms till timeout)
2022-03-30 20:47:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1758841ms till timeout)
2022-03-30 20:47:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1757837ms till timeout)
2022-03-30 20:47:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1756834ms till timeout)
2022-03-30 20:47:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1755830ms till timeout)
2022-03-30 20:47:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:47:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1754827ms till timeout)
2022-03-30 20:47:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1753824ms till timeout)
2022-03-30 20:47:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1752820ms till timeout)
2022-03-30 20:47:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1751817ms till timeout)
2022-03-30 20:47:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1750814ms till timeout)
2022-03-30 20:47:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:47:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1749810ms till timeout)
2022-03-30 20:47:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1748807ms till timeout)
2022-03-30 20:47:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1747803ms till timeout)
2022-03-30 20:47:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1746799ms till timeout)
2022-03-30 20:47:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1745796ms till timeout)
2022-03-30 20:47:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:47:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1744792ms till timeout)
2022-03-30 20:47:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1743789ms till timeout)
2022-03-30 20:47:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1742785ms till timeout)
2022-03-30 20:47:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1741780ms till timeout)
2022-03-30 20:47:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1740772ms till timeout)
2022-03-30 20:47:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:47:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1739767ms till timeout)
2022-03-30 20:47:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1738763ms till timeout)
2022-03-30 20:47:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1737760ms till timeout)
2022-03-30 20:47:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1736756ms till timeout)
2022-03-30 20:47:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1735753ms till timeout)
2022-03-30 20:47:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:47:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1734749ms till timeout)
2022-03-30 20:47:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1733745ms till timeout)
2022-03-30 20:47:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1732742ms till timeout)
2022-03-30 20:47:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1731738ms till timeout)
2022-03-30 20:47:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1730735ms till timeout)
2022-03-30 20:47:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:47:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1729731ms till timeout)
2022-03-30 20:47:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1728728ms till timeout)
2022-03-30 20:47:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1727724ms till timeout)
2022-03-30 20:47:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1726720ms till timeout)
2022-03-30 20:47:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1725717ms till timeout)
2022-03-30 20:47:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:47:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1724713ms till timeout)
2022-03-30 20:47:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1723710ms till timeout)
2022-03-30 20:47:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1722706ms till timeout)
2022-03-30 20:47:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1721703ms till timeout)
2022-03-30 20:47:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1720699ms till timeout)
2022-03-30 20:47:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:47:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1719696ms till timeout)
2022-03-30 20:47:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1718692ms till timeout)
2022-03-30 20:47:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1717689ms till timeout)
2022-03-30 20:47:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1716686ms till timeout)
2022-03-30 20:47:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1715682ms till timeout)
2022-03-30 20:47:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:47:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1714679ms till timeout)
2022-03-30 20:47:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1713676ms till timeout)
2022-03-30 20:47:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1712673ms till timeout)
2022-03-30 20:47:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1711670ms till timeout)
2022-03-30 20:47:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1710666ms till timeout)
2022-03-30 20:47:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:47:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1709663ms till timeout)
2022-03-30 20:47:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1708660ms till timeout)
2022-03-30 20:47:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1707657ms till timeout)
2022-03-30 20:47:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1706654ms till timeout)
2022-03-30 20:47:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1705650ms till timeout)
2022-03-30 20:47:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:47:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1704647ms till timeout)
2022-03-30 20:47:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1703644ms till timeout)
2022-03-30 20:47:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1702639ms till timeout)
2022-03-30 20:47:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1701636ms till timeout)
2022-03-30 20:48:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1700632ms till timeout)
2022-03-30 20:48:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:48:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1699628ms till timeout)
2022-03-30 20:48:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1698623ms till timeout)
2022-03-30 20:48:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1697619ms till timeout)
2022-03-30 20:48:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1696615ms till timeout)
2022-03-30 20:48:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1695612ms till timeout)
2022-03-30 20:48:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:48:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1694609ms till timeout)
2022-03-30 20:48:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1693605ms till timeout)
2022-03-30 20:48:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1692602ms till timeout)
2022-03-30 20:48:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1691599ms till timeout)
2022-03-30 20:48:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1690596ms till timeout)
2022-03-30 20:48:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:48:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1689592ms till timeout)
2022-03-30 20:48:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1688589ms till timeout)
2022-03-30 20:48:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1687585ms till timeout)
2022-03-30 20:48:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1686582ms till timeout)
2022-03-30 20:48:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1685579ms till timeout)
2022-03-30 20:48:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:48:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1684575ms till timeout)
2022-03-30 20:48:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1683571ms till timeout)
2022-03-30 20:48:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1682568ms till timeout)
2022-03-30 20:48:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1681564ms till timeout)
2022-03-30 20:48:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1680558ms till timeout)
2022-03-30 20:48:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:48:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1679553ms till timeout)
2022-03-30 20:48:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1678550ms till timeout)
2022-03-30 20:48:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1677546ms till timeout)
2022-03-30 20:48:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1676542ms till timeout)
2022-03-30 20:48:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1675539ms till timeout)
2022-03-30 20:48:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:48:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1674535ms till timeout)
2022-03-30 20:48:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1673532ms till timeout)
2022-03-30 20:48:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1672528ms till timeout)
2022-03-30 20:48:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1671525ms till timeout)
2022-03-30 20:48:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1670521ms till timeout)
2022-03-30 20:48:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:48:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1669517ms till timeout)
2022-03-30 20:48:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1668514ms till timeout)
2022-03-30 20:48:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1667510ms till timeout)
2022-03-30 20:48:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1666507ms till timeout)
2022-03-30 20:48:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1665503ms till timeout)
2022-03-30 20:48:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:48:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1664500ms till timeout)
2022-03-30 20:48:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1663496ms till timeout)
2022-03-30 20:48:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1662492ms till timeout)
2022-03-30 20:48:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1661489ms till timeout)
2022-03-30 20:48:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1660485ms till timeout)
2022-03-30 20:48:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:48:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1659482ms till timeout)
2022-03-30 20:48:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1658478ms till timeout)
2022-03-30 20:48:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1657474ms till timeout)
2022-03-30 20:48:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1656470ms till timeout)
2022-03-30 20:48:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1655467ms till timeout)
2022-03-30 20:48:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:48:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1654463ms till timeout)
2022-03-30 20:48:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1653460ms till timeout)
2022-03-30 20:48:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1652455ms till timeout)
2022-03-30 20:48:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1651452ms till timeout)
2022-03-30 20:48:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1650448ms till timeout)
2022-03-30 20:48:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:48:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1649444ms till timeout)
2022-03-30 20:48:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1648441ms till timeout)
2022-03-30 20:48:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1647437ms till timeout)
2022-03-30 20:48:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1646433ms till timeout)
2022-03-30 20:48:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1645430ms till timeout)
2022-03-30 20:48:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:48:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1644427ms till timeout)
2022-03-30 20:48:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1643423ms till timeout)
2022-03-30 20:48:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1642420ms till timeout)
2022-03-30 20:48:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1641416ms till timeout)
2022-03-30 20:49:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1640412ms till timeout)
2022-03-30 20:49:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:49:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1639409ms till timeout)
2022-03-30 20:49:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1638405ms till timeout)
2022-03-30 20:49:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1637401ms till timeout)
2022-03-30 20:49:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1636398ms till timeout)
2022-03-30 20:49:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1635394ms till timeout)
2022-03-30 20:49:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:49:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1634391ms till timeout)
2022-03-30 20:49:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1633387ms till timeout)
2022-03-30 20:49:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1632380ms till timeout)
2022-03-30 20:49:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1631377ms till timeout)
2022-03-30 20:49:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1630374ms till timeout)
2022-03-30 20:49:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:49:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1629370ms till timeout)
2022-03-30 20:49:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1628366ms till timeout)
2022-03-30 20:49:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1627363ms till timeout)
2022-03-30 20:49:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1626359ms till timeout)
2022-03-30 20:49:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1625356ms till timeout)
2022-03-30 20:49:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:49:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1624353ms till timeout)
2022-03-30 20:49:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1623349ms till timeout)
2022-03-30 20:49:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1622346ms till timeout)
2022-03-30 20:49:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Kafka: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (1621342ms till timeout)
2022-03-30 20:49:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: metrics-cluster-name is in desired state: Ready
2022-03-30 20:49:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:second-kafka-cluster
2022-03-30 20:49:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: second-kafka-cluster will have desired state: Ready
2022-03-30 20:49:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: second-kafka-cluster will have desired state: Ready
2022-03-30 20:49:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: second-kafka-cluster is in desired state: Ready
2022-03-30 20:49:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:infra-namespace-kafka-clients
2022-03-30 20:49:20 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: infra-namespace-kafka-clients will be ready
2022-03-30 20:49:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: infra-namespace-kafka-clients will be ready
2022-03-30 20:49:20 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: infra-namespace-kafka-clients is ready
2022-03-30 20:49:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:second-metrics-cluster-test-kafka-clients
2022-03-30 20:49:20 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: second-metrics-cluster-test-kafka-clients will be ready
2022-03-30 20:49:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: second-metrics-cluster-test-kafka-clients will be ready
2022-03-30 20:49:20 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: second-metrics-cluster-test-kafka-clients is ready
2022-03-30 20:49:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge my-bridge in namespace infra-namespace
2022-03-30 20:49:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaBridge:my-bridge
2022-03-30 20:49:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: my-bridge will have desired state: Ready
2022-03-30 20:49:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaBridge: my-bridge will have desired state: Ready
2022-03-30 20:49:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (479998ms till timeout)
2022-03-30 20:49:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:49:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (478995ms till timeout)
2022-03-30 20:49:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (477992ms till timeout)
2022-03-30 20:49:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (476989ms till timeout)
2022-03-30 20:49:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (475986ms till timeout)
2022-03-30 20:49:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (474982ms till timeout)
2022-03-30 20:49:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:49:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (473979ms till timeout)
2022-03-30 20:49:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (472976ms till timeout)
2022-03-30 20:49:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (471973ms till timeout)
2022-03-30 20:49:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (470970ms till timeout)
2022-03-30 20:49:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (469966ms till timeout)
2022-03-30 20:49:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:49:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (468963ms till timeout)
2022-03-30 20:49:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (467960ms till timeout)
2022-03-30 20:49:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (466957ms till timeout)
2022-03-30 20:49:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (465953ms till timeout)
2022-03-30 20:49:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (464950ms till timeout)
2022-03-30 20:49:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:49:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (463947ms till timeout)
2022-03-30 20:49:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (462944ms till timeout)
2022-03-30 20:49:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (461941ms till timeout)
2022-03-30 20:49:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (460936ms till timeout)
2022-03-30 20:49:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (459933ms till timeout)
2022-03-30 20:49:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:49:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (458930ms till timeout)
2022-03-30 20:49:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (457927ms till timeout)
2022-03-30 20:49:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (456924ms till timeout)
2022-03-30 20:49:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (455921ms till timeout)
2022-03-30 20:49:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaBridge: my-bridge will have desired state: Ready not ready, will try again in 1000 ms (454918ms till timeout)
2022-03-30 20:49:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:49:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: my-bridge is in desired state: Ready
2022-03-30 20:49:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 mm2-cluster in namespace infra-namespace
2022-03-30 20:49:46 [ForkJoinPool-3-worker-3] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkamirrormaker2s' with unstable version 'v1beta2'
2022-03-30 20:49:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaMirrorMaker2:mm2-cluster
2022-03-30 20:49:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: mm2-cluster will have desired state: Ready
2022-03-30 20:49:46 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaMirrorMaker2: mm2-cluster will have desired state: Ready
2022-03-30 20:49:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 20:49:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-30 20:49:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (597990ms till timeout)
2022-03-30 20:49:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (596987ms till timeout)
2022-03-30 20:49:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:49:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (595983ms till timeout)
2022-03-30 20:49:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (594980ms till timeout)
2022-03-30 20:49:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (593977ms till timeout)
2022-03-30 20:49:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (592971ms till timeout)
2022-03-30 20:49:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (591967ms till timeout)
2022-03-30 20:49:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:49:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (590964ms till timeout)
2022-03-30 20:49:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (589958ms till timeout)
2022-03-30 20:49:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (588954ms till timeout)
2022-03-30 20:49:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (587951ms till timeout)
2022-03-30 20:49:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (586948ms till timeout)
2022-03-30 20:50:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:50:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (585944ms till timeout)
2022-03-30 20:50:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (584941ms till timeout)
2022-03-30 20:50:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (583938ms till timeout)
2022-03-30 20:50:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (582934ms till timeout)
2022-03-30 20:50:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (581932ms till timeout)
2022-03-30 20:50:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:50:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (580928ms till timeout)
2022-03-30 20:50:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (579925ms till timeout)
2022-03-30 20:50:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (578922ms till timeout)
2022-03-30 20:50:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (577919ms till timeout)
2022-03-30 20:50:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (576915ms till timeout)
2022-03-30 20:50:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:50:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (575912ms till timeout)
2022-03-30 20:50:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (574909ms till timeout)
2022-03-30 20:50:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (573906ms till timeout)
2022-03-30 20:50:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (572902ms till timeout)
2022-03-30 20:50:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (571899ms till timeout)
2022-03-30 20:50:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:50:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (570896ms till timeout)
2022-03-30 20:50:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (569892ms till timeout)
2022-03-30 20:50:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (568889ms till timeout)
2022-03-30 20:50:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (567886ms till timeout)
2022-03-30 20:50:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (566882ms till timeout)
2022-03-30 20:50:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:50:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (565879ms till timeout)
2022-03-30 20:50:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (564875ms till timeout)
2022-03-30 20:50:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (563872ms till timeout)
2022-03-30 20:50:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (562869ms till timeout)
2022-03-30 20:50:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (561866ms till timeout)
2022-03-30 20:50:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:50:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (560862ms till timeout)
2022-03-30 20:50:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (559859ms till timeout)
2022-03-30 20:50:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (558856ms till timeout)
2022-03-30 20:50:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (557853ms till timeout)
2022-03-30 20:50:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (556850ms till timeout)
2022-03-30 20:50:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:50:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (555847ms till timeout)
2022-03-30 20:50:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (554843ms till timeout)
2022-03-30 20:50:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (553840ms till timeout)
2022-03-30 20:50:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (552836ms till timeout)
2022-03-30 20:50:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (551833ms till timeout)
2022-03-30 20:50:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:50:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (550830ms till timeout)
2022-03-30 20:50:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (549826ms till timeout)
2022-03-30 20:50:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (548823ms till timeout)
2022-03-30 20:50:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (547820ms till timeout)
2022-03-30 20:50:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (546816ms till timeout)
2022-03-30 20:50:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:50:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (545813ms till timeout)
2022-03-30 20:50:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (544810ms till timeout)
2022-03-30 20:50:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (543806ms till timeout)
2022-03-30 20:50:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (542803ms till timeout)
2022-03-30 20:50:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (541799ms till timeout)
2022-03-30 20:50:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:50:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (540796ms till timeout)
2022-03-30 20:50:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (539792ms till timeout)
2022-03-30 20:50:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (538789ms till timeout)
2022-03-30 20:50:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (537786ms till timeout)
2022-03-30 20:50:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: mm2-cluster will have desired state: Ready not ready, will try again in 1000 ms (536782ms till timeout)
2022-03-30 20:50:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:50:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: mm2-cluster is in desired state: Ready
2022-03-30 20:50:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1905774485-827577292 in namespace infra-namespace
2022-03-30 20:50:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1905774485-827577292
2022-03-30 20:50:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1905774485-827577292 will have desired state: Ready
2022-03-30 20:50:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1905774485-827577292 will have desired state: Ready
2022-03-30 20:50:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1905774485-827577292 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 20:50:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1905774485-827577292 is in desired state: Ready
2022-03-30 20:50:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1550308369-1840108023 in namespace infra-namespace
2022-03-30 20:50:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1550308369-1840108023
2022-03-30 20:50:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1550308369-1840108023 will have desired state: Ready
2022-03-30 20:50:51 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1550308369-1840108023 will have desired state: Ready
2022-03-30 20:50:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1550308369-1840108023 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 20:50:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1550308369-1840108023 is in desired state: Ready
2022-03-30 20:50:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-2049647524-1750726403 in namespace infra-namespace
2022-03-30 20:50:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-2049647524-1750726403
2022-03-30 20:50:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-2049647524-1750726403 will have desired state: Ready
2022-03-30 20:50:52 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-2049647524-1750726403 will have desired state: Ready
2022-03-30 20:50:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-2049647524-1750726403 will have desired state: Ready not ready, will try again in 1000 ms (179997ms till timeout)
2022-03-30 20:50:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-2049647524-1750726403 is in desired state: Ready
2022-03-30 20:50:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-2102088411-591776996 in namespace infra-namespace
2022-03-30 20:50:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-2102088411-591776996
2022-03-30 20:50:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-2102088411-591776996 will have desired state: Ready
2022-03-30 20:50:53 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-2102088411-591776996 will have desired state: Ready
2022-03-30 20:50:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-2102088411-591776996 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 20:50:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-2102088411-591776996 is in desired state: Ready
2022-03-30 20:50:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1903507259-1821354793 in namespace infra-namespace
2022-03-30 20:50:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1903507259-1821354793
2022-03-30 20:50:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1903507259-1821354793 will have desired state: Ready
2022-03-30 20:50:54 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-user-1903507259-1821354793 will have desired state: Ready
2022-03-30 20:50:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaUser: my-user-1903507259-1821354793 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 20:50:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:50:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1903507259-1821354793 is in desired state: Ready
2022-03-30 20:50:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect metrics-cluster-name in namespace infra-namespace
2022-03-30 20:50:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaConnect:metrics-cluster-name
2022-03-30 20:50:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: metrics-cluster-name will have desired state: Ready
2022-03-30 20:50:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaConnect: metrics-cluster-name will have desired state: Ready
2022-03-30 20:50:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 20:50:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (598996ms till timeout)
2022-03-30 20:50:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (597992ms till timeout)
2022-03-30 20:50:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (596989ms till timeout)
2022-03-30 20:50:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (595986ms till timeout)
2022-03-30 20:51:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:51:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (594983ms till timeout)
2022-03-30 20:51:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (593979ms till timeout)
2022-03-30 20:51:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (592976ms till timeout)
2022-03-30 20:51:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (591973ms till timeout)
2022-03-30 20:51:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (590969ms till timeout)
2022-03-30 20:51:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:51:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (589966ms till timeout)
2022-03-30 20:51:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (588963ms till timeout)
2022-03-30 20:51:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (587960ms till timeout)
2022-03-30 20:51:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (586953ms till timeout)
2022-03-30 20:51:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (585950ms till timeout)
2022-03-30 20:51:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:51:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (584947ms till timeout)
2022-03-30 20:51:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (583944ms till timeout)
2022-03-30 20:51:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (582941ms till timeout)
2022-03-30 20:51:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (581937ms till timeout)
2022-03-30 20:51:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (580934ms till timeout)
2022-03-30 20:51:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:51:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (579931ms till timeout)
2022-03-30 20:51:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (578928ms till timeout)
2022-03-30 20:51:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (577925ms till timeout)
2022-03-30 20:51:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (576921ms till timeout)
2022-03-30 20:51:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (575918ms till timeout)
2022-03-30 20:51:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:51:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (574915ms till timeout)
2022-03-30 20:51:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (573912ms till timeout)
2022-03-30 20:51:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (572908ms till timeout)
2022-03-30 20:51:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (571905ms till timeout)
2022-03-30 20:51:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (570902ms till timeout)
2022-03-30 20:51:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:51:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (569899ms till timeout)
2022-03-30 20:51:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (568896ms till timeout)
2022-03-30 20:51:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (567893ms till timeout)
2022-03-30 20:51:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (566890ms till timeout)
2022-03-30 20:51:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (565886ms till timeout)
2022-03-30 20:51:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:51:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (564883ms till timeout)
2022-03-30 20:51:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (563880ms till timeout)
2022-03-30 20:51:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (562877ms till timeout)
2022-03-30 20:51:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (561873ms till timeout)
2022-03-30 20:51:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (560870ms till timeout)
2022-03-30 20:51:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:51:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (559866ms till timeout)
2022-03-30 20:51:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (558863ms till timeout)
2022-03-30 20:51:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (557860ms till timeout)
2022-03-30 20:51:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (556857ms till timeout)
2022-03-30 20:51:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (555854ms till timeout)
2022-03-30 20:51:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:51:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (554850ms till timeout)
2022-03-30 20:51:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (553847ms till timeout)
2022-03-30 20:51:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (552844ms till timeout)
2022-03-30 20:51:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (551841ms till timeout)
2022-03-30 20:51:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (550838ms till timeout)
2022-03-30 20:51:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:51:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (549834ms till timeout)
2022-03-30 20:51:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (548831ms till timeout)
2022-03-30 20:51:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (547828ms till timeout)
2022-03-30 20:51:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (546825ms till timeout)
2022-03-30 20:51:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (545821ms till timeout)
2022-03-30 20:51:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:51:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (544818ms till timeout)
2022-03-30 20:51:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (543815ms till timeout)
2022-03-30 20:51:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (542811ms till timeout)
2022-03-30 20:51:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (541808ms till timeout)
2022-03-30 20:51:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (540804ms till timeout)
2022-03-30 20:51:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:51:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (539801ms till timeout)
2022-03-30 20:51:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaConnect: metrics-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (538798ms till timeout)
2022-03-30 20:51:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: metrics-cluster-name is in desired state: Ready
2022-03-30 20:51:58 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:72] Apply NetworkPolicy access to cluster-operator from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={})
2022-03-30 20:51:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyResource:88] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=cluster-operator-allow, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8080, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/kind=cluster-operator}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 20:51:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy cluster-operator-allow in namespace infra-namespace
2022-03-30 20:51:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:cluster-operator-allow
2022-03-30 20:51:58 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:90] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}) successfully created
2022-03-30 20:51:58 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:104] Apply NetworkPolicy access to metrics-cluster-name-entity-operator from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={})
2022-03-30 20:51:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyResource:126] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=metrics-cluster-name-entity-operator-allow, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8080, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8081, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-entity-operator}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 20:51:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy metrics-cluster-name-entity-operator-allow in namespace infra-namespace
2022-03-30 20:51:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:metrics-cluster-name-entity-operator-allow
2022-03-30 20:51:58 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:128] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}) successfully created
2022-03-30 20:51:58 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:104] Apply NetworkPolicy access to second-kafka-cluster-entity-operator from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={})
2022-03-30 20:51:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyResource:126] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=second-kafka-cluster-entity-operator-allow, namespace=second-metrics-cluster-test, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8080, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8081, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=second-kafka-cluster, strimzi.io/kind=Kafka, strimzi.io/name=second-kafka-cluster-entity-operator}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 20:51:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy second-kafka-cluster-entity-operator-allow in namespace second-metrics-cluster-test
2022-03-30 20:51:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:second-kafka-cluster-entity-operator-allow
2022-03-30 20:51:58 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:128] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}) successfully created
2022-03-30 20:51:58 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:141] Apply NetworkPolicy access to metrics-cluster-name-kafka-exporter from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={})
2022-03-30 20:51:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyResource:159] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=metrics-cluster-name-kafka-exporter-allow, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9404, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 20:51:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy metrics-cluster-name-kafka-exporter-allow in namespace infra-namespace
2022-03-30 20:51:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:metrics-cluster-name-kafka-exporter-allow
2022-03-30 20:51:58 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:161] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}) successfully created
2022-03-30 20:51:58 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:141] Apply NetworkPolicy access to second-kafka-cluster-kafka-exporter from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={})
2022-03-30 20:51:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [NetworkPolicyResource:159] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=second-kafka-cluster-kafka-exporter-allow, namespace=second-metrics-cluster-test, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9404, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=second-kafka-cluster, strimzi.io/kind=Kafka, strimzi.io/name=second-kafka-cluster-kafka-exporter}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 20:51:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy second-kafka-cluster-kafka-exporter-allow in namespace second-metrics-cluster-test
2022-03-30 20:51:58 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:second-kafka-cluster-kafka-exporter-allow
2022-03-30 20:51:58 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:161] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}) successfully created
2022-03-30 20:52:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:52:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:52:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:52:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:52:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:52:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:52:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:52:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:52:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:52:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:52:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:52:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:53:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:53:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:53:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:53:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:53:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 20:53:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-84mgz -n infra-namespace -- curl 172.17.0.16:9404
2022-03-30 20:53:20 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.16 from Pod infra-namespace-kafka-clients-748578f786-84mgz finished with return code: 0
2022-03-30 20:53:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-84mgz -n infra-namespace -- curl 172.17.0.18:9404
2022-03-30 20:53:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:53:22 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.18 from Pod infra-namespace-kafka-clients-748578f786-84mgz finished with return code: 0
2022-03-30 20:53:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-84mgz -n infra-namespace -- curl 172.17.0.17:9404
2022-03-30 20:53:24 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.17 from Pod infra-namespace-kafka-clients-748578f786-84mgz finished with return code: 0
2022-03-30 20:53:24 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 20:53:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-84mgz -n infra-namespace -- curl 172.17.0.9:9404
2022-03-30 20:53:24 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.9 from Pod infra-namespace-kafka-clients-748578f786-84mgz finished with return code: 0
2022-03-30 20:53:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-84mgz -n infra-namespace -- curl 172.17.0.10:9404
2022-03-30 20:53:25 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.10 from Pod infra-namespace-kafka-clients-748578f786-84mgz finished with return code: 0
2022-03-30 20:53:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-84mgz -n infra-namespace -- curl 172.17.0.12:9404
2022-03-30 20:53:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:53:25 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.12 from Pod infra-namespace-kafka-clients-748578f786-84mgz finished with return code: 0
2022-03-30 20:53:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 20:53:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-84mgz -n infra-namespace -- curl 172.17.0.24:9404
2022-03-30 20:53:26 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.24 from Pod infra-namespace-kafka-clients-748578f786-84mgz finished with return code: 0
2022-03-30 20:53:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 20:53:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-84mgz -n infra-namespace -- curl 172.17.0.21:9404/metrics
2022-03-30 20:53:26 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.21 from Pod infra-namespace-kafka-clients-748578f786-84mgz finished with return code: 0
2022-03-30 20:53:26 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 20:53:26 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testReconcileStateMetricInTopicOperator-STARTED
2022-03-30 20:53:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 20:53:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 20:53:26 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 20:53:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff, testSendSimpleMessageTls=my-cluster-ca7325d7, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e, testUpdateUser=my-cluster-6e6fa8fe, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37, testCruiseControlBasicAPIRequests=my-cluster-8aed4745, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba, testSendMessagesTlsScramSha=my-cluster-84882f67, testReceiveSimpleMessageTls=my-cluster-2e450144, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273}
2022-03-30 20:53:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-838849261-890460734, testSendSimpleMessageTls=my-user-1309491966-2015599610, testReconcileStateMetricInTopicOperator=my-user-2124671027-1015880288, testUpdateUser=my-user-1387197422-925410353, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1886945581-244087591, testSendMessagesCustomListenerTlsScramSha=my-user-1118022021-805525218, testCruiseControlBasicAPIRequests=my-user-1861484466-561878159, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1851888062-673454065, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1731919654-539989781, testSendMessagesTlsScramSha=my-user-586569987-2105688079, testReceiveSimpleMessageTls=my-user-1669668617-1031881775, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677, testRackAwareConnectCorrectDeployment=my-user-1051420592-1240796798}
2022-03-30 20:53:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-2138084746-704381117, testSendSimpleMessageTls=my-topic-435427300-917779110, testReconcileStateMetricInTopicOperator=my-topic-1267576332-2046216344, testUpdateUser=my-topic-1875783706-1763804507, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-306860904-106838350, testSendMessagesCustomListenerTlsScramSha=my-topic-963888555-1712711326, testCruiseControlBasicAPIRequests=my-topic-1631570507-1308241024, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-307985263-926887950, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1672280868-1119644965, testSendMessagesTlsScramSha=my-topic-255809133-823300208, testReceiveSimpleMessageTls=my-topic-368564676-637091599, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206, testRackAwareConnectCorrectDeployment=my-topic-189499239-153088857}
2022-03-30 20:53:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff-kafka-clients, testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e-kafka-clients, testUpdateUser=my-cluster-6e6fa8fe-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-8aed4745-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba-kafka-clients, testSendMessagesTlsScramSha=my-cluster-84882f67-kafka-clients, testReceiveSimpleMessageTls=my-cluster-2e450144-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273-kafka-clients}
2022-03-30 20:53:26 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-metrics-cluster-test
2022-03-30 20:53:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1267576332-2046216344 in namespace second-metrics-cluster-test
2022-03-30 20:53:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1267576332-2046216344
2022-03-30 20:53:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1267576332-2046216344 will have desired state: Ready
2022-03-30 20:53:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1267576332-2046216344 will have desired state: Ready
2022-03-30 20:53:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1267576332-2046216344 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 20:53:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1267576332-2046216344 is in desired state: Ready
2022-03-30 20:53:27 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 20:53:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec second-metrics-cluster-test-kafka-clients-6c57d6898-k9wz8 -n second-metrics-cluster-test -- curl 172.17.0.14:8080/metrics
2022-03-30 20:53:28 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.14 from Pod second-metrics-cluster-test-kafka-clients-6c57d6898-k9wz8 finished with return code: 0
2022-03-30 20:53:28 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:553] Checking if resource state metric reason message is "none" and KafkaTopic is ready
2022-03-30 20:53:28 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:556] Changing topic name in spec.topicName
2022-03-30 20:53:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1267576332-2046216344 will have desired state: NotReady
2022-03-30 20:53:28 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1267576332-2046216344 will have desired state: NotReady
2022-03-30 20:53:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1267576332-2046216344 will have desired state: NotReady not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 20:53:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1267576332-2046216344 is in desired state: NotReady
2022-03-30 20:53:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 20:53:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec second-metrics-cluster-test-kafka-clients-6c57d6898-k9wz8 -n second-metrics-cluster-test -- curl 172.17.0.14:8080/metrics
2022-03-30 20:53:29 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.14 from Pod second-metrics-cluster-test-kafka-clients-6c57d6898-k9wz8 finished with return code: 0
2022-03-30 20:53:29 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:564] Changing back to it's original name and scaling replicas to be higher number
2022-03-30 20:53:29 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:132] Waiting for KafkaTopic change my-topic-1267576332-2046216344
2022-03-30 20:53:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic change my-topic-1267576332-2046216344
2022-03-30 20:53:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 20:53:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec second-metrics-cluster-test-kafka-clients-6c57d6898-k9wz8 -n second-metrics-cluster-test -- curl 172.17.0.14:8080/metrics
2022-03-30 20:53:29 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.14 from Pod second-metrics-cluster-test-kafka-clients-6c57d6898-k9wz8 finished with return code: 0
2022-03-30 20:53:29 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:576] Scaling replicas to be higher than before
2022-03-30 20:53:29 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:132] Waiting for KafkaTopic change my-topic-1267576332-2046216344
2022-03-30 20:53:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic change my-topic-1267576332-2046216344
2022-03-30 20:53:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 20:53:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec second-metrics-cluster-test-kafka-clients-6c57d6898-k9wz8 -n second-metrics-cluster-test -- curl 172.17.0.14:8080/metrics
2022-03-30 20:53:29 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.14 from Pod second-metrics-cluster-test-kafka-clients-6c57d6898-k9wz8 finished with return code: 0
2022-03-30 20:53:29 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:584] Changing KafkaTopic's spec to correct state
2022-03-30 20:53:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1267576332-2046216344 will have desired state: Ready
2022-03-30 20:53:29 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1267576332-2046216344 will have desired state: Ready
2022-03-30 20:53:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1267576332-2046216344 will have desired state: Ready not ready, will try again in 1000 ms (179996ms till timeout)
2022-03-30 20:53:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:53:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1267576332-2046216344 is in desired state: Ready
2022-03-30 20:53:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 20:53:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec second-metrics-cluster-test-kafka-clients-6c57d6898-k9wz8 -n second-metrics-cluster-test -- curl 172.17.0.14:8080/metrics
2022-03-30 20:53:30 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.14 from Pod second-metrics-cluster-test-kafka-clients-6c57d6898-k9wz8 finished with return code: 0
2022-03-30 20:53:30 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 20:53:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 20:53:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 20:53:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:53:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testReconcileStateMetricInTopicOperator
2022-03-30 20:53:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1267576332-2046216344 in namespace second-metrics-cluster-test
2022-03-30 20:53:30 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1267576332-2046216344
2022-03-30 20:53:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1267576332-2046216344 not ready, will try again in 10000 ms (179994ms till timeout)
2022-03-30 20:53:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:53:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:53:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:53:40 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testReconcileStateMetricInTopicOperator-FINISHED
2022-03-30 20:53:40 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 20:53:40 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 20:53:40 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaExporterDataAfterExchange-STARTED
2022-03-30 20:53:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 20:53:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 20:53:40 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 20:53:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff, testSendSimpleMessageTls=my-cluster-ca7325d7, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e, testKafkaExporterDataAfterExchange=my-cluster-e86a0839, testUpdateUser=my-cluster-6e6fa8fe, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37, testCruiseControlBasicAPIRequests=my-cluster-8aed4745, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba, testSendMessagesTlsScramSha=my-cluster-84882f67, testReceiveSimpleMessageTls=my-cluster-2e450144, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273}
2022-03-30 20:53:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-838849261-890460734, testSendSimpleMessageTls=my-user-1309491966-2015599610, testReconcileStateMetricInTopicOperator=my-user-2124671027-1015880288, testKafkaExporterDataAfterExchange=my-user-766848479-1588072395, testUpdateUser=my-user-1387197422-925410353, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1886945581-244087591, testSendMessagesCustomListenerTlsScramSha=my-user-1118022021-805525218, testCruiseControlBasicAPIRequests=my-user-1861484466-561878159, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1851888062-673454065, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1731919654-539989781, testSendMessagesTlsScramSha=my-user-586569987-2105688079, testReceiveSimpleMessageTls=my-user-1669668617-1031881775, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677, testRackAwareConnectCorrectDeployment=my-user-1051420592-1240796798}
2022-03-30 20:53:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-2138084746-704381117, testSendSimpleMessageTls=my-topic-435427300-917779110, testReconcileStateMetricInTopicOperator=my-topic-1267576332-2046216344, testKafkaExporterDataAfterExchange=my-topic-1937812163-1733130103, testUpdateUser=my-topic-1875783706-1763804507, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-306860904-106838350, testSendMessagesCustomListenerTlsScramSha=my-topic-963888555-1712711326, testCruiseControlBasicAPIRequests=my-topic-1631570507-1308241024, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-307985263-926887950, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1672280868-1119644965, testSendMessagesTlsScramSha=my-topic-255809133-823300208, testReceiveSimpleMessageTls=my-topic-368564676-637091599, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206, testRackAwareConnectCorrectDeployment=my-topic-189499239-153088857}
2022-03-30 20:53:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff-kafka-clients, testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e-kafka-clients, testKafkaExporterDataAfterExchange=my-cluster-e86a0839-kafka-clients, testUpdateUser=my-cluster-6e6fa8fe-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-8aed4745-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba-kafka-clients, testSendMessagesTlsScramSha=my-cluster-84882f67-kafka-clients, testReceiveSimpleMessageTls=my-cluster-2e450144-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273-kafka-clients}
2022-03-30 20:53:40 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-30 20:53:40 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@10b01a54, which are set.
2022-03-30 20:53:40 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@7ce1e228, messages=[], arguments=[--topic, my-topic-1550308369-1840108023, --bootstrap-server, metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092, --max-messages, 5000], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='infra-namespace-kafka-clients-748578f786-84mgz', podNamespace='infra-namespace', bootstrapServer='metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092', topicName='my-topic-1550308369-1840108023', maxMessages=5000, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@10b01a54}
2022-03-30 20:53:40 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:94] Producing 5000 messages to metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092:my-topic-1550308369-1840108023 from pod infra-namespace-kafka-clients-748578f786-84mgz
2022-03-30 20:53:40 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec infra-namespace-kafka-clients-748578f786-84mgz -n infra-namespace -- /opt/kafka/producer.sh --topic my-topic-1550308369-1840108023 --bootstrap-server metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092 --max-messages 5000
2022-03-30 20:53:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-84mgz -n infra-namespace -- /opt/kafka/producer.sh --topic my-topic-1550308369-1840108023 --bootstrap-server metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092 --max-messages 5000
2022-03-30 20:53:43 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-03-30 20:53:43 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:101] Producer produced 5000 messages
2022-03-30 20:53:43 [ForkJoinPool-3-worker-3] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5842c5c0, which are set.
2022-03-30 20:53:43 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@a2b2054, messages=[], arguments=[--group-id, my-consumer-group-1481814101, --topic, my-topic-1550308369-1840108023, --group-instance-id, instance1747082772, --bootstrap-server, metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092, --max-messages, 5000], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='infra-namespace-kafka-clients-748578f786-84mgz', podNamespace='infra-namespace', bootstrapServer='metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092', topicName='my-topic-1550308369-1840108023', maxMessages=5000, kafkaUsername='null', consumerGroupName='my-consumer-group-1481814101', consumerInstanceId='instance1747082772', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5842c5c0}
2022-03-30 20:53:43 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:157] Consuming 5000 messages from metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092#my-topic-1550308369-1840108023 from pod infra-namespace-kafka-clients-748578f786-84mgz
2022-03-30 20:53:43 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec infra-namespace-kafka-clients-748578f786-84mgz -n infra-namespace -- /opt/kafka/consumer.sh --group-id my-consumer-group-1481814101 --topic my-topic-1550308369-1840108023 --group-instance-id instance1747082772 --bootstrap-server metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092 --max-messages 5000
2022-03-30 20:53:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-84mgz -n infra-namespace -- /opt/kafka/consumer.sh --group-id my-consumer-group-1481814101 --topic my-topic-1550308369-1840108023 --group-instance-id instance1747082772 --bootstrap-server metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092 --max-messages 5000
2022-03-30 20:53:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:53:49 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-30 20:53:49 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 5000 messages
2022-03-30 20:53:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 20:53:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-84mgz -n infra-namespace -- curl 172.17.0.21:9404/metrics
2022-03-30 20:53:49 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.21 from Pod infra-namespace-kafka-clients-748578f786-84mgz finished with return code: 0
2022-03-30 20:53:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 20:53:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 20:53:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:53:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context testKafkaExporterDataAfterExchange is everything deleted.
2022-03-30 20:53:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:53:49 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaExporterDataAfterExchange-FINISHED
2022-03-30 20:53:49 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 20:53:49 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 20:53:49 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaExporterDifferentSetting-STARTED
2022-03-30 20:53:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 20:53:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 20:53:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaExporterDifferentSetting
2022-03-30 20:53:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 1
2022-03-30 20:53:49 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:230] testKafkaExporterDifferentSetting test now can proceed its execution
2022-03-30 20:53:49 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 20:53:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff, testSendSimpleMessageTls=my-cluster-ca7325d7, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e, testKafkaExporterDataAfterExchange=my-cluster-e86a0839, testUpdateUser=my-cluster-6e6fa8fe, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37, testCruiseControlBasicAPIRequests=my-cluster-8aed4745, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba, testSendMessagesTlsScramSha=my-cluster-84882f67, testReceiveSimpleMessageTls=my-cluster-2e450144, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273}
2022-03-30 20:53:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-838849261-890460734, testSendSimpleMessageTls=my-user-1309491966-2015599610, testReconcileStateMetricInTopicOperator=my-user-2124671027-1015880288, testKafkaExporterDataAfterExchange=my-user-766848479-1588072395, testUpdateUser=my-user-1387197422-925410353, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1886945581-244087591, testSendMessagesCustomListenerTlsScramSha=my-user-1118022021-805525218, testCruiseControlBasicAPIRequests=my-user-1861484466-561878159, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1851888062-673454065, testKafkaExporterDifferentSetting=my-user-988491821-279531432, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1731919654-539989781, testSendMessagesTlsScramSha=my-user-586569987-2105688079, testReceiveSimpleMessageTls=my-user-1669668617-1031881775, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677, testRackAwareConnectCorrectDeployment=my-user-1051420592-1240796798}
2022-03-30 20:53:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-2138084746-704381117, testSendSimpleMessageTls=my-topic-435427300-917779110, testReconcileStateMetricInTopicOperator=my-topic-1267576332-2046216344, testKafkaExporterDataAfterExchange=my-topic-1937812163-1733130103, testUpdateUser=my-topic-1875783706-1763804507, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-306860904-106838350, testSendMessagesCustomListenerTlsScramSha=my-topic-963888555-1712711326, testCruiseControlBasicAPIRequests=my-topic-1631570507-1308241024, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-307985263-926887950, testKafkaExporterDifferentSetting=my-topic-568145531-1290731867, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1672280868-1119644965, testSendMessagesTlsScramSha=my-topic-255809133-823300208, testReceiveSimpleMessageTls=my-topic-368564676-637091599, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206, testRackAwareConnectCorrectDeployment=my-topic-189499239-153088857}
2022-03-30 20:53:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff-kafka-clients, testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e-kafka-clients, testKafkaExporterDataAfterExchange=my-cluster-e86a0839-kafka-clients, testUpdateUser=my-cluster-6e6fa8fe-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-8aed4745-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634-kafka-clients, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba-kafka-clients, testSendMessagesTlsScramSha=my-cluster-84882f67-kafka-clients, testReceiveSimpleMessageTls=my-cluster-2e450144-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273-kafka-clients}
2022-03-30 20:53:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec metrics-cluster-name-kafka-exporter-8454677f49-7k9nc -n infra-namespace -- cat /tmp/run.sh
2022-03-30 20:53:49 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:608] Metrics collection for pod metrics-cluster-name-kafka-exporter-8454677f49-7k9nc return code - 0
2022-03-30 20:53:50 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment metrics-cluster-name-kafka-exporter rolling update
2022-03-30 20:53:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace
2022-03-30 20:53:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-7k9nc=f3ef73f9-5b7b-4603-93cc-44015521bc5a}
2022-03-30 20:53:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-7k9nc=f3ef73f9-5b7b-4603-93cc-44015521bc5a}
2022-03-30 20:53:50 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {metrics-cluster-name-kafka-exporter-8454677f49-7k9nc=f3ef73f9-5b7b-4603-93cc-44015521bc5a}
2022-03-30 20:53:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace not ready, will try again in 5000 ms (599994ms till timeout)
2022-03-30 20:53:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:53:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-7k9nc=f3ef73f9-5b7b-4603-93cc-44015521bc5a}
2022-03-30 20:53:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-64f75c558b-sbsqh=3bf01dd5-9bb0-47b0-b14c-9cd5cabfc283, metrics-cluster-name-kafka-exporter-8454677f49-7k9nc=f3ef73f9-5b7b-4603-93cc-44015521bc5a}
2022-03-30 20:53:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {metrics-cluster-name-kafka-exporter-8454677f49-7k9nc=f3ef73f9-5b7b-4603-93cc-44015521bc5a}
2022-03-30 20:53:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace not ready, will try again in 5000 ms (594987ms till timeout)
2022-03-30 20:53:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:54:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-7k9nc=f3ef73f9-5b7b-4603-93cc-44015521bc5a}
2022-03-30 20:54:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-64f75c558b-sbsqh=3bf01dd5-9bb0-47b0-b14c-9cd5cabfc283, metrics-cluster-name-kafka-exporter-8454677f49-7k9nc=f3ef73f9-5b7b-4603-93cc-44015521bc5a}
2022-03-30 20:54:00 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {metrics-cluster-name-kafka-exporter-8454677f49-7k9nc=f3ef73f9-5b7b-4603-93cc-44015521bc5a}
2022-03-30 20:54:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace not ready, will try again in 5000 ms (589980ms till timeout)
2022-03-30 20:54:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:54:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-7k9nc=f3ef73f9-5b7b-4603-93cc-44015521bc5a}
2022-03-30 20:54:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-64f75c558b-sbsqh=3bf01dd5-9bb0-47b0-b14c-9cd5cabfc283, metrics-cluster-name-kafka-exporter-8454677f49-7k9nc=f3ef73f9-5b7b-4603-93cc-44015521bc5a}
2022-03-30 20:54:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:122] Some pods still need to roll: {metrics-cluster-name-kafka-exporter-8454677f49-7k9nc=f3ef73f9-5b7b-4603-93cc-44015521bc5a}
2022-03-30 20:54:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Deployment metrics-cluster-name-kafka-exporter rolling update in namespace:infra-namespace not ready, will try again in 5000 ms (584973ms till timeout)
2022-03-30 20:54:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:54:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:113] Existing snapshot: {metrics-cluster-name-kafka-exporter-8454677f49-7k9nc=f3ef73f9-5b7b-4603-93cc-44015521bc5a}
2022-03-30 20:54:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:115] Current  snapshot: {metrics-cluster-name-kafka-exporter-64f75c558b-sbsqh=3bf01dd5-9bb0-47b0-b14c-9cd5cabfc283}
2022-03-30 20:54:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DeploymentUtils:119] All pods seem to have rolled
2022-03-30 20:54:10 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: metrics-cluster-name-kafka-exporter will be ready
2022-03-30 20:54:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: metrics-cluster-name-kafka-exporter will be ready
2022-03-30 20:54:10 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: metrics-cluster-name-kafka-exporter is ready
2022-03-30 20:54:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready
2022-03-30 20:54:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-64f75c558b-sbsqh not ready: metrics-cluster-name-kafka-exporter)
2022-03-30 20:54:10 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-64f75c558b-sbsqh are ready
2022-03-30 20:54:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-30 20:54:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:54:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-64f75c558b-sbsqh not ready: metrics-cluster-name-kafka-exporter)
2022-03-30 20:54:11 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-64f75c558b-sbsqh are ready
2022-03-30 20:54:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598991ms till timeout)
2022-03-30 20:54:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-64f75c558b-sbsqh not ready: metrics-cluster-name-kafka-exporter)
2022-03-30 20:54:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-64f75c558b-sbsqh are ready
2022-03-30 20:54:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597986ms till timeout)
2022-03-30 20:54:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-64f75c558b-sbsqh not ready: metrics-cluster-name-kafka-exporter)
2022-03-30 20:54:13 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-64f75c558b-sbsqh are ready
2022-03-30 20:54:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596980ms till timeout)
2022-03-30 20:54:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-64f75c558b-sbsqh not ready: metrics-cluster-name-kafka-exporter)
2022-03-30 20:54:14 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-64f75c558b-sbsqh are ready
2022-03-30 20:54:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595975ms till timeout)
2022-03-30 20:54:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-64f75c558b-sbsqh not ready: metrics-cluster-name-kafka-exporter)
2022-03-30 20:54:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-64f75c558b-sbsqh are ready
2022-03-30 20:54:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594971ms till timeout)
2022-03-30 20:54:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:54:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-64f75c558b-sbsqh not ready: metrics-cluster-name-kafka-exporter)
2022-03-30 20:54:16 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-64f75c558b-sbsqh are ready
2022-03-30 20:54:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593966ms till timeout)
2022-03-30 20:54:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-64f75c558b-sbsqh not ready: metrics-cluster-name-kafka-exporter)
2022-03-30 20:54:17 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-64f75c558b-sbsqh are ready
2022-03-30 20:54:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592962ms till timeout)
2022-03-30 20:54:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-64f75c558b-sbsqh not ready: metrics-cluster-name-kafka-exporter)
2022-03-30 20:54:18 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-64f75c558b-sbsqh are ready
2022-03-30 20:54:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591957ms till timeout)
2022-03-30 20:54:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-64f75c558b-sbsqh not ready: metrics-cluster-name-kafka-exporter)
2022-03-30 20:54:19 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-64f75c558b-sbsqh are ready
2022-03-30 20:54:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=metrics-cluster-name, strimzi.io/kind=Kafka, strimzi.io/name=metrics-cluster-name-kafka-exporter}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590953ms till timeout)
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod metrics-cluster-name-kafka-exporter-64f75c558b-sbsqh not ready: metrics-cluster-name-kafka-exporter)
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [PodUtils:106] Pods metrics-cluster-name-kafka-exporter-64f75c558b-sbsqh are ready
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment metrics-cluster-name-kafka-exporter rolling update finished
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec metrics-cluster-name-kafka-exporter-64f75c558b-sbsqh -n infra-namespace -- cat /tmp/run.sh
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:608] Metrics collection for pod metrics-cluster-name-kafka-exporter-64f75c558b-sbsqh return code - 0
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context testKafkaExporterDifferentSetting is everything deleted.
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:267] testKafkaExporterDifferentSetting - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendSimpleMessageTls, testUpdateUser, testCruiseControlBasicAPIRequests, testReceiveSimpleMessageTls, testSendMessagesTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testKafkaExporterDifferentSetting] to and randomly select one to start execution
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaExporterDifferentSetting
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 0
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaExporterDifferentSetting-FINISHED
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaConnectResponse-STARTED
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaConnectResponse
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 1
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:230] testKafkaConnectResponse test now can proceed its execution
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff, testKafkaConnectResponse=my-cluster-2fe1834d, testSendSimpleMessageTls=my-cluster-ca7325d7, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e, testKafkaExporterDataAfterExchange=my-cluster-e86a0839, testUpdateUser=my-cluster-6e6fa8fe, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37, testCruiseControlBasicAPIRequests=my-cluster-8aed4745, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba, testSendMessagesTlsScramSha=my-cluster-84882f67, testReceiveSimpleMessageTls=my-cluster-2e450144, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273}
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-838849261-890460734, testKafkaConnectResponse=my-user-1873693835-1689263597, testSendSimpleMessageTls=my-user-1309491966-2015599610, testReconcileStateMetricInTopicOperator=my-user-2124671027-1015880288, testKafkaExporterDataAfterExchange=my-user-766848479-1588072395, testUpdateUser=my-user-1387197422-925410353, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1886945581-244087591, testSendMessagesCustomListenerTlsScramSha=my-user-1118022021-805525218, testCruiseControlBasicAPIRequests=my-user-1861484466-561878159, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1851888062-673454065, testKafkaExporterDifferentSetting=my-user-988491821-279531432, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1731919654-539989781, testSendMessagesTlsScramSha=my-user-586569987-2105688079, testReceiveSimpleMessageTls=my-user-1669668617-1031881775, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677, testRackAwareConnectCorrectDeployment=my-user-1051420592-1240796798}
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-2138084746-704381117, testKafkaConnectResponse=my-topic-680219046-979906955, testSendSimpleMessageTls=my-topic-435427300-917779110, testReconcileStateMetricInTopicOperator=my-topic-1267576332-2046216344, testKafkaExporterDataAfterExchange=my-topic-1937812163-1733130103, testUpdateUser=my-topic-1875783706-1763804507, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-306860904-106838350, testSendMessagesCustomListenerTlsScramSha=my-topic-963888555-1712711326, testCruiseControlBasicAPIRequests=my-topic-1631570507-1308241024, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-307985263-926887950, testKafkaExporterDifferentSetting=my-topic-568145531-1290731867, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1672280868-1119644965, testSendMessagesTlsScramSha=my-topic-255809133-823300208, testReceiveSimpleMessageTls=my-topic-368564676-637091599, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206, testRackAwareConnectCorrectDeployment=my-topic-189499239-153088857}
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff-kafka-clients, testKafkaConnectResponse=my-cluster-2fe1834d-kafka-clients, testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e-kafka-clients, testKafkaExporterDataAfterExchange=my-cluster-e86a0839-kafka-clients, testUpdateUser=my-cluster-6e6fa8fe-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-8aed4745-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634-kafka-clients, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba-kafka-clients, testSendMessagesTlsScramSha=my-cluster-84882f67-kafka-clients, testReceiveSimpleMessageTls=my-cluster-2e450144-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273-kafka-clients}
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-84mgz -n infra-namespace -- curl 172.17.0.24:9404
2022-03-30 20:54:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.24 from Pod infra-namespace-kafka-clients-748578f786-84mgz finished with return code: 0
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context testKafkaConnectResponse is everything deleted.
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:267] testKafkaConnectResponse - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendSimpleMessageTls, testUpdateUser, testCruiseControlBasicAPIRequests, testReceiveSimpleMessageTls, testSendMessagesTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testKafkaExporterDifferentSetting, testKafkaConnectResponse] to and randomly select one to start execution
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaConnectResponse
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 0
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaConnectResponse-FINISHED
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaConnectRequests-STARTED
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaConnectRequests
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 1
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:230] testKafkaConnectRequests test now can proceed its execution
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff, testKafkaConnectResponse=my-cluster-2fe1834d, testSendSimpleMessageTls=my-cluster-ca7325d7, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e, testKafkaExporterDataAfterExchange=my-cluster-e86a0839, testUpdateUser=my-cluster-6e6fa8fe, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37, testCruiseControlBasicAPIRequests=my-cluster-8aed4745, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba, testKafkaConnectRequests=my-cluster-d1a00fce, testSendMessagesTlsScramSha=my-cluster-84882f67, testReceiveSimpleMessageTls=my-cluster-2e450144, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273}
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-838849261-890460734, testKafkaConnectResponse=my-user-1873693835-1689263597, testSendSimpleMessageTls=my-user-1309491966-2015599610, testReconcileStateMetricInTopicOperator=my-user-2124671027-1015880288, testKafkaExporterDataAfterExchange=my-user-766848479-1588072395, testUpdateUser=my-user-1387197422-925410353, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1886945581-244087591, testSendMessagesCustomListenerTlsScramSha=my-user-1118022021-805525218, testCruiseControlBasicAPIRequests=my-user-1861484466-561878159, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1851888062-673454065, testKafkaExporterDifferentSetting=my-user-988491821-279531432, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1731919654-539989781, testKafkaConnectRequests=my-user-1167366435-1145507543, testSendMessagesTlsScramSha=my-user-586569987-2105688079, testReceiveSimpleMessageTls=my-user-1669668617-1031881775, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677, testRackAwareConnectCorrectDeployment=my-user-1051420592-1240796798}
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-2138084746-704381117, testKafkaConnectResponse=my-topic-680219046-979906955, testSendSimpleMessageTls=my-topic-435427300-917779110, testReconcileStateMetricInTopicOperator=my-topic-1267576332-2046216344, testKafkaExporterDataAfterExchange=my-topic-1937812163-1733130103, testUpdateUser=my-topic-1875783706-1763804507, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-306860904-106838350, testSendMessagesCustomListenerTlsScramSha=my-topic-963888555-1712711326, testCruiseControlBasicAPIRequests=my-topic-1631570507-1308241024, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-307985263-926887950, testKafkaExporterDifferentSetting=my-topic-568145531-1290731867, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1672280868-1119644965, testKafkaConnectRequests=my-topic-873062135-1706759978, testSendMessagesTlsScramSha=my-topic-255809133-823300208, testReceiveSimpleMessageTls=my-topic-368564676-637091599, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206, testRackAwareConnectCorrectDeployment=my-topic-189499239-153088857}
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff-kafka-clients, testKafkaConnectResponse=my-cluster-2fe1834d-kafka-clients, testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e-kafka-clients, testKafkaExporterDataAfterExchange=my-cluster-e86a0839-kafka-clients, testUpdateUser=my-cluster-6e6fa8fe-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-8aed4745-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634-kafka-clients, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba-kafka-clients, testKafkaConnectRequests=my-cluster-d1a00fce-kafka-clients, testSendMessagesTlsScramSha=my-cluster-84882f67-kafka-clients, testReceiveSimpleMessageTls=my-cluster-2e450144-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273-kafka-clients}
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 20:54:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-84mgz -n infra-namespace -- curl 172.17.0.24:9404
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.24 from Pod infra-namespace-kafka-clients-748578f786-84mgz finished with return code: 0
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context testKafkaConnectRequests is everything deleted.
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:267] testKafkaConnectRequests - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendSimpleMessageTls, testUpdateUser, testCruiseControlBasicAPIRequests, testReceiveSimpleMessageTls, testSendMessagesTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testKafkaExporterDifferentSetting, testKafkaConnectResponse, testKafkaConnectRequests] to and randomly select one to start execution
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaConnectRequests
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 0
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaConnectRequests-FINISHED
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testTopicOperatorMetrics-STARTED
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testTopicOperatorMetrics
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 1
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:230] testTopicOperatorMetrics test now can proceed its execution
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff, testKafkaConnectResponse=my-cluster-2fe1834d, testSendSimpleMessageTls=my-cluster-ca7325d7, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e, testKafkaExporterDataAfterExchange=my-cluster-e86a0839, testUpdateUser=my-cluster-6e6fa8fe, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d, testTopicOperatorMetrics=my-cluster-216189cb, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37, testCruiseControlBasicAPIRequests=my-cluster-8aed4745, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba, testKafkaConnectRequests=my-cluster-d1a00fce, testSendMessagesTlsScramSha=my-cluster-84882f67, testReceiveSimpleMessageTls=my-cluster-2e450144, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273}
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-838849261-890460734, testKafkaConnectResponse=my-user-1873693835-1689263597, testSendSimpleMessageTls=my-user-1309491966-2015599610, testReconcileStateMetricInTopicOperator=my-user-2124671027-1015880288, testKafkaExporterDataAfterExchange=my-user-766848479-1588072395, testUpdateUser=my-user-1387197422-925410353, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1886945581-244087591, testTopicOperatorMetrics=my-user-779424699-479796683, testSendMessagesCustomListenerTlsScramSha=my-user-1118022021-805525218, testCruiseControlBasicAPIRequests=my-user-1861484466-561878159, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1851888062-673454065, testKafkaExporterDifferentSetting=my-user-988491821-279531432, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1731919654-539989781, testKafkaConnectRequests=my-user-1167366435-1145507543, testSendMessagesTlsScramSha=my-user-586569987-2105688079, testReceiveSimpleMessageTls=my-user-1669668617-1031881775, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677, testRackAwareConnectCorrectDeployment=my-user-1051420592-1240796798}
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-2138084746-704381117, testKafkaConnectResponse=my-topic-680219046-979906955, testSendSimpleMessageTls=my-topic-435427300-917779110, testReconcileStateMetricInTopicOperator=my-topic-1267576332-2046216344, testKafkaExporterDataAfterExchange=my-topic-1937812163-1733130103, testUpdateUser=my-topic-1875783706-1763804507, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-306860904-106838350, testTopicOperatorMetrics=my-topic-1965542273-995674430, testSendMessagesCustomListenerTlsScramSha=my-topic-963888555-1712711326, testCruiseControlBasicAPIRequests=my-topic-1631570507-1308241024, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-307985263-926887950, testKafkaExporterDifferentSetting=my-topic-568145531-1290731867, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1672280868-1119644965, testKafkaConnectRequests=my-topic-873062135-1706759978, testSendMessagesTlsScramSha=my-topic-255809133-823300208, testReceiveSimpleMessageTls=my-topic-368564676-637091599, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206, testRackAwareConnectCorrectDeployment=my-topic-189499239-153088857}
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff-kafka-clients, testKafkaConnectResponse=my-cluster-2fe1834d-kafka-clients, testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e-kafka-clients, testKafkaExporterDataAfterExchange=my-cluster-e86a0839-kafka-clients, testUpdateUser=my-cluster-6e6fa8fe-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d-kafka-clients, testTopicOperatorMetrics=my-cluster-216189cb-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-8aed4745-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634-kafka-clients, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba-kafka-clients, testKafkaConnectRequests=my-cluster-d1a00fce-kafka-clients, testSendMessagesTlsScramSha=my-cluster-84882f67-kafka-clients, testReceiveSimpleMessageTls=my-cluster-2e450144-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273-kafka-clients}
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-84mgz -n infra-namespace -- curl 172.17.0.19:8080/metrics
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.19 from Pod infra-namespace-kafka-clients-748578f786-84mgz finished with return code: 0
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get KafkaTopic -o jsonpath={range .items[*]}{.metadata.name} 
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get KafkaTopic -o jsonpath={range .items[*]}{.metadata.name} 
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: consumer-offsets---84e7a678d08f4bd226872e5cdd4eb527fadc1c6a
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: heartbeats
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: metrics-cluster-name-connect-config
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: metrics-cluster-name-connect-offsets
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: metrics-cluster-name-connect-status
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: mirrormaker2-cluster-configs
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: mirrormaker2-cluster-offsets
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: mirrormaker2-cluster-status
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: my-topic-1550308369-1840108023
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: my-topic-1905774485-827577292
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: my-topic-2049647524-1750726403
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: second-kafka-cluster.checkpoints.internal
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: strimzi-store-topic---effb8e3e057afce1ecf67c3f5d8e4e3ff177fc55
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: strimzi-topic-operator-kstreams-topic-store-changelog---b75e702040b99be8a9263134de3507fc0cc4017b
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: strimzi.cruisecontrol.metrics
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: strimzi.cruisecontrol.modeltrainingsamples
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:368] KafkaTopic: strimzi.cruisecontrol.partitionmetricsamples
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context testTopicOperatorMetrics is everything deleted.
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:267] testTopicOperatorMetrics - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendSimpleMessageTls, testUpdateUser, testCruiseControlBasicAPIRequests, testReceiveSimpleMessageTls, testSendMessagesTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testKafkaExporterDifferentSetting, testKafkaConnectResponse, testKafkaConnectRequests, testTopicOperatorMetrics] to and randomly select one to start execution
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testTopicOperatorMetrics
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 0
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testTopicOperatorMetrics-FINISHED
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testCruiseControlMetrics-STARTED
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testCruiseControlMetrics
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 1
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:230] testCruiseControlMetrics test now can proceed its execution
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff, testKafkaConnectResponse=my-cluster-2fe1834d, testSendSimpleMessageTls=my-cluster-ca7325d7, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e, testKafkaExporterDataAfterExchange=my-cluster-e86a0839, testUpdateUser=my-cluster-6e6fa8fe, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d, testTopicOperatorMetrics=my-cluster-216189cb, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37, testCruiseControlBasicAPIRequests=my-cluster-8aed4745, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba, testKafkaConnectRequests=my-cluster-d1a00fce, testSendMessagesTlsScramSha=my-cluster-84882f67, testCruiseControlMetrics=my-cluster-f49dfd8c, testReceiveSimpleMessageTls=my-cluster-2e450144, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273}
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-838849261-890460734, testKafkaConnectResponse=my-user-1873693835-1689263597, testSendSimpleMessageTls=my-user-1309491966-2015599610, testReconcileStateMetricInTopicOperator=my-user-2124671027-1015880288, testKafkaExporterDataAfterExchange=my-user-766848479-1588072395, testUpdateUser=my-user-1387197422-925410353, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1886945581-244087591, testTopicOperatorMetrics=my-user-779424699-479796683, testSendMessagesCustomListenerTlsScramSha=my-user-1118022021-805525218, testCruiseControlBasicAPIRequests=my-user-1861484466-561878159, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1851888062-673454065, testKafkaExporterDifferentSetting=my-user-988491821-279531432, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1731919654-539989781, testKafkaConnectRequests=my-user-1167366435-1145507543, testSendMessagesTlsScramSha=my-user-586569987-2105688079, testCruiseControlMetrics=my-user-958954083-1578303162, testReceiveSimpleMessageTls=my-user-1669668617-1031881775, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677, testRackAwareConnectCorrectDeployment=my-user-1051420592-1240796798}
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-2138084746-704381117, testKafkaConnectResponse=my-topic-680219046-979906955, testSendSimpleMessageTls=my-topic-435427300-917779110, testReconcileStateMetricInTopicOperator=my-topic-1267576332-2046216344, testKafkaExporterDataAfterExchange=my-topic-1937812163-1733130103, testUpdateUser=my-topic-1875783706-1763804507, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-306860904-106838350, testTopicOperatorMetrics=my-topic-1965542273-995674430, testSendMessagesCustomListenerTlsScramSha=my-topic-963888555-1712711326, testCruiseControlBasicAPIRequests=my-topic-1631570507-1308241024, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-307985263-926887950, testKafkaExporterDifferentSetting=my-topic-568145531-1290731867, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1672280868-1119644965, testKafkaConnectRequests=my-topic-873062135-1706759978, testSendMessagesTlsScramSha=my-topic-255809133-823300208, testCruiseControlMetrics=my-topic-1917112707-1243703175, testReceiveSimpleMessageTls=my-topic-368564676-637091599, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206, testRackAwareConnectCorrectDeployment=my-topic-189499239-153088857}
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff-kafka-clients, testKafkaConnectResponse=my-cluster-2fe1834d-kafka-clients, testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e-kafka-clients, testKafkaExporterDataAfterExchange=my-cluster-e86a0839-kafka-clients, testUpdateUser=my-cluster-6e6fa8fe-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d-kafka-clients, testTopicOperatorMetrics=my-cluster-216189cb-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-8aed4745-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634-kafka-clients, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba-kafka-clients, testKafkaConnectRequests=my-cluster-d1a00fce-kafka-clients, testSendMessagesTlsScramSha=my-cluster-84882f67-kafka-clients, testCruiseControlMetrics=my-cluster-f49dfd8c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-2e450144-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273-kafka-clients}
2022-03-30 20:54:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace exec metrics-cluster-name-cruise-control-6cdc867f99-dp58m -c cruise-control -- /bin/bash -c curl -XGET localhost:9404/metrics
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace exec metrics-cluster-name-cruise-control-6cdc867f99-dp58m -c cruise-control -- /bin/bash -c curl -XGET localhost:9404/metrics
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:450] Verifying that we have more than 0 groups
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_info{runtime="OpenJDK Runtime Environment",vendor="Red Hat, Inc.",version="11.0.14.1+1-LTS",} -> 1.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_total{pool="CodeHeap 'profiled nmethods'",} -> 1.3953536E7
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_total{pool="G1 Old Gen",} -> 2.3284656E7
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_total{pool="G1 Eden Space",} -> 5.46308096E8
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_total{pool="CodeHeap 'non-profiled nmethods'",} -> 3279360.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_total{pool="G1 Survivor Space",} -> 1.1534336E7
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_total{pool="Compressed Class Space",} -> 5457216.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_total{pool="Metaspace",} -> 4.7999512E7
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_total{pool="CodeHeap 'non-nmethods'",} -> 1483136.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_current -> 54.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_daemon -> 36.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_peak -> 54.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_started_total -> 69.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_deadlocked -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_deadlocked_monitor -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_state{state="RUNNABLE",} -> 14.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_state{state="TIMED_WAITING",} -> 24.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_state{state="WAITING",} -> 16.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_state{state="BLOCKED",} -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_state{state="TERMINATED",} -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_threads_state{state="NEW",} -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_objects_pending_finalization -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_bytes_used{area="heap",} -> 4.2222896E7
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_bytes_used{area="nonheap",} -> 7.3119656E7
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_bytes_committed{area="heap",} -> 8.68220928E8
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_bytes_committed{area="nonheap",} -> 7.6546048E7
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_bytes_max{area="heap",} -> 8.37812224E9
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_bytes_max{area="nonheap",} -> -1.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_bytes_init{area="heap",} -> 1.34217728E8
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_bytes_init{area="nonheap",} -> 7667712.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_used{pool="CodeHeap 'non-nmethods'",} -> 1470464.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_used{pool="Metaspace",} -> 4.8307624E7
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_used{pool="CodeHeap 'profiled nmethods'",} -> 1.4456576E7
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_used{pool="Compressed Class Space",} -> 5477248.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_used{pool="G1 Eden Space",} -> 1.7825792E7
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_used{pool="G1 Old Gen",} -> 2.02028E7
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_used{pool="G1 Survivor Space",} -> 4194304.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_used{pool="CodeHeap 'non-profiled nmethods'",} -> 3407744.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_committed{pool="CodeHeap 'non-nmethods'",} -> 2555904.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_committed{pool="Metaspace",} -> 4.9938432E7
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_committed{pool="CodeHeap 'profiled nmethods'",} -> 1.4483456E7
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_committed{pool="Compressed Class Space",} -> 6160384.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_committed{pool="G1 Eden Space",} -> 4.718592E7
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_committed{pool="G1 Old Gen",} -> 8.16840704E8
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_committed{pool="G1 Survivor Space",} -> 4194304.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_committed{pool="CodeHeap 'non-profiled nmethods'",} -> 3407872.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_max{pool="CodeHeap 'non-nmethods'",} -> 5828608.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_max{pool="Metaspace",} -> -1.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_max{pool="CodeHeap 'profiled nmethods'",} -> 1.22912768E8
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_max{pool="Compressed Class Space",} -> 1.073741824E9
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_max{pool="G1 Eden Space",} -> -1.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_max{pool="G1 Old Gen",} -> 8.37812224E9
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_max{pool="G1 Survivor Space",} -> -1.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_max{pool="CodeHeap 'non-profiled nmethods'",} -> 1.22916864E8
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_init{pool="CodeHeap 'non-nmethods'",} -> 2555904.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_init{pool="Metaspace",} -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_init{pool="CodeHeap 'profiled nmethods'",} -> 2555904.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_init{pool="Compressed Class Space",} -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_init{pool="G1 Eden Space",} -> 7340032.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_init{pool="G1 Old Gen",} -> 1.26877696E8
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_init{pool="G1 Survivor Space",} -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_bytes_init{pool="CodeHeap 'non-profiled nmethods'",} -> 2555904.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_used_bytes{pool="G1 Eden Space",} -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_used_bytes{pool="G1 Old Gen",} -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_used_bytes{pool="G1 Survivor Space",} -> 4194304.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_committed_bytes{pool="G1 Eden Space",} -> 4.718592E7
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_committed_bytes{pool="G1 Old Gen",} -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_committed_bytes{pool="G1 Survivor Space",} -> 4194304.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_max_bytes{pool="G1 Eden Space",} -> -1.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_max_bytes{pool="G1 Old Gen",} -> 8.37812224E9
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_max_bytes{pool="G1 Survivor Space",} -> -1.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_init_bytes{pool="G1 Eden Space",} -> 7340032.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_init_bytes{pool="G1 Old Gen",} -> 1.26877696E8
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_collection_init_bytes{pool="G1 Survivor Space",} -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jmx_exporter_build_info{version="0.16.1",name="jmx_prometheus_javaagent",} -> 1.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_buffer_pool_used_bytes{pool="mapped",} -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_buffer_pool_used_bytes{pool="direct",} -> 331444.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_buffer_pool_capacity_bytes{pool="mapped",} -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_buffer_pool_capacity_bytes{pool="direct",} -> 331444.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_buffer_pool_used_buffers{pool="mapped",} -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_buffer_pool_used_buffers{pool="direct",} -> 24.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] process_cpu_seconds_total -> 19.49
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] process_start_time_seconds -> 1.648673317378E9
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] process_open_fds -> 169.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] process_max_fds -> 1048576.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] process_virtual_memory_bytes -> 1.4651133952E10
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] process_resident_memory_bytes -> 3.60255488E8
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_gc_collection_seconds_count{gc="G1 Young Generation",} -> 25.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_gc_collection_seconds_sum{gc="G1 Young Generation",} -> 0.262
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_gc_collection_seconds_count{gc="G1 Old Generation",} -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_gc_collection_seconds_sum{gc="G1 Old Generation",} -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jmx_config_reload_success_total -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jmx_config_reload_failure_total -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_mean -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_max -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_75thpercentile -> 28.264588999999997
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_request_rate_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_aborting_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_execution_started_kafka_assigner_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_min -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_has_partitions_with_isr_greater_than_replicas_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_count -> 1.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_min -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_failure_rate_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_mean -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_request_rate_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_request_rate_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_stddev -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_rate_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_request_rate_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_stddev -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_enabled_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_request_rate_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_request_rate_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_ongoing_anomaly_duration_ms_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_request_rate_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_request_rate_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_min -> 25.448406
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_stddev -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_request_rate_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_max -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_stddev -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_min -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_min -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_request_rate_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_aborted_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_request_rate_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_request_rate_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_mean -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_usertaskmanager_num_active_sessions_number -> 12.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_request_rate_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_mean -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_inter_broker_partition_movements_per_broker_cap_value -> 5.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_request_rate_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_min -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_min -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_automated_rightsizing_rate_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_request_rate_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_total_monitored_windows_value -> 2.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_request_rate_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_mean -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_95thpercentile -> 64.083648
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_request_rate_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_number_of_self_healing_failed_to_start_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_fiveminuterate -> 0.010430640751379987
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_movements_global_cap_value -> 1000.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_min -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_99thpercentile -> 197.20353
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_request_rate_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_mean -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_rate_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_request_rate_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_metadata_factor_number -> 1512.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_failure_rate_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_min -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_automated_rightsizing_rate_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_999thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_min -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_max -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_max -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_request_rate_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_max -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_request_rate_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_75thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_98thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_999thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_max -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_request_rate_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_stddev -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_rate_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_mean -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_request_rate_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_999thpercentile -> 78.450965
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_dead_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_number_of_self_healing_failed_to_start_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_75thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_999thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_min -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_mean -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_oneminuterate -> 7.96150839048494E-4
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_98thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_request_rate_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_total_monitored_windows_number -> 2.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_in_progress_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_request_rate_meanrate -> 0.1974872010295393
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_95thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_50thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_rate_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_request_rate_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_request_rate_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_stddev -> 22.774265396047024
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_stddev -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_request_rate_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_right_sized_number -> 1.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_request_rate_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_50thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_enabled_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_rate_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_request_rate_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_95thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_99thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_mean -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_rate_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_request_rate_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_execution_started_kafka_assigner_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_max -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_request_rate_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_mean -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_valid_windows_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_999thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_min -> 36.30188
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_99thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_completed_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_request_rate_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_rate_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_stddev -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_rate_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_request_rate_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_request_rate_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_mean -> 36.30188
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_dead_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_request_rate_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_completed_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_failure_rate_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_95thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_enabled_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_valid_windows_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_mean -> 78.450965
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_request_rate_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_ongoing_execution_non_kafka_assigner_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_request_rate_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_98thpercentile -> 36.30188
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_stddev -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_request_rate_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_50thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_stddev -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_has_unfixable_goals_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_stddev -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_meanrate -> 0.002992317131718777
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_pending_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_count -> 5.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_oneminuterate -> 8.653395709915322E-4
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_request_rate_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_stddev -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_stddev -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_monitored_partitions_percentage_value -> 0.5969387888908386
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_rate_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_rate_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_num_suspect_metric_anomalies_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_mean -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_max -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_request_rate_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_max -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_number_of_self_healing_started_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_request_rate_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_mean -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_75thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_enabled_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_mean -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_request_rate_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_rate_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_98thpercentile -> 64.083648
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_num_topics_number -> 17.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_dead_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_has_partitions_with_isr_greater_than_replicas_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_aborted_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_mean_time_to_start_fix_ms_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_monitored_partitions_percentage_number -> 0.5969387888908386
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_75thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_stddev -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_mean -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_stddev -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_request_rate_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_dead_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_98thpercentile -> 78.450965
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_max -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_execution_stopped_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_dead_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_failure_rate_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_pending_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_request_rate_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_max -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_request_rate_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_request_rate_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_fiveminuterate -> 0.001814211694082754
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_request_rate_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_request_rate_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_min -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_mean -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_98thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_aborted_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_rate_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_in_progress_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_stddev -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_stddev -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_aborting_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_stddev -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_98thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_fiveminuterate -> 0.001844701935204127
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_rate_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_rate_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_50thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_min -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_under_provisioned_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_enabled_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_pending_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_stddev -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_max -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_mean -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_aborted_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_50thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_request_rate_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_request_rate_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_request_rate_fifteenminuterate -> 0.2
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_num_topics_value -> 17.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_aborting_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_partition_movements_per_broker_cap_number -> 2.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_max -> 36.30188
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_request_rate_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_enabled_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_stddev -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_mean -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_request_rate_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_failure_rate_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_request_rate_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_ongoing_execution_kafka_assigner_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_movements_global_cap_number -> 1000.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_request_rate_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_stddev -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_rate_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_max -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_count -> 1.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_has_partitions_with_replication_factor_greater_than_num_racks_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_rate_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_dead_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_98thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_aborting_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_max -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_99thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_request_rate_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_completed_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_min -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_75thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_50thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_enabled_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_request_rate_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_min -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_request_rate_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_request_rate_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_request_rate_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_request_rate_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_75thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_min -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_in_progress_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_ongoing_execution_non_kafka_assigner_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_75thpercentile -> 36.30188
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_enabled_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_95thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_min -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_request_rate_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_num_recent_metric_anomalies_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_stddev -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_mean -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_in_progress_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_enabled_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_request_rate_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_over_provisioned_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_999thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_rate_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_request_rate_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_completed_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_usertaskmanager_num_active_sessions_value -> 12.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_under_provisioned_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_999thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_in_progress_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_has_partitions_with_replication_factor_greater_than_num_racks_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_max -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_min -> 78.450965
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_metadata_factor_value -> 1512.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_min -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_stddev -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_999thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_request_rate_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_in_progress_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_request_rate_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_request_rate_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_right_sized_value -> 1.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_meanrate -> 0.0029155177467890723
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_stddev -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_completed_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_fifteenminuterate -> 0.004710506511337888
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_request_rate_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_50thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_request_rate_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_95thpercentile -> 78.450965
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_request_rate_oneminuterate -> 0.2
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_rate_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_usertaskmanager_num_active_user_tasks_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_num_persistent_metric_anomalies_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_95thpercentile -> 36.30188
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_balancedness_score_number -> 100.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_max -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_failure_rate_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_rate_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_automated_rightsizing_rate_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_98thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_99thpercentile -> 78.450965
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_rate_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_request_rate_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_stddev -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_request_rate_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_aborted_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_request_rate_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_request_rate_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_mean -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_mean -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_mean -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_fifteenminuterate -> 9.071785630234758E-4
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_request_rate_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_99thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_request_rate_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_usertaskmanager_num_active_user_tasks_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_request_rate_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_enabled_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_request_rate_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_request_rate_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_95thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_mean -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_stddev -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_stddev -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_min -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_request_rate_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_99thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_request_rate_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_mean -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_max -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_num_persistent_metric_anomalies_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_max -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_completed_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_oneminuterate -> 0.015240369283104429
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_request_rate_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_stddev -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_inter_broker_partition_movements_per_broker_cap_number -> 5.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_rate_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_mean -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_98thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_ongoing_anomaly_duration_ms_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_aborting_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_request_rate_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_failure_rate_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_request_rate_count -> 66.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_request_rate_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_98thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_request_rate_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_fifteenminuterate -> 9.12232469559705E-4
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_min -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_75thpercentile -> 78.450965
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_min -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_min -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_mean_time_to_start_fix_ms_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_execution_started_non_kafka_assigner_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_over_provisioned_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_max -> 197.20353
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_min -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_rate_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_rate_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_999thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_execution_stopped_by_user_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_max -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_rate_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_min -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_max -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_stddev -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_replica_action_aborted_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_ongoing_execution_kafka_assigner_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_mean -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_request_rate_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_request_rate_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_999thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_number_of_self_healing_started_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_num_suspect_metric_anomalies_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_execution_stopped_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_failure_rate_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_99thpercentile -> 36.30188
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_failure_rate_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_max -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_max -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_max -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_max -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_request_rate_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_max -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_mean -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_min -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_request_rate_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_rate_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_50thpercentile -> 25.448406
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_request_rate_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_mean -> 31.440851676479408
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_request_rate_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_intra_broker_partition_movements_per_broker_cap_value -> 2.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_rate_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_rate_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_max -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_min -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_request_rate_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_999thpercentile -> 36.30188
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_mean -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_balancedness_score_value -> 100.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_failure_rate_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_50thpercentile -> 78.450965
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_95thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_add_broker_successful_request_execution_timer_mean -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_has_unfixable_goals_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_cluster_model_creation_timer_50thpercentile -> 36.30188
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_pending_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_min -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_99thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_enabled_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rebalance_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_automated_rightsizing_rate_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_rate_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_remove_broker_request_rate_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_replica_action_pending_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_proposals_request_rate_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_min -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_stddev -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_num_partitions_with_extrapolations_number -> 117.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_execution_started_non_kafka_assigner_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_request_rate_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_mean -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_max -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_min -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_self_healing_fix_generation_timer_max -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_goaloptimizer_proposal_computation_timer_75thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_99thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_topic_anomaly_rate_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_stddev -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_request_rate_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_95thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_stop_proposal_execution_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_train_successful_request_execution_timer_min -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_maintenance_event_self_healing_fix_generation_timer_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_successful_request_execution_timer_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_request_rate_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_successful_request_execution_timer_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_50thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_review_board_request_rate_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_num_recent_metric_anomalies_value -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_metric_anomaly_self_healing_fix_generation_timer_99thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_demote_broker_successful_request_execution_timer_99thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_state_request_rate_fiveminuterate -> 0.2
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_fix_generation_timer_75thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_execution_stopped_by_user_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_partition_load_successful_request_execution_timer_count -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_rate_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_request_rate_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_automated_rightsizing_rate_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_self_healing_fix_generation_timer_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_aborting_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_loadmonitor_num_partitions_with_extrapolations_value -> 117.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_999thpercentile -> 197.20353
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_load_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_disk_failure_self_healing_fix_generation_timer_stddev -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_bootstrap_request_rate_oneminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_goal_violation_detection_timer_max -> 78.450965
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_rightsize_successful_request_execution_timer_75thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_executor_leadership_action_pending_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_fix_offline_replicas_successful_request_execution_timer_98thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_partition_samples_fetcher_timer_meanrate -> 0.014840804694489326
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_anomalydetector_broker_failure_self_healing_enabled_number -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_user_tasks_successful_request_execution_timer_fiveminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_resume_sampling_successful_request_execution_timer_50thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_95thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_pause_sampling_request_rate_fifteenminuterate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_admin_request_rate_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_metricfetchermanager_training_samples_fetcher_timer_max -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_kafka_cluster_state_successful_request_execution_timer_95thpercentile -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] kafka_cruisecontrol_kafkacruisecontrolservlet_topic_configuration_successful_request_execution_timer_meanrate -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jmx_scrape_duration_seconds -> 0.254187822
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jmx_scrape_error -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jmx_scrape_cached_beans -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_classes_loaded -> 8684.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_classes_loaded_total -> 8684.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_classes_unloaded_total -> 0.0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jmx_config_reload_failure_created -> 1.648673317558E9
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jmx_config_reload_success_created -> 1.648673317557E9
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_created{pool="CodeHeap 'profiled nmethods'",} -> 1.64867331785E9
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_created{pool="G1 Old Gen",} -> 1.648673317853E9
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_created{pool="G1 Eden Space",} -> 1.648673317853E9
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_created{pool="CodeHeap 'non-profiled nmethods'",} -> 1.648673317853E9
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_created{pool="G1 Survivor Space",} -> 1.648673317853E9
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_created{pool="Compressed Class Space",} -> 1.648673317853E9
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_created{pool="Metaspace",} -> 1.648673317853E9
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [MetricsIsolatedST:459] jvm_memory_pool_allocated_bytes_created{pool="CodeHeap 'non-nmethods'",} 1.648673317853E9 -> 
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context testCruiseControlMetrics is everything deleted.
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:267] testCruiseControlMetrics - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendSimpleMessageTls, testUpdateUser, testCruiseControlBasicAPIRequests, testReceiveSimpleMessageTls, testSendMessagesTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testKafkaExporterDifferentSetting, testKafkaConnectResponse, testKafkaConnectRequests, testTopicOperatorMetrics, testCruiseControlMetrics] to and randomly select one to start execution
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testCruiseControlMetrics
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testCruiseControlMetrics-FINISHED
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testClusterOperatorMetrics-STARTED
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testClusterOperatorMetrics
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 1
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:230] testClusterOperatorMetrics test now can proceed its execution
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff, testKafkaConnectResponse=my-cluster-2fe1834d, testSendSimpleMessageTls=my-cluster-ca7325d7, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e, testKafkaExporterDataAfterExchange=my-cluster-e86a0839, testUpdateUser=my-cluster-6e6fa8fe, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d, testTopicOperatorMetrics=my-cluster-216189cb, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37, testCruiseControlBasicAPIRequests=my-cluster-8aed4745, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba, testKafkaConnectRequests=my-cluster-d1a00fce, testSendMessagesTlsScramSha=my-cluster-84882f67, testCruiseControlMetrics=my-cluster-f49dfd8c, testReceiveSimpleMessageTls=my-cluster-2e450144, testClusterOperatorMetrics=my-cluster-cfe17ead, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273}
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-838849261-890460734, testKafkaConnectResponse=my-user-1873693835-1689263597, testSendSimpleMessageTls=my-user-1309491966-2015599610, testReconcileStateMetricInTopicOperator=my-user-2124671027-1015880288, testKafkaExporterDataAfterExchange=my-user-766848479-1588072395, testUpdateUser=my-user-1387197422-925410353, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1886945581-244087591, testTopicOperatorMetrics=my-user-779424699-479796683, testSendMessagesCustomListenerTlsScramSha=my-user-1118022021-805525218, testCruiseControlBasicAPIRequests=my-user-1861484466-561878159, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1851888062-673454065, testKafkaExporterDifferentSetting=my-user-988491821-279531432, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1731919654-539989781, testKafkaConnectRequests=my-user-1167366435-1145507543, testSendMessagesTlsScramSha=my-user-586569987-2105688079, testCruiseControlMetrics=my-user-958954083-1578303162, testReceiveSimpleMessageTls=my-user-1669668617-1031881775, testClusterOperatorMetrics=my-user-467081550-2107716793, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677, testRackAwareConnectCorrectDeployment=my-user-1051420592-1240796798}
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-2138084746-704381117, testKafkaConnectResponse=my-topic-680219046-979906955, testSendSimpleMessageTls=my-topic-435427300-917779110, testReconcileStateMetricInTopicOperator=my-topic-1267576332-2046216344, testKafkaExporterDataAfterExchange=my-topic-1937812163-1733130103, testUpdateUser=my-topic-1875783706-1763804507, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-306860904-106838350, testTopicOperatorMetrics=my-topic-1965542273-995674430, testSendMessagesCustomListenerTlsScramSha=my-topic-963888555-1712711326, testCruiseControlBasicAPIRequests=my-topic-1631570507-1308241024, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-307985263-926887950, testKafkaExporterDifferentSetting=my-topic-568145531-1290731867, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1672280868-1119644965, testKafkaConnectRequests=my-topic-873062135-1706759978, testSendMessagesTlsScramSha=my-topic-255809133-823300208, testCruiseControlMetrics=my-topic-1917112707-1243703175, testReceiveSimpleMessageTls=my-topic-368564676-637091599, testClusterOperatorMetrics=my-topic-1416314702-1707243111, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206, testRackAwareConnectCorrectDeployment=my-topic-189499239-153088857}
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff-kafka-clients, testKafkaConnectResponse=my-cluster-2fe1834d-kafka-clients, testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e-kafka-clients, testKafkaExporterDataAfterExchange=my-cluster-e86a0839-kafka-clients, testUpdateUser=my-cluster-6e6fa8fe-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d-kafka-clients, testTopicOperatorMetrics=my-cluster-216189cb-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-8aed4745-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634-kafka-clients, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba-kafka-clients, testKafkaConnectRequests=my-cluster-d1a00fce-kafka-clients, testSendMessagesTlsScramSha=my-cluster-84882f67-kafka-clients, testCruiseControlMetrics=my-cluster-f49dfd8c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-2e450144-kafka-clients, testClusterOperatorMetrics=my-cluster-cfe17ead-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273-kafka-clients}
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-84mgz -n infra-namespace -- curl 172.17.0.5:8080/metrics
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.5 from Pod infra-namespace-kafka-clients-748578f786-84mgz finished with return code: 0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context testClusterOperatorMetrics is everything deleted.
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:267] testClusterOperatorMetrics - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendSimpleMessageTls, testUpdateUser, testCruiseControlBasicAPIRequests, testReceiveSimpleMessageTls, testSendMessagesTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testKafkaExporterDifferentSetting, testKafkaConnectResponse, testKafkaConnectRequests, testTopicOperatorMetrics, testCruiseControlMetrics, testClusterOperatorMetrics] to and randomly select one to start execution
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testClusterOperatorMetrics
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 0
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testClusterOperatorMetrics-FINISHED
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaMetricsSettings-STARTED
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaMetricsSettings
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 1
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:230] testKafkaMetricsSettings test now can proceed its execution
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff, testKafkaConnectResponse=my-cluster-2fe1834d, testSendSimpleMessageTls=my-cluster-ca7325d7, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e, testKafkaExporterDataAfterExchange=my-cluster-e86a0839, testUpdateUser=my-cluster-6e6fa8fe, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d, testTopicOperatorMetrics=my-cluster-216189cb, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37, testCruiseControlBasicAPIRequests=my-cluster-8aed4745, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba, testKafkaConnectRequests=my-cluster-d1a00fce, testSendMessagesTlsScramSha=my-cluster-84882f67, testCruiseControlMetrics=my-cluster-f49dfd8c, testReceiveSimpleMessageTls=my-cluster-2e450144, testClusterOperatorMetrics=my-cluster-cfe17ead, testKafkaMetricsSettings=my-cluster-b7e9a534, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273}
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-838849261-890460734, testKafkaConnectResponse=my-user-1873693835-1689263597, testSendSimpleMessageTls=my-user-1309491966-2015599610, testReconcileStateMetricInTopicOperator=my-user-2124671027-1015880288, testKafkaExporterDataAfterExchange=my-user-766848479-1588072395, testUpdateUser=my-user-1387197422-925410353, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1886945581-244087591, testTopicOperatorMetrics=my-user-779424699-479796683, testSendMessagesCustomListenerTlsScramSha=my-user-1118022021-805525218, testCruiseControlBasicAPIRequests=my-user-1861484466-561878159, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1851888062-673454065, testKafkaExporterDifferentSetting=my-user-988491821-279531432, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1731919654-539989781, testKafkaConnectRequests=my-user-1167366435-1145507543, testSendMessagesTlsScramSha=my-user-586569987-2105688079, testCruiseControlMetrics=my-user-958954083-1578303162, testReceiveSimpleMessageTls=my-user-1669668617-1031881775, testClusterOperatorMetrics=my-user-467081550-2107716793, testKafkaMetricsSettings=my-user-1339734863-677347400, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677, testRackAwareConnectCorrectDeployment=my-user-1051420592-1240796798}
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-2138084746-704381117, testKafkaConnectResponse=my-topic-680219046-979906955, testSendSimpleMessageTls=my-topic-435427300-917779110, testReconcileStateMetricInTopicOperator=my-topic-1267576332-2046216344, testKafkaExporterDataAfterExchange=my-topic-1937812163-1733130103, testUpdateUser=my-topic-1875783706-1763804507, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-306860904-106838350, testTopicOperatorMetrics=my-topic-1965542273-995674430, testSendMessagesCustomListenerTlsScramSha=my-topic-963888555-1712711326, testCruiseControlBasicAPIRequests=my-topic-1631570507-1308241024, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-307985263-926887950, testKafkaExporterDifferentSetting=my-topic-568145531-1290731867, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1672280868-1119644965, testKafkaConnectRequests=my-topic-873062135-1706759978, testSendMessagesTlsScramSha=my-topic-255809133-823300208, testCruiseControlMetrics=my-topic-1917112707-1243703175, testReceiveSimpleMessageTls=my-topic-368564676-637091599, testClusterOperatorMetrics=my-topic-1416314702-1707243111, testKafkaMetricsSettings=my-topic-256215769-877131003, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206, testRackAwareConnectCorrectDeployment=my-topic-189499239-153088857}
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff-kafka-clients, testKafkaConnectResponse=my-cluster-2fe1834d-kafka-clients, testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e-kafka-clients, testKafkaExporterDataAfterExchange=my-cluster-e86a0839-kafka-clients, testUpdateUser=my-cluster-6e6fa8fe-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d-kafka-clients, testTopicOperatorMetrics=my-cluster-216189cb-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-8aed4745-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634-kafka-clients, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba-kafka-clients, testKafkaConnectRequests=my-cluster-d1a00fce-kafka-clients, testSendMessagesTlsScramSha=my-cluster-84882f67-kafka-clients, testCruiseControlMetrics=my-cluster-f49dfd8c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-2e450144-kafka-clients, testClusterOperatorMetrics=my-cluster-cfe17ead-kafka-clients, testKafkaMetricsSettings=my-cluster-b7e9a534-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273-kafka-clients}
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: second-kafka-cluster are stable
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for pod with prefixsecond-kafka-cluster is present.
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Pods stability in phase Running
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 50
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 50
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 50
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 50
2022-03-30 20:54:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (299991ms till timeout)
2022-03-30 20:54:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 49
2022-03-30 20:54:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 49
2022-03-30 20:54:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 49
2022-03-30 20:54:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 49
2022-03-30 20:54:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (298981ms till timeout)
2022-03-30 20:54:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 48
2022-03-30 20:54:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 48
2022-03-30 20:54:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 48
2022-03-30 20:54:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 48
2022-03-30 20:54:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (297972ms till timeout)
2022-03-30 20:54:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:54:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 47
2022-03-30 20:54:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 47
2022-03-30 20:54:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 47
2022-03-30 20:54:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 47
2022-03-30 20:54:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (296962ms till timeout)
2022-03-30 20:54:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 46
2022-03-30 20:54:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 46
2022-03-30 20:54:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 46
2022-03-30 20:54:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 46
2022-03-30 20:54:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (295952ms till timeout)
2022-03-30 20:54:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 45
2022-03-30 20:54:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 45
2022-03-30 20:54:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 45
2022-03-30 20:54:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 45
2022-03-30 20:54:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (294943ms till timeout)
2022-03-30 20:54:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 44
2022-03-30 20:54:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 44
2022-03-30 20:54:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 44
2022-03-30 20:54:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 44
2022-03-30 20:54:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (293935ms till timeout)
2022-03-30 20:54:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 43
2022-03-30 20:54:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 43
2022-03-30 20:54:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 43
2022-03-30 20:54:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 43
2022-03-30 20:54:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (292925ms till timeout)
2022-03-30 20:54:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:54:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 42
2022-03-30 20:54:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 42
2022-03-30 20:54:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 42
2022-03-30 20:54:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 42
2022-03-30 20:54:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (291916ms till timeout)
2022-03-30 20:54:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 41
2022-03-30 20:54:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 41
2022-03-30 20:54:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 41
2022-03-30 20:54:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 41
2022-03-30 20:54:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (290865ms till timeout)
2022-03-30 20:54:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 40
2022-03-30 20:54:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 40
2022-03-30 20:54:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 40
2022-03-30 20:54:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 40
2022-03-30 20:54:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (289855ms till timeout)
2022-03-30 20:54:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 39
2022-03-30 20:54:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 39
2022-03-30 20:54:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 39
2022-03-30 20:54:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 39
2022-03-30 20:54:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (288846ms till timeout)
2022-03-30 20:54:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 38
2022-03-30 20:54:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 38
2022-03-30 20:54:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 38
2022-03-30 20:54:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 38
2022-03-30 20:54:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (287837ms till timeout)
2022-03-30 20:54:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:54:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 37
2022-03-30 20:54:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 37
2022-03-30 20:54:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 37
2022-03-30 20:54:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 37
2022-03-30 20:54:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (286827ms till timeout)
2022-03-30 20:54:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 36
2022-03-30 20:54:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 36
2022-03-30 20:54:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 36
2022-03-30 20:54:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 36
2022-03-30 20:54:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (285817ms till timeout)
2022-03-30 20:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 35
2022-03-30 20:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 35
2022-03-30 20:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 35
2022-03-30 20:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 35
2022-03-30 20:54:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (284808ms till timeout)
2022-03-30 20:54:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 34
2022-03-30 20:54:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 34
2022-03-30 20:54:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 34
2022-03-30 20:54:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 34
2022-03-30 20:54:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (283798ms till timeout)
2022-03-30 20:54:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 33
2022-03-30 20:54:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 33
2022-03-30 20:54:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 33
2022-03-30 20:54:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 33
2022-03-30 20:54:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (282789ms till timeout)
2022-03-30 20:54:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:54:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 32
2022-03-30 20:54:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 32
2022-03-30 20:54:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 32
2022-03-30 20:54:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 32
2022-03-30 20:54:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (281779ms till timeout)
2022-03-30 20:54:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 31
2022-03-30 20:54:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 31
2022-03-30 20:54:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 31
2022-03-30 20:54:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 31
2022-03-30 20:54:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (280770ms till timeout)
2022-03-30 20:54:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 30
2022-03-30 20:54:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 30
2022-03-30 20:54:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 30
2022-03-30 20:54:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 30
2022-03-30 20:54:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (279760ms till timeout)
2022-03-30 20:54:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 29
2022-03-30 20:54:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 29
2022-03-30 20:54:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 29
2022-03-30 20:54:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 29
2022-03-30 20:54:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (278751ms till timeout)
2022-03-30 20:54:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 28
2022-03-30 20:54:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 28
2022-03-30 20:54:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 28
2022-03-30 20:54:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 28
2022-03-30 20:54:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (277741ms till timeout)
2022-03-30 20:54:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:54:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 27
2022-03-30 20:54:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 27
2022-03-30 20:54:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 27
2022-03-30 20:54:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 27
2022-03-30 20:54:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (276732ms till timeout)
2022-03-30 20:54:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 26
2022-03-30 20:54:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 26
2022-03-30 20:54:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 26
2022-03-30 20:54:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 26
2022-03-30 20:54:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (275722ms till timeout)
2022-03-30 20:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 25
2022-03-30 20:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 25
2022-03-30 20:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 25
2022-03-30 20:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 25
2022-03-30 20:54:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (274713ms till timeout)
2022-03-30 20:54:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 24
2022-03-30 20:54:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 24
2022-03-30 20:54:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 24
2022-03-30 20:54:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 24
2022-03-30 20:54:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (273704ms till timeout)
2022-03-30 20:54:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 23
2022-03-30 20:54:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 23
2022-03-30 20:54:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 23
2022-03-30 20:54:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 23
2022-03-30 20:54:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (272693ms till timeout)
2022-03-30 20:54:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:54:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 22
2022-03-30 20:54:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 22
2022-03-30 20:54:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 22
2022-03-30 20:54:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 22
2022-03-30 20:54:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (271684ms till timeout)
2022-03-30 20:54:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 21
2022-03-30 20:54:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 21
2022-03-30 20:54:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 21
2022-03-30 20:54:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 21
2022-03-30 20:54:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (270674ms till timeout)
2022-03-30 20:54:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 20
2022-03-30 20:54:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 20
2022-03-30 20:54:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 20
2022-03-30 20:54:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 20
2022-03-30 20:54:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (269665ms till timeout)
2022-03-30 20:54:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 19
2022-03-30 20:54:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 19
2022-03-30 20:54:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 19
2022-03-30 20:54:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 19
2022-03-30 20:54:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (268655ms till timeout)
2022-03-30 20:54:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 18
2022-03-30 20:54:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 18
2022-03-30 20:54:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 18
2022-03-30 20:54:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 18
2022-03-30 20:54:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (267646ms till timeout)
2022-03-30 20:54:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:54:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 17
2022-03-30 20:54:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 17
2022-03-30 20:54:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 17
2022-03-30 20:54:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 17
2022-03-30 20:54:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (266636ms till timeout)
2022-03-30 20:54:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 16
2022-03-30 20:54:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 16
2022-03-30 20:54:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 16
2022-03-30 20:54:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 16
2022-03-30 20:54:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (265627ms till timeout)
2022-03-30 20:54:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 15
2022-03-30 20:54:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 15
2022-03-30 20:54:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 15
2022-03-30 20:54:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 15
2022-03-30 20:54:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (264618ms till timeout)
2022-03-30 20:54:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 14
2022-03-30 20:54:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 14
2022-03-30 20:54:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 14
2022-03-30 20:54:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 14
2022-03-30 20:54:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (263608ms till timeout)
2022-03-30 20:54:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 13
2022-03-30 20:54:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 13
2022-03-30 20:54:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 13
2022-03-30 20:54:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 13
2022-03-30 20:54:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (262599ms till timeout)
2022-03-30 20:55:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:55:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 12
2022-03-30 20:55:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 12
2022-03-30 20:55:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 12
2022-03-30 20:55:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 12
2022-03-30 20:55:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (261588ms till timeout)
2022-03-30 20:55:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 11
2022-03-30 20:55:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 11
2022-03-30 20:55:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 11
2022-03-30 20:55:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 11
2022-03-30 20:55:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (260579ms till timeout)
2022-03-30 20:55:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 10
2022-03-30 20:55:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 10
2022-03-30 20:55:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 10
2022-03-30 20:55:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 10
2022-03-30 20:55:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (259563ms till timeout)
2022-03-30 20:55:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 9
2022-03-30 20:55:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 9
2022-03-30 20:55:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 9
2022-03-30 20:55:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 9
2022-03-30 20:55:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (258553ms till timeout)
2022-03-30 20:55:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 8
2022-03-30 20:55:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 8
2022-03-30 20:55:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 8
2022-03-30 20:55:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 8
2022-03-30 20:55:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (257544ms till timeout)
2022-03-30 20:55:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:55:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 7
2022-03-30 20:55:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 7
2022-03-30 20:55:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 7
2022-03-30 20:55:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 7
2022-03-30 20:55:06 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (256534ms till timeout)
2022-03-30 20:55:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 6
2022-03-30 20:55:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 6
2022-03-30 20:55:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 6
2022-03-30 20:55:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 6
2022-03-30 20:55:07 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (255525ms till timeout)
2022-03-30 20:55:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 5
2022-03-30 20:55:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 5
2022-03-30 20:55:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 5
2022-03-30 20:55:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 5
2022-03-30 20:55:08 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (254516ms till timeout)
2022-03-30 20:55:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 4
2022-03-30 20:55:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 4
2022-03-30 20:55:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 4
2022-03-30 20:55:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 4
2022-03-30 20:55:09 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (253505ms till timeout)
2022-03-30 20:55:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 3
2022-03-30 20:55:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 3
2022-03-30 20:55:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 3
2022-03-30 20:55:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 3
2022-03-30 20:55:10 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (252455ms till timeout)
2022-03-30 20:55:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:55:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 2
2022-03-30 20:55:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 2
2022-03-30 20:55:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 2
2022-03-30 20:55:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 2
2022-03-30 20:55:11 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (251446ms till timeout)
2022-03-30 20:55:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 1
2022-03-30 20:55:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 1
2022-03-30 20:55:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 1
2022-03-30 20:55:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 1
2022-03-30 20:55:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:335] All pods are stable second-kafka-cluster-entity-operator-5f8949dc9c-84c2x ,second-kafka-cluster-kafka-0 ,second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll ,second-kafka-cluster-zookeeper-0
2022-03-30 20:55:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: second-kafka-cluster are stable
2022-03-30 20:55:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for pod with prefixsecond-kafka-cluster is present.
2022-03-30 20:55:12 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Pods stability in phase Running
2022-03-30 20:55:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 50
2022-03-30 20:55:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 50
2022-03-30 20:55:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 50
2022-03-30 20:55:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 50
2022-03-30 20:55:12 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (299991ms till timeout)
2022-03-30 20:55:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 49
2022-03-30 20:55:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 49
2022-03-30 20:55:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 49
2022-03-30 20:55:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 49
2022-03-30 20:55:13 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (298982ms till timeout)
2022-03-30 20:55:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 48
2022-03-30 20:55:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 48
2022-03-30 20:55:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 48
2022-03-30 20:55:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 48
2022-03-30 20:55:14 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (297972ms till timeout)
2022-03-30 20:55:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 47
2022-03-30 20:55:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 47
2022-03-30 20:55:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 47
2022-03-30 20:55:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 47
2022-03-30 20:55:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (296962ms till timeout)
2022-03-30 20:55:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:55:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 46
2022-03-30 20:55:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 46
2022-03-30 20:55:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 46
2022-03-30 20:55:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 46
2022-03-30 20:55:16 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (295953ms till timeout)
2022-03-30 20:55:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 45
2022-03-30 20:55:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 45
2022-03-30 20:55:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 45
2022-03-30 20:55:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 45
2022-03-30 20:55:17 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (294943ms till timeout)
2022-03-30 20:55:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 44
2022-03-30 20:55:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 44
2022-03-30 20:55:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 44
2022-03-30 20:55:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 44
2022-03-30 20:55:18 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (293934ms till timeout)
2022-03-30 20:55:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 43
2022-03-30 20:55:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 43
2022-03-30 20:55:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 43
2022-03-30 20:55:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 43
2022-03-30 20:55:19 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (292924ms till timeout)
2022-03-30 20:55:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 42
2022-03-30 20:55:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 42
2022-03-30 20:55:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 42
2022-03-30 20:55:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 42
2022-03-30 20:55:20 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (291915ms till timeout)
2022-03-30 20:55:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:55:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 41
2022-03-30 20:55:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 41
2022-03-30 20:55:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 41
2022-03-30 20:55:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 41
2022-03-30 20:55:21 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (290905ms till timeout)
2022-03-30 20:55:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 40
2022-03-30 20:55:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 40
2022-03-30 20:55:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 40
2022-03-30 20:55:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 40
2022-03-30 20:55:22 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (289896ms till timeout)
2022-03-30 20:55:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 39
2022-03-30 20:55:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 39
2022-03-30 20:55:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 39
2022-03-30 20:55:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 39
2022-03-30 20:55:23 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (288886ms till timeout)
2022-03-30 20:55:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 38
2022-03-30 20:55:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 38
2022-03-30 20:55:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 38
2022-03-30 20:55:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 38
2022-03-30 20:55:24 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (287877ms till timeout)
2022-03-30 20:55:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 37
2022-03-30 20:55:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 37
2022-03-30 20:55:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 37
2022-03-30 20:55:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 37
2022-03-30 20:55:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (286867ms till timeout)
2022-03-30 20:55:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:55:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 36
2022-03-30 20:55:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 36
2022-03-30 20:55:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 36
2022-03-30 20:55:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 36
2022-03-30 20:55:26 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (285858ms till timeout)
2022-03-30 20:55:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 35
2022-03-30 20:55:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 35
2022-03-30 20:55:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 35
2022-03-30 20:55:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 35
2022-03-30 20:55:27 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (284849ms till timeout)
2022-03-30 20:55:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 34
2022-03-30 20:55:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 34
2022-03-30 20:55:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 34
2022-03-30 20:55:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 34
2022-03-30 20:55:28 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (283840ms till timeout)
2022-03-30 20:55:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 33
2022-03-30 20:55:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 33
2022-03-30 20:55:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 33
2022-03-30 20:55:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 33
2022-03-30 20:55:29 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (282829ms till timeout)
2022-03-30 20:55:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 32
2022-03-30 20:55:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 32
2022-03-30 20:55:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 32
2022-03-30 20:55:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 32
2022-03-30 20:55:30 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (281820ms till timeout)
2022-03-30 20:55:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 31
2022-03-30 20:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 31
2022-03-30 20:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 31
2022-03-30 20:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 31
2022-03-30 20:55:31 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (280769ms till timeout)
2022-03-30 20:55:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 30
2022-03-30 20:55:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 30
2022-03-30 20:55:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 30
2022-03-30 20:55:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 30
2022-03-30 20:55:32 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (279759ms till timeout)
2022-03-30 20:55:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 29
2022-03-30 20:55:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 29
2022-03-30 20:55:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 29
2022-03-30 20:55:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 29
2022-03-30 20:55:33 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (278750ms till timeout)
2022-03-30 20:55:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 28
2022-03-30 20:55:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 28
2022-03-30 20:55:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 28
2022-03-30 20:55:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 28
2022-03-30 20:55:34 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (277741ms till timeout)
2022-03-30 20:55:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 27
2022-03-30 20:55:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 27
2022-03-30 20:55:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 27
2022-03-30 20:55:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 27
2022-03-30 20:55:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (276731ms till timeout)
2022-03-30 20:55:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:55:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 26
2022-03-30 20:55:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 26
2022-03-30 20:55:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 26
2022-03-30 20:55:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 26
2022-03-30 20:55:36 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (275722ms till timeout)
2022-03-30 20:55:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 25
2022-03-30 20:55:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 25
2022-03-30 20:55:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 25
2022-03-30 20:55:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 25
2022-03-30 20:55:37 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (274713ms till timeout)
2022-03-30 20:55:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 24
2022-03-30 20:55:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 24
2022-03-30 20:55:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 24
2022-03-30 20:55:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 24
2022-03-30 20:55:38 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (273704ms till timeout)
2022-03-30 20:55:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 23
2022-03-30 20:55:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 23
2022-03-30 20:55:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 23
2022-03-30 20:55:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 23
2022-03-30 20:55:39 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (272694ms till timeout)
2022-03-30 20:55:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 22
2022-03-30 20:55:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 22
2022-03-30 20:55:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 22
2022-03-30 20:55:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 22
2022-03-30 20:55:40 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (271685ms till timeout)
2022-03-30 20:55:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:55:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 21
2022-03-30 20:55:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 21
2022-03-30 20:55:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 21
2022-03-30 20:55:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 21
2022-03-30 20:55:41 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (270676ms till timeout)
2022-03-30 20:55:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 20
2022-03-30 20:55:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 20
2022-03-30 20:55:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 20
2022-03-30 20:55:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 20
2022-03-30 20:55:42 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (269667ms till timeout)
2022-03-30 20:55:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 19
2022-03-30 20:55:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 19
2022-03-30 20:55:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 19
2022-03-30 20:55:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 19
2022-03-30 20:55:43 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (268657ms till timeout)
2022-03-30 20:55:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 18
2022-03-30 20:55:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 18
2022-03-30 20:55:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 18
2022-03-30 20:55:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 18
2022-03-30 20:55:44 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (267648ms till timeout)
2022-03-30 20:55:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 17
2022-03-30 20:55:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 17
2022-03-30 20:55:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 17
2022-03-30 20:55:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 17
2022-03-30 20:55:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (266638ms till timeout)
2022-03-30 20:55:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:55:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 16
2022-03-30 20:55:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 16
2022-03-30 20:55:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 16
2022-03-30 20:55:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 16
2022-03-30 20:55:46 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (265629ms till timeout)
2022-03-30 20:55:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 15
2022-03-30 20:55:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 15
2022-03-30 20:55:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 15
2022-03-30 20:55:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 15
2022-03-30 20:55:47 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (264620ms till timeout)
2022-03-30 20:55:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 14
2022-03-30 20:55:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 14
2022-03-30 20:55:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 14
2022-03-30 20:55:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 14
2022-03-30 20:55:48 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (263610ms till timeout)
2022-03-30 20:55:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 13
2022-03-30 20:55:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 13
2022-03-30 20:55:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 13
2022-03-30 20:55:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 13
2022-03-30 20:55:49 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (262600ms till timeout)
2022-03-30 20:55:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:55:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 12
2022-03-30 20:55:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 12
2022-03-30 20:55:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 12
2022-03-30 20:55:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 12
2022-03-30 20:55:50 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (261591ms till timeout)
2022-03-30 20:55:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 11
2022-03-30 20:55:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 11
2022-03-30 20:55:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 11
2022-03-30 20:55:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 11
2022-03-30 20:55:51 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (260581ms till timeout)
2022-03-30 20:55:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 10
2022-03-30 20:55:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 10
2022-03-30 20:55:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 10
2022-03-30 20:55:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 10
2022-03-30 20:55:52 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (259572ms till timeout)
2022-03-30 20:55:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 9
2022-03-30 20:55:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 9
2022-03-30 20:55:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 9
2022-03-30 20:55:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 9
2022-03-30 20:55:53 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (258562ms till timeout)
2022-03-30 20:55:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 8
2022-03-30 20:55:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 8
2022-03-30 20:55:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 8
2022-03-30 20:55:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 8
2022-03-30 20:55:54 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (257553ms till timeout)
2022-03-30 20:55:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:55:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 7
2022-03-30 20:55:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 7
2022-03-30 20:55:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 7
2022-03-30 20:55:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 7
2022-03-30 20:55:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (256544ms till timeout)
2022-03-30 20:55:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 6
2022-03-30 20:55:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 6
2022-03-30 20:55:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 6
2022-03-30 20:55:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 6
2022-03-30 20:55:56 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (255535ms till timeout)
2022-03-30 20:55:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 5
2022-03-30 20:55:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 5
2022-03-30 20:55:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 5
2022-03-30 20:55:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 5
2022-03-30 20:55:57 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (254525ms till timeout)
2022-03-30 20:55:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 4
2022-03-30 20:55:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 4
2022-03-30 20:55:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 4
2022-03-30 20:55:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 4
2022-03-30 20:55:58 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (253516ms till timeout)
2022-03-30 20:55:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 3
2022-03-30 20:55:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 3
2022-03-30 20:55:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 3
2022-03-30 20:55:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 3
2022-03-30 20:55:59 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (252506ms till timeout)
2022-03-30 20:56:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 2
2022-03-30 20:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 2
2022-03-30 20:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 2
2022-03-30 20:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 2
2022-03-30 20:56:00 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Pods stability in phase Running not ready, will try again in 1000 ms (251497ms till timeout)
2022-03-30 20:56:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-84c2x is in the Running state. Remaining seconds pod to be stable 1
2022-03-30 20:56:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 1
2022-03-30 20:56:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll is in the Running state. Remaining seconds pod to be stable 1
2022-03-30 20:56:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 1
2022-03-30 20:56:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:335] All pods are stable second-kafka-cluster-entity-operator-5f8949dc9c-84c2x ,second-kafka-cluster-kafka-0 ,second-kafka-cluster-kafka-exporter-6dfb7ccc69-cd7ll ,second-kafka-cluster-zookeeper-0
2022-03-30 20:56:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 20:56:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 20:56:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:56:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context testKafkaMetricsSettings is everything deleted.
2022-03-30 20:56:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:56:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:267] testKafkaMetricsSettings - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendSimpleMessageTls, testUpdateUser, testCruiseControlBasicAPIRequests, testReceiveSimpleMessageTls, testSendMessagesTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testKafkaExporterDifferentSetting, testKafkaConnectResponse, testKafkaConnectRequests, testTopicOperatorMetrics, testCruiseControlMetrics, testClusterOperatorMetrics, testKafkaMetricsSettings] to and randomly select one to start execution
2022-03-30 20:56:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaMetricsSettings
2022-03-30 20:56:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 0
2022-03-30 20:56:01 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaMetricsSettings-FINISHED
2022-03-30 20:56:01 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 20:56:01 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 20:56:01 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaBridgeMetrics-STARTED
2022-03-30 20:56:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 20:56:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 20:56:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaBridgeMetrics
2022-03-30 20:56:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 1
2022-03-30 20:56:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:230] testKafkaBridgeMetrics test now can proceed its execution
2022-03-30 20:56:01 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 20:56:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff, testKafkaConnectResponse=my-cluster-2fe1834d, testKafkaBridgeMetrics=my-cluster-9178bcb3, testSendSimpleMessageTls=my-cluster-ca7325d7, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e, testKafkaExporterDataAfterExchange=my-cluster-e86a0839, testUpdateUser=my-cluster-6e6fa8fe, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d, testTopicOperatorMetrics=my-cluster-216189cb, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37, testCruiseControlBasicAPIRequests=my-cluster-8aed4745, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba, testKafkaConnectRequests=my-cluster-d1a00fce, testSendMessagesTlsScramSha=my-cluster-84882f67, testCruiseControlMetrics=my-cluster-f49dfd8c, testReceiveSimpleMessageTls=my-cluster-2e450144, testClusterOperatorMetrics=my-cluster-cfe17ead, testKafkaMetricsSettings=my-cluster-b7e9a534, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273}
2022-03-30 20:56:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-838849261-890460734, testKafkaConnectResponse=my-user-1873693835-1689263597, testKafkaBridgeMetrics=my-user-829221238-874616880, testSendSimpleMessageTls=my-user-1309491966-2015599610, testReconcileStateMetricInTopicOperator=my-user-2124671027-1015880288, testKafkaExporterDataAfterExchange=my-user-766848479-1588072395, testUpdateUser=my-user-1387197422-925410353, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1886945581-244087591, testTopicOperatorMetrics=my-user-779424699-479796683, testSendMessagesCustomListenerTlsScramSha=my-user-1118022021-805525218, testCruiseControlBasicAPIRequests=my-user-1861484466-561878159, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1851888062-673454065, testKafkaExporterDifferentSetting=my-user-988491821-279531432, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1731919654-539989781, testKafkaConnectRequests=my-user-1167366435-1145507543, testSendMessagesTlsScramSha=my-user-586569987-2105688079, testCruiseControlMetrics=my-user-958954083-1578303162, testReceiveSimpleMessageTls=my-user-1669668617-1031881775, testClusterOperatorMetrics=my-user-467081550-2107716793, testKafkaMetricsSettings=my-user-1339734863-677347400, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677, testRackAwareConnectCorrectDeployment=my-user-1051420592-1240796798}
2022-03-30 20:56:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-2138084746-704381117, testKafkaConnectResponse=my-topic-680219046-979906955, testKafkaBridgeMetrics=my-topic-1063894720-254889472, testSendSimpleMessageTls=my-topic-435427300-917779110, testReconcileStateMetricInTopicOperator=my-topic-1267576332-2046216344, testKafkaExporterDataAfterExchange=my-topic-1937812163-1733130103, testUpdateUser=my-topic-1875783706-1763804507, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-306860904-106838350, testTopicOperatorMetrics=my-topic-1965542273-995674430, testSendMessagesCustomListenerTlsScramSha=my-topic-963888555-1712711326, testCruiseControlBasicAPIRequests=my-topic-1631570507-1308241024, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-307985263-926887950, testKafkaExporterDifferentSetting=my-topic-568145531-1290731867, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1672280868-1119644965, testKafkaConnectRequests=my-topic-873062135-1706759978, testSendMessagesTlsScramSha=my-topic-255809133-823300208, testCruiseControlMetrics=my-topic-1917112707-1243703175, testReceiveSimpleMessageTls=my-topic-368564676-637091599, testClusterOperatorMetrics=my-topic-1416314702-1707243111, testKafkaMetricsSettings=my-topic-256215769-877131003, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206, testRackAwareConnectCorrectDeployment=my-topic-189499239-153088857}
2022-03-30 20:56:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff-kafka-clients, testKafkaConnectResponse=my-cluster-2fe1834d-kafka-clients, testKafkaBridgeMetrics=my-cluster-9178bcb3-kafka-clients, testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e-kafka-clients, testKafkaExporterDataAfterExchange=my-cluster-e86a0839-kafka-clients, testUpdateUser=my-cluster-6e6fa8fe-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d-kafka-clients, testTopicOperatorMetrics=my-cluster-216189cb-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-8aed4745-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634-kafka-clients, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba-kafka-clients, testKafkaConnectRequests=my-cluster-d1a00fce-kafka-clients, testSendMessagesTlsScramSha=my-cluster-84882f67-kafka-clients, testCruiseControlMetrics=my-cluster-f49dfd8c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-2e450144-kafka-clients, testClusterOperatorMetrics=my-cluster-cfe17ead-kafka-clients, testKafkaMetricsSettings=my-cluster-b7e9a534-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273-kafka-clients}
2022-03-30 20:56:01 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-30 20:56:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job bridge-producer in namespace infra-namespace
2022-03-30 20:56:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:bridge-producer
2022-03-30 20:56:01 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: bridge-producer will be in active state
2022-03-30 20:56:01 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for job active
2022-03-30 20:56:01 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] job active not ready, will try again in 1000 ms (179992ms till timeout)
2022-03-30 20:56:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job bridge-consumer in namespace infra-namespace
2022-03-30 20:56:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:bridge-consumer
2022-03-30 20:56:02 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: bridge-consumer will be in active state
2022-03-30 20:56:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for job active
2022-03-30 20:56:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaProducer metrics will be available
2022-03-30 20:56:02 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:420] Looking for 'strimzi_bridge_kafka_producer_count' in bridge metrics
2022-03-30 20:56:02 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 20:56:02 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-84mgz -n infra-namespace -- curl 172.17.0.22:8080/metrics
2022-03-30 20:56:03 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.22 from Pod infra-namespace-kafka-clients-748578f786-84mgz finished with return code: 0
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testUserOperatorMetrics-STARTED
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testUserOperatorMetrics
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 2
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:230] testUserOperatorMetrics test now can proceed its execution
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 20:56:03 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] KafkaProducer metrics will be available not ready, will try again in 1000 ms (299473ms till timeout)
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff, testKafkaConnectResponse=my-cluster-2fe1834d, testKafkaBridgeMetrics=my-cluster-9178bcb3, testSendSimpleMessageTls=my-cluster-ca7325d7, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e, testKafkaExporterDataAfterExchange=my-cluster-e86a0839, testUpdateUser=my-cluster-6e6fa8fe, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d, testTopicOperatorMetrics=my-cluster-216189cb, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37, testCruiseControlBasicAPIRequests=my-cluster-8aed4745, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba, testKafkaConnectRequests=my-cluster-d1a00fce, testSendMessagesTlsScramSha=my-cluster-84882f67, testCruiseControlMetrics=my-cluster-f49dfd8c, testReceiveSimpleMessageTls=my-cluster-2e450144, testClusterOperatorMetrics=my-cluster-cfe17ead, testUserOperatorMetrics=my-cluster-d671df34, testKafkaMetricsSettings=my-cluster-b7e9a534, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273}
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-838849261-890460734, testKafkaConnectResponse=my-user-1873693835-1689263597, testKafkaBridgeMetrics=my-user-829221238-874616880, testSendSimpleMessageTls=my-user-1309491966-2015599610, testReconcileStateMetricInTopicOperator=my-user-2124671027-1015880288, testKafkaExporterDataAfterExchange=my-user-766848479-1588072395, testUpdateUser=my-user-1387197422-925410353, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1886945581-244087591, testTopicOperatorMetrics=my-user-779424699-479796683, testSendMessagesCustomListenerTlsScramSha=my-user-1118022021-805525218, testCruiseControlBasicAPIRequests=my-user-1861484466-561878159, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1851888062-673454065, testKafkaExporterDifferentSetting=my-user-988491821-279531432, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1731919654-539989781, testKafkaConnectRequests=my-user-1167366435-1145507543, testSendMessagesTlsScramSha=my-user-586569987-2105688079, testCruiseControlMetrics=my-user-958954083-1578303162, testReceiveSimpleMessageTls=my-user-1669668617-1031881775, testClusterOperatorMetrics=my-user-467081550-2107716793, testUserOperatorMetrics=my-user-1969851044-309723582, testKafkaMetricsSettings=my-user-1339734863-677347400, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677, testRackAwareConnectCorrectDeployment=my-user-1051420592-1240796798}
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-2138084746-704381117, testKafkaConnectResponse=my-topic-680219046-979906955, testKafkaBridgeMetrics=my-topic-1063894720-254889472, testSendSimpleMessageTls=my-topic-435427300-917779110, testReconcileStateMetricInTopicOperator=my-topic-1267576332-2046216344, testKafkaExporterDataAfterExchange=my-topic-1937812163-1733130103, testUpdateUser=my-topic-1875783706-1763804507, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-306860904-106838350, testTopicOperatorMetrics=my-topic-1965542273-995674430, testSendMessagesCustomListenerTlsScramSha=my-topic-963888555-1712711326, testCruiseControlBasicAPIRequests=my-topic-1631570507-1308241024, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-307985263-926887950, testKafkaExporterDifferentSetting=my-topic-568145531-1290731867, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1672280868-1119644965, testKafkaConnectRequests=my-topic-873062135-1706759978, testSendMessagesTlsScramSha=my-topic-255809133-823300208, testCruiseControlMetrics=my-topic-1917112707-1243703175, testReceiveSimpleMessageTls=my-topic-368564676-637091599, testClusterOperatorMetrics=my-topic-1416314702-1707243111, testUserOperatorMetrics=my-topic-1544093300-974581436, testKafkaMetricsSettings=my-topic-256215769-877131003, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206, testRackAwareConnectCorrectDeployment=my-topic-189499239-153088857}
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff-kafka-clients, testKafkaConnectResponse=my-cluster-2fe1834d-kafka-clients, testKafkaBridgeMetrics=my-cluster-9178bcb3-kafka-clients, testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e-kafka-clients, testKafkaExporterDataAfterExchange=my-cluster-e86a0839-kafka-clients, testUpdateUser=my-cluster-6e6fa8fe-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d-kafka-clients, testTopicOperatorMetrics=my-cluster-216189cb-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-8aed4745-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634-kafka-clients, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba-kafka-clients, testKafkaConnectRequests=my-cluster-d1a00fce-kafka-clients, testSendMessagesTlsScramSha=my-cluster-84882f67-kafka-clients, testCruiseControlMetrics=my-cluster-f49dfd8c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-2e450144-kafka-clients, testClusterOperatorMetrics=my-cluster-cfe17ead-kafka-clients, testUserOperatorMetrics=my-cluster-d671df34-kafka-clients, testKafkaMetricsSettings=my-cluster-b7e9a534-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273-kafka-clients}
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-84mgz -n infra-namespace -- curl 172.17.0.19:8081/metrics
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.19 from Pod infra-namespace-kafka-clients-748578f786-84mgz finished with return code: 0
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:346] In context testUserOperatorMetrics is everything deleted.
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:267] testUserOperatorMetrics - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendSimpleMessageTls, testUpdateUser, testCruiseControlBasicAPIRequests, testReceiveSimpleMessageTls, testSendMessagesTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testKafkaExporterDifferentSetting, testKafkaConnectResponse, testKafkaConnectRequests, testTopicOperatorMetrics, testCruiseControlMetrics, testClusterOperatorMetrics, testKafkaMetricsSettings, testKafkaBridgeMetrics, testUserOperatorMetrics] to and randomly select one to start execution
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testUserOperatorMetrics
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 1
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testUserOperatorMetrics-FINISHED
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testZookeeperWatchersCount-STARTED
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testZookeeperWatchersCount
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 2
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:230] testZookeeperWatchersCount test now can proceed its execution
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff, testKafkaConnectResponse=my-cluster-2fe1834d, testKafkaBridgeMetrics=my-cluster-9178bcb3, testSendSimpleMessageTls=my-cluster-ca7325d7, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e, testKafkaExporterDataAfterExchange=my-cluster-e86a0839, testUpdateUser=my-cluster-6e6fa8fe, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d, testTopicOperatorMetrics=my-cluster-216189cb, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37, testCruiseControlBasicAPIRequests=my-cluster-8aed4745, testZookeeperWatchersCount=my-cluster-22c0f078, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba, testKafkaConnectRequests=my-cluster-d1a00fce, testSendMessagesTlsScramSha=my-cluster-84882f67, testCruiseControlMetrics=my-cluster-f49dfd8c, testReceiveSimpleMessageTls=my-cluster-2e450144, testClusterOperatorMetrics=my-cluster-cfe17ead, testUserOperatorMetrics=my-cluster-d671df34, testKafkaMetricsSettings=my-cluster-b7e9a534, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273}
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-user-838849261-890460734, testKafkaConnectResponse=my-user-1873693835-1689263597, testKafkaBridgeMetrics=my-user-829221238-874616880, testSendSimpleMessageTls=my-user-1309491966-2015599610, testReconcileStateMetricInTopicOperator=my-user-2124671027-1015880288, testKafkaExporterDataAfterExchange=my-user-766848479-1588072395, testUpdateUser=my-user-1387197422-925410353, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1886945581-244087591, testTopicOperatorMetrics=my-user-779424699-479796683, testSendMessagesCustomListenerTlsScramSha=my-user-1118022021-805525218, testCruiseControlBasicAPIRequests=my-user-1861484466-561878159, testZookeeperWatchersCount=my-user-1490797769-719890298, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1851888062-673454065, testKafkaExporterDifferentSetting=my-user-988491821-279531432, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1731919654-539989781, testKafkaConnectRequests=my-user-1167366435-1145507543, testSendMessagesTlsScramSha=my-user-586569987-2105688079, testCruiseControlMetrics=my-user-958954083-1578303162, testReceiveSimpleMessageTls=my-user-1669668617-1031881775, testClusterOperatorMetrics=my-user-467081550-2107716793, testUserOperatorMetrics=my-user-1969851044-309723582, testKafkaMetricsSettings=my-user-1339734863-677347400, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677, testRackAwareConnectCorrectDeployment=my-user-1051420592-1240796798}
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-topic-2138084746-704381117, testKafkaConnectResponse=my-topic-680219046-979906955, testKafkaBridgeMetrics=my-topic-1063894720-254889472, testSendSimpleMessageTls=my-topic-435427300-917779110, testReconcileStateMetricInTopicOperator=my-topic-1267576332-2046216344, testKafkaExporterDataAfterExchange=my-topic-1937812163-1733130103, testUpdateUser=my-topic-1875783706-1763804507, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-306860904-106838350, testTopicOperatorMetrics=my-topic-1965542273-995674430, testSendMessagesCustomListenerTlsScramSha=my-topic-963888555-1712711326, testCruiseControlBasicAPIRequests=my-topic-1631570507-1308241024, testZookeeperWatchersCount=my-topic-1959052705-1334872805, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-307985263-926887950, testKafkaExporterDifferentSetting=my-topic-568145531-1290731867, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1672280868-1119644965, testKafkaConnectRequests=my-topic-873062135-1706759978, testSendMessagesTlsScramSha=my-topic-255809133-823300208, testCruiseControlMetrics=my-topic-1917112707-1243703175, testReceiveSimpleMessageTls=my-topic-368564676-637091599, testClusterOperatorMetrics=my-topic-1416314702-1707243111, testUserOperatorMetrics=my-topic-1544093300-974581436, testKafkaMetricsSettings=my-topic-256215769-877131003, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206, testRackAwareConnectCorrectDeployment=my-topic-189499239-153088857}
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff-kafka-clients, testKafkaConnectResponse=my-cluster-2fe1834d-kafka-clients, testKafkaBridgeMetrics=my-cluster-9178bcb3-kafka-clients, testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e-kafka-clients, testKafkaExporterDataAfterExchange=my-cluster-e86a0839-kafka-clients, testUpdateUser=my-cluster-6e6fa8fe-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d-kafka-clients, testTopicOperatorMetrics=my-cluster-216189cb-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-8aed4745-kafka-clients, testZookeeperWatchersCount=my-cluster-22c0f078-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634-kafka-clients, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba-kafka-clients, testKafkaConnectRequests=my-cluster-d1a00fce-kafka-clients, testSendMessagesTlsScramSha=my-cluster-84882f67-kafka-clients, testCruiseControlMetrics=my-cluster-f49dfd8c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-2e450144-kafka-clients, testClusterOperatorMetrics=my-cluster-cfe17ead-kafka-clients, testUserOperatorMetrics=my-cluster-d671df34-kafka-clients, testKafkaMetricsSettings=my-cluster-b7e9a534-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273-kafka-clients}
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:346] In context testZookeeperWatchersCount is everything deleted.
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:267] testZookeeperWatchersCount - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendSimpleMessageTls, testUpdateUser, testCruiseControlBasicAPIRequests, testReceiveSimpleMessageTls, testSendMessagesTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testKafkaExporterDifferentSetting, testKafkaConnectResponse, testKafkaConnectRequests, testTopicOperatorMetrics, testCruiseControlMetrics, testClusterOperatorMetrics, testKafkaMetricsSettings, testKafkaBridgeMetrics, testUserOperatorMetrics, testZookeeperWatchersCount] to and randomly select one to start execution
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testZookeeperWatchersCount
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 1
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testZookeeperWatchersCount-FINISHED
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaConnectIoNetwork-STARTED
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaConnectIoNetwork
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 2
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:230] testKafkaConnectIoNetwork test now can proceed its execution
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaConnectResponse=my-cluster-2fe1834d, testSendSimpleMessageTls=my-cluster-ca7325d7, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e, testKafkaConnectIoNetwork=my-cluster-2e56d7a8, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37, testCruiseControlBasicAPIRequests=my-cluster-8aed4745, testZookeeperWatchersCount=my-cluster-22c0f078, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba, testSendMessagesTlsScramSha=my-cluster-84882f67, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff, testKafkaBridgeMetrics=my-cluster-9178bcb3, testKafkaExporterDataAfterExchange=my-cluster-e86a0839, testUpdateUser=my-cluster-6e6fa8fe, testTopicOperatorMetrics=my-cluster-216189cb, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60, testKafkaConnectRequests=my-cluster-d1a00fce, testCruiseControlMetrics=my-cluster-f49dfd8c, testReceiveSimpleMessageTls=my-cluster-2e450144, testClusterOperatorMetrics=my-cluster-cfe17ead, testUserOperatorMetrics=my-cluster-d671df34, testKafkaMetricsSettings=my-cluster-b7e9a534}
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaConnectResponse=my-user-1873693835-1689263597, testSendSimpleMessageTls=my-user-1309491966-2015599610, testReconcileStateMetricInTopicOperator=my-user-2124671027-1015880288, testKafkaConnectIoNetwork=my-user-1245775831-860629339, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1886945581-244087591, testSendMessagesCustomListenerTlsScramSha=my-user-1118022021-805525218, testCruiseControlBasicAPIRequests=my-user-1861484466-561878159, testZookeeperWatchersCount=my-user-1490797769-719890298, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1851888062-673454065, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1731919654-539989781, testSendMessagesTlsScramSha=my-user-586569987-2105688079, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677, testRackAwareConnectCorrectDeployment=my-user-1051420592-1240796798, testKafkaAndZookeeperScaleUpScaleDown=my-user-838849261-890460734, testKafkaBridgeMetrics=my-user-829221238-874616880, testKafkaExporterDataAfterExchange=my-user-766848479-1588072395, testUpdateUser=my-user-1387197422-925410353, testTopicOperatorMetrics=my-user-779424699-479796683, testKafkaExporterDifferentSetting=my-user-988491821-279531432, testKafkaConnectRequests=my-user-1167366435-1145507543, testCruiseControlMetrics=my-user-958954083-1578303162, testReceiveSimpleMessageTls=my-user-1669668617-1031881775, testClusterOperatorMetrics=my-user-467081550-2107716793, testUserOperatorMetrics=my-user-1969851044-309723582, testKafkaMetricsSettings=my-user-1339734863-677347400}
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaConnectResponse=my-topic-680219046-979906955, testSendSimpleMessageTls=my-topic-435427300-917779110, testReconcileStateMetricInTopicOperator=my-topic-1267576332-2046216344, testKafkaConnectIoNetwork=my-topic-1593682376-1381222647, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-306860904-106838350, testSendMessagesCustomListenerTlsScramSha=my-topic-963888555-1712711326, testCruiseControlBasicAPIRequests=my-topic-1631570507-1308241024, testZookeeperWatchersCount=my-topic-1959052705-1334872805, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-307985263-926887950, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1672280868-1119644965, testSendMessagesTlsScramSha=my-topic-255809133-823300208, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206, testRackAwareConnectCorrectDeployment=my-topic-189499239-153088857, testKafkaAndZookeeperScaleUpScaleDown=my-topic-2138084746-704381117, testKafkaBridgeMetrics=my-topic-1063894720-254889472, testKafkaExporterDataAfterExchange=my-topic-1937812163-1733130103, testUpdateUser=my-topic-1875783706-1763804507, testTopicOperatorMetrics=my-topic-1965542273-995674430, testKafkaExporterDifferentSetting=my-topic-568145531-1290731867, testKafkaConnectRequests=my-topic-873062135-1706759978, testCruiseControlMetrics=my-topic-1917112707-1243703175, testReceiveSimpleMessageTls=my-topic-368564676-637091599, testClusterOperatorMetrics=my-topic-1416314702-1707243111, testUserOperatorMetrics=my-topic-1544093300-974581436, testKafkaMetricsSettings=my-topic-256215769-877131003}
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaConnectResponse=my-cluster-2fe1834d-kafka-clients, testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e-kafka-clients, testKafkaConnectIoNetwork=my-cluster-2e56d7a8-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-8aed4745-kafka-clients, testZookeeperWatchersCount=my-cluster-22c0f078-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba-kafka-clients, testSendMessagesTlsScramSha=my-cluster-84882f67-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff-kafka-clients, testKafkaBridgeMetrics=my-cluster-9178bcb3-kafka-clients, testKafkaExporterDataAfterExchange=my-cluster-e86a0839-kafka-clients, testUpdateUser=my-cluster-6e6fa8fe-kafka-clients, testTopicOperatorMetrics=my-cluster-216189cb-kafka-clients, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60-kafka-clients, testKafkaConnectRequests=my-cluster-d1a00fce-kafka-clients, testCruiseControlMetrics=my-cluster-f49dfd8c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-2e450144-kafka-clients, testClusterOperatorMetrics=my-cluster-cfe17ead-kafka-clients, testUserOperatorMetrics=my-cluster-d671df34-kafka-clients, testKafkaMetricsSettings=my-cluster-b7e9a534-kafka-clients}
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 20:56:03 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-84mgz -n infra-namespace -- curl 172.17.0.24:9404
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.24 from Pod infra-namespace-kafka-clients-748578f786-84mgz finished with return code: 0
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:346] In context testKafkaConnectIoNetwork is everything deleted.
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:267] testKafkaConnectIoNetwork - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendSimpleMessageTls, testUpdateUser, testCruiseControlBasicAPIRequests, testReceiveSimpleMessageTls, testSendMessagesTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testKafkaExporterDifferentSetting, testKafkaConnectResponse, testKafkaConnectRequests, testTopicOperatorMetrics, testCruiseControlMetrics, testClusterOperatorMetrics, testKafkaMetricsSettings, testKafkaBridgeMetrics, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaConnectIoNetwork] to and randomly select one to start execution
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaConnectIoNetwork
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 1
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaConnectIoNetwork-FINISHED
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaBrokersCount-STARTED
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaBrokersCount
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 2
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:230] testKafkaBrokersCount test now can proceed its execution
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaConnectResponse=my-cluster-2fe1834d, testSendSimpleMessageTls=my-cluster-ca7325d7, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e, testKafkaConnectIoNetwork=my-cluster-2e56d7a8, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37, testCruiseControlBasicAPIRequests=my-cluster-8aed4745, testZookeeperWatchersCount=my-cluster-22c0f078, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba, testSendMessagesTlsScramSha=my-cluster-84882f67, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff, testKafkaBridgeMetrics=my-cluster-9178bcb3, testKafkaExporterDataAfterExchange=my-cluster-e86a0839, testUpdateUser=my-cluster-6e6fa8fe, testTopicOperatorMetrics=my-cluster-216189cb, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60, testKafkaConnectRequests=my-cluster-d1a00fce, testCruiseControlMetrics=my-cluster-f49dfd8c, testReceiveSimpleMessageTls=my-cluster-2e450144, testClusterOperatorMetrics=my-cluster-cfe17ead, testKafkaBrokersCount=my-cluster-a943baf0, testUserOperatorMetrics=my-cluster-d671df34, testKafkaMetricsSettings=my-cluster-b7e9a534}
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaConnectResponse=my-user-1873693835-1689263597, testSendSimpleMessageTls=my-user-1309491966-2015599610, testReconcileStateMetricInTopicOperator=my-user-2124671027-1015880288, testKafkaConnectIoNetwork=my-user-1245775831-860629339, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1886945581-244087591, testSendMessagesCustomListenerTlsScramSha=my-user-1118022021-805525218, testCruiseControlBasicAPIRequests=my-user-1861484466-561878159, testZookeeperWatchersCount=my-user-1490797769-719890298, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1851888062-673454065, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1731919654-539989781, testSendMessagesTlsScramSha=my-user-586569987-2105688079, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677, testRackAwareConnectCorrectDeployment=my-user-1051420592-1240796798, testKafkaAndZookeeperScaleUpScaleDown=my-user-838849261-890460734, testKafkaBridgeMetrics=my-user-829221238-874616880, testKafkaExporterDataAfterExchange=my-user-766848479-1588072395, testUpdateUser=my-user-1387197422-925410353, testTopicOperatorMetrics=my-user-779424699-479796683, testKafkaExporterDifferentSetting=my-user-988491821-279531432, testKafkaConnectRequests=my-user-1167366435-1145507543, testCruiseControlMetrics=my-user-958954083-1578303162, testReceiveSimpleMessageTls=my-user-1669668617-1031881775, testClusterOperatorMetrics=my-user-467081550-2107716793, testKafkaBrokersCount=my-user-114552180-150189557, testUserOperatorMetrics=my-user-1969851044-309723582, testKafkaMetricsSettings=my-user-1339734863-677347400}
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaConnectResponse=my-topic-680219046-979906955, testSendSimpleMessageTls=my-topic-435427300-917779110, testReconcileStateMetricInTopicOperator=my-topic-1267576332-2046216344, testKafkaConnectIoNetwork=my-topic-1593682376-1381222647, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-306860904-106838350, testSendMessagesCustomListenerTlsScramSha=my-topic-963888555-1712711326, testCruiseControlBasicAPIRequests=my-topic-1631570507-1308241024, testZookeeperWatchersCount=my-topic-1959052705-1334872805, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-307985263-926887950, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1672280868-1119644965, testSendMessagesTlsScramSha=my-topic-255809133-823300208, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206, testRackAwareConnectCorrectDeployment=my-topic-189499239-153088857, testKafkaAndZookeeperScaleUpScaleDown=my-topic-2138084746-704381117, testKafkaBridgeMetrics=my-topic-1063894720-254889472, testKafkaExporterDataAfterExchange=my-topic-1937812163-1733130103, testUpdateUser=my-topic-1875783706-1763804507, testTopicOperatorMetrics=my-topic-1965542273-995674430, testKafkaExporterDifferentSetting=my-topic-568145531-1290731867, testKafkaConnectRequests=my-topic-873062135-1706759978, testCruiseControlMetrics=my-topic-1917112707-1243703175, testReceiveSimpleMessageTls=my-topic-368564676-637091599, testClusterOperatorMetrics=my-topic-1416314702-1707243111, testKafkaBrokersCount=my-topic-222235994-1274701867, testUserOperatorMetrics=my-topic-1544093300-974581436, testKafkaMetricsSettings=my-topic-256215769-877131003}
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaConnectResponse=my-cluster-2fe1834d-kafka-clients, testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e-kafka-clients, testKafkaConnectIoNetwork=my-cluster-2e56d7a8-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-8aed4745-kafka-clients, testZookeeperWatchersCount=my-cluster-22c0f078-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba-kafka-clients, testSendMessagesTlsScramSha=my-cluster-84882f67-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff-kafka-clients, testKafkaBridgeMetrics=my-cluster-9178bcb3-kafka-clients, testKafkaExporterDataAfterExchange=my-cluster-e86a0839-kafka-clients, testUpdateUser=my-cluster-6e6fa8fe-kafka-clients, testTopicOperatorMetrics=my-cluster-216189cb-kafka-clients, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60-kafka-clients, testKafkaConnectRequests=my-cluster-d1a00fce-kafka-clients, testCruiseControlMetrics=my-cluster-f49dfd8c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-2e450144-kafka-clients, testClusterOperatorMetrics=my-cluster-cfe17ead-kafka-clients, testKafkaBrokersCount=my-cluster-a943baf0-kafka-clients, testUserOperatorMetrics=my-cluster-d671df34-kafka-clients, testKafkaMetricsSettings=my-cluster-b7e9a534-kafka-clients}
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:346] In context testKafkaBrokersCount is everything deleted.
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:267] testKafkaBrokersCount - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendSimpleMessageTls, testUpdateUser, testCruiseControlBasicAPIRequests, testReceiveSimpleMessageTls, testSendMessagesTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testKafkaExporterDifferentSetting, testKafkaConnectResponse, testKafkaConnectRequests, testTopicOperatorMetrics, testCruiseControlMetrics, testClusterOperatorMetrics, testKafkaMetricsSettings, testKafkaBridgeMetrics, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaConnectIoNetwork, testKafkaBrokersCount] to and randomly select one to start execution
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaBrokersCount
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 1
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaBrokersCount-FINISHED
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaActiveControllers-STARTED
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaActiveControllers
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 2
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:230] testKafkaActiveControllers test now can proceed its execution
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaConnectResponse=my-cluster-2fe1834d, testSendSimpleMessageTls=my-cluster-ca7325d7, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e, testKafkaConnectIoNetwork=my-cluster-2e56d7a8, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37, testCruiseControlBasicAPIRequests=my-cluster-8aed4745, testZookeeperWatchersCount=my-cluster-22c0f078, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba, testSendMessagesTlsScramSha=my-cluster-84882f67, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff, testKafkaBridgeMetrics=my-cluster-9178bcb3, testKafkaExporterDataAfterExchange=my-cluster-e86a0839, testUpdateUser=my-cluster-6e6fa8fe, testTopicOperatorMetrics=my-cluster-216189cb, testKafkaActiveControllers=my-cluster-ede2d53e, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60, testKafkaConnectRequests=my-cluster-d1a00fce, testCruiseControlMetrics=my-cluster-f49dfd8c, testReceiveSimpleMessageTls=my-cluster-2e450144, testClusterOperatorMetrics=my-cluster-cfe17ead, testKafkaBrokersCount=my-cluster-a943baf0, testUserOperatorMetrics=my-cluster-d671df34, testKafkaMetricsSettings=my-cluster-b7e9a534}
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaConnectResponse=my-user-1873693835-1689263597, testSendSimpleMessageTls=my-user-1309491966-2015599610, testReconcileStateMetricInTopicOperator=my-user-2124671027-1015880288, testKafkaConnectIoNetwork=my-user-1245775831-860629339, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1886945581-244087591, testSendMessagesCustomListenerTlsScramSha=my-user-1118022021-805525218, testCruiseControlBasicAPIRequests=my-user-1861484466-561878159, testZookeeperWatchersCount=my-user-1490797769-719890298, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1851888062-673454065, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1731919654-539989781, testSendMessagesTlsScramSha=my-user-586569987-2105688079, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677, testRackAwareConnectCorrectDeployment=my-user-1051420592-1240796798, testKafkaAndZookeeperScaleUpScaleDown=my-user-838849261-890460734, testKafkaBridgeMetrics=my-user-829221238-874616880, testKafkaExporterDataAfterExchange=my-user-766848479-1588072395, testUpdateUser=my-user-1387197422-925410353, testTopicOperatorMetrics=my-user-779424699-479796683, testKafkaActiveControllers=my-user-1066197147-6205877, testKafkaExporterDifferentSetting=my-user-988491821-279531432, testKafkaConnectRequests=my-user-1167366435-1145507543, testCruiseControlMetrics=my-user-958954083-1578303162, testReceiveSimpleMessageTls=my-user-1669668617-1031881775, testClusterOperatorMetrics=my-user-467081550-2107716793, testKafkaBrokersCount=my-user-114552180-150189557, testUserOperatorMetrics=my-user-1969851044-309723582, testKafkaMetricsSettings=my-user-1339734863-677347400}
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaConnectResponse=my-topic-680219046-979906955, testSendSimpleMessageTls=my-topic-435427300-917779110, testReconcileStateMetricInTopicOperator=my-topic-1267576332-2046216344, testKafkaConnectIoNetwork=my-topic-1593682376-1381222647, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-306860904-106838350, testSendMessagesCustomListenerTlsScramSha=my-topic-963888555-1712711326, testCruiseControlBasicAPIRequests=my-topic-1631570507-1308241024, testZookeeperWatchersCount=my-topic-1959052705-1334872805, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-307985263-926887950, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1672280868-1119644965, testSendMessagesTlsScramSha=my-topic-255809133-823300208, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206, testRackAwareConnectCorrectDeployment=my-topic-189499239-153088857, testKafkaAndZookeeperScaleUpScaleDown=my-topic-2138084746-704381117, testKafkaBridgeMetrics=my-topic-1063894720-254889472, testKafkaExporterDataAfterExchange=my-topic-1937812163-1733130103, testUpdateUser=my-topic-1875783706-1763804507, testTopicOperatorMetrics=my-topic-1965542273-995674430, testKafkaActiveControllers=my-topic-1135110290-1277775808, testKafkaExporterDifferentSetting=my-topic-568145531-1290731867, testKafkaConnectRequests=my-topic-873062135-1706759978, testCruiseControlMetrics=my-topic-1917112707-1243703175, testReceiveSimpleMessageTls=my-topic-368564676-637091599, testClusterOperatorMetrics=my-topic-1416314702-1707243111, testKafkaBrokersCount=my-topic-222235994-1274701867, testUserOperatorMetrics=my-topic-1544093300-974581436, testKafkaMetricsSettings=my-topic-256215769-877131003}
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaConnectResponse=my-cluster-2fe1834d-kafka-clients, testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e-kafka-clients, testKafkaConnectIoNetwork=my-cluster-2e56d7a8-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-8aed4745-kafka-clients, testZookeeperWatchersCount=my-cluster-22c0f078-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba-kafka-clients, testSendMessagesTlsScramSha=my-cluster-84882f67-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff-kafka-clients, testKafkaBridgeMetrics=my-cluster-9178bcb3-kafka-clients, testKafkaExporterDataAfterExchange=my-cluster-e86a0839-kafka-clients, testUpdateUser=my-cluster-6e6fa8fe-kafka-clients, testTopicOperatorMetrics=my-cluster-216189cb-kafka-clients, testKafkaActiveControllers=my-cluster-ede2d53e-kafka-clients, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60-kafka-clients, testKafkaConnectRequests=my-cluster-d1a00fce-kafka-clients, testCruiseControlMetrics=my-cluster-f49dfd8c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-2e450144-kafka-clients, testClusterOperatorMetrics=my-cluster-cfe17ead-kafka-clients, testKafkaBrokersCount=my-cluster-a943baf0-kafka-clients, testUserOperatorMetrics=my-cluster-d671df34-kafka-clients, testKafkaMetricsSettings=my-cluster-b7e9a534-kafka-clients}
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:346] In context testKafkaActiveControllers is everything deleted.
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:267] testKafkaActiveControllers - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendSimpleMessageTls, testUpdateUser, testCruiseControlBasicAPIRequests, testReceiveSimpleMessageTls, testSendMessagesTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testKafkaExporterDifferentSetting, testKafkaConnectResponse, testKafkaConnectRequests, testTopicOperatorMetrics, testCruiseControlMetrics, testClusterOperatorMetrics, testKafkaMetricsSettings, testKafkaBridgeMetrics, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaConnectIoNetwork, testKafkaBrokersCount, testKafkaActiveControllers] to and randomly select one to start execution
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaActiveControllers
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 1
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaActiveControllers-FINISHED
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaTopicUnderReplicatedPartitions-STARTED
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaTopicUnderReplicatedPartitions
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 2
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:230] testKafkaTopicUnderReplicatedPartitions test now can proceed its execution
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaConnectResponse=my-cluster-2fe1834d, testSendSimpleMessageTls=my-cluster-ca7325d7, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e, testKafkaConnectIoNetwork=my-cluster-2e56d7a8, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37, testCruiseControlBasicAPIRequests=my-cluster-8aed4745, testZookeeperWatchersCount=my-cluster-22c0f078, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba, testKafkaTopicUnderReplicatedPartitions=my-cluster-d686e2cd, testSendMessagesTlsScramSha=my-cluster-84882f67, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff, testKafkaBridgeMetrics=my-cluster-9178bcb3, testKafkaExporterDataAfterExchange=my-cluster-e86a0839, testUpdateUser=my-cluster-6e6fa8fe, testTopicOperatorMetrics=my-cluster-216189cb, testKafkaActiveControllers=my-cluster-ede2d53e, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60, testKafkaConnectRequests=my-cluster-d1a00fce, testCruiseControlMetrics=my-cluster-f49dfd8c, testReceiveSimpleMessageTls=my-cluster-2e450144, testClusterOperatorMetrics=my-cluster-cfe17ead, testKafkaBrokersCount=my-cluster-a943baf0, testUserOperatorMetrics=my-cluster-d671df34, testKafkaMetricsSettings=my-cluster-b7e9a534}
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaConnectResponse=my-user-1873693835-1689263597, testSendSimpleMessageTls=my-user-1309491966-2015599610, testReconcileStateMetricInTopicOperator=my-user-2124671027-1015880288, testKafkaConnectIoNetwork=my-user-1245775831-860629339, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1886945581-244087591, testSendMessagesCustomListenerTlsScramSha=my-user-1118022021-805525218, testCruiseControlBasicAPIRequests=my-user-1861484466-561878159, testZookeeperWatchersCount=my-user-1490797769-719890298, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1851888062-673454065, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1731919654-539989781, testKafkaTopicUnderReplicatedPartitions=my-user-1731286414-663678983, testSendMessagesTlsScramSha=my-user-586569987-2105688079, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677, testRackAwareConnectCorrectDeployment=my-user-1051420592-1240796798, testKafkaAndZookeeperScaleUpScaleDown=my-user-838849261-890460734, testKafkaBridgeMetrics=my-user-829221238-874616880, testKafkaExporterDataAfterExchange=my-user-766848479-1588072395, testUpdateUser=my-user-1387197422-925410353, testTopicOperatorMetrics=my-user-779424699-479796683, testKafkaActiveControllers=my-user-1066197147-6205877, testKafkaExporterDifferentSetting=my-user-988491821-279531432, testKafkaConnectRequests=my-user-1167366435-1145507543, testCruiseControlMetrics=my-user-958954083-1578303162, testReceiveSimpleMessageTls=my-user-1669668617-1031881775, testClusterOperatorMetrics=my-user-467081550-2107716793, testKafkaBrokersCount=my-user-114552180-150189557, testUserOperatorMetrics=my-user-1969851044-309723582, testKafkaMetricsSettings=my-user-1339734863-677347400}
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaConnectResponse=my-topic-680219046-979906955, testSendSimpleMessageTls=my-topic-435427300-917779110, testReconcileStateMetricInTopicOperator=my-topic-1267576332-2046216344, testKafkaConnectIoNetwork=my-topic-1593682376-1381222647, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-306860904-106838350, testSendMessagesCustomListenerTlsScramSha=my-topic-963888555-1712711326, testCruiseControlBasicAPIRequests=my-topic-1631570507-1308241024, testZookeeperWatchersCount=my-topic-1959052705-1334872805, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-307985263-926887950, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1672280868-1119644965, testKafkaTopicUnderReplicatedPartitions=my-topic-2103629457-772081434, testSendMessagesTlsScramSha=my-topic-255809133-823300208, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206, testRackAwareConnectCorrectDeployment=my-topic-189499239-153088857, testKafkaAndZookeeperScaleUpScaleDown=my-topic-2138084746-704381117, testKafkaBridgeMetrics=my-topic-1063894720-254889472, testKafkaExporterDataAfterExchange=my-topic-1937812163-1733130103, testUpdateUser=my-topic-1875783706-1763804507, testTopicOperatorMetrics=my-topic-1965542273-995674430, testKafkaActiveControllers=my-topic-1135110290-1277775808, testKafkaExporterDifferentSetting=my-topic-568145531-1290731867, testKafkaConnectRequests=my-topic-873062135-1706759978, testCruiseControlMetrics=my-topic-1917112707-1243703175, testReceiveSimpleMessageTls=my-topic-368564676-637091599, testClusterOperatorMetrics=my-topic-1416314702-1707243111, testKafkaBrokersCount=my-topic-222235994-1274701867, testUserOperatorMetrics=my-topic-1544093300-974581436, testKafkaMetricsSettings=my-topic-256215769-877131003}
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaConnectResponse=my-cluster-2fe1834d-kafka-clients, testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e-kafka-clients, testKafkaConnectIoNetwork=my-cluster-2e56d7a8-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-8aed4745-kafka-clients, testZookeeperWatchersCount=my-cluster-22c0f078-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-d686e2cd-kafka-clients, testSendMessagesTlsScramSha=my-cluster-84882f67-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff-kafka-clients, testKafkaBridgeMetrics=my-cluster-9178bcb3-kafka-clients, testKafkaExporterDataAfterExchange=my-cluster-e86a0839-kafka-clients, testUpdateUser=my-cluster-6e6fa8fe-kafka-clients, testTopicOperatorMetrics=my-cluster-216189cb-kafka-clients, testKafkaActiveControllers=my-cluster-ede2d53e-kafka-clients, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60-kafka-clients, testKafkaConnectRequests=my-cluster-d1a00fce-kafka-clients, testCruiseControlMetrics=my-cluster-f49dfd8c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-2e450144-kafka-clients, testClusterOperatorMetrics=my-cluster-cfe17ead-kafka-clients, testKafkaBrokersCount=my-cluster-a943baf0-kafka-clients, testUserOperatorMetrics=my-cluster-d671df34-kafka-clients, testKafkaMetricsSettings=my-cluster-b7e9a534-kafka-clients}
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:346] In context testKafkaTopicUnderReplicatedPartitions is everything deleted.
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:267] testKafkaTopicUnderReplicatedPartitions - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendSimpleMessageTls, testUpdateUser, testCruiseControlBasicAPIRequests, testReceiveSimpleMessageTls, testSendMessagesTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testKafkaExporterDifferentSetting, testKafkaConnectResponse, testKafkaConnectRequests, testTopicOperatorMetrics, testCruiseControlMetrics, testClusterOperatorMetrics, testKafkaMetricsSettings, testKafkaBridgeMetrics, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaConnectIoNetwork, testKafkaBrokersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions] to and randomly select one to start execution
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaTopicUnderReplicatedPartitions
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 1
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaTopicUnderReplicatedPartitions-FINISHED
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testZookeeperQuorumSize-STARTED
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testZookeeperQuorumSize
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 2
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:230] testZookeeperQuorumSize test now can proceed its execution
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaConnectResponse=my-cluster-2fe1834d, testSendSimpleMessageTls=my-cluster-ca7325d7, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e, testKafkaConnectIoNetwork=my-cluster-2e56d7a8, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37, testCruiseControlBasicAPIRequests=my-cluster-8aed4745, testZookeeperWatchersCount=my-cluster-22c0f078, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba, testKafkaTopicUnderReplicatedPartitions=my-cluster-d686e2cd, testSendMessagesTlsScramSha=my-cluster-84882f67, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273, testZookeeperQuorumSize=my-cluster-16088501, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff, testKafkaBridgeMetrics=my-cluster-9178bcb3, testKafkaExporterDataAfterExchange=my-cluster-e86a0839, testUpdateUser=my-cluster-6e6fa8fe, testTopicOperatorMetrics=my-cluster-216189cb, testKafkaActiveControllers=my-cluster-ede2d53e, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60, testKafkaConnectRequests=my-cluster-d1a00fce, testCruiseControlMetrics=my-cluster-f49dfd8c, testReceiveSimpleMessageTls=my-cluster-2e450144, testClusterOperatorMetrics=my-cluster-cfe17ead, testKafkaBrokersCount=my-cluster-a943baf0, testUserOperatorMetrics=my-cluster-d671df34, testKafkaMetricsSettings=my-cluster-b7e9a534}
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaConnectResponse=my-user-1873693835-1689263597, testSendSimpleMessageTls=my-user-1309491966-2015599610, testReconcileStateMetricInTopicOperator=my-user-2124671027-1015880288, testKafkaConnectIoNetwork=my-user-1245775831-860629339, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1886945581-244087591, testSendMessagesCustomListenerTlsScramSha=my-user-1118022021-805525218, testCruiseControlBasicAPIRequests=my-user-1861484466-561878159, testZookeeperWatchersCount=my-user-1490797769-719890298, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1851888062-673454065, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1731919654-539989781, testKafkaTopicUnderReplicatedPartitions=my-user-1731286414-663678983, testSendMessagesTlsScramSha=my-user-586569987-2105688079, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677, testRackAwareConnectCorrectDeployment=my-user-1051420592-1240796798, testZookeeperQuorumSize=my-user-2117010560-1601663415, testKafkaAndZookeeperScaleUpScaleDown=my-user-838849261-890460734, testKafkaBridgeMetrics=my-user-829221238-874616880, testKafkaExporterDataAfterExchange=my-user-766848479-1588072395, testUpdateUser=my-user-1387197422-925410353, testTopicOperatorMetrics=my-user-779424699-479796683, testKafkaActiveControllers=my-user-1066197147-6205877, testKafkaExporterDifferentSetting=my-user-988491821-279531432, testKafkaConnectRequests=my-user-1167366435-1145507543, testCruiseControlMetrics=my-user-958954083-1578303162, testReceiveSimpleMessageTls=my-user-1669668617-1031881775, testClusterOperatorMetrics=my-user-467081550-2107716793, testKafkaBrokersCount=my-user-114552180-150189557, testUserOperatorMetrics=my-user-1969851044-309723582, testKafkaMetricsSettings=my-user-1339734863-677347400}
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaConnectResponse=my-topic-680219046-979906955, testSendSimpleMessageTls=my-topic-435427300-917779110, testReconcileStateMetricInTopicOperator=my-topic-1267576332-2046216344, testKafkaConnectIoNetwork=my-topic-1593682376-1381222647, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-306860904-106838350, testSendMessagesCustomListenerTlsScramSha=my-topic-963888555-1712711326, testCruiseControlBasicAPIRequests=my-topic-1631570507-1308241024, testZookeeperWatchersCount=my-topic-1959052705-1334872805, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-307985263-926887950, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1672280868-1119644965, testKafkaTopicUnderReplicatedPartitions=my-topic-2103629457-772081434, testSendMessagesTlsScramSha=my-topic-255809133-823300208, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206, testRackAwareConnectCorrectDeployment=my-topic-189499239-153088857, testZookeeperQuorumSize=my-topic-192478281-1148042900, testKafkaAndZookeeperScaleUpScaleDown=my-topic-2138084746-704381117, testKafkaBridgeMetrics=my-topic-1063894720-254889472, testKafkaExporterDataAfterExchange=my-topic-1937812163-1733130103, testUpdateUser=my-topic-1875783706-1763804507, testTopicOperatorMetrics=my-topic-1965542273-995674430, testKafkaActiveControllers=my-topic-1135110290-1277775808, testKafkaExporterDifferentSetting=my-topic-568145531-1290731867, testKafkaConnectRequests=my-topic-873062135-1706759978, testCruiseControlMetrics=my-topic-1917112707-1243703175, testReceiveSimpleMessageTls=my-topic-368564676-637091599, testClusterOperatorMetrics=my-topic-1416314702-1707243111, testKafkaBrokersCount=my-topic-222235994-1274701867, testUserOperatorMetrics=my-topic-1544093300-974581436, testKafkaMetricsSettings=my-topic-256215769-877131003}
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaConnectResponse=my-cluster-2fe1834d-kafka-clients, testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e-kafka-clients, testKafkaConnectIoNetwork=my-cluster-2e56d7a8-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-8aed4745-kafka-clients, testZookeeperWatchersCount=my-cluster-22c0f078-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-d686e2cd-kafka-clients, testSendMessagesTlsScramSha=my-cluster-84882f67-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273-kafka-clients, testZookeeperQuorumSize=my-cluster-16088501-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff-kafka-clients, testKafkaBridgeMetrics=my-cluster-9178bcb3-kafka-clients, testKafkaExporterDataAfterExchange=my-cluster-e86a0839-kafka-clients, testUpdateUser=my-cluster-6e6fa8fe-kafka-clients, testTopicOperatorMetrics=my-cluster-216189cb-kafka-clients, testKafkaActiveControllers=my-cluster-ede2d53e-kafka-clients, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60-kafka-clients, testKafkaConnectRequests=my-cluster-d1a00fce-kafka-clients, testCruiseControlMetrics=my-cluster-f49dfd8c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-2e450144-kafka-clients, testClusterOperatorMetrics=my-cluster-cfe17ead-kafka-clients, testKafkaBrokersCount=my-cluster-a943baf0-kafka-clients, testUserOperatorMetrics=my-cluster-d671df34-kafka-clients, testKafkaMetricsSettings=my-cluster-b7e9a534-kafka-clients}
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:346] In context testZookeeperQuorumSize is everything deleted.
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:267] testZookeeperQuorumSize - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendSimpleMessageTls, testUpdateUser, testCruiseControlBasicAPIRequests, testReceiveSimpleMessageTls, testSendMessagesTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testKafkaExporterDifferentSetting, testKafkaConnectResponse, testKafkaConnectRequests, testTopicOperatorMetrics, testCruiseControlMetrics, testClusterOperatorMetrics, testKafkaMetricsSettings, testKafkaBridgeMetrics, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaConnectIoNetwork, testKafkaBrokersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize] to and randomly select one to start execution
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testZookeeperQuorumSize
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 1
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testZookeeperQuorumSize-FINISHED
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testZookeeperAliveConnections-STARTED
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testZookeeperAliveConnections
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 2
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:230] testZookeeperAliveConnections test now can proceed its execution
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaConnectResponse=my-cluster-2fe1834d, testSendSimpleMessageTls=my-cluster-ca7325d7, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e, testKafkaConnectIoNetwork=my-cluster-2e56d7a8, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37, testCruiseControlBasicAPIRequests=my-cluster-8aed4745, testZookeeperWatchersCount=my-cluster-22c0f078, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634, testZookeeperAliveConnections=my-cluster-b34ddefa, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba, testKafkaTopicUnderReplicatedPartitions=my-cluster-d686e2cd, testSendMessagesTlsScramSha=my-cluster-84882f67, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273, testZookeeperQuorumSize=my-cluster-16088501, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff, testKafkaBridgeMetrics=my-cluster-9178bcb3, testKafkaExporterDataAfterExchange=my-cluster-e86a0839, testUpdateUser=my-cluster-6e6fa8fe, testTopicOperatorMetrics=my-cluster-216189cb, testKafkaActiveControllers=my-cluster-ede2d53e, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60, testKafkaConnectRequests=my-cluster-d1a00fce, testCruiseControlMetrics=my-cluster-f49dfd8c, testReceiveSimpleMessageTls=my-cluster-2e450144, testClusterOperatorMetrics=my-cluster-cfe17ead, testKafkaBrokersCount=my-cluster-a943baf0, testUserOperatorMetrics=my-cluster-d671df34, testKafkaMetricsSettings=my-cluster-b7e9a534}
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaConnectResponse=my-user-1873693835-1689263597, testSendSimpleMessageTls=my-user-1309491966-2015599610, testReconcileStateMetricInTopicOperator=my-user-2124671027-1015880288, testKafkaConnectIoNetwork=my-user-1245775831-860629339, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1886945581-244087591, testSendMessagesCustomListenerTlsScramSha=my-user-1118022021-805525218, testCruiseControlBasicAPIRequests=my-user-1861484466-561878159, testZookeeperWatchersCount=my-user-1490797769-719890298, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1851888062-673454065, testZookeeperAliveConnections=my-user-2146410634-1587438750, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1731919654-539989781, testKafkaTopicUnderReplicatedPartitions=my-user-1731286414-663678983, testSendMessagesTlsScramSha=my-user-586569987-2105688079, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677, testRackAwareConnectCorrectDeployment=my-user-1051420592-1240796798, testZookeeperQuorumSize=my-user-2117010560-1601663415, testKafkaAndZookeeperScaleUpScaleDown=my-user-838849261-890460734, testKafkaBridgeMetrics=my-user-829221238-874616880, testKafkaExporterDataAfterExchange=my-user-766848479-1588072395, testUpdateUser=my-user-1387197422-925410353, testTopicOperatorMetrics=my-user-779424699-479796683, testKafkaActiveControllers=my-user-1066197147-6205877, testKafkaExporterDifferentSetting=my-user-988491821-279531432, testKafkaConnectRequests=my-user-1167366435-1145507543, testCruiseControlMetrics=my-user-958954083-1578303162, testReceiveSimpleMessageTls=my-user-1669668617-1031881775, testClusterOperatorMetrics=my-user-467081550-2107716793, testKafkaBrokersCount=my-user-114552180-150189557, testUserOperatorMetrics=my-user-1969851044-309723582, testKafkaMetricsSettings=my-user-1339734863-677347400}
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaConnectResponse=my-topic-680219046-979906955, testSendSimpleMessageTls=my-topic-435427300-917779110, testReconcileStateMetricInTopicOperator=my-topic-1267576332-2046216344, testKafkaConnectIoNetwork=my-topic-1593682376-1381222647, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-306860904-106838350, testSendMessagesCustomListenerTlsScramSha=my-topic-963888555-1712711326, testCruiseControlBasicAPIRequests=my-topic-1631570507-1308241024, testZookeeperWatchersCount=my-topic-1959052705-1334872805, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-307985263-926887950, testZookeeperAliveConnections=my-topic-1555557100-1445821129, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1672280868-1119644965, testKafkaTopicUnderReplicatedPartitions=my-topic-2103629457-772081434, testSendMessagesTlsScramSha=my-topic-255809133-823300208, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206, testRackAwareConnectCorrectDeployment=my-topic-189499239-153088857, testZookeeperQuorumSize=my-topic-192478281-1148042900, testKafkaAndZookeeperScaleUpScaleDown=my-topic-2138084746-704381117, testKafkaBridgeMetrics=my-topic-1063894720-254889472, testKafkaExporterDataAfterExchange=my-topic-1937812163-1733130103, testUpdateUser=my-topic-1875783706-1763804507, testTopicOperatorMetrics=my-topic-1965542273-995674430, testKafkaActiveControllers=my-topic-1135110290-1277775808, testKafkaExporterDifferentSetting=my-topic-568145531-1290731867, testKafkaConnectRequests=my-topic-873062135-1706759978, testCruiseControlMetrics=my-topic-1917112707-1243703175, testReceiveSimpleMessageTls=my-topic-368564676-637091599, testClusterOperatorMetrics=my-topic-1416314702-1707243111, testKafkaBrokersCount=my-topic-222235994-1274701867, testUserOperatorMetrics=my-topic-1544093300-974581436, testKafkaMetricsSettings=my-topic-256215769-877131003}
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaConnectResponse=my-cluster-2fe1834d-kafka-clients, testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e-kafka-clients, testKafkaConnectIoNetwork=my-cluster-2e56d7a8-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-8aed4745-kafka-clients, testZookeeperWatchersCount=my-cluster-22c0f078-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634-kafka-clients, testZookeeperAliveConnections=my-cluster-b34ddefa-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-d686e2cd-kafka-clients, testSendMessagesTlsScramSha=my-cluster-84882f67-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273-kafka-clients, testZookeeperQuorumSize=my-cluster-16088501-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff-kafka-clients, testKafkaBridgeMetrics=my-cluster-9178bcb3-kafka-clients, testKafkaExporterDataAfterExchange=my-cluster-e86a0839-kafka-clients, testUpdateUser=my-cluster-6e6fa8fe-kafka-clients, testTopicOperatorMetrics=my-cluster-216189cb-kafka-clients, testKafkaActiveControllers=my-cluster-ede2d53e-kafka-clients, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60-kafka-clients, testKafkaConnectRequests=my-cluster-d1a00fce-kafka-clients, testCruiseControlMetrics=my-cluster-f49dfd8c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-2e450144-kafka-clients, testClusterOperatorMetrics=my-cluster-cfe17ead-kafka-clients, testKafkaBrokersCount=my-cluster-a943baf0-kafka-clients, testUserOperatorMetrics=my-cluster-d671df34-kafka-clients, testKafkaMetricsSettings=my-cluster-b7e9a534-kafka-clients}
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:346] In context testZookeeperAliveConnections is everything deleted.
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:267] testZookeeperAliveConnections - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendSimpleMessageTls, testUpdateUser, testCruiseControlBasicAPIRequests, testReceiveSimpleMessageTls, testSendMessagesTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testKafkaExporterDifferentSetting, testKafkaConnectResponse, testKafkaConnectRequests, testTopicOperatorMetrics, testCruiseControlMetrics, testClusterOperatorMetrics, testKafkaMetricsSettings, testKafkaBridgeMetrics, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaConnectIoNetwork, testKafkaBrokersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize, testZookeeperAliveConnections] to and randomly select one to start execution
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testZookeeperAliveConnections
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 1
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testZookeeperAliveConnections-FINISHED
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaTopicPartitions-STARTED
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testKafkaTopicPartitions
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 2
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:230] testKafkaTopicPartitions test now can proceed its execution
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaConnectResponse=my-cluster-2fe1834d, testSendSimpleMessageTls=my-cluster-ca7325d7, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e, testKafkaConnectIoNetwork=my-cluster-2e56d7a8, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37, testCruiseControlBasicAPIRequests=my-cluster-8aed4745, testZookeeperWatchersCount=my-cluster-22c0f078, testKafkaTopicPartitions=my-cluster-0afaffb5, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634, testZookeeperAliveConnections=my-cluster-b34ddefa, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba, testKafkaTopicUnderReplicatedPartitions=my-cluster-d686e2cd, testSendMessagesTlsScramSha=my-cluster-84882f67, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273, testZookeeperQuorumSize=my-cluster-16088501, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff, testKafkaBridgeMetrics=my-cluster-9178bcb3, testKafkaExporterDataAfterExchange=my-cluster-e86a0839, testUpdateUser=my-cluster-6e6fa8fe, testTopicOperatorMetrics=my-cluster-216189cb, testKafkaActiveControllers=my-cluster-ede2d53e, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60, testKafkaConnectRequests=my-cluster-d1a00fce, testCruiseControlMetrics=my-cluster-f49dfd8c, testReceiveSimpleMessageTls=my-cluster-2e450144, testClusterOperatorMetrics=my-cluster-cfe17ead, testKafkaBrokersCount=my-cluster-a943baf0, testUserOperatorMetrics=my-cluster-d671df34, testKafkaMetricsSettings=my-cluster-b7e9a534}
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaConnectResponse=my-user-1873693835-1689263597, testSendSimpleMessageTls=my-user-1309491966-2015599610, testReconcileStateMetricInTopicOperator=my-user-2124671027-1015880288, testKafkaConnectIoNetwork=my-user-1245775831-860629339, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1886945581-244087591, testSendMessagesCustomListenerTlsScramSha=my-user-1118022021-805525218, testCruiseControlBasicAPIRequests=my-user-1861484466-561878159, testZookeeperWatchersCount=my-user-1490797769-719890298, testKafkaTopicPartitions=my-user-2007150773-1723968056, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1851888062-673454065, testZookeeperAliveConnections=my-user-2146410634-1587438750, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1731919654-539989781, testKafkaTopicUnderReplicatedPartitions=my-user-1731286414-663678983, testSendMessagesTlsScramSha=my-user-586569987-2105688079, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677, testRackAwareConnectCorrectDeployment=my-user-1051420592-1240796798, testZookeeperQuorumSize=my-user-2117010560-1601663415, testKafkaAndZookeeperScaleUpScaleDown=my-user-838849261-890460734, testKafkaBridgeMetrics=my-user-829221238-874616880, testKafkaExporterDataAfterExchange=my-user-766848479-1588072395, testUpdateUser=my-user-1387197422-925410353, testTopicOperatorMetrics=my-user-779424699-479796683, testKafkaActiveControllers=my-user-1066197147-6205877, testKafkaExporterDifferentSetting=my-user-988491821-279531432, testKafkaConnectRequests=my-user-1167366435-1145507543, testCruiseControlMetrics=my-user-958954083-1578303162, testReceiveSimpleMessageTls=my-user-1669668617-1031881775, testClusterOperatorMetrics=my-user-467081550-2107716793, testKafkaBrokersCount=my-user-114552180-150189557, testUserOperatorMetrics=my-user-1969851044-309723582, testKafkaMetricsSettings=my-user-1339734863-677347400}
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaConnectResponse=my-topic-680219046-979906955, testSendSimpleMessageTls=my-topic-435427300-917779110, testReconcileStateMetricInTopicOperator=my-topic-1267576332-2046216344, testKafkaConnectIoNetwork=my-topic-1593682376-1381222647, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-306860904-106838350, testSendMessagesCustomListenerTlsScramSha=my-topic-963888555-1712711326, testCruiseControlBasicAPIRequests=my-topic-1631570507-1308241024, testZookeeperWatchersCount=my-topic-1959052705-1334872805, testKafkaTopicPartitions=my-topic-1875007668-599688839, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-307985263-926887950, testZookeeperAliveConnections=my-topic-1555557100-1445821129, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1672280868-1119644965, testKafkaTopicUnderReplicatedPartitions=my-topic-2103629457-772081434, testSendMessagesTlsScramSha=my-topic-255809133-823300208, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206, testRackAwareConnectCorrectDeployment=my-topic-189499239-153088857, testZookeeperQuorumSize=my-topic-192478281-1148042900, testKafkaAndZookeeperScaleUpScaleDown=my-topic-2138084746-704381117, testKafkaBridgeMetrics=my-topic-1063894720-254889472, testKafkaExporterDataAfterExchange=my-topic-1937812163-1733130103, testUpdateUser=my-topic-1875783706-1763804507, testTopicOperatorMetrics=my-topic-1965542273-995674430, testKafkaActiveControllers=my-topic-1135110290-1277775808, testKafkaExporterDifferentSetting=my-topic-568145531-1290731867, testKafkaConnectRequests=my-topic-873062135-1706759978, testCruiseControlMetrics=my-topic-1917112707-1243703175, testReceiveSimpleMessageTls=my-topic-368564676-637091599, testClusterOperatorMetrics=my-topic-1416314702-1707243111, testKafkaBrokersCount=my-topic-222235994-1274701867, testUserOperatorMetrics=my-topic-1544093300-974581436, testKafkaMetricsSettings=my-topic-256215769-877131003}
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaConnectResponse=my-cluster-2fe1834d-kafka-clients, testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e-kafka-clients, testKafkaConnectIoNetwork=my-cluster-2e56d7a8-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-8aed4745-kafka-clients, testZookeeperWatchersCount=my-cluster-22c0f078-kafka-clients, testKafkaTopicPartitions=my-cluster-0afaffb5-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634-kafka-clients, testZookeeperAliveConnections=my-cluster-b34ddefa-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-d686e2cd-kafka-clients, testSendMessagesTlsScramSha=my-cluster-84882f67-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273-kafka-clients, testZookeeperQuorumSize=my-cluster-16088501-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff-kafka-clients, testKafkaBridgeMetrics=my-cluster-9178bcb3-kafka-clients, testKafkaExporterDataAfterExchange=my-cluster-e86a0839-kafka-clients, testUpdateUser=my-cluster-6e6fa8fe-kafka-clients, testTopicOperatorMetrics=my-cluster-216189cb-kafka-clients, testKafkaActiveControllers=my-cluster-ede2d53e-kafka-clients, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60-kafka-clients, testKafkaConnectRequests=my-cluster-d1a00fce-kafka-clients, testCruiseControlMetrics=my-cluster-f49dfd8c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-2e450144-kafka-clients, testClusterOperatorMetrics=my-cluster-cfe17ead-kafka-clients, testKafkaBrokersCount=my-cluster-a943baf0-kafka-clients, testUserOperatorMetrics=my-cluster-d671df34-kafka-clients, testKafkaMetricsSettings=my-cluster-b7e9a534-kafka-clients}
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:346] In context testKafkaTopicPartitions is everything deleted.
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:267] testKafkaTopicPartitions - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendSimpleMessageTls, testUpdateUser, testCruiseControlBasicAPIRequests, testReceiveSimpleMessageTls, testSendMessagesTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testKafkaExporterDifferentSetting, testKafkaConnectResponse, testKafkaConnectRequests, testTopicOperatorMetrics, testCruiseControlMetrics, testClusterOperatorMetrics, testKafkaMetricsSettings, testKafkaBridgeMetrics, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaConnectIoNetwork, testKafkaBrokersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions] to and randomly select one to start execution
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaTopicPartitions
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 1
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaTopicPartitions-FINISHED
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testMirrorMaker2Metrics-STARTED
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:659] [metrics.MetricsIsolatedST - Before Each] - Setup test case environment
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:77] [metrics.MetricsIsolatedST] - Adding parallel test: testMirrorMaker2Metrics
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:81] [metrics.MetricsIsolatedST] - Parallel test count: 2
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:230] testMirrorMaker2Metrics test now can proceed its execution
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaConnectResponse=my-cluster-2fe1834d, testSendSimpleMessageTls=my-cluster-ca7325d7, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e, testKafkaConnectIoNetwork=my-cluster-2e56d7a8, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37, testCruiseControlBasicAPIRequests=my-cluster-8aed4745, testZookeeperWatchersCount=my-cluster-22c0f078, testKafkaTopicPartitions=my-cluster-0afaffb5, testMirrorMaker2Metrics=my-cluster-5da4fc90, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634, testZookeeperAliveConnections=my-cluster-b34ddefa, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba, testKafkaTopicUnderReplicatedPartitions=my-cluster-d686e2cd, testSendMessagesTlsScramSha=my-cluster-84882f67, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273, testZookeeperQuorumSize=my-cluster-16088501, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff, testKafkaBridgeMetrics=my-cluster-9178bcb3, testKafkaExporterDataAfterExchange=my-cluster-e86a0839, testUpdateUser=my-cluster-6e6fa8fe, testTopicOperatorMetrics=my-cluster-216189cb, testKafkaActiveControllers=my-cluster-ede2d53e, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60, testKafkaConnectRequests=my-cluster-d1a00fce, testCruiseControlMetrics=my-cluster-f49dfd8c, testReceiveSimpleMessageTls=my-cluster-2e450144, testClusterOperatorMetrics=my-cluster-cfe17ead, testKafkaBrokersCount=my-cluster-a943baf0, testUserOperatorMetrics=my-cluster-d671df34, testKafkaMetricsSettings=my-cluster-b7e9a534}
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaConnectResponse=my-user-1873693835-1689263597, testSendSimpleMessageTls=my-user-1309491966-2015599610, testReconcileStateMetricInTopicOperator=my-user-2124671027-1015880288, testKafkaConnectIoNetwork=my-user-1245775831-860629339, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1886945581-244087591, testSendMessagesCustomListenerTlsScramSha=my-user-1118022021-805525218, testCruiseControlBasicAPIRequests=my-user-1861484466-561878159, testZookeeperWatchersCount=my-user-1490797769-719890298, testKafkaTopicPartitions=my-user-2007150773-1723968056, testMirrorMaker2Metrics=my-user-1300203975-2042223970, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1851888062-673454065, testZookeeperAliveConnections=my-user-2146410634-1587438750, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1731919654-539989781, testKafkaTopicUnderReplicatedPartitions=my-user-1731286414-663678983, testSendMessagesTlsScramSha=my-user-586569987-2105688079, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677, testRackAwareConnectCorrectDeployment=my-user-1051420592-1240796798, testZookeeperQuorumSize=my-user-2117010560-1601663415, testKafkaAndZookeeperScaleUpScaleDown=my-user-838849261-890460734, testKafkaBridgeMetrics=my-user-829221238-874616880, testKafkaExporterDataAfterExchange=my-user-766848479-1588072395, testUpdateUser=my-user-1387197422-925410353, testTopicOperatorMetrics=my-user-779424699-479796683, testKafkaActiveControllers=my-user-1066197147-6205877, testKafkaExporterDifferentSetting=my-user-988491821-279531432, testKafkaConnectRequests=my-user-1167366435-1145507543, testCruiseControlMetrics=my-user-958954083-1578303162, testReceiveSimpleMessageTls=my-user-1669668617-1031881775, testClusterOperatorMetrics=my-user-467081550-2107716793, testKafkaBrokersCount=my-user-114552180-150189557, testUserOperatorMetrics=my-user-1969851044-309723582, testKafkaMetricsSettings=my-user-1339734863-677347400}
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaConnectResponse=my-topic-680219046-979906955, testSendSimpleMessageTls=my-topic-435427300-917779110, testReconcileStateMetricInTopicOperator=my-topic-1267576332-2046216344, testKafkaConnectIoNetwork=my-topic-1593682376-1381222647, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-306860904-106838350, testSendMessagesCustomListenerTlsScramSha=my-topic-963888555-1712711326, testCruiseControlBasicAPIRequests=my-topic-1631570507-1308241024, testZookeeperWatchersCount=my-topic-1959052705-1334872805, testKafkaTopicPartitions=my-topic-1875007668-599688839, testMirrorMaker2Metrics=my-topic-643348725-45724501, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-307985263-926887950, testZookeeperAliveConnections=my-topic-1555557100-1445821129, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1672280868-1119644965, testKafkaTopicUnderReplicatedPartitions=my-topic-2103629457-772081434, testSendMessagesTlsScramSha=my-topic-255809133-823300208, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206, testRackAwareConnectCorrectDeployment=my-topic-189499239-153088857, testZookeeperQuorumSize=my-topic-192478281-1148042900, testKafkaAndZookeeperScaleUpScaleDown=my-topic-2138084746-704381117, testKafkaBridgeMetrics=my-topic-1063894720-254889472, testKafkaExporterDataAfterExchange=my-topic-1937812163-1733130103, testUpdateUser=my-topic-1875783706-1763804507, testTopicOperatorMetrics=my-topic-1965542273-995674430, testKafkaActiveControllers=my-topic-1135110290-1277775808, testKafkaExporterDifferentSetting=my-topic-568145531-1290731867, testKafkaConnectRequests=my-topic-873062135-1706759978, testCruiseControlMetrics=my-topic-1917112707-1243703175, testReceiveSimpleMessageTls=my-topic-368564676-637091599, testClusterOperatorMetrics=my-topic-1416314702-1707243111, testKafkaBrokersCount=my-topic-222235994-1274701867, testUserOperatorMetrics=my-topic-1544093300-974581436, testKafkaMetricsSettings=my-topic-256215769-877131003}
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaConnectResponse=my-cluster-2fe1834d-kafka-clients, testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e-kafka-clients, testKafkaConnectIoNetwork=my-cluster-2e56d7a8-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-8aed4745-kafka-clients, testZookeeperWatchersCount=my-cluster-22c0f078-kafka-clients, testKafkaTopicPartitions=my-cluster-0afaffb5-kafka-clients, testMirrorMaker2Metrics=my-cluster-5da4fc90-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634-kafka-clients, testZookeeperAliveConnections=my-cluster-b34ddefa-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-d686e2cd-kafka-clients, testSendMessagesTlsScramSha=my-cluster-84882f67-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273-kafka-clients, testZookeeperQuorumSize=my-cluster-16088501-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff-kafka-clients, testKafkaBridgeMetrics=my-cluster-9178bcb3-kafka-clients, testKafkaExporterDataAfterExchange=my-cluster-e86a0839-kafka-clients, testUpdateUser=my-cluster-6e6fa8fe-kafka-clients, testTopicOperatorMetrics=my-cluster-216189cb-kafka-clients, testKafkaActiveControllers=my-cluster-ede2d53e-kafka-clients, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60-kafka-clients, testKafkaConnectRequests=my-cluster-d1a00fce-kafka-clients, testCruiseControlMetrics=my-cluster-f49dfd8c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-2e450144-kafka-clients, testClusterOperatorMetrics=my-cluster-cfe17ead-kafka-clients, testKafkaBrokersCount=my-cluster-a943baf0-kafka-clients, testUserOperatorMetrics=my-cluster-d671df34-kafka-clients, testKafkaMetricsSettings=my-cluster-b7e9a534-kafka-clients}
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 20:56:04 [ForkJoinPool-3-worker-9] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-84mgz -n infra-namespace -- curl 172.17.0.23:9404
2022-03-30 20:56:04 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:420] Looking for 'strimzi_bridge_kafka_producer_count' in bridge metrics
2022-03-30 20:56:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 20:56:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-84mgz -n infra-namespace -- curl 172.17.0.22:8080/metrics
2022-03-30 20:56:04 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.22 from Pod infra-namespace-kafka-clients-748578f786-84mgz finished with return code: 0
2022-03-30 20:56:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for KafkaConsumer metrics will be available
2022-03-30 20:56:04 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:428] Looking for 'strimzi_bridge_kafka_consumer_connection_count' in bridge metrics
2022-03-30 20:56:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for metrics has data
2022-03-30 20:56:04 [ForkJoinPool-3-worker-3] [30mTRACE[m [Exec:248] Running command - kubectl exec infra-namespace-kafka-clients-748578f786-84mgz -n infra-namespace -- curl 172.17.0.22:8080/metrics
2022-03-30 20:56:04 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.22 from Pod infra-namespace-kafka-clients-748578f786-84mgz finished with return code: 0
2022-03-30 20:56:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 20:56:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 20:56:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:56:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaBridgeMetrics
2022-03-30 20:56:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job bridge-consumer in namespace infra-namespace
2022-03-30 20:56:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:bridge-consumer
2022-03-30 20:56:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job bridge-producer in namespace infra-namespace
2022-03-30 20:56:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:bridge-producer
2022-03-30 20:56:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:56:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:267] testKafkaBridgeMetrics - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendSimpleMessageTls, testUpdateUser, testCruiseControlBasicAPIRequests, testReceiveSimpleMessageTls, testSendMessagesTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testKafkaExporterDifferentSetting, testKafkaConnectResponse, testKafkaConnectRequests, testTopicOperatorMetrics, testCruiseControlMetrics, testClusterOperatorMetrics, testKafkaMetricsSettings, testKafkaBridgeMetrics, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaConnectIoNetwork, testKafkaBrokersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics] to and randomly select one to start execution
2022-03-30 20:56:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testKafkaBridgeMetrics
2022-03-30 20:56:04 [ForkJoinPool-3-worker-3] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 1
2022-03-30 20:56:04 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaBridgeMetrics-FINISHED
2022-03-30 20:56:04 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 20:56:05 [ForkJoinPool-3-worker-9] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.23 from Pod infra-namespace-kafka-clients-748578f786-84mgz finished with return code: 0
2022-03-30 20:56:05 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 20:56:05 [ForkJoinPool-3-worker-9] [36mDEBUG[m [AbstractST:675] [metrics.MetricsIsolatedST - After Each] - Clean up after test
2022-03-30 20:56:05 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:56:05 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:346] In context testMirrorMaker2Metrics is everything deleted.
2022-03-30 20:56:05 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:56:05 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:267] testMirrorMaker2Metrics - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendSimpleMessageTls, testUpdateUser, testCruiseControlBasicAPIRequests, testReceiveSimpleMessageTls, testSendMessagesTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testKafkaExporterDifferentSetting, testKafkaConnectResponse, testKafkaConnectRequests, testTopicOperatorMetrics, testCruiseControlMetrics, testClusterOperatorMetrics, testKafkaMetricsSettings, testKafkaBridgeMetrics, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaConnectIoNetwork, testKafkaBrokersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics] to and randomly select one to start execution
2022-03-30 20:56:05 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:93] [metrics.MetricsIsolatedST] - Removing parallel test: testMirrorMaker2Metrics
2022-03-30 20:56:05 [ForkJoinPool-3-worker-9] [36mDEBUG[m [SuiteThreadController:97] [metrics.MetricsIsolatedST] - Parallel test count: 0
2022-03-30 20:56:05 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testMirrorMaker2Metrics-FINISHED
2022-03-30 20:56:05 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 20:56:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 20:56:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractST:690] [metrics.MetricsIsolatedST - After All] - Clean up after test suite
2022-03-30 20:56:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:56:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for MetricsIsolatedST
2022-03-30 20:56:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1903507259-1821354793 in namespace infra-namespace
2022-03-30 20:56:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1903507259-1821354793
2022-03-30 20:56:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1903507259-1821354793 not ready, will try again in 10000 ms (179991ms till timeout)
2022-03-30 20:56:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:56:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:56:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect metrics-cluster-name in namespace infra-namespace
2022-03-30 20:56:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaConnect:metrics-cluster-name
2022-03-30 20:56:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaConnect:metrics-cluster-name not ready, will try again in 10000 ms (599987ms till timeout)
2022-03-30 20:56:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:56:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:56:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-2102088411-591776996 in namespace infra-namespace
2022-03-30 20:56:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-2102088411-591776996
2022-03-30 20:56:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-2102088411-591776996 not ready, will try again in 10000 ms (179993ms till timeout)
2022-03-30 20:56:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:56:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:56:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-2049647524-1750726403 in namespace infra-namespace
2022-03-30 20:56:35 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-2049647524-1750726403
2022-03-30 20:56:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-2049647524-1750726403 not ready, will try again in 10000 ms (179994ms till timeout)
2022-03-30 20:56:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:56:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:56:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy second-kafka-cluster-entity-operator-allow in namespace second-metrics-cluster-test
2022-03-30 20:56:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:second-kafka-cluster-entity-operator-allow
2022-03-30 20:56:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy second-kafka-cluster-kafka-exporter-allow in namespace second-metrics-cluster-test
2022-03-30 20:56:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:second-kafka-cluster-kafka-exporter-allow
2022-03-30 20:56:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy metrics-cluster-name-kafka-exporter-allow in namespace infra-namespace
2022-03-30 20:56:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:metrics-cluster-name-kafka-exporter-allow
2022-03-30 20:56:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy metrics-cluster-name-entity-operator-allow in namespace infra-namespace
2022-03-30 20:56:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:metrics-cluster-name-entity-operator-allow
2022-03-30 20:56:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy cluster-operator-allow in namespace infra-namespace
2022-03-30 20:56:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:cluster-operator-allow
2022-03-30 20:56:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 mm2-cluster in namespace infra-namespace
2022-03-30 20:56:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaMirrorMaker2:mm2-cluster
2022-03-30 20:56:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaMirrorMaker2:mm2-cluster not ready, will try again in 10000 ms (599989ms till timeout)
2022-03-30 20:56:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:56:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:56:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge my-bridge in namespace infra-namespace
2022-03-30 20:56:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaBridge:my-bridge
2022-03-30 20:56:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaBridge:my-bridge not ready, will try again in 10000 ms (479995ms till timeout)
2022-03-30 20:56:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:57:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:57:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1550308369-1840108023 in namespace infra-namespace
2022-03-30 20:57:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1550308369-1840108023
2022-03-30 20:57:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1550308369-1840108023 not ready, will try again in 10000 ms (179994ms till timeout)
2022-03-30 20:57:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:57:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:57:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1905774485-827577292 in namespace infra-namespace
2022-03-30 20:57:15 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1905774485-827577292
2022-03-30 20:57:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1905774485-827577292 not ready, will try again in 10000 ms (179995ms till timeout)
2022-03-30 20:57:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:57:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:57:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment infra-namespace-kafka-clients in namespace infra-namespace
2022-03-30 20:57:25 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:infra-namespace-kafka-clients
2022-03-30 20:57:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:infra-namespace-kafka-clients not ready, will try again in 10000 ms (479991ms till timeout)
2022-03-30 20:57:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:57:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:57:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:infra-namespace-kafka-clients not ready, will try again in 10000 ms (469982ms till timeout)
2022-03-30 20:57:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:57:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:57:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:infra-namespace-kafka-clients not ready, will try again in 10000 ms (459973ms till timeout)
2022-03-30 20:57:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:57:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:57:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:infra-namespace-kafka-clients not ready, will try again in 10000 ms (449963ms till timeout)
2022-03-30 20:57:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:58:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:58:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment second-metrics-cluster-test-kafka-clients in namespace second-metrics-cluster-test
2022-03-30 20:58:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:second-metrics-cluster-test-kafka-clients
2022-03-30 20:58:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:second-metrics-cluster-test-kafka-clients not ready, will try again in 10000 ms (479989ms till timeout)
2022-03-30 20:58:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:58:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:58:15 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:second-metrics-cluster-test-kafka-clients not ready, will try again in 10000 ms (469980ms till timeout)
2022-03-30 20:58:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:58:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:58:25 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:second-metrics-cluster-test-kafka-clients not ready, will try again in 10000 ms (459971ms till timeout)
2022-03-30 20:58:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:58:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:58:35 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:second-metrics-cluster-test-kafka-clients not ready, will try again in 10000 ms (449962ms till timeout)
2022-03-30 20:58:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:58:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:58:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka second-kafka-cluster in namespace second-metrics-cluster-test
2022-03-30 20:58:45 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:second-kafka-cluster
2022-03-30 20:58:45 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:second-kafka-cluster not ready, will try again in 10000 ms (839958ms till timeout)
2022-03-30 20:58:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:58:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:58:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka metrics-cluster-name in namespace infra-namespace
2022-03-30 20:58:55 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace infra-namespace, for cruise control Kafka cluster metrics-cluster-name
2022-03-30 20:58:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:metrics-cluster-name
2022-03-30 20:58:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:metrics-cluster-name not ready, will try again in 10000 ms (839997ms till timeout)
2022-03-30 20:58:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:59:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMaker2IsolatedST is waiting to lock to be released.
2022-03-30 20:59:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 20, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3,536.207 s - in io.strimzi.systemtest.metrics.MetricsIsolatedST
2022-03-30 20:59:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:136] Suite mirrormaker.MirrorMaker2IsolatedST has locked the @IsolatedSuite and other @IsolatedSuites must wait until lock is released.
2022-03-30 20:59:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 20:59:05 [ForkJoinPool-3-worker-7] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-30 20:59:05 [ForkJoinPool-3-worker-7] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-30 20:59:05 [ForkJoinPool-3-worker-7] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-30 20:59:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 20:59:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-30 20:59:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 20:59:05 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 20:59:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 20:59:05 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 20:59:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 20:59:05 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io not ready, will try again in 10000 ms (179992ms till timeout)
2022-03-30 20:59:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 20:59:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:59:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 20:59:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 20:59:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 20:59:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 20:59:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 20:59:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:59:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 20:59:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 20:59:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 20:59:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179915ms till timeout)
2022-03-30 20:59:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 20:59:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 20:59:15 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 20:59:15 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 20:59:15 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io not ready, will try again in 10000 ms (179992ms till timeout)
2022-03-30 20:59:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace second-metrics-cluster-test
2022-03-30 20:59:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 20:59:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 20:59:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 20:59:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:59:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 20:59:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator not ready, will try again in 10000 ms (479943ms till timeout)
2022-03-30 20:59:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 20:59:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 20:59:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 20:59:25 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 20:59:25 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179949ms till timeout)
2022-03-30 20:59:25 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace second-metrics-cluster-test
2022-03-30 20:59:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 20:59:25 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 20:59:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 20:59:25 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 20:59:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 20:59:25 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 20:59:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 20:59:25 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 20:59:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 20:59:25 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 20:59:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 20:59:26 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 20:59:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 20:59:26 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 20:59:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 20:59:26 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 20:59:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 20:59:26 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 20:59:26 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 20:59:26 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179816ms till timeout)
2022-03-30 20:59:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 20:59:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 20:59:35 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 20:59:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 20:59:35 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 20:59:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 20:59:35 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 20:59:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 20:59:35 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 20:59:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 20:59:35 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 20:59:35 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 20:59:36 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 20:59:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-30 20:59:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 20:59:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 20:59:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v93309
2022-03-30 20:59:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v93309
2022-03-30 20:59:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=93309&allowWatchBookmarks=true&watch=true...
2022-03-30 20:59:36 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 20:59:36 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 93310
2022-03-30 20:59:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 20:59:41 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 93515
2022-03-30 20:59:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 20:59:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 20:59:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:00:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:00:02 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 93543
2022-03-30 21:00:02 [ForkJoinPool-3-worker-7] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: second-metrics-cluster-test
2022-03-30 21:00:02 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v93515 in namespace default
2022-03-30 21:00:02 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@25972335
2022-03-30 21:00:02 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 21:00:02 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@6ef3637a
2022-03-30 21:00:02 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@6ef3637a
2022-03-30 21:00:02 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@6ef3637a
2022-03-30 21:00:03 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 21:00:03 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 21:00:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 21:00:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 21:00:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v93544
2022-03-30 21:00:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v93544
2022-03-30 21:00:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dsecond-metrics-cluster-test&resourceVersion=93544&allowWatchBookmarks=true&watch=true...
2022-03-30 21:00:03 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 21:00:03 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 93545
2022-03-30 21:00:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:00:08 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 93619
2022-03-30 21:00:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:00:13 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 93622
2022-03-30 21:00:13 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v93619 in namespace default
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5a34beae
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=30000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-30 21:00:13 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@1da13fe
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@5314833a, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5a34beae, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=30000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-30 21:00:13 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 21:00:13 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@6fc776a
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-30 21:00:13 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@6fc776a
2022-03-30 21:00:13 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@6fc776a
2022-03-30 21:00:13 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 21:00:13 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T21:00:13Z",
        "name": "infra-namespace",
        "resourceVersion": "93623",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "19732781-3dcd-4655-a354-a301ae710b27"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 21:00:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 21:00:14 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 21:00:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 21:00:14 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 21:00:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 21:00:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479997ms till timeout)
2022-03-30 21:00:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478994ms till timeout)
2022-03-30 21:00:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:00:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477991ms till timeout)
2022-03-30 21:00:17 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476988ms till timeout)
2022-03-30 21:00:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475985ms till timeout)
2022-03-30 21:00:19 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474982ms till timeout)
2022-03-30 21:00:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473979ms till timeout)
2022-03-30 21:00:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:00:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472976ms till timeout)
2022-03-30 21:00:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471973ms till timeout)
2022-03-30 21:00:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470970ms till timeout)
2022-03-30 21:00:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469967ms till timeout)
2022-03-30 21:00:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468964ms till timeout)
2022-03-30 21:00:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:00:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467962ms till timeout)
2022-03-30 21:00:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466959ms till timeout)
2022-03-30 21:00:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465956ms till timeout)
2022-03-30 21:00:29 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464953ms till timeout)
2022-03-30 21:00:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463950ms till timeout)
2022-03-30 21:00:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:00:31 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462947ms till timeout)
2022-03-30 21:00:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461944ms till timeout)
2022-03-30 21:00:33 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-30 21:00:33 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-30 21:00:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-30 21:00:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-df5tv not ready: strimzi-cluster-operator)
2022-03-30 21:00:33 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-df5tv are ready
2022-03-30 21:00:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-30 21:00:34 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-df5tv not ready: strimzi-cluster-operator)
2022-03-30 21:00:34 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-df5tv are ready
2022-03-30 21:00:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-30 21:00:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-df5tv not ready: strimzi-cluster-operator)
2022-03-30 21:00:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-df5tv are ready
2022-03-30 21:00:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597990ms till timeout)
2022-03-30 21:00:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:00:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-df5tv not ready: strimzi-cluster-operator)
2022-03-30 21:00:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-df5tv are ready
2022-03-30 21:00:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596987ms till timeout)
2022-03-30 21:00:37 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-df5tv not ready: strimzi-cluster-operator)
2022-03-30 21:00:37 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-df5tv are ready
2022-03-30 21:00:37 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595984ms till timeout)
2022-03-30 21:00:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-df5tv not ready: strimzi-cluster-operator)
2022-03-30 21:00:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-df5tv are ready
2022-03-30 21:00:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594980ms till timeout)
2022-03-30 21:00:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-df5tv not ready: strimzi-cluster-operator)
2022-03-30 21:00:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-df5tv are ready
2022-03-30 21:00:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593976ms till timeout)
2022-03-30 21:00:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-df5tv not ready: strimzi-cluster-operator)
2022-03-30 21:00:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-df5tv are ready
2022-03-30 21:00:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592973ms till timeout)
2022-03-30 21:00:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:00:41 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-df5tv not ready: strimzi-cluster-operator)
2022-03-30 21:00:41 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-df5tv are ready
2022-03-30 21:00:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591969ms till timeout)
2022-03-30 21:00:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-df5tv not ready: strimzi-cluster-operator)
2022-03-30 21:00:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-df5tv are ready
2022-03-30 21:00:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590966ms till timeout)
2022-03-30 21:00:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-77554ffdfb-df5tv not ready: strimzi-cluster-operator)
2022-03-30 21:00:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-77554ffdfb-df5tv are ready
2022-03-30 21:00:43 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-30 21:00:43 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 21:00:43 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testMirrorMaker2TlsAndTlsClientAuth-STARTED
2022-03-30 21:00:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 21:00:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:659] [mirrormaker.MirrorMaker2IsolatedST - Before Each] - Setup test case environment
2022-03-30 21:00:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:77] [mirrormaker.MirrorMaker2IsolatedST] - Adding parallel test: testMirrorMaker2TlsAndTlsClientAuth
2022-03-30 21:00:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:81] [mirrormaker.MirrorMaker2IsolatedST] - Parallel test count: 1
2022-03-30 21:00:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:230] testMirrorMaker2TlsAndTlsClientAuth test now can proceed its execution
2022-03-30 21:00:43 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 21:00:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaConnectResponse=my-cluster-2fe1834d, testSendSimpleMessageTls=my-cluster-ca7325d7, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e, testKafkaConnectIoNetwork=my-cluster-2e56d7a8, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37, testCruiseControlBasicAPIRequests=my-cluster-8aed4745, testZookeeperWatchersCount=my-cluster-22c0f078, testKafkaTopicPartitions=my-cluster-0afaffb5, testMirrorMaker2Metrics=my-cluster-5da4fc90, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634, testZookeeperAliveConnections=my-cluster-b34ddefa, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba, testKafkaTopicUnderReplicatedPartitions=my-cluster-d686e2cd, testSendMessagesTlsScramSha=my-cluster-84882f67, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-52516fd7, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273, testZookeeperQuorumSize=my-cluster-16088501, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff, testKafkaBridgeMetrics=my-cluster-9178bcb3, testKafkaExporterDataAfterExchange=my-cluster-e86a0839, testUpdateUser=my-cluster-6e6fa8fe, testTopicOperatorMetrics=my-cluster-216189cb, testKafkaActiveControllers=my-cluster-ede2d53e, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60, testKafkaConnectRequests=my-cluster-d1a00fce, testCruiseControlMetrics=my-cluster-f49dfd8c, testReceiveSimpleMessageTls=my-cluster-2e450144, testClusterOperatorMetrics=my-cluster-cfe17ead, testKafkaBrokersCount=my-cluster-a943baf0, testUserOperatorMetrics=my-cluster-d671df34, testKafkaMetricsSettings=my-cluster-b7e9a534}
2022-03-30 21:00:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaConnectResponse=my-user-1873693835-1689263597, testSendSimpleMessageTls=my-user-1309491966-2015599610, testReconcileStateMetricInTopicOperator=my-user-2124671027-1015880288, testKafkaConnectIoNetwork=my-user-1245775831-860629339, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1886945581-244087591, testSendMessagesCustomListenerTlsScramSha=my-user-1118022021-805525218, testCruiseControlBasicAPIRequests=my-user-1861484466-561878159, testZookeeperWatchersCount=my-user-1490797769-719890298, testKafkaTopicPartitions=my-user-2007150773-1723968056, testMirrorMaker2Metrics=my-user-1300203975-2042223970, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1851888062-673454065, testZookeeperAliveConnections=my-user-2146410634-1587438750, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1731919654-539989781, testKafkaTopicUnderReplicatedPartitions=my-user-1731286414-663678983, testSendMessagesTlsScramSha=my-user-586569987-2105688079, testMirrorMaker2TlsAndTlsClientAuth=my-user-2042967534-325538616, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677, testRackAwareConnectCorrectDeployment=my-user-1051420592-1240796798, testZookeeperQuorumSize=my-user-2117010560-1601663415, testKafkaAndZookeeperScaleUpScaleDown=my-user-838849261-890460734, testKafkaBridgeMetrics=my-user-829221238-874616880, testKafkaExporterDataAfterExchange=my-user-766848479-1588072395, testUpdateUser=my-user-1387197422-925410353, testTopicOperatorMetrics=my-user-779424699-479796683, testKafkaActiveControllers=my-user-1066197147-6205877, testKafkaExporterDifferentSetting=my-user-988491821-279531432, testKafkaConnectRequests=my-user-1167366435-1145507543, testCruiseControlMetrics=my-user-958954083-1578303162, testReceiveSimpleMessageTls=my-user-1669668617-1031881775, testClusterOperatorMetrics=my-user-467081550-2107716793, testKafkaBrokersCount=my-user-114552180-150189557, testUserOperatorMetrics=my-user-1969851044-309723582, testKafkaMetricsSettings=my-user-1339734863-677347400}
2022-03-30 21:00:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaConnectResponse=my-topic-680219046-979906955, testSendSimpleMessageTls=my-topic-435427300-917779110, testReconcileStateMetricInTopicOperator=my-topic-1267576332-2046216344, testKafkaConnectIoNetwork=my-topic-1593682376-1381222647, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-306860904-106838350, testSendMessagesCustomListenerTlsScramSha=my-topic-963888555-1712711326, testCruiseControlBasicAPIRequests=my-topic-1631570507-1308241024, testZookeeperWatchersCount=my-topic-1959052705-1334872805, testKafkaTopicPartitions=my-topic-1875007668-599688839, testMirrorMaker2Metrics=my-topic-643348725-45724501, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-307985263-926887950, testZookeeperAliveConnections=my-topic-1555557100-1445821129, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1672280868-1119644965, testKafkaTopicUnderReplicatedPartitions=my-topic-2103629457-772081434, testSendMessagesTlsScramSha=my-topic-255809133-823300208, testMirrorMaker2TlsAndTlsClientAuth=my-topic-247064469-1645874446, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206, testRackAwareConnectCorrectDeployment=my-topic-189499239-153088857, testZookeeperQuorumSize=my-topic-192478281-1148042900, testKafkaAndZookeeperScaleUpScaleDown=my-topic-2138084746-704381117, testKafkaBridgeMetrics=my-topic-1063894720-254889472, testKafkaExporterDataAfterExchange=my-topic-1937812163-1733130103, testUpdateUser=my-topic-1875783706-1763804507, testTopicOperatorMetrics=my-topic-1965542273-995674430, testKafkaActiveControllers=my-topic-1135110290-1277775808, testKafkaExporterDifferentSetting=my-topic-568145531-1290731867, testKafkaConnectRequests=my-topic-873062135-1706759978, testCruiseControlMetrics=my-topic-1917112707-1243703175, testReceiveSimpleMessageTls=my-topic-368564676-637091599, testClusterOperatorMetrics=my-topic-1416314702-1707243111, testKafkaBrokersCount=my-topic-222235994-1274701867, testUserOperatorMetrics=my-topic-1544093300-974581436, testKafkaMetricsSettings=my-topic-256215769-877131003}
2022-03-30 21:00:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaConnectResponse=my-cluster-2fe1834d-kafka-clients, testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e-kafka-clients, testKafkaConnectIoNetwork=my-cluster-2e56d7a8-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-8aed4745-kafka-clients, testZookeeperWatchersCount=my-cluster-22c0f078-kafka-clients, testKafkaTopicPartitions=my-cluster-0afaffb5-kafka-clients, testMirrorMaker2Metrics=my-cluster-5da4fc90-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634-kafka-clients, testZookeeperAliveConnections=my-cluster-b34ddefa-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-d686e2cd-kafka-clients, testSendMessagesTlsScramSha=my-cluster-84882f67-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-52516fd7-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273-kafka-clients, testZookeeperQuorumSize=my-cluster-16088501-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff-kafka-clients, testKafkaBridgeMetrics=my-cluster-9178bcb3-kafka-clients, testKafkaExporterDataAfterExchange=my-cluster-e86a0839-kafka-clients, testUpdateUser=my-cluster-6e6fa8fe-kafka-clients, testTopicOperatorMetrics=my-cluster-216189cb-kafka-clients, testKafkaActiveControllers=my-cluster-ede2d53e-kafka-clients, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60-kafka-clients, testKafkaConnectRequests=my-cluster-d1a00fce-kafka-clients, testCruiseControlMetrics=my-cluster-f49dfd8c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-2e450144-kafka-clients, testClusterOperatorMetrics=my-cluster-cfe17ead-kafka-clients, testKafkaBrokersCount=my-cluster-a943baf0-kafka-clients, testUserOperatorMetrics=my-cluster-d671df34-kafka-clients, testKafkaMetricsSettings=my-cluster-b7e9a534-kafka-clients}
2022-03-30 21:00:43 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-7 for test case:testMirrorMaker2TlsAndTlsClientAuth
2022-03-30 21:00:43 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-7
2022-03-30 21:00:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-7
2022-03-30 21:00:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-7 -o json
2022-03-30 21:00:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-7 -o json
2022-03-30 21:00:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:00:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T21:00:43Z",
        "name": "namespace-7",
        "resourceVersion": "93720",
        "selfLink": "/api/v1/namespaces/namespace-7",
        "uid": "cb54739a-ad18-4edb-8d22-ccb92f75e688"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 21:00:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@529573c9=[namespace-7]}
2022-03-30 21:00:43 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-7
2022-03-30 21:00:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-7, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 21:00:43 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-7
2022-03-30 21:00:43 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-52516fd7-source in namespace namespace-7
2022-03-30 21:00:43 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-30 21:00:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-52516fd7-source
2022-03-30 21:00:43 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-52516fd7-source will have desired state: Ready
2022-03-30 21:00:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-52516fd7-source will have desired state: Ready
2022-03-30 21:00:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (839998ms till timeout)
2022-03-30 21:00:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (838995ms till timeout)
2022-03-30 21:00:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (837991ms till timeout)
2022-03-30 21:00:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:00:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (836988ms till timeout)
2022-03-30 21:00:47 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (835986ms till timeout)
2022-03-30 21:00:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (834983ms till timeout)
2022-03-30 21:00:49 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (833979ms till timeout)
2022-03-30 21:00:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (832977ms till timeout)
2022-03-30 21:00:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:00:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (831973ms till timeout)
2022-03-30 21:00:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (830970ms till timeout)
2022-03-30 21:00:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (829967ms till timeout)
2022-03-30 21:00:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (828964ms till timeout)
2022-03-30 21:00:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (827961ms till timeout)
2022-03-30 21:00:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:00:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (826958ms till timeout)
2022-03-30 21:00:57 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (825955ms till timeout)
2022-03-30 21:00:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (824952ms till timeout)
2022-03-30 21:00:59 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (823949ms till timeout)
2022-03-30 21:01:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (822945ms till timeout)
2022-03-30 21:01:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:01:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (821942ms till timeout)
2022-03-30 21:01:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (820939ms till timeout)
2022-03-30 21:01:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (819935ms till timeout)
2022-03-30 21:01:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (818932ms till timeout)
2022-03-30 21:01:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (817929ms till timeout)
2022-03-30 21:01:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:01:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (816926ms till timeout)
2022-03-30 21:01:07 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (815923ms till timeout)
2022-03-30 21:01:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (814920ms till timeout)
2022-03-30 21:01:09 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (813917ms till timeout)
2022-03-30 21:01:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (812914ms till timeout)
2022-03-30 21:01:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:01:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (811911ms till timeout)
2022-03-30 21:01:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (810907ms till timeout)
2022-03-30 21:01:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (809904ms till timeout)
2022-03-30 21:01:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (808901ms till timeout)
2022-03-30 21:01:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (807897ms till timeout)
2022-03-30 21:01:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:01:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (806894ms till timeout)
2022-03-30 21:01:17 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (805891ms till timeout)
2022-03-30 21:01:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (804888ms till timeout)
2022-03-30 21:01:19 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (803885ms till timeout)
2022-03-30 21:01:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (802882ms till timeout)
2022-03-30 21:01:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:01:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (801878ms till timeout)
2022-03-30 21:01:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (800875ms till timeout)
2022-03-30 21:01:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (799872ms till timeout)
2022-03-30 21:01:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (798870ms till timeout)
2022-03-30 21:01:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (797866ms till timeout)
2022-03-30 21:01:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:01:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (796863ms till timeout)
2022-03-30 21:01:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (795860ms till timeout)
2022-03-30 21:01:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (794857ms till timeout)
2022-03-30 21:01:29 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (793853ms till timeout)
2022-03-30 21:01:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (792850ms till timeout)
2022-03-30 21:01:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:01:31 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (791847ms till timeout)
2022-03-30 21:01:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (790843ms till timeout)
2022-03-30 21:01:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (789840ms till timeout)
2022-03-30 21:01:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (788837ms till timeout)
2022-03-30 21:01:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (787834ms till timeout)
2022-03-30 21:01:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:01:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (786830ms till timeout)
2022-03-30 21:01:37 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (785827ms till timeout)
2022-03-30 21:01:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (784824ms till timeout)
2022-03-30 21:01:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (783822ms till timeout)
2022-03-30 21:01:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (782819ms till timeout)
2022-03-30 21:01:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:01:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (781815ms till timeout)
2022-03-30 21:01:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (780812ms till timeout)
2022-03-30 21:01:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (779809ms till timeout)
2022-03-30 21:01:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (778806ms till timeout)
2022-03-30 21:01:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (777802ms till timeout)
2022-03-30 21:01:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:01:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (776800ms till timeout)
2022-03-30 21:01:47 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (775796ms till timeout)
2022-03-30 21:01:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (774793ms till timeout)
2022-03-30 21:01:49 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (773790ms till timeout)
2022-03-30 21:01:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-source will have desired state: Ready not ready, will try again in 1000 ms (772787ms till timeout)
2022-03-30 21:01:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:01:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-52516fd7-source is in desired state: Ready
2022-03-30 21:01:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-52516fd7-target in namespace namespace-7
2022-03-30 21:01:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-30 21:01:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-52516fd7-target
2022-03-30 21:01:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-52516fd7-target will have desired state: Ready
2022-03-30 21:01:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-52516fd7-target will have desired state: Ready
2022-03-30 21:01:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (839998ms till timeout)
2022-03-30 21:01:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (838995ms till timeout)
2022-03-30 21:01:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (837992ms till timeout)
2022-03-30 21:01:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (836989ms till timeout)
2022-03-30 21:01:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (835986ms till timeout)
2022-03-30 21:01:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:01:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (834984ms till timeout)
2022-03-30 21:01:57 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (833980ms till timeout)
2022-03-30 21:01:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (832978ms till timeout)
2022-03-30 21:01:59 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (831975ms till timeout)
2022-03-30 21:02:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (830972ms till timeout)
2022-03-30 21:02:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:02:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (829969ms till timeout)
2022-03-30 21:02:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (828966ms till timeout)
2022-03-30 21:02:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (827963ms till timeout)
2022-03-30 21:02:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (826960ms till timeout)
2022-03-30 21:02:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (825957ms till timeout)
2022-03-30 21:02:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:02:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (824954ms till timeout)
2022-03-30 21:02:07 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (823950ms till timeout)
2022-03-30 21:02:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (822948ms till timeout)
2022-03-30 21:02:09 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (821945ms till timeout)
2022-03-30 21:02:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (820942ms till timeout)
2022-03-30 21:02:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:02:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (819939ms till timeout)
2022-03-30 21:02:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (818936ms till timeout)
2022-03-30 21:02:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (817934ms till timeout)
2022-03-30 21:02:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (816931ms till timeout)
2022-03-30 21:02:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (815928ms till timeout)
2022-03-30 21:02:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:02:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (814925ms till timeout)
2022-03-30 21:02:17 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (813922ms till timeout)
2022-03-30 21:02:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (812919ms till timeout)
2022-03-30 21:02:19 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (811916ms till timeout)
2022-03-30 21:02:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (810913ms till timeout)
2022-03-30 21:02:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:02:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (809910ms till timeout)
2022-03-30 21:02:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (808907ms till timeout)
2022-03-30 21:02:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (807904ms till timeout)
2022-03-30 21:02:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (806901ms till timeout)
2022-03-30 21:02:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (805898ms till timeout)
2022-03-30 21:02:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:02:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (804894ms till timeout)
2022-03-30 21:02:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (803891ms till timeout)
2022-03-30 21:02:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (802888ms till timeout)
2022-03-30 21:02:29 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (801885ms till timeout)
2022-03-30 21:02:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (800881ms till timeout)
2022-03-30 21:02:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:02:31 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (799878ms till timeout)
2022-03-30 21:02:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (798875ms till timeout)
2022-03-30 21:02:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (797872ms till timeout)
2022-03-30 21:02:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (796869ms till timeout)
2022-03-30 21:02:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (795866ms till timeout)
2022-03-30 21:02:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:02:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (794863ms till timeout)
2022-03-30 21:02:37 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (793860ms till timeout)
2022-03-30 21:02:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (792857ms till timeout)
2022-03-30 21:02:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (791854ms till timeout)
2022-03-30 21:02:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (790850ms till timeout)
2022-03-30 21:02:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:02:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (789846ms till timeout)
2022-03-30 21:02:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (788843ms till timeout)
2022-03-30 21:02:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (787837ms till timeout)
2022-03-30 21:02:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (786834ms till timeout)
2022-03-30 21:02:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:02:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (785831ms till timeout)
2022-03-30 21:02:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (784828ms till timeout)
2022-03-30 21:02:47 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (783825ms till timeout)
2022-03-30 21:02:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (782822ms till timeout)
2022-03-30 21:02:49 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (781819ms till timeout)
2022-03-30 21:02:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:02:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (780816ms till timeout)
2022-03-30 21:02:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (779813ms till timeout)
2022-03-30 21:02:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (778811ms till timeout)
2022-03-30 21:02:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (777808ms till timeout)
2022-03-30 21:02:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (776805ms till timeout)
2022-03-30 21:02:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:02:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (775802ms till timeout)
2022-03-30 21:02:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (774799ms till timeout)
2022-03-30 21:02:57 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (773796ms till timeout)
2022-03-30 21:02:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (772794ms till timeout)
2022-03-30 21:02:59 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-52516fd7-target will have desired state: Ready not ready, will try again in 1000 ms (771791ms till timeout)
2022-03-30 21:03:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:03:00 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-52516fd7-target is in desired state: Ready
2022-03-30 21:03:00 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic mirrormaker2-topic-example-1455540718 in namespace namespace-7
2022-03-30 21:03:00 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-30 21:03:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:mirrormaker2-topic-example-1455540718
2022-03-30 21:03:00 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: mirrormaker2-topic-example-1455540718 will have desired state: Ready
2022-03-30 21:03:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: mirrormaker2-topic-example-1455540718 will have desired state: Ready
2022-03-30 21:03:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaTopic: mirrormaker2-topic-example-1455540718 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 21:03:01 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: mirrormaker2-topic-example-1455540718 is in desired state: Ready
2022-03-30 21:03:01 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-52516fd7-my-user-source in namespace namespace-7
2022-03-30 21:03:01 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-30 21:03:01 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-cluster-52516fd7-my-user-source
2022-03-30 21:03:01 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-52516fd7-my-user-source will have desired state: Ready
2022-03-30 21:03:01 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-cluster-52516fd7-my-user-source will have desired state: Ready
2022-03-30 21:03:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaUser: my-cluster-52516fd7-my-user-source will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 21:03:02 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-52516fd7-my-user-source is in desired state: Ready
2022-03-30 21:03:02 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-52516fd7-my-user-target in namespace namespace-7
2022-03-30 21:03:02 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-30 21:03:02 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-cluster-52516fd7-my-user-target
2022-03-30 21:03:02 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-52516fd7-my-user-target will have desired state: Ready
2022-03-30 21:03:02 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-cluster-52516fd7-my-user-target will have desired state: Ready
2022-03-30 21:03:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaUser: my-cluster-52516fd7-my-user-target will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 21:03:03 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-52516fd7-my-user-target is in desired state: Ready
2022-03-30 21:03:03 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-52516fd7-kafka-clients in namespace namespace-7
2022-03-30 21:03:03 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-30 21:03:03 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for pod with prefixmy-cluster-52516fd7-kafka-clients is present.
2022-03-30 21:03:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] pod with prefixmy-cluster-52516fd7-kafka-clients is present. not ready, will try again in 10000 ms (299996ms till timeout)
2022-03-30 21:03:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:03:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:03:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-247064469-1645874446-test-1 in namespace namespace-7
2022-03-30 21:03:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-30 21:03:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-247064469-1645874446-test-1
2022-03-30 21:03:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-247064469-1645874446-test-1 will have desired state: Ready
2022-03-30 21:03:13 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-247064469-1645874446-test-1 will have desired state: Ready
2022-03-30 21:03:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-247064469-1645874446-test-1 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 21:03:14 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-247064469-1645874446-test-1 is in desired state: Ready
2022-03-30 21:03:14 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-247064469-1645874446-test-2 in namespace namespace-7
2022-03-30 21:03:14 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-30 21:03:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-247064469-1645874446-test-2
2022-03-30 21:03:14 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-247064469-1645874446-test-2 will have desired state: Ready
2022-03-30 21:03:14 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-247064469-1645874446-test-2 will have desired state: Ready
2022-03-30 21:03:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-247064469-1645874446-test-2 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 21:03:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:03:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-247064469-1645874446-test-2 is in desired state: Ready
2022-03-30 21:03:15 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-30 21:03:15 [ForkJoinPool-3-worker-7] [32mINFO [m [MirrorMaker2IsolatedST:328] Setting topic to my-topic-247064469-1645874446-test-2, cluster to my-cluster-52516fd7-target and changing user to my-cluster-52516fd7-my-user-target
2022-03-30 21:03:15 [ForkJoinPool-3-worker-7] [32mINFO [m [MirrorMaker2IsolatedST:337] Sending messages to - topic my-topic-247064469-1645874446-test-2, cluster my-cluster-52516fd7-target and message count of 200
2022-03-30 21:03:15 [ForkJoinPool-3-worker-7] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5284213a, which are set.
2022-03-30 21:03:15 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@28bd8aed, messages=[], arguments=[USER=my_cluster_52516fd7_my_user_target, --topic, my-topic-247064469-1645874446-test-2, --bootstrap-server, my-cluster-52516fd7-target-kafka-bootstrap.namespace-7.svc:9093, --max-messages, 200], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-52516fd7-kafka-clients-54849f8974-wcltl', podNamespace='namespace-7', bootstrapServer='my-cluster-52516fd7-target-kafka-bootstrap.namespace-7.svc:9093', topicName='my-topic-247064469-1645874446-test-2', maxMessages=200, kafkaUsername='my-cluster-52516fd7-my-user-target', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5284213a}
2022-03-30 21:03:15 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-52516fd7-target-kafka-bootstrap.namespace-7.svc:9093:my-topic-247064469-1645874446-test-2 from pod my-cluster-52516fd7-kafka-clients-54849f8974-wcltl
2022-03-30 21:03:15 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-52516fd7-kafka-clients-54849f8974-wcltl -n namespace-7 -- /opt/kafka/producer.sh USER=my_cluster_52516fd7_my_user_target --topic my-topic-247064469-1645874446-test-2 --bootstrap-server my-cluster-52516fd7-target-kafka-bootstrap.namespace-7.svc:9093 --max-messages 200
2022-03-30 21:03:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-52516fd7-kafka-clients-54849f8974-wcltl -n namespace-7 -- /opt/kafka/producer.sh USER=my_cluster_52516fd7_my_user_target --topic my-topic-247064469-1645874446-test-2 --bootstrap-server my-cluster-52516fd7-target-kafka-bootstrap.namespace-7.svc:9093 --max-messages 200
2022-03-30 21:03:19 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-30 21:03:19 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-03-30 21:03:19 [ForkJoinPool-3-worker-7] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5538070a, which are set.
2022-03-30 21:03:19 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@26466166, messages=[], arguments=[USER=my_cluster_52516fd7_my_user_target, --group-id, my-consumer-group-1751936383, --topic, my-topic-247064469-1645874446-test-2, --group-instance-id, instance2141691805, --bootstrap-server, my-cluster-52516fd7-target-kafka-bootstrap.namespace-7.svc:9093, --max-messages, 200], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-52516fd7-kafka-clients-54849f8974-wcltl', podNamespace='namespace-7', bootstrapServer='my-cluster-52516fd7-target-kafka-bootstrap.namespace-7.svc:9093', topicName='my-topic-247064469-1645874446-test-2', maxMessages=200, kafkaUsername='my-cluster-52516fd7-my-user-target', consumerGroupName='my-consumer-group-1751936383', consumerInstanceId='instance2141691805', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5538070a}
2022-03-30 21:03:19 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-52516fd7-target-kafka-bootstrap.namespace-7.svc:9093:my-topic-247064469-1645874446-test-2 from pod my-cluster-52516fd7-kafka-clients-54849f8974-wcltl
2022-03-30 21:03:19 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-52516fd7-kafka-clients-54849f8974-wcltl -n namespace-7 -- /opt/kafka/consumer.sh USER=my_cluster_52516fd7_my_user_target --group-id my-consumer-group-1751936383 --topic my-topic-247064469-1645874446-test-2 --group-instance-id instance2141691805 --bootstrap-server my-cluster-52516fd7-target-kafka-bootstrap.namespace-7.svc:9093 --max-messages 200
2022-03-30 21:03:19 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-52516fd7-kafka-clients-54849f8974-wcltl -n namespace-7 -- /opt/kafka/consumer.sh USER=my_cluster_52516fd7_my_user_target --group-id my-consumer-group-1751936383 --topic my-topic-247064469-1645874446-test-2 --group-instance-id instance2141691805 --bootstrap-server my-cluster-52516fd7-target-kafka-bootstrap.namespace-7.svc:9093 --max-messages 200
2022-03-30 21:03:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:03:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:03:26 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 21:03:26 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-03-30 21:03:26 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-52516fd7 in namespace namespace-7
2022-03-30 21:03:26 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-03-30 21:03:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaMirrorMaker2:my-cluster-52516fd7
2022-03-30 21:03:26 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready
2022-03-30 21:03:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready
2022-03-30 21:03:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-30 21:03:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-30 21:03:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (597991ms till timeout)
2022-03-30 21:03:29 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (596987ms till timeout)
2022-03-30 21:03:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (595984ms till timeout)
2022-03-30 21:03:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:03:31 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (594980ms till timeout)
2022-03-30 21:03:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (593977ms till timeout)
2022-03-30 21:03:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (592974ms till timeout)
2022-03-30 21:03:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (591969ms till timeout)
2022-03-30 21:03:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (590966ms till timeout)
2022-03-30 21:03:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:03:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (589962ms till timeout)
2022-03-30 21:03:37 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (588959ms till timeout)
2022-03-30 21:03:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (587956ms till timeout)
2022-03-30 21:03:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (586952ms till timeout)
2022-03-30 21:03:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (585949ms till timeout)
2022-03-30 21:03:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:03:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (584945ms till timeout)
2022-03-30 21:03:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (583942ms till timeout)
2022-03-30 21:03:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (582939ms till timeout)
2022-03-30 21:03:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (581935ms till timeout)
2022-03-30 21:03:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (580932ms till timeout)
2022-03-30 21:03:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:03:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (579929ms till timeout)
2022-03-30 21:03:47 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (578925ms till timeout)
2022-03-30 21:03:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (577922ms till timeout)
2022-03-30 21:03:49 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (576919ms till timeout)
2022-03-30 21:03:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (575916ms till timeout)
2022-03-30 21:03:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:03:51 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (574912ms till timeout)
2022-03-30 21:03:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (573909ms till timeout)
2022-03-30 21:03:53 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (572906ms till timeout)
2022-03-30 21:03:54 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (571902ms till timeout)
2022-03-30 21:03:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (570899ms till timeout)
2022-03-30 21:03:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:03:56 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (569895ms till timeout)
2022-03-30 21:03:57 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (568892ms till timeout)
2022-03-30 21:03:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (567889ms till timeout)
2022-03-30 21:03:59 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (566886ms till timeout)
2022-03-30 21:04:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (565882ms till timeout)
2022-03-30 21:04:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:04:01 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (564879ms till timeout)
2022-03-30 21:04:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (563875ms till timeout)
2022-03-30 21:04:03 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (562872ms till timeout)
2022-03-30 21:04:04 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (561868ms till timeout)
2022-03-30 21:04:05 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (560865ms till timeout)
2022-03-30 21:04:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:04:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (559862ms till timeout)
2022-03-30 21:04:07 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (558859ms till timeout)
2022-03-30 21:04:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (557855ms till timeout)
2022-03-30 21:04:09 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (556852ms till timeout)
2022-03-30 21:04:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (555849ms till timeout)
2022-03-30 21:04:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:04:11 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (554845ms till timeout)
2022-03-30 21:04:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (553842ms till timeout)
2022-03-30 21:04:13 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (552839ms till timeout)
2022-03-30 21:04:14 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (551835ms till timeout)
2022-03-30 21:04:15 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (550832ms till timeout)
2022-03-30 21:04:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:04:16 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (549829ms till timeout)
2022-03-30 21:04:17 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (548825ms till timeout)
2022-03-30 21:04:18 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (547822ms till timeout)
2022-03-30 21:04:19 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (546819ms till timeout)
2022-03-30 21:04:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (545815ms till timeout)
2022-03-30 21:04:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:04:21 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (544812ms till timeout)
2022-03-30 21:04:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (543809ms till timeout)
2022-03-30 21:04:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (542806ms till timeout)
2022-03-30 21:04:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (541802ms till timeout)
2022-03-30 21:04:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (540799ms till timeout)
2022-03-30 21:04:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:04:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (539795ms till timeout)
2022-03-30 21:04:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (538792ms till timeout)
2022-03-30 21:04:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (537789ms till timeout)
2022-03-30 21:04:29 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (536786ms till timeout)
2022-03-30 21:04:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (535782ms till timeout)
2022-03-30 21:04:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:04:31 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (534779ms till timeout)
2022-03-30 21:04:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (533776ms till timeout)
2022-03-30 21:04:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker2: my-cluster-52516fd7 will have desired state: Ready not ready, will try again in 1000 ms (532773ms till timeout)
2022-03-30 21:04:34 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-52516fd7 is in desired state: Ready
2022-03-30 21:04:34 [ForkJoinPool-3-worker-7] [32mINFO [m [MirrorMaker2IsolatedST:397] Setting topic to mirrormaker2-topic-example-1455540718, cluster to my-cluster-52516fd7-source and changing user to my-cluster-52516fd7-my-user-source
2022-03-30 21:04:34 [ForkJoinPool-3-worker-7] [32mINFO [m [MirrorMaker2IsolatedST:407] Sending messages to - topic mirrormaker2-topic-example-1455540718, cluster my-cluster-52516fd7-source and message count of 200
2022-03-30 21:04:34 [ForkJoinPool-3-worker-7] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7b222563, which are set.
2022-03-30 21:04:34 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@570fa73f, messages=[], arguments=[USER=my_cluster_52516fd7_my_user_source, --topic, mirrormaker2-topic-example-1455540718, --bootstrap-server, my-cluster-52516fd7-source-kafka-bootstrap.namespace-7.svc:9093, --max-messages, 200], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-52516fd7-kafka-clients-54849f8974-wcltl', podNamespace='namespace-7', bootstrapServer='my-cluster-52516fd7-source-kafka-bootstrap.namespace-7.svc:9093', topicName='mirrormaker2-topic-example-1455540718', maxMessages=200, kafkaUsername='my-cluster-52516fd7-my-user-source', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7b222563}
2022-03-30 21:04:34 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-52516fd7-source-kafka-bootstrap.namespace-7.svc:9093:mirrormaker2-topic-example-1455540718 from pod my-cluster-52516fd7-kafka-clients-54849f8974-wcltl
2022-03-30 21:04:34 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-52516fd7-kafka-clients-54849f8974-wcltl -n namespace-7 -- /opt/kafka/producer.sh USER=my_cluster_52516fd7_my_user_source --topic mirrormaker2-topic-example-1455540718 --bootstrap-server my-cluster-52516fd7-source-kafka-bootstrap.namespace-7.svc:9093 --max-messages 200
2022-03-30 21:04:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-52516fd7-kafka-clients-54849f8974-wcltl -n namespace-7 -- /opt/kafka/producer.sh USER=my_cluster_52516fd7_my_user_source --topic mirrormaker2-topic-example-1455540718 --bootstrap-server my-cluster-52516fd7-source-kafka-bootstrap.namespace-7.svc:9093 --max-messages 200
2022-03-30 21:04:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:04:38 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-30 21:04:38 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-03-30 21:04:38 [ForkJoinPool-3-worker-7] [32mINFO [m [MirrorMaker2IsolatedST:411] Receiving messages from - topic mirrormaker2-topic-example-1455540718, cluster my-cluster-52516fd7-source and message count of 200
2022-03-30 21:04:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@12a5266e, which are set.
2022-03-30 21:04:38 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@78c61064, messages=[], arguments=[USER=my_cluster_52516fd7_my_user_source, --group-id, my-consumer-group-1751936383, --topic, mirrormaker2-topic-example-1455540718, --group-instance-id, instance1844145759, --bootstrap-server, my-cluster-52516fd7-source-kafka-bootstrap.namespace-7.svc:9093, --max-messages, 200], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-52516fd7-kafka-clients-54849f8974-wcltl', podNamespace='namespace-7', bootstrapServer='my-cluster-52516fd7-source-kafka-bootstrap.namespace-7.svc:9093', topicName='mirrormaker2-topic-example-1455540718', maxMessages=200, kafkaUsername='my-cluster-52516fd7-my-user-source', consumerGroupName='my-consumer-group-1751936383', consumerInstanceId='instance1844145759', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@12a5266e}
2022-03-30 21:04:38 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-52516fd7-source-kafka-bootstrap.namespace-7.svc:9093:mirrormaker2-topic-example-1455540718 from pod my-cluster-52516fd7-kafka-clients-54849f8974-wcltl
2022-03-30 21:04:38 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-52516fd7-kafka-clients-54849f8974-wcltl -n namespace-7 -- /opt/kafka/consumer.sh USER=my_cluster_52516fd7_my_user_source --group-id my-consumer-group-1751936383 --topic mirrormaker2-topic-example-1455540718 --group-instance-id instance1844145759 --bootstrap-server my-cluster-52516fd7-source-kafka-bootstrap.namespace-7.svc:9093 --max-messages 200
2022-03-30 21:04:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-52516fd7-kafka-clients-54849f8974-wcltl -n namespace-7 -- /opt/kafka/consumer.sh USER=my_cluster_52516fd7_my_user_source --group-id my-consumer-group-1751936383 --topic mirrormaker2-topic-example-1455540718 --group-instance-id instance1844145759 --bootstrap-server my-cluster-52516fd7-source-kafka-bootstrap.namespace-7.svc:9093 --max-messages 200
2022-03-30 21:04:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:04:45 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 21:04:45 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-03-30 21:04:45 [ForkJoinPool-3-worker-7] [32mINFO [m [MirrorMaker2IsolatedST:418] Now setting topic to my-cluster-52516fd7-source.mirrormaker2-topic-example-1455540718, cluster to my-cluster-52516fd7-target and user to my-cluster-52516fd7-my-user-target - the messages should be mirrored
2022-03-30 21:04:45 [ForkJoinPool-3-worker-7] [32mINFO [m [MirrorMaker2IsolatedST:427] Consumer in target cluster and topic should receive 200 messages
2022-03-30 21:04:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@525a761c, which are set.
2022-03-30 21:04:45 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@4713f481, messages=[], arguments=[USER=my_cluster_52516fd7_my_user_target, --group-id, my-consumer-group-1751936383, --topic, my-cluster-52516fd7-source.mirrormaker2-topic-example-1455540718, --group-instance-id, instance63983851, --bootstrap-server, my-cluster-52516fd7-target-kafka-bootstrap.namespace-7.svc:9093, --max-messages, 200], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-52516fd7-kafka-clients-54849f8974-wcltl', podNamespace='namespace-7', bootstrapServer='my-cluster-52516fd7-target-kafka-bootstrap.namespace-7.svc:9093', topicName='my-cluster-52516fd7-source.mirrormaker2-topic-example-1455540718', maxMessages=200, kafkaUsername='my-cluster-52516fd7-my-user-target', consumerGroupName='my-consumer-group-1751936383', consumerInstanceId='instance63983851', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@525a761c}
2022-03-30 21:04:45 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-52516fd7-target-kafka-bootstrap.namespace-7.svc:9093:my-cluster-52516fd7-source.mirrormaker2-topic-example-1455540718 from pod my-cluster-52516fd7-kafka-clients-54849f8974-wcltl
2022-03-30 21:04:45 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-52516fd7-kafka-clients-54849f8974-wcltl -n namespace-7 -- /opt/kafka/consumer.sh USER=my_cluster_52516fd7_my_user_target --group-id my-consumer-group-1751936383 --topic my-cluster-52516fd7-source.mirrormaker2-topic-example-1455540718 --group-instance-id instance63983851 --bootstrap-server my-cluster-52516fd7-target-kafka-bootstrap.namespace-7.svc:9093 --max-messages 200
2022-03-30 21:04:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-52516fd7-kafka-clients-54849f8974-wcltl -n namespace-7 -- /opt/kafka/consumer.sh USER=my_cluster_52516fd7_my_user_target --group-id my-consumer-group-1751936383 --topic my-cluster-52516fd7-source.mirrormaker2-topic-example-1455540718 --group-instance-id instance63983851 --bootstrap-server my-cluster-52516fd7-target-kafka-bootstrap.namespace-7.svc:9093 --max-messages 200
2022-03-30 21:04:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:04:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:04:51 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 21:04:51 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-03-30 21:04:51 [ForkJoinPool-3-worker-7] [32mINFO [m [MirrorMaker2IsolatedST:432] Messages successfully mirrored
2022-03-30 21:04:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 21:04:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:675] [mirrormaker.MirrorMaker2IsolatedST - After Each] - Clean up after test
2022-03-30 21:04:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 21:04:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testMirrorMaker2TlsAndTlsClientAuth
2022-03-30 21:04:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-52516fd7-kafka-clients in namespace namespace-7
2022-03-30 21:04:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic mirrormaker2-topic-example-1455540718 in namespace namespace-7
2022-03-30 21:04:51 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-52516fd7-kafka-clients
2022-03-30 21:04:52 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:mirrormaker2-topic-example-1455540718
2022-03-30 21:04:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:mirrormaker2-topic-example-1455540718 not ready, will try again in 10000 ms (179993ms till timeout)
2022-03-30 21:04:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-52516fd7-kafka-clients not ready, will try again in 10000 ms (479986ms till timeout)
2022-03-30 21:04:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:05:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:05:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:mirrormaker2-topic-example-1455540718 not ready, will try again in 10000 ms (169987ms till timeout)
2022-03-30 21:05:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-52516fd7-kafka-clients not ready, will try again in 10000 ms (469976ms till timeout)
2022-03-30 21:05:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:05:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:05:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:mirrormaker2-topic-example-1455540718 not ready, will try again in 10000 ms (159981ms till timeout)
2022-03-30 21:05:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-52516fd7-kafka-clients not ready, will try again in 10000 ms (459968ms till timeout)
2022-03-30 21:05:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:05:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:05:22 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:mirrormaker2-topic-example-1455540718 not ready, will try again in 10000 ms (149974ms till timeout)
2022-03-30 21:05:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-52516fd7-kafka-clients not ready, will try again in 10000 ms (449958ms till timeout)
2022-03-30 21:05:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:05:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:05:32 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:mirrormaker2-topic-example-1455540718 not ready, will try again in 10000 ms (139968ms till timeout)
2022-03-30 21:05:32 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-52516fd7-my-user-target in namespace namespace-7
2022-03-30 21:05:32 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-cluster-52516fd7-my-user-target
2022-03-30 21:05:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-cluster-52516fd7-my-user-target not ready, will try again in 10000 ms (179994ms till timeout)
2022-03-30 21:05:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:05:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:05:42 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:mirrormaker2-topic-example-1455540718 not ready, will try again in 10000 ms (129961ms till timeout)
2022-03-30 21:05:42 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-247064469-1645874446-test-2 in namespace namespace-7
2022-03-30 21:05:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-247064469-1645874446-test-2
2022-03-30 21:05:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-247064469-1645874446-test-2 not ready, will try again in 10000 ms (179993ms till timeout)
2022-03-30 21:05:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:05:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:05:52 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:mirrormaker2-topic-example-1455540718 not ready, will try again in 10000 ms (119955ms till timeout)
2022-03-30 21:05:52 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-52516fd7 in namespace namespace-7
2022-03-30 21:05:52 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaMirrorMaker2:my-cluster-52516fd7
2022-03-30 21:05:52 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaMirrorMaker2:my-cluster-52516fd7 not ready, will try again in 10000 ms (599992ms till timeout)
2022-03-30 21:05:55 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:06:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:06:02 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-52516fd7-my-user-source in namespace namespace-7
2022-03-30 21:06:02 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-cluster-52516fd7-my-user-source
2022-03-30 21:06:02 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-cluster-52516fd7-my-user-source not ready, will try again in 10000 ms (179994ms till timeout)
2022-03-30 21:06:02 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-247064469-1645874446-test-1 in namespace namespace-7
2022-03-30 21:06:02 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-247064469-1645874446-test-1
2022-03-30 21:06:02 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-247064469-1645874446-test-1 not ready, will try again in 10000 ms (179994ms till timeout)
2022-03-30 21:06:05 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:06:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:06:12 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-52516fd7-target in namespace namespace-7
2022-03-30 21:06:12 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-52516fd7-target
2022-03-30 21:06:12 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-52516fd7-target not ready, will try again in 10000 ms (839957ms till timeout)
2022-03-30 21:06:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-52516fd7-source in namespace namespace-7
2022-03-30 21:06:12 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-52516fd7-source
2022-03-30 21:06:12 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-52516fd7-source not ready, will try again in 10000 ms (839997ms till timeout)
2022-03-30 21:06:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:06:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:06:22 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 21:06:22 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-7 for test case:testMirrorMaker2TlsAndTlsClientAuth
2022-03-30 21:06:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-7 removal
2022-03-30 21:06:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:22 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:06:22 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (479929ms till timeout)
2022-03-30 21:06:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:23 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:06:23 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (478852ms till timeout)
2022-03-30 21:06:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:24 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:06:24 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (477767ms till timeout)
2022-03-30 21:06:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:25 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:06:25 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (476695ms till timeout)
2022-03-30 21:06:25 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:06:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:26 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:06:26 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (475624ms till timeout)
2022-03-30 21:06:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:27 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:06:27 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (474547ms till timeout)
2022-03-30 21:06:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:28 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:06:28 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (473470ms till timeout)
2022-03-30 21:06:29 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:29 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:29 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:06:29 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (472399ms till timeout)
2022-03-30 21:06:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:06:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:06:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (471325ms till timeout)
2022-03-30 21:06:31 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:31 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:31 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:06:31 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (470252ms till timeout)
2022-03-30 21:06:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:32 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:32 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:06:32 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (469172ms till timeout)
2022-03-30 21:06:33 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:34 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:34 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:06:34 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (468104ms till timeout)
2022-03-30 21:06:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:35 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:06:35 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (467030ms till timeout)
2022-03-30 21:06:35 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:06:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:36 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:06:36 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (465957ms till timeout)
2022-03-30 21:06:37 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:37 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:37 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:06:37 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (464884ms till timeout)
2022-03-30 21:06:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:38 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:06:38 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (463815ms till timeout)
2022-03-30 21:06:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:39 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:06:39 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (462736ms till timeout)
2022-03-30 21:06:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:06:40 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (461660ms till timeout)
2022-03-30 21:06:40 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:06:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:41 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:41 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:06:41 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (460588ms till timeout)
2022-03-30 21:06:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:42 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:06:42 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (459520ms till timeout)
2022-03-30 21:06:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:43 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:06:43 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (458448ms till timeout)
2022-03-30 21:06:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:44 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:06:44 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (457370ms till timeout)
2022-03-30 21:06:45 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:127] Suite watcher.AllNamespaceIsolatedST is waiting to lock to be released.
2022-03-30 21:06:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:45 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:06:45 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (456291ms till timeout)
2022-03-30 21:06:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:46 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:06:46 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (455214ms till timeout)
2022-03-30 21:06:47 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:47 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:47 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:06:47 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (454146ms till timeout)
2022-03-30 21:06:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-7 get Namespace namespace-7 -o yaml
2022-03-30 21:06:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 21:06:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 21:06:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-7" not found
2022-03-30 21:06:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 21:06:49 [ForkJoinPool-3-worker-7] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@529573c9=[]}
2022-03-30 21:06:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:267] testMirrorMaker2TlsAndTlsClientAuth - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendSimpleMessageTls, testUpdateUser, testCruiseControlBasicAPIRequests, testReceiveSimpleMessageTls, testSendMessagesTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testKafkaExporterDifferentSetting, testKafkaConnectResponse, testKafkaConnectRequests, testTopicOperatorMetrics, testCruiseControlMetrics, testClusterOperatorMetrics, testKafkaMetricsSettings, testKafkaBridgeMetrics, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaConnectIoNetwork, testKafkaBrokersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testMirrorMaker2TlsAndTlsClientAuth] to and randomly select one to start execution
2022-03-30 21:06:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:93] [mirrormaker.MirrorMaker2IsolatedST] - Removing parallel test: testMirrorMaker2TlsAndTlsClientAuth
2022-03-30 21:06:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [SuiteThreadController:97] [mirrormaker.MirrorMaker2IsolatedST] - Parallel test count: 0
2022-03-30 21:06:49 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testMirrorMaker2TlsAndTlsClientAuth-FINISHED
2022-03-30 21:06:49 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 21:06:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 21:06:49 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractST:690] [mirrormaker.MirrorMaker2IsolatedST - After All] - Clean up after test suite
2022-03-30 21:06:49 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 21:06:49 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:346] In context MirrorMaker2IsolatedST is everything deleted.
2022-03-30 21:06:49 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3,040.416 s - in io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST
2022-03-30 21:06:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SuiteThreadController:136] Suite watcher.AllNamespaceIsolatedST has locked the @IsolatedSuite and other @IsolatedSuites must wait until lock is released.
2022-03-30 21:06:50 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:06:50 [ForkJoinPool-3-worker-1] [32mINFO [m [AllNamespaceIsolatedST:190] Creating resources before the test class
2022-03-30 21:06:50 [ForkJoinPool-3-worker-1] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-30 21:06:50 [ForkJoinPool-3-worker-1] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-30 21:06:50 [ForkJoinPool-3-worker-1] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-30 21:06:50 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 21:06:50 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-30 21:06:50 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 21:06:50 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 21:06:50 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 21:06:50 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 21:06:50 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 21:06:50 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 21:06:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io not ready, will try again in 10000 ms (179985ms till timeout)
2022-03-30 21:06:50 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 21:06:50 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 21:06:50 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 21:06:50 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 21:06:50 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179963ms till timeout)
2022-03-30 21:06:50 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 21:06:50 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 21:06:50 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 21:06:50 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 21:06:50 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 21:06:50 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 21:06:50 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 21:06:50 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 21:06:50 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 21:06:50 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 21:06:50 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 21:06:50 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 21:06:50 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179905ms till timeout)
2022-03-30 21:06:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:07:00 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:07:00 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 21:07:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 21:07:00 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 21:07:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 21:07:00 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 21:07:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 21:07:00 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 21:07:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 21:07:00 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 21:07:00 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 21:07:00 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 21:07:00 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io not ready, will try again in 10000 ms (179991ms till timeout)
2022-03-30 21:07:00 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 21:07:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 21:07:00 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 21:07:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179809ms till timeout)
2022-03-30 21:07:01 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io not ready, will try again in 10000 ms (179912ms till timeout)
2022-03-30 21:07:05 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:07:10 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:07:10 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 21:07:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 21:07:10 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 21:07:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 21:07:10 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 21:07:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 21:07:10 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 21:07:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 21:07:10 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 21:07:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 21:07:10 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 21:07:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 21:07:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io not ready, will try again in 10000 ms (179982ms till timeout)
2022-03-30 21:07:10 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 21:07:10 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 21:07:15 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:07:20 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:07:20 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 21:07:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-30 21:07:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 21:07:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 21:07:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v94770
2022-03-30 21:07:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v94770
2022-03-30 21:07:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=94770&allowWatchBookmarks=true&watch=true...
2022-03-30 21:07:20 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 21:07:21 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 94771
2022-03-30 21:07:25 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:07:25 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 94793
2022-03-30 21:07:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:07:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 94800
2022-03-30 21:07:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v94793 in namespace default
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5a34beae
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace, second-namespace-test, third-namespace-test]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-30 21:07:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@3ce138ef
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@5314833a, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5a34beae, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace, second-namespace-test, third-namespace-test], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-30 21:07:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-30 21:07:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@38deabab
2022-03-30 21:07:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@38deabab
2022-03-30 21:07:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@38deabab
2022-03-30 21:07:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 21:07:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace infra-namespace -o json
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace infra-namespace -o json
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T21:07:31Z",
        "name": "infra-namespace",
        "resourceVersion": "94801",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "8f37bd02-94ae-4ae9-916e-0279461e10eb"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: second-namespace-test
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Namespace second-namespace-test
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace second-namespace-test -o json
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace second-namespace-test -o json
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T21:07:31Z",
        "name": "second-namespace-test",
        "resourceVersion": "94805",
        "selfLink": "/api/v1/namespaces/second-namespace-test",
        "uid": "f7fca110-b259-4f19-9b91-8a9d7eb6301a"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace, second-namespace-test]}
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: third-namespace-test
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Namespace third-namespace-test
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-7 get Namespace third-namespace-test -o json
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-7 get Namespace third-namespace-test -o json
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T21:07:31Z",
        "name": "third-namespace-test",
        "resourceVersion": "94809",
        "selfLink": "/api/v1/namespaces/third-namespace-test",
        "uid": "6290e375-c01e-4a00-b679-2e8c365cd99b"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace, second-namespace-test, third-namespace-test]}
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=second-namespace-test, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: second-namespace-test
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=third-namespace-test, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: third-namespace-test
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace second-namespace-test
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace second-namespace-test
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace third-namespace-test
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace third-namespace-test
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 21:07:31 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 21:07:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479987ms till timeout)
2022-03-30 21:07:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478984ms till timeout)
2022-03-30 21:07:34 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477981ms till timeout)
2022-03-30 21:07:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476978ms till timeout)
2022-03-30 21:07:35 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:07:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475975ms till timeout)
2022-03-30 21:07:37 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474972ms till timeout)
2022-03-30 21:07:38 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473968ms till timeout)
2022-03-30 21:07:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472965ms till timeout)
2022-03-30 21:07:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471962ms till timeout)
2022-03-30 21:07:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:07:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470959ms till timeout)
2022-03-30 21:07:42 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469956ms till timeout)
2022-03-30 21:07:43 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468952ms till timeout)
2022-03-30 21:07:44 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467949ms till timeout)
2022-03-30 21:07:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466946ms till timeout)
2022-03-30 21:07:45 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:07:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465943ms till timeout)
2022-03-30 21:07:47 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464940ms till timeout)
2022-03-30 21:07:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463937ms till timeout)
2022-03-30 21:07:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462934ms till timeout)
2022-03-30 21:07:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461931ms till timeout)
2022-03-30 21:07:50 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:07:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (460928ms till timeout)
2022-03-30 21:07:52 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (459925ms till timeout)
2022-03-30 21:07:53 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (458922ms till timeout)
2022-03-30 21:07:54 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (457919ms till timeout)
2022-03-30 21:07:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (456916ms till timeout)
2022-03-30 21:07:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:07:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (455913ms till timeout)
2022-03-30 21:07:57 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (454910ms till timeout)
2022-03-30 21:07:58 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (453907ms till timeout)
2022-03-30 21:07:59 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (452904ms till timeout)
2022-03-30 21:08:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (451900ms till timeout)
2022-03-30 21:08:00 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:08:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (450897ms till timeout)
2022-03-30 21:08:02 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (449894ms till timeout)
2022-03-30 21:08:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (448891ms till timeout)
2022-03-30 21:08:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (447888ms till timeout)
2022-03-30 21:08:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (446885ms till timeout)
2022-03-30 21:08:05 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:08:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (445882ms till timeout)
2022-03-30 21:08:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (444878ms till timeout)
2022-03-30 21:08:08 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (443875ms till timeout)
2022-03-30 21:08:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (442872ms till timeout)
2022-03-30 21:08:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (441869ms till timeout)
2022-03-30 21:08:10 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:08:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (440866ms till timeout)
2022-03-30 21:08:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (439863ms till timeout)
2022-03-30 21:08:13 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-30 21:08:13 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-30 21:08:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-30 21:08:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-9xdtb not ready: strimzi-cluster-operator)
2022-03-30 21:08:13 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-9xdtb are ready
2022-03-30 21:08:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599998ms till timeout)
2022-03-30 21:08:14 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-9xdtb not ready: strimzi-cluster-operator)
2022-03-30 21:08:14 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-9xdtb are ready
2022-03-30 21:08:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598994ms till timeout)
2022-03-30 21:08:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-9xdtb not ready: strimzi-cluster-operator)
2022-03-30 21:08:15 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-9xdtb are ready
2022-03-30 21:08:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597990ms till timeout)
2022-03-30 21:08:15 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:08:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-9xdtb not ready: strimzi-cluster-operator)
2022-03-30 21:08:16 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-9xdtb are ready
2022-03-30 21:08:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596987ms till timeout)
2022-03-30 21:08:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-9xdtb not ready: strimzi-cluster-operator)
2022-03-30 21:08:17 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-9xdtb are ready
2022-03-30 21:08:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595983ms till timeout)
2022-03-30 21:08:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-9xdtb not ready: strimzi-cluster-operator)
2022-03-30 21:08:18 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-9xdtb are ready
2022-03-30 21:08:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594980ms till timeout)
2022-03-30 21:08:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-9xdtb not ready: strimzi-cluster-operator)
2022-03-30 21:08:19 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-9xdtb are ready
2022-03-30 21:08:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593976ms till timeout)
2022-03-30 21:08:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-9xdtb not ready: strimzi-cluster-operator)
2022-03-30 21:08:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-9xdtb are ready
2022-03-30 21:08:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592973ms till timeout)
2022-03-30 21:08:20 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:08:21 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-9xdtb not ready: strimzi-cluster-operator)
2022-03-30 21:08:21 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-9xdtb are ready
2022-03-30 21:08:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591969ms till timeout)
2022-03-30 21:08:22 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-9xdtb not ready: strimzi-cluster-operator)
2022-03-30 21:08:22 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-9xdtb are ready
2022-03-30 21:08:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590966ms till timeout)
2022-03-30 21:08:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-78689684d4-9xdtb not ready: strimzi-cluster-operator)
2022-03-30 21:08:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-78689684d4-9xdtb are ready
2022-03-30 21:08:23 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-30 21:08:23 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: third-namespace-test
2022-03-30 21:08:23 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster in namespace third-namespace-test
2022-03-30 21:08:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster
2022-03-30 21:08:23 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster will have desired state: Ready
2022-03-30 21:08:23 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster will have desired state: Ready
2022-03-30 21:08:23 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (839998ms till timeout)
2022-03-30 21:08:24 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (838995ms till timeout)
2022-03-30 21:08:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (837992ms till timeout)
2022-03-30 21:08:25 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:08:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (836989ms till timeout)
2022-03-30 21:08:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (835986ms till timeout)
2022-03-30 21:08:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (834983ms till timeout)
2022-03-30 21:08:29 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (833980ms till timeout)
2022-03-30 21:08:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (832976ms till timeout)
2022-03-30 21:08:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:08:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (831973ms till timeout)
2022-03-30 21:08:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (830970ms till timeout)
2022-03-30 21:08:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (829967ms till timeout)
2022-03-30 21:08:34 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (828964ms till timeout)
2022-03-30 21:08:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (827960ms till timeout)
2022-03-30 21:08:35 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:08:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (826957ms till timeout)
2022-03-30 21:08:37 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (825955ms till timeout)
2022-03-30 21:08:38 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (824952ms till timeout)
2022-03-30 21:08:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (823949ms till timeout)
2022-03-30 21:08:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (822945ms till timeout)
2022-03-30 21:08:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:08:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (821942ms till timeout)
2022-03-30 21:08:42 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (820940ms till timeout)
2022-03-30 21:08:43 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (819937ms till timeout)
2022-03-30 21:08:44 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (818934ms till timeout)
2022-03-30 21:08:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (817930ms till timeout)
2022-03-30 21:08:45 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:08:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (816928ms till timeout)
2022-03-30 21:08:47 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (815924ms till timeout)
2022-03-30 21:08:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (814921ms till timeout)
2022-03-30 21:08:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (813918ms till timeout)
2022-03-30 21:08:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (812915ms till timeout)
2022-03-30 21:08:50 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:08:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (811912ms till timeout)
2022-03-30 21:08:52 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (810908ms till timeout)
2022-03-30 21:08:53 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (809905ms till timeout)
2022-03-30 21:08:54 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (808902ms till timeout)
2022-03-30 21:08:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (807898ms till timeout)
2022-03-30 21:08:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:08:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (806895ms till timeout)
2022-03-30 21:08:57 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (805892ms till timeout)
2022-03-30 21:08:58 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (804889ms till timeout)
2022-03-30 21:08:59 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (803886ms till timeout)
2022-03-30 21:09:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (802882ms till timeout)
2022-03-30 21:09:00 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:09:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (801879ms till timeout)
2022-03-30 21:09:02 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (800876ms till timeout)
2022-03-30 21:09:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (799873ms till timeout)
2022-03-30 21:09:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (798870ms till timeout)
2022-03-30 21:09:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (797866ms till timeout)
2022-03-30 21:09:05 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:09:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (796863ms till timeout)
2022-03-30 21:09:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (795860ms till timeout)
2022-03-30 21:09:08 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (794857ms till timeout)
2022-03-30 21:09:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (793853ms till timeout)
2022-03-30 21:09:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (792850ms till timeout)
2022-03-30 21:09:10 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:09:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (791847ms till timeout)
2022-03-30 21:09:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (790844ms till timeout)
2022-03-30 21:09:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (789841ms till timeout)
2022-03-30 21:09:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (788838ms till timeout)
2022-03-30 21:09:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (787835ms till timeout)
2022-03-30 21:09:15 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:09:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (786831ms till timeout)
2022-03-30 21:09:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (785828ms till timeout)
2022-03-30 21:09:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (784825ms till timeout)
2022-03-30 21:09:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (783822ms till timeout)
2022-03-30 21:09:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (782818ms till timeout)
2022-03-30 21:09:20 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:09:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (781815ms till timeout)
2022-03-30 21:09:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (780812ms till timeout)
2022-03-30 21:09:23 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (779809ms till timeout)
2022-03-30 21:09:24 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (778805ms till timeout)
2022-03-30 21:09:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (777802ms till timeout)
2022-03-30 21:09:25 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:09:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (776799ms till timeout)
2022-03-30 21:09:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (775796ms till timeout)
2022-03-30 21:09:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (774793ms till timeout)
2022-03-30 21:09:29 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (773790ms till timeout)
2022-03-30 21:09:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (772787ms till timeout)
2022-03-30 21:09:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:09:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (771783ms till timeout)
2022-03-30 21:09:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (770780ms till timeout)
2022-03-30 21:09:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (769777ms till timeout)
2022-03-30 21:09:34 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (768774ms till timeout)
2022-03-30 21:09:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster will have desired state: Ready not ready, will try again in 1000 ms (767771ms till timeout)
2022-03-30 21:09:35 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:09:36 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster is in desired state: Ready
2022-03-30 21:09:36 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-namespace-test
2022-03-30 21:09:36 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-second in namespace second-namespace-test
2022-03-30 21:09:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-second
2022-03-30 21:09:36 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-second will have desired state: Ready
2022-03-30 21:09:36 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-second will have desired state: Ready
2022-03-30 21:09:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (839998ms till timeout)
2022-03-30 21:09:37 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (838996ms till timeout)
2022-03-30 21:09:38 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (837993ms till timeout)
2022-03-30 21:09:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (836990ms till timeout)
2022-03-30 21:09:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (835987ms till timeout)
2022-03-30 21:09:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:09:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (834983ms till timeout)
2022-03-30 21:09:42 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (833932ms till timeout)
2022-03-30 21:09:43 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (832929ms till timeout)
2022-03-30 21:09:44 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (831926ms till timeout)
2022-03-30 21:09:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (830923ms till timeout)
2022-03-30 21:09:45 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:09:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (829919ms till timeout)
2022-03-30 21:09:47 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (828916ms till timeout)
2022-03-30 21:09:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (827914ms till timeout)
2022-03-30 21:09:49 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (826911ms till timeout)
2022-03-30 21:09:50 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (825907ms till timeout)
2022-03-30 21:09:50 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:09:51 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (824904ms till timeout)
2022-03-30 21:09:52 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (823901ms till timeout)
2022-03-30 21:09:53 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (822898ms till timeout)
2022-03-30 21:09:54 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (821895ms till timeout)
2022-03-30 21:09:55 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (820892ms till timeout)
2022-03-30 21:09:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:09:56 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (819889ms till timeout)
2022-03-30 21:09:57 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (818886ms till timeout)
2022-03-30 21:09:58 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (817883ms till timeout)
2022-03-30 21:09:59 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (816879ms till timeout)
2022-03-30 21:10:00 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (815876ms till timeout)
2022-03-30 21:10:00 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:10:01 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (814873ms till timeout)
2022-03-30 21:10:02 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (813869ms till timeout)
2022-03-30 21:10:03 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (812866ms till timeout)
2022-03-30 21:10:04 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (811864ms till timeout)
2022-03-30 21:10:05 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (810860ms till timeout)
2022-03-30 21:10:05 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:10:06 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (809855ms till timeout)
2022-03-30 21:10:07 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (808852ms till timeout)
2022-03-30 21:10:08 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (807848ms till timeout)
2022-03-30 21:10:09 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (806842ms till timeout)
2022-03-30 21:10:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (805837ms till timeout)
2022-03-30 21:10:10 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:10:11 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (804831ms till timeout)
2022-03-30 21:10:12 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (803828ms till timeout)
2022-03-30 21:10:13 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (802825ms till timeout)
2022-03-30 21:10:14 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (801822ms till timeout)
2022-03-30 21:10:15 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (800820ms till timeout)
2022-03-30 21:10:15 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:10:16 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (799817ms till timeout)
2022-03-30 21:10:17 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (798814ms till timeout)
2022-03-30 21:10:18 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (797811ms till timeout)
2022-03-30 21:10:19 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (796808ms till timeout)
2022-03-30 21:10:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (795805ms till timeout)
2022-03-30 21:10:20 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:10:21 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (794802ms till timeout)
2022-03-30 21:10:22 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (793799ms till timeout)
2022-03-30 21:10:23 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (792796ms till timeout)
2022-03-30 21:10:24 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (791794ms till timeout)
2022-03-30 21:10:25 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:10:25 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (790791ms till timeout)
2022-03-30 21:10:26 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (789788ms till timeout)
2022-03-30 21:10:27 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (788785ms till timeout)
2022-03-30 21:10:28 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (787783ms till timeout)
2022-03-30 21:10:29 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (786779ms till timeout)
2022-03-30 21:10:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:10:30 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (785777ms till timeout)
2022-03-30 21:10:31 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (784773ms till timeout)
2022-03-30 21:10:32 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (783770ms till timeout)
2022-03-30 21:10:33 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (782767ms till timeout)
2022-03-30 21:10:34 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (781764ms till timeout)
2022-03-30 21:10:35 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:10:35 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (780761ms till timeout)
2022-03-30 21:10:36 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (779757ms till timeout)
2022-03-30 21:10:37 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (778754ms till timeout)
2022-03-30 21:10:38 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (777751ms till timeout)
2022-03-30 21:10:39 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (776748ms till timeout)
2022-03-30 21:10:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:10:40 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (775745ms till timeout)
2022-03-30 21:10:41 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (774742ms till timeout)
2022-03-30 21:10:42 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (773738ms till timeout)
2022-03-30 21:10:43 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (772735ms till timeout)
2022-03-30 21:10:44 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (771732ms till timeout)
2022-03-30 21:10:45 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:10:45 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (770729ms till timeout)
2022-03-30 21:10:46 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (769726ms till timeout)
2022-03-30 21:10:47 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-second will have desired state: Ready not ready, will try again in 1000 ms (768723ms till timeout)
2022-03-30 21:10:48 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-second is in desired state: Ready
2022-03-30 21:10:48 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 21:10:48 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 21:10:48 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.watcher.AllNamespaceIsolatedST.testKafkaInDifferentNsThanClusterOperator-STARTED
2022-03-30 21:10:48 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 21:10:48 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:659] [watcher.AllNamespaceIsolatedST - Before Each] - Setup test case environment
2022-03-30 21:10:48 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 21:10:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaConnectResponse=my-cluster-2fe1834d, testSendSimpleMessageTls=my-cluster-ca7325d7, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e, testKafkaConnectIoNetwork=my-cluster-2e56d7a8, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37, testCruiseControlBasicAPIRequests=my-cluster-8aed4745, testZookeeperWatchersCount=my-cluster-22c0f078, testKafkaTopicPartitions=my-cluster-0afaffb5, testMirrorMaker2Metrics=my-cluster-5da4fc90, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634, testZookeeperAliveConnections=my-cluster-b34ddefa, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba, testKafkaTopicUnderReplicatedPartitions=my-cluster-d686e2cd, testSendMessagesTlsScramSha=my-cluster-84882f67, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-52516fd7, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273, testZookeeperQuorumSize=my-cluster-16088501, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff, testKafkaBridgeMetrics=my-cluster-9178bcb3, testKafkaExporterDataAfterExchange=my-cluster-e86a0839, testUpdateUser=my-cluster-6e6fa8fe, testTopicOperatorMetrics=my-cluster-216189cb, testKafkaInDifferentNsThanClusterOperator=my-cluster-e569de99, testKafkaActiveControllers=my-cluster-ede2d53e, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60, testKafkaConnectRequests=my-cluster-d1a00fce, testCruiseControlMetrics=my-cluster-f49dfd8c, testReceiveSimpleMessageTls=my-cluster-2e450144, testClusterOperatorMetrics=my-cluster-cfe17ead, testKafkaBrokersCount=my-cluster-a943baf0, testUserOperatorMetrics=my-cluster-d671df34, testKafkaMetricsSettings=my-cluster-b7e9a534}
2022-03-30 21:10:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaConnectResponse=my-user-1873693835-1689263597, testSendSimpleMessageTls=my-user-1309491966-2015599610, testReconcileStateMetricInTopicOperator=my-user-2124671027-1015880288, testKafkaConnectIoNetwork=my-user-1245775831-860629339, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1886945581-244087591, testSendMessagesCustomListenerTlsScramSha=my-user-1118022021-805525218, testCruiseControlBasicAPIRequests=my-user-1861484466-561878159, testZookeeperWatchersCount=my-user-1490797769-719890298, testKafkaTopicPartitions=my-user-2007150773-1723968056, testMirrorMaker2Metrics=my-user-1300203975-2042223970, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1851888062-673454065, testZookeeperAliveConnections=my-user-2146410634-1587438750, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1731919654-539989781, testKafkaTopicUnderReplicatedPartitions=my-user-1731286414-663678983, testSendMessagesTlsScramSha=my-user-586569987-2105688079, testMirrorMaker2TlsAndTlsClientAuth=my-user-2042967534-325538616, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677, testRackAwareConnectCorrectDeployment=my-user-1051420592-1240796798, testZookeeperQuorumSize=my-user-2117010560-1601663415, testKafkaAndZookeeperScaleUpScaleDown=my-user-838849261-890460734, testKafkaBridgeMetrics=my-user-829221238-874616880, testKafkaExporterDataAfterExchange=my-user-766848479-1588072395, testUpdateUser=my-user-1387197422-925410353, testTopicOperatorMetrics=my-user-779424699-479796683, testKafkaInDifferentNsThanClusterOperator=my-user-1419436610-24498042, testKafkaActiveControllers=my-user-1066197147-6205877, testKafkaExporterDifferentSetting=my-user-988491821-279531432, testKafkaConnectRequests=my-user-1167366435-1145507543, testCruiseControlMetrics=my-user-958954083-1578303162, testReceiveSimpleMessageTls=my-user-1669668617-1031881775, testClusterOperatorMetrics=my-user-467081550-2107716793, testKafkaBrokersCount=my-user-114552180-150189557, testUserOperatorMetrics=my-user-1969851044-309723582, testKafkaMetricsSettings=my-user-1339734863-677347400}
2022-03-30 21:10:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaConnectResponse=my-topic-680219046-979906955, testSendSimpleMessageTls=my-topic-435427300-917779110, testReconcileStateMetricInTopicOperator=my-topic-1267576332-2046216344, testKafkaConnectIoNetwork=my-topic-1593682376-1381222647, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-306860904-106838350, testSendMessagesCustomListenerTlsScramSha=my-topic-963888555-1712711326, testCruiseControlBasicAPIRequests=my-topic-1631570507-1308241024, testZookeeperWatchersCount=my-topic-1959052705-1334872805, testKafkaTopicPartitions=my-topic-1875007668-599688839, testMirrorMaker2Metrics=my-topic-643348725-45724501, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-307985263-926887950, testZookeeperAliveConnections=my-topic-1555557100-1445821129, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1672280868-1119644965, testKafkaTopicUnderReplicatedPartitions=my-topic-2103629457-772081434, testSendMessagesTlsScramSha=my-topic-255809133-823300208, testMirrorMaker2TlsAndTlsClientAuth=my-topic-247064469-1645874446, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206, testRackAwareConnectCorrectDeployment=my-topic-189499239-153088857, testZookeeperQuorumSize=my-topic-192478281-1148042900, testKafkaAndZookeeperScaleUpScaleDown=my-topic-2138084746-704381117, testKafkaBridgeMetrics=my-topic-1063894720-254889472, testKafkaExporterDataAfterExchange=my-topic-1937812163-1733130103, testUpdateUser=my-topic-1875783706-1763804507, testTopicOperatorMetrics=my-topic-1965542273-995674430, testKafkaInDifferentNsThanClusterOperator=my-topic-822742038-519807677, testKafkaActiveControllers=my-topic-1135110290-1277775808, testKafkaExporterDifferentSetting=my-topic-568145531-1290731867, testKafkaConnectRequests=my-topic-873062135-1706759978, testCruiseControlMetrics=my-topic-1917112707-1243703175, testReceiveSimpleMessageTls=my-topic-368564676-637091599, testClusterOperatorMetrics=my-topic-1416314702-1707243111, testKafkaBrokersCount=my-topic-222235994-1274701867, testUserOperatorMetrics=my-topic-1544093300-974581436, testKafkaMetricsSettings=my-topic-256215769-877131003}
2022-03-30 21:10:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaConnectResponse=my-cluster-2fe1834d-kafka-clients, testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e-kafka-clients, testKafkaConnectIoNetwork=my-cluster-2e56d7a8-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-8aed4745-kafka-clients, testZookeeperWatchersCount=my-cluster-22c0f078-kafka-clients, testKafkaTopicPartitions=my-cluster-0afaffb5-kafka-clients, testMirrorMaker2Metrics=my-cluster-5da4fc90-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634-kafka-clients, testZookeeperAliveConnections=my-cluster-b34ddefa-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-d686e2cd-kafka-clients, testSendMessagesTlsScramSha=my-cluster-84882f67-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-52516fd7-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273-kafka-clients, testZookeeperQuorumSize=my-cluster-16088501-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff-kafka-clients, testKafkaBridgeMetrics=my-cluster-9178bcb3-kafka-clients, testKafkaExporterDataAfterExchange=my-cluster-e86a0839-kafka-clients, testUpdateUser=my-cluster-6e6fa8fe-kafka-clients, testTopicOperatorMetrics=my-cluster-216189cb-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-e569de99-kafka-clients, testKafkaActiveControllers=my-cluster-ede2d53e-kafka-clients, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60-kafka-clients, testKafkaConnectRequests=my-cluster-d1a00fce-kafka-clients, testCruiseControlMetrics=my-cluster-f49dfd8c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-2e450144-kafka-clients, testClusterOperatorMetrics=my-cluster-cfe17ead-kafka-clients, testKafkaBrokersCount=my-cluster-a943baf0-kafka-clients, testUserOperatorMetrics=my-cluster-d671df34-kafka-clients, testKafkaMetricsSettings=my-cluster-b7e9a534-kafka-clients}
2022-03-30 21:10:48 [ForkJoinPool-3-worker-1] [32mINFO [m [AllNamespaceIsolatedST:82] Deploying Kafka cluster in different namespace than CO when CO watches all namespaces
2022-03-30 21:10:48 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-namespace-test
2022-03-30 21:10:48 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractNamespaceST:46] Check if Kafka Cluster my-cluster-second in namespace second-namespace-test
2022-03-30 21:10:48 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Kafka Cluster status is not in desired state: Ready
2022-03-30 21:10:48 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractNamespaceST:51] Kafka condition status: True
2022-03-30 21:10:48 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractNamespaceST:52] Kafka condition type: Ready
2022-03-30 21:10:48 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 21:10:48 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 21:10:48 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:675] [watcher.AllNamespaceIsolatedST - After Each] - Clean up after test
2022-03-30 21:10:48 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 21:10:48 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:346] In context testKafkaInDifferentNsThanClusterOperator is everything deleted.
2022-03-30 21:10:48 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 21:10:48 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.watcher.AllNamespaceIsolatedST.testKafkaInDifferentNsThanClusterOperator-FINISHED
2022-03-30 21:10:48 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 21:10:48 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 21:10:48 [ForkJoinPool-3-worker-1] [36mDEBUG[m [AbstractST:690] [watcher.AllNamespaceIsolatedST - After All] - Clean up after test suite
2022-03-30 21:10:48 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 21:10:48 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for AllNamespaceIsolatedST
2022-03-30 21:10:48 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-second in namespace second-namespace-test
2022-03-30 21:10:48 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster in namespace third-namespace-test
2022-03-30 21:10:48 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-second
2022-03-30 21:10:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster
2022-03-30 21:10:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-second not ready, will try again in 10000 ms (839996ms till timeout)
2022-03-30 21:10:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster not ready, will try again in 10000 ms (839997ms till timeout)
2022-03-30 21:10:50 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:10:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:10:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster not ready, will try again in 10000 ms (829994ms till timeout)
2022-03-30 21:11:00 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:11:05 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:127] Suite mirrormaker.MirrorMakerIsolatedST is waiting to lock to be released.
2022-03-30 21:11:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3,807.841 s - in io.strimzi.systemtest.watcher.AllNamespaceIsolatedST
2022-03-30 21:11:10 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:136] Suite mirrormaker.MirrorMakerIsolatedST has locked the @IsolatedSuite and other @IsolatedSuites must wait until lock is released.
2022-03-30 21:11:10 [ForkJoinPool-3-worker-15] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-30 21:11:10 [ForkJoinPool-3-worker-15] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-30 21:11:10 [ForkJoinPool-3-worker-15] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-30 21:11:10 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 21:11:10 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-30 21:11:10 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 21:11:10 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace third-namespace-test
2022-03-30 21:11:10 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 21:11:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 21:11:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 21:11:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 21:11:10 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 21:11:10 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 21:11:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 21:11:10 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace third-namespace-test
2022-03-30 21:11:10 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 21:11:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 21:11:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 21:11:10 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 21:11:10 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io not ready, will try again in 10000 ms (179986ms till timeout)
2022-03-30 21:11:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 21:11:10 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 21:11:10 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 21:11:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 21:11:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 21:11:10 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 21:11:10 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 21:11:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 21:11:10 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace second-namespace-test
2022-03-30 21:11:10 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 21:11:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 21:11:10 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 21:11:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 21:11:10 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 21:11:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 21:11:10 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator not ready, will try again in 10000 ms (479960ms till timeout)
2022-03-30 21:11:10 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 21:11:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace second-namespace-test
2022-03-30 21:11:10 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 21:11:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 21:11:10 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 21:11:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 21:11:10 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 21:11:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 21:11:10 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 21:11:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 21:11:10 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 21:11:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 21:11:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 21:11:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 21:11:10 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 21:11:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 21:11:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179959ms till timeout)
2022-03-30 21:11:10 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 21:11:11 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179823ms till timeout)
2022-03-30 21:11:20 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 21:11:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 21:11:20 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 21:11:20 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 21:11:20 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 21:11:20 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 21:11:20 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io not ready, will try again in 10000 ms (179985ms till timeout)
2022-03-30 21:11:20 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io not ready, will try again in 10000 ms (179979ms till timeout)
2022-03-30 21:11:20 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 21:11:20 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 21:11:20 [ForkJoinPool-3-worker-9] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io not ready, will try again in 10000 ms (179993ms till timeout)
2022-03-30 21:11:21 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 21:11:21 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 21:11:21 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179902ms till timeout)
2022-03-30 21:11:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 21:11:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 21:11:30 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 21:11:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 21:11:30 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 21:11:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 21:11:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 21:11:30 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io not ready, will try again in 10000 ms (179977ms till timeout)
2022-03-30 21:11:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 21:11:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 21:11:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 21:11:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 21:11:30 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 21:11:30 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 21:11:30 [ForkJoinPool-3-worker-9] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 21:11:40 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 21:11:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: second-namespace-test
2022-03-30 21:11:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-30 21:11:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 21:11:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 21:11:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 21:11:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 21:11:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v95767
2022-03-30 21:11:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v95767
2022-03-30 21:11:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dsecond-namespace-test&resourceVersion=95767&allowWatchBookmarks=true&watch=true...
2022-03-30 21:11:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v95767
2022-03-30 21:11:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v95767
2022-03-30 21:11:40 [ForkJoinPool-3-worker-7] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=95767&allowWatchBookmarks=true&watch=true...
2022-03-30 21:11:40 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 21:11:40 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 21:11:41 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 95768
2022-03-30 21:11:41 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 95769
2022-03-30 21:11:45 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 95834
2022-03-30 21:11:45 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 95837
2022-03-30 21:11:45 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v95834 in namespace default
2022-03-30 21:11:45 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@70511eb2
2022-03-30 21:11:45 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 21:11:45 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@746e8460
2022-03-30 21:11:45 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@746e8460
2022-03-30 21:11:45 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@746e8460
2022-03-30 21:11:45 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 21:11:45 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 21:11:46 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 95862
2022-03-30 21:11:46 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 95863
2022-03-30 21:11:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: third-namespace-test
2022-03-30 21:11:46 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v95862 in namespace default
2022-03-30 21:11:46 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@16ce22b1
2022-03-30 21:11:46 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 21:11:46 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@12f1a4b0
2022-03-30 21:11:46 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@12f1a4b0
2022-03-30 21:11:46 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@12f1a4b0
2022-03-30 21:11:46 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 21:11:46 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 21:11:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 21:11:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 21:11:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v95865
2022-03-30 21:11:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v95865
2022-03-30 21:11:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dthird-namespace-test&resourceVersion=95865&allowWatchBookmarks=true&watch=true...
2022-03-30 21:11:46 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 21:11:51 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 95905
2022-03-30 21:11:51 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 95906
2022-03-30 21:11:51 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v95905 in namespace default
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5a34beae
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=120000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-30 21:11:51 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@1529a7b
2022-03-30 21:11:51 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@5314833a, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5a34beae, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=120000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-30 21:11:51 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@66eefb4a
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-30 21:11:51 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@66eefb4a
2022-03-30 21:11:51 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@66eefb4a
2022-03-30 21:11:51 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 21:11:51 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace infra-namespace -o json
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T21:11:51Z",
        "name": "infra-namespace",
        "resourceVersion": "95907",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "b6f578dd-1254-4049-b620-adc984e98e0a"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: Crd
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-30 21:11:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479998ms till timeout)
2022-03-30 21:11:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478995ms till timeout)
2022-03-30 21:11:53 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477992ms till timeout)
2022-03-30 21:11:54 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476989ms till timeout)
2022-03-30 21:11:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475986ms till timeout)
2022-03-30 21:11:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474983ms till timeout)
2022-03-30 21:11:57 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473980ms till timeout)
2022-03-30 21:11:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472977ms till timeout)
2022-03-30 21:11:59 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471974ms till timeout)
2022-03-30 21:12:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470970ms till timeout)
2022-03-30 21:12:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469967ms till timeout)
2022-03-30 21:12:02 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468964ms till timeout)
2022-03-30 21:12:03 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467961ms till timeout)
2022-03-30 21:12:04 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466958ms till timeout)
2022-03-30 21:12:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465954ms till timeout)
2022-03-30 21:12:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464951ms till timeout)
2022-03-30 21:12:07 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463948ms till timeout)
2022-03-30 21:12:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462945ms till timeout)
2022-03-30 21:12:09 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461942ms till timeout)
2022-03-30 21:12:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (460939ms till timeout)
2022-03-30 21:12:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (459936ms till timeout)
2022-03-30 21:12:12 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (458933ms till timeout)
2022-03-30 21:12:13 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (457930ms till timeout)
2022-03-30 21:12:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (456927ms till timeout)
2022-03-30 21:12:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (455924ms till timeout)
2022-03-30 21:12:17 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (454921ms till timeout)
2022-03-30 21:12:18 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (453918ms till timeout)
2022-03-30 21:12:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (452915ms till timeout)
2022-03-30 21:12:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (451912ms till timeout)
2022-03-30 21:12:21 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-30 21:12:21 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-30 21:12:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-30 21:12:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-zs44r not ready: strimzi-cluster-operator)
2022-03-30 21:12:21 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-zs44r are ready
2022-03-30 21:12:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599997ms till timeout)
2022-03-30 21:12:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-zs44r not ready: strimzi-cluster-operator)
2022-03-30 21:12:22 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-zs44r are ready
2022-03-30 21:12:22 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598993ms till timeout)
2022-03-30 21:12:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-zs44r not ready: strimzi-cluster-operator)
2022-03-30 21:12:23 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-zs44r are ready
2022-03-30 21:12:23 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597990ms till timeout)
2022-03-30 21:12:24 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-zs44r not ready: strimzi-cluster-operator)
2022-03-30 21:12:24 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-zs44r are ready
2022-03-30 21:12:24 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596986ms till timeout)
2022-03-30 21:12:25 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-zs44r not ready: strimzi-cluster-operator)
2022-03-30 21:12:25 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-zs44r are ready
2022-03-30 21:12:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595983ms till timeout)
2022-03-30 21:12:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-zs44r not ready: strimzi-cluster-operator)
2022-03-30 21:12:26 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-zs44r are ready
2022-03-30 21:12:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594979ms till timeout)
2022-03-30 21:12:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-zs44r not ready: strimzi-cluster-operator)
2022-03-30 21:12:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-zs44r are ready
2022-03-30 21:12:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593975ms till timeout)
2022-03-30 21:12:28 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-zs44r not ready: strimzi-cluster-operator)
2022-03-30 21:12:28 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-zs44r are ready
2022-03-30 21:12:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592972ms till timeout)
2022-03-30 21:12:29 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-zs44r not ready: strimzi-cluster-operator)
2022-03-30 21:12:29 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-zs44r are ready
2022-03-30 21:12:29 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591968ms till timeout)
2022-03-30 21:12:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-zs44r not ready: strimzi-cluster-operator)
2022-03-30 21:12:30 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-zs44r are ready
2022-03-30 21:12:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590965ms till timeout)
2022-03-30 21:12:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-7588bbf888-zs44r not ready: strimzi-cluster-operator)
2022-03-30 21:12:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [PodUtils:106] Pods strimzi-cluster-operator-7588bbf888-zs44r are ready
2022-03-30 21:12:31 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-30 21:12:31 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-03-30 21:12:31 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST.testMirrorMakerTlsAuthenticated-STARTED
2022-03-30 21:12:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:658] ============================================================================
2022-03-30 21:12:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:659] [mirrormaker.MirrorMakerIsolatedST - Before Each] - Setup test case environment
2022-03-30 21:12:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:77] [mirrormaker.MirrorMakerIsolatedST] - Adding parallel test: testMirrorMakerTlsAuthenticated
2022-03-30 21:12:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:81] [mirrormaker.MirrorMakerIsolatedST] - Parallel test count: 1
2022-03-30 21:12:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:230] testMirrorMakerTlsAuthenticated test now can proceed its execution
2022-03-30 21:12:31 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-30 21:12:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [AbstractST:606] CLUSTER_NAMES_MAP: {testKafkaConnectResponse=my-cluster-2fe1834d, testSendSimpleMessageTls=my-cluster-ca7325d7, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e, testKafkaConnectIoNetwork=my-cluster-2e56d7a8, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37, testCruiseControlBasicAPIRequests=my-cluster-8aed4745, testZookeeperWatchersCount=my-cluster-22c0f078, testKafkaTopicPartitions=my-cluster-0afaffb5, testMirrorMaker2Metrics=my-cluster-5da4fc90, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634, testZookeeperAliveConnections=my-cluster-b34ddefa, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba, testKafkaTopicUnderReplicatedPartitions=my-cluster-d686e2cd, testSendMessagesTlsScramSha=my-cluster-84882f67, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-52516fd7, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273, testZookeeperQuorumSize=my-cluster-16088501, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff, testKafkaBridgeMetrics=my-cluster-9178bcb3, testKafkaExporterDataAfterExchange=my-cluster-e86a0839, testUpdateUser=my-cluster-6e6fa8fe, testTopicOperatorMetrics=my-cluster-216189cb, testKafkaInDifferentNsThanClusterOperator=my-cluster-e569de99, testKafkaActiveControllers=my-cluster-ede2d53e, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60, testKafkaConnectRequests=my-cluster-d1a00fce, testCruiseControlMetrics=my-cluster-f49dfd8c, testReceiveSimpleMessageTls=my-cluster-2e450144, testClusterOperatorMetrics=my-cluster-cfe17ead, testKafkaBrokersCount=my-cluster-a943baf0, testUserOperatorMetrics=my-cluster-d671df34, testKafkaMetricsSettings=my-cluster-b7e9a534, testMirrorMakerTlsAuthenticated=my-cluster-7f6f1191}
2022-03-30 21:12:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [AbstractST:607] USERS_NAME_MAP: {testKafkaConnectResponse=my-user-1873693835-1689263597, testSendSimpleMessageTls=my-user-1309491966-2015599610, testReconcileStateMetricInTopicOperator=my-user-2124671027-1015880288, testKafkaConnectIoNetwork=my-user-1245775831-860629339, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-user-1886945581-244087591, testSendMessagesCustomListenerTlsScramSha=my-user-1118022021-805525218, testCruiseControlBasicAPIRequests=my-user-1861484466-561878159, testZookeeperWatchersCount=my-user-1490797769-719890298, testKafkaTopicPartitions=my-user-2007150773-1723968056, testMirrorMaker2Metrics=my-user-1300203975-2042223970, testMultiNodeKafkaConnectWithConnectorCreation=my-user-1851888062-673454065, testZookeeperAliveConnections=my-user-2146410634-1587438750, testAutoRenewAllCaCertsTriggeredByAnno=my-user-1731919654-539989781, testKafkaTopicUnderReplicatedPartitions=my-user-1731286414-663678983, testSendMessagesTlsScramSha=my-user-586569987-2105688079, testMirrorMaker2TlsAndTlsClientAuth=my-user-2042967534-325538616, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-user-899980519-1426343677, testRackAwareConnectCorrectDeployment=my-user-1051420592-1240796798, testZookeeperQuorumSize=my-user-2117010560-1601663415, testKafkaAndZookeeperScaleUpScaleDown=my-user-838849261-890460734, testKafkaBridgeMetrics=my-user-829221238-874616880, testKafkaExporterDataAfterExchange=my-user-766848479-1588072395, testUpdateUser=my-user-1387197422-925410353, testTopicOperatorMetrics=my-user-779424699-479796683, testKafkaInDifferentNsThanClusterOperator=my-user-1419436610-24498042, testKafkaActiveControllers=my-user-1066197147-6205877, testKafkaExporterDifferentSetting=my-user-988491821-279531432, testKafkaConnectRequests=my-user-1167366435-1145507543, testCruiseControlMetrics=my-user-958954083-1578303162, testReceiveSimpleMessageTls=my-user-1669668617-1031881775, testClusterOperatorMetrics=my-user-467081550-2107716793, testKafkaBrokersCount=my-user-114552180-150189557, testUserOperatorMetrics=my-user-1969851044-309723582, testKafkaMetricsSettings=my-user-1339734863-677347400, testMirrorMakerTlsAuthenticated=my-user-292682336-94618297}
2022-03-30 21:12:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [AbstractST:608] TOPIC_NAMES_MAP: {testKafkaConnectResponse=my-topic-680219046-979906955, testSendSimpleMessageTls=my-topic-435427300-917779110, testReconcileStateMetricInTopicOperator=my-topic-1267576332-2046216344, testKafkaConnectIoNetwork=my-topic-1593682376-1381222647, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-topic-306860904-106838350, testSendMessagesCustomListenerTlsScramSha=my-topic-963888555-1712711326, testCruiseControlBasicAPIRequests=my-topic-1631570507-1308241024, testZookeeperWatchersCount=my-topic-1959052705-1334872805, testKafkaTopicPartitions=my-topic-1875007668-599688839, testMirrorMaker2Metrics=my-topic-643348725-45724501, testMultiNodeKafkaConnectWithConnectorCreation=my-topic-307985263-926887950, testZookeeperAliveConnections=my-topic-1555557100-1445821129, testAutoRenewAllCaCertsTriggeredByAnno=my-topic-1672280868-1119644965, testKafkaTopicUnderReplicatedPartitions=my-topic-2103629457-772081434, testSendMessagesTlsScramSha=my-topic-255809133-823300208, testMirrorMaker2TlsAndTlsClientAuth=my-topic-247064469-1645874446, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-topic-599799964-1691466206, testRackAwareConnectCorrectDeployment=my-topic-189499239-153088857, testZookeeperQuorumSize=my-topic-192478281-1148042900, testKafkaAndZookeeperScaleUpScaleDown=my-topic-2138084746-704381117, testKafkaBridgeMetrics=my-topic-1063894720-254889472, testKafkaExporterDataAfterExchange=my-topic-1937812163-1733130103, testUpdateUser=my-topic-1875783706-1763804507, testTopicOperatorMetrics=my-topic-1965542273-995674430, testKafkaInDifferentNsThanClusterOperator=my-topic-822742038-519807677, testKafkaActiveControllers=my-topic-1135110290-1277775808, testKafkaExporterDifferentSetting=my-topic-568145531-1290731867, testKafkaConnectRequests=my-topic-873062135-1706759978, testCruiseControlMetrics=my-topic-1917112707-1243703175, testReceiveSimpleMessageTls=my-topic-368564676-637091599, testClusterOperatorMetrics=my-topic-1416314702-1707243111, testKafkaBrokersCount=my-topic-222235994-1274701867, testUserOperatorMetrics=my-topic-1544093300-974581436, testKafkaMetricsSettings=my-topic-256215769-877131003, testMirrorMakerTlsAuthenticated=my-topic-52565379-1117408619}
2022-03-30 21:12:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [AbstractST:609] THIS IS CLIENTS MAP: {testKafkaConnectResponse=my-cluster-2fe1834d-kafka-clients, testSendSimpleMessageTls=my-cluster-ca7325d7-kafka-clients, testReconcileStateMetricInTopicOperator=my-cluster-2c97166e-kafka-clients, testKafkaConnectIoNetwork=my-cluster-2e56d7a8-kafka-clients, testCruiseControlWithRebalanceResourceAndRefreshAnnotation=my-cluster-f141387d-kafka-clients, testSendMessagesCustomListenerTlsScramSha=my-cluster-d4963f37-kafka-clients, testCruiseControlBasicAPIRequests=my-cluster-8aed4745-kafka-clients, testZookeeperWatchersCount=my-cluster-22c0f078-kafka-clients, testKafkaTopicPartitions=my-cluster-0afaffb5-kafka-clients, testMirrorMaker2Metrics=my-cluster-5da4fc90-kafka-clients, testMultiNodeKafkaConnectWithConnectorCreation=my-cluster-b42ad634-kafka-clients, testZookeeperAliveConnections=my-cluster-b34ddefa-kafka-clients, testAutoRenewAllCaCertsTriggeredByAnno=my-cluster-cae962ba-kafka-clients, testKafkaTopicUnderReplicatedPartitions=my-cluster-d686e2cd-kafka-clients, testSendMessagesTlsScramSha=my-cluster-84882f67-kafka-clients, testMirrorMaker2TlsAndTlsClientAuth=my-cluster-52516fd7-kafka-clients, testCruiseControlBasicAPIRequestsWithSecurityDisabled=my-cluster-45fb7140-kafka-clients, testRackAwareConnectCorrectDeployment=my-cluster-6fad9273-kafka-clients, testZookeeperQuorumSize=my-cluster-16088501-kafka-clients, testKafkaAndZookeeperScaleUpScaleDown=my-cluster-2a79abff-kafka-clients, testKafkaBridgeMetrics=my-cluster-9178bcb3-kafka-clients, testKafkaExporterDataAfterExchange=my-cluster-e86a0839-kafka-clients, testUpdateUser=my-cluster-6e6fa8fe-kafka-clients, testTopicOperatorMetrics=my-cluster-216189cb-kafka-clients, testKafkaInDifferentNsThanClusterOperator=my-cluster-e569de99-kafka-clients, testKafkaActiveControllers=my-cluster-ede2d53e-kafka-clients, testKafkaExporterDifferentSetting=my-cluster-0ecbcb60-kafka-clients, testKafkaConnectRequests=my-cluster-d1a00fce-kafka-clients, testCruiseControlMetrics=my-cluster-f49dfd8c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-2e450144-kafka-clients, testClusterOperatorMetrics=my-cluster-cfe17ead-kafka-clients, testKafkaBrokersCount=my-cluster-a943baf0-kafka-clients, testUserOperatorMetrics=my-cluster-d671df34-kafka-clients, testKafkaMetricsSettings=my-cluster-b7e9a534-kafka-clients, testMirrorMakerTlsAuthenticated=my-cluster-7f6f1191-kafka-clients}
2022-03-30 21:12:31 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-8 for test case:testMirrorMakerTlsAuthenticated
2022-03-30 21:12:31 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-8
2022-03-30 21:12:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-8
2022-03-30 21:12:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace infra-namespace get Namespace namespace-8 -o json
2022-03-30 21:12:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace infra-namespace get Namespace namespace-8 -o json
2022-03-30 21:12:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:12:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2022-03-30T21:12:31Z",
        "name": "namespace-8",
        "resourceVersion": "96010",
        "selfLink": "/api/v1/namespaces/namespace-8",
        "uid": "a7749605-4f71-426e-b7b0-037b10c53594"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-30 21:12:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@6a42f58d=[namespace-8]}
2022-03-30 21:12:31 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-8
2022-03-30 21:12:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-8, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-30 21:12:31 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-8
2022-03-30 21:12:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-7f6f1191-source in namespace namespace-8
2022-03-30 21:12:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-30 21:12:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-7f6f1191-source
2022-03-30 21:12:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-7f6f1191-source will have desired state: Ready
2022-03-30 21:12:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-7f6f1191-source will have desired state: Ready
2022-03-30 21:12:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (839998ms till timeout)
2022-03-30 21:12:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (838995ms till timeout)
2022-03-30 21:12:33 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (837992ms till timeout)
2022-03-30 21:12:34 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (836989ms till timeout)
2022-03-30 21:12:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (835986ms till timeout)
2022-03-30 21:12:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (834984ms till timeout)
2022-03-30 21:12:37 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (833981ms till timeout)
2022-03-30 21:12:38 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (832977ms till timeout)
2022-03-30 21:12:39 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (831974ms till timeout)
2022-03-30 21:12:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (830971ms till timeout)
2022-03-30 21:12:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (829968ms till timeout)
2022-03-30 21:12:42 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (828965ms till timeout)
2022-03-30 21:12:43 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (827962ms till timeout)
2022-03-30 21:12:44 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (826959ms till timeout)
2022-03-30 21:12:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (825956ms till timeout)
2022-03-30 21:12:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (824953ms till timeout)
2022-03-30 21:12:47 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (823950ms till timeout)
2022-03-30 21:12:48 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (822947ms till timeout)
2022-03-30 21:12:49 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (821943ms till timeout)
2022-03-30 21:12:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (820940ms till timeout)
2022-03-30 21:12:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (819937ms till timeout)
2022-03-30 21:12:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (818934ms till timeout)
2022-03-30 21:12:53 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (817931ms till timeout)
2022-03-30 21:12:54 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (816928ms till timeout)
2022-03-30 21:12:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (815925ms till timeout)
2022-03-30 21:12:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (814922ms till timeout)
2022-03-30 21:12:57 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (813919ms till timeout)
2022-03-30 21:12:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (812916ms till timeout)
2022-03-30 21:12:59 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (811913ms till timeout)
2022-03-30 21:13:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (810910ms till timeout)
2022-03-30 21:13:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (809906ms till timeout)
2022-03-30 21:13:02 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (808903ms till timeout)
2022-03-30 21:13:03 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (807900ms till timeout)
2022-03-30 21:13:04 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (806897ms till timeout)
2022-03-30 21:13:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (805894ms till timeout)
2022-03-30 21:13:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (804890ms till timeout)
2022-03-30 21:13:07 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (803887ms till timeout)
2022-03-30 21:13:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (802884ms till timeout)
2022-03-30 21:13:09 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (801881ms till timeout)
2022-03-30 21:13:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (800878ms till timeout)
2022-03-30 21:13:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (799875ms till timeout)
2022-03-30 21:13:12 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (798872ms till timeout)
2022-03-30 21:13:13 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (797869ms till timeout)
2022-03-30 21:13:14 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (796866ms till timeout)
2022-03-30 21:13:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (795863ms till timeout)
2022-03-30 21:13:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (794860ms till timeout)
2022-03-30 21:13:17 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (793857ms till timeout)
2022-03-30 21:13:18 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (792854ms till timeout)
2022-03-30 21:13:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (791850ms till timeout)
2022-03-30 21:13:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (790847ms till timeout)
2022-03-30 21:13:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (789844ms till timeout)
2022-03-30 21:13:22 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (788841ms till timeout)
2022-03-30 21:13:23 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (787838ms till timeout)
2022-03-30 21:13:24 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (786835ms till timeout)
2022-03-30 21:13:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (785832ms till timeout)
2022-03-30 21:13:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (784829ms till timeout)
2022-03-30 21:13:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (783826ms till timeout)
2022-03-30 21:13:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (782823ms till timeout)
2022-03-30 21:13:29 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (781819ms till timeout)
2022-03-30 21:13:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (780816ms till timeout)
2022-03-30 21:13:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (779813ms till timeout)
2022-03-30 21:13:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (778810ms till timeout)
2022-03-30 21:13:33 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (777807ms till timeout)
2022-03-30 21:13:34 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (776804ms till timeout)
2022-03-30 21:13:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (775801ms till timeout)
2022-03-30 21:13:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (774797ms till timeout)
2022-03-30 21:13:37 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (773794ms till timeout)
2022-03-30 21:13:38 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (772791ms till timeout)
2022-03-30 21:13:39 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (771788ms till timeout)
2022-03-30 21:13:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (770785ms till timeout)
2022-03-30 21:13:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (769782ms till timeout)
2022-03-30 21:13:42 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (768779ms till timeout)
2022-03-30 21:13:43 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-source will have desired state: Ready not ready, will try again in 1000 ms (767775ms till timeout)
2022-03-30 21:13:44 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-7f6f1191-source is in desired state: Ready
2022-03-30 21:13:44 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-7f6f1191-target in namespace namespace-8
2022-03-30 21:13:44 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-30 21:13:44 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-7f6f1191-target
2022-03-30 21:13:44 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-7f6f1191-target will have desired state: Ready
2022-03-30 21:13:44 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Kafka: my-cluster-7f6f1191-target will have desired state: Ready
2022-03-30 21:13:44 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (839998ms till timeout)
2022-03-30 21:13:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (838993ms till timeout)
2022-03-30 21:13:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (837990ms till timeout)
2022-03-30 21:13:47 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (836987ms till timeout)
2022-03-30 21:13:48 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (835985ms till timeout)
2022-03-30 21:13:49 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (834981ms till timeout)
2022-03-30 21:13:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (833979ms till timeout)
2022-03-30 21:13:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (832976ms till timeout)
2022-03-30 21:13:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (831973ms till timeout)
2022-03-30 21:13:53 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (830969ms till timeout)
2022-03-30 21:13:54 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (829967ms till timeout)
2022-03-30 21:13:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (828960ms till timeout)
2022-03-30 21:13:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (827957ms till timeout)
2022-03-30 21:13:57 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (826954ms till timeout)
2022-03-30 21:13:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (825951ms till timeout)
2022-03-30 21:13:59 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (824947ms till timeout)
2022-03-30 21:14:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (823945ms till timeout)
2022-03-30 21:14:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (822941ms till timeout)
2022-03-30 21:14:02 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (821938ms till timeout)
2022-03-30 21:14:03 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (820935ms till timeout)
2022-03-30 21:14:04 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (819932ms till timeout)
2022-03-30 21:14:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (818929ms till timeout)
2022-03-30 21:14:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (817925ms till timeout)
2022-03-30 21:14:07 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (816922ms till timeout)
2022-03-30 21:14:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (815919ms till timeout)
2022-03-30 21:14:09 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (814915ms till timeout)
2022-03-30 21:14:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (813912ms till timeout)
2022-03-30 21:14:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (812909ms till timeout)
2022-03-30 21:14:12 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (811906ms till timeout)
2022-03-30 21:14:13 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (810903ms till timeout)
2022-03-30 21:14:14 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (809900ms till timeout)
2022-03-30 21:14:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (808897ms till timeout)
2022-03-30 21:14:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (807893ms till timeout)
2022-03-30 21:14:17 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (806890ms till timeout)
2022-03-30 21:14:18 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (805887ms till timeout)
2022-03-30 21:14:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (804884ms till timeout)
2022-03-30 21:14:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (803881ms till timeout)
2022-03-30 21:14:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (802878ms till timeout)
2022-03-30 21:14:22 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (801875ms till timeout)
2022-03-30 21:14:23 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (800872ms till timeout)
2022-03-30 21:14:24 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (799870ms till timeout)
2022-03-30 21:14:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (798866ms till timeout)
2022-03-30 21:14:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (797863ms till timeout)
2022-03-30 21:14:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (796861ms till timeout)
2022-03-30 21:14:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (795858ms till timeout)
2022-03-30 21:14:29 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (794855ms till timeout)
2022-03-30 21:14:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (793852ms till timeout)
2022-03-30 21:14:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (792849ms till timeout)
2022-03-30 21:14:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (791846ms till timeout)
2022-03-30 21:14:33 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (790843ms till timeout)
2022-03-30 21:14:34 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (789840ms till timeout)
2022-03-30 21:14:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (788837ms till timeout)
2022-03-30 21:14:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (787834ms till timeout)
2022-03-30 21:14:37 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (786830ms till timeout)
2022-03-30 21:14:38 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (785827ms till timeout)
2022-03-30 21:14:39 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (784822ms till timeout)
2022-03-30 21:14:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (783819ms till timeout)
2022-03-30 21:14:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (782816ms till timeout)
2022-03-30 21:14:42 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (781813ms till timeout)
2022-03-30 21:14:43 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (780810ms till timeout)
2022-03-30 21:14:44 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (779807ms till timeout)
2022-03-30 21:14:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (778803ms till timeout)
2022-03-30 21:14:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (777800ms till timeout)
2022-03-30 21:14:47 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (776797ms till timeout)
2022-03-30 21:14:48 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (775794ms till timeout)
2022-03-30 21:14:49 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (774791ms till timeout)
2022-03-30 21:14:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (773787ms till timeout)
2022-03-30 21:14:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (772784ms till timeout)
2022-03-30 21:14:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (771781ms till timeout)
2022-03-30 21:14:53 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (770778ms till timeout)
2022-03-30 21:14:54 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (769775ms till timeout)
2022-03-30 21:14:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (768772ms till timeout)
2022-03-30 21:14:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (767769ms till timeout)
2022-03-30 21:14:57 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Kafka: my-cluster-7f6f1191-target will have desired state: Ready not ready, will try again in 1000 ms (766766ms till timeout)
2022-03-30 21:14:58 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-7f6f1191-target is in desired state: Ready
2022-03-30 21:14:58 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1151337119-304751814-source-820891023 in namespace namespace-8
2022-03-30 21:14:58 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-30 21:14:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1151337119-304751814-source-820891023
2022-03-30 21:14:58 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1151337119-304751814-source-820891023 will have desired state: Ready
2022-03-30 21:14:58 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-1151337119-304751814-source-820891023 will have desired state: Ready
2022-03-30 21:14:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-1151337119-304751814-source-820891023 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 21:14:59 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1151337119-304751814-source-820891023 is in desired state: Ready
2022-03-30 21:14:59 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-7f6f1191-my-user-source in namespace namespace-8
2022-03-30 21:14:59 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-30 21:14:59 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-cluster-7f6f1191-my-user-source
2022-03-30 21:14:59 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-7f6f1191-my-user-source will have desired state: Ready
2022-03-30 21:14:59 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-cluster-7f6f1191-my-user-source will have desired state: Ready
2022-03-30 21:14:59 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaUser: my-cluster-7f6f1191-my-user-source will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 21:15:00 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-7f6f1191-my-user-source is in desired state: Ready
2022-03-30 21:15:00 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-7f6f1191-my-user-target in namespace namespace-8
2022-03-30 21:15:00 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-30 21:15:00 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-cluster-7f6f1191-my-user-target
2022-03-30 21:15:00 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-7f6f1191-my-user-target will have desired state: Ready
2022-03-30 21:15:00 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for KafkaUser: my-cluster-7f6f1191-my-user-target will have desired state: Ready
2022-03-30 21:15:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaUser: my-cluster-7f6f1191-my-user-target will have desired state: Ready not ready, will try again in 1000 ms (179997ms till timeout)
2022-03-30 21:15:01 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-7f6f1191-my-user-target is in desired state: Ready
2022-03-30 21:15:01 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-7f6f1191-kafka-clients in namespace namespace-8
2022-03-30 21:15:01 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-30 21:15:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-7f6f1191-kafka-clients
2022-03-30 21:15:01 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-7f6f1191-kafka-clients will be ready
2022-03-30 21:15:01 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Wait for Deployment: my-cluster-7f6f1191-kafka-clients will be ready
2022-03-30 21:15:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-7f6f1191-kafka-clients will be ready not ready, will try again in 1000 ms (479997ms till timeout)
2022-03-30 21:15:02 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Wait for Deployment: my-cluster-7f6f1191-kafka-clients will be ready not ready, will try again in 1000 ms (478993ms till timeout)
2022-03-30 21:15:03 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-7f6f1191-kafka-clients is ready
2022-03-30 21:15:03 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for pod with prefixmy-cluster-7f6f1191-kafka-clients is present.
2022-03-30 21:15:03 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-52565379-1117408619-test-1 in namespace namespace-8
2022-03-30 21:15:03 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-30 21:15:03 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-52565379-1117408619-test-1
2022-03-30 21:15:03 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-52565379-1117408619-test-1 will have desired state: Ready
2022-03-30 21:15:03 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-52565379-1117408619-test-1 will have desired state: Ready
2022-03-30 21:15:03 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-52565379-1117408619-test-1 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 21:15:04 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-52565379-1117408619-test-1 is in desired state: Ready
2022-03-30 21:15:04 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-52565379-1117408619-test-2 in namespace namespace-8
2022-03-30 21:15:04 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-30 21:15:04 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-52565379-1117408619-test-2
2022-03-30 21:15:04 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-52565379-1117408619-test-2 will have desired state: Ready
2022-03-30 21:15:04 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for KafkaTopic: my-topic-52565379-1117408619-test-2 will have desired state: Ready
2022-03-30 21:15:04 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaTopic: my-topic-52565379-1117408619-test-2 will have desired state: Ready not ready, will try again in 1000 ms (179998ms till timeout)
2022-03-30 21:15:05 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-52565379-1117408619-test-2 is in desired state: Ready
2022-03-30 21:15:05 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-30 21:15:05 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Producer and consumer will successfully send and receive messages.
2022-03-30 21:15:05 [ForkJoinPool-3-worker-15] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@69b902de, which are set.
2022-03-30 21:15:05 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@6a9417d6, messages=[], arguments=[USER=my_cluster_7f6f1191_my_user_source, --topic, my-topic-52565379-1117408619-test-1, --bootstrap-server, my-cluster-7f6f1191-source-kafka-bootstrap.namespace-8.svc:9093, --max-messages, 200], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-7f6f1191-kafka-clients-5df55595d6-qwsrs', podNamespace='namespace-8', bootstrapServer='my-cluster-7f6f1191-source-kafka-bootstrap.namespace-8.svc:9093', topicName='my-topic-52565379-1117408619-test-1', maxMessages=200, kafkaUsername='my-cluster-7f6f1191-my-user-source', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@69b902de}
2022-03-30 21:15:05 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-7f6f1191-source-kafka-bootstrap.namespace-8.svc:9093:my-topic-52565379-1117408619-test-1 from pod my-cluster-7f6f1191-kafka-clients-5df55595d6-qwsrs
2022-03-30 21:15:05 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-7f6f1191-kafka-clients-5df55595d6-qwsrs -n namespace-8 -- /opt/kafka/producer.sh USER=my_cluster_7f6f1191_my_user_source --topic my-topic-52565379-1117408619-test-1 --bootstrap-server my-cluster-7f6f1191-source-kafka-bootstrap.namespace-8.svc:9093 --max-messages 200
2022-03-30 21:15:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-7f6f1191-kafka-clients-5df55595d6-qwsrs -n namespace-8 -- /opt/kafka/producer.sh USER=my_cluster_7f6f1191_my_user_source --topic my-topic-52565379-1117408619-test-1 --bootstrap-server my-cluster-7f6f1191-source-kafka-bootstrap.namespace-8.svc:9093 --max-messages 200
2022-03-30 21:15:09 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-30 21:15:09 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-03-30 21:15:09 [ForkJoinPool-3-worker-15] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6752980c, which are set.
2022-03-30 21:15:09 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@465cc9ca, messages=[], arguments=[USER=my_cluster_7f6f1191_my_user_source, --group-id, my-consumer-group-1691728300, --topic, my-topic-52565379-1117408619-test-1, --group-instance-id, instance2049932734, --bootstrap-server, my-cluster-7f6f1191-source-kafka-bootstrap.namespace-8.svc:9093, --max-messages, 200], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-7f6f1191-kafka-clients-5df55595d6-qwsrs', podNamespace='namespace-8', bootstrapServer='my-cluster-7f6f1191-source-kafka-bootstrap.namespace-8.svc:9093', topicName='my-topic-52565379-1117408619-test-1', maxMessages=200, kafkaUsername='my-cluster-7f6f1191-my-user-source', consumerGroupName='my-consumer-group-1691728300', consumerInstanceId='instance2049932734', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6752980c}
2022-03-30 21:15:09 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-7f6f1191-source-kafka-bootstrap.namespace-8.svc:9093:my-topic-52565379-1117408619-test-1 from pod my-cluster-7f6f1191-kafka-clients-5df55595d6-qwsrs
2022-03-30 21:15:09 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-7f6f1191-kafka-clients-5df55595d6-qwsrs -n namespace-8 -- /opt/kafka/consumer.sh USER=my_cluster_7f6f1191_my_user_source --group-id my-consumer-group-1691728300 --topic my-topic-52565379-1117408619-test-1 --group-instance-id instance2049932734 --bootstrap-server my-cluster-7f6f1191-source-kafka-bootstrap.namespace-8.svc:9093 --max-messages 200
2022-03-30 21:15:09 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-7f6f1191-kafka-clients-5df55595d6-qwsrs -n namespace-8 -- /opt/kafka/consumer.sh USER=my_cluster_7f6f1191_my_user_source --group-id my-consumer-group-1691728300 --topic my-topic-52565379-1117408619-test-1 --group-instance-id instance2049932734 --bootstrap-server my-cluster-7f6f1191-source-kafka-bootstrap.namespace-8.svc:9093 --max-messages 200
2022-03-30 21:15:16 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 21:15:16 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-03-30 21:15:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Producer and consumer will successfully send and receive messages.
2022-03-30 21:15:16 [ForkJoinPool-3-worker-15] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@c3c5c70, which are set.
2022-03-30 21:15:16 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@61c9f2b5, messages=[], arguments=[USER=my_cluster_7f6f1191_my_user_target, --topic, my-topic-52565379-1117408619-test-2, --bootstrap-server, my-cluster-7f6f1191-target-kafka-bootstrap.namespace-8.svc:9093, --max-messages, 200], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-7f6f1191-kafka-clients-5df55595d6-qwsrs', podNamespace='namespace-8', bootstrapServer='my-cluster-7f6f1191-target-kafka-bootstrap.namespace-8.svc:9093', topicName='my-topic-52565379-1117408619-test-2', maxMessages=200, kafkaUsername='my-cluster-7f6f1191-my-user-target', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@c3c5c70}
2022-03-30 21:15:16 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-7f6f1191-target-kafka-bootstrap.namespace-8.svc:9093:my-topic-52565379-1117408619-test-2 from pod my-cluster-7f6f1191-kafka-clients-5df55595d6-qwsrs
2022-03-30 21:15:16 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-7f6f1191-kafka-clients-5df55595d6-qwsrs -n namespace-8 -- /opt/kafka/producer.sh USER=my_cluster_7f6f1191_my_user_target --topic my-topic-52565379-1117408619-test-2 --bootstrap-server my-cluster-7f6f1191-target-kafka-bootstrap.namespace-8.svc:9093 --max-messages 200
2022-03-30 21:15:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-7f6f1191-kafka-clients-5df55595d6-qwsrs -n namespace-8 -- /opt/kafka/producer.sh USER=my_cluster_7f6f1191_my_user_target --topic my-topic-52565379-1117408619-test-2 --bootstrap-server my-cluster-7f6f1191-target-kafka-bootstrap.namespace-8.svc:9093 --max-messages 200
2022-03-30 21:15:20 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-30 21:15:20 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-03-30 21:15:20 [ForkJoinPool-3-worker-15] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@12a88478, which are set.
2022-03-30 21:15:20 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@329a0d7e, messages=[], arguments=[USER=my_cluster_7f6f1191_my_user_target, --group-id, my-consumer-group-226119380, --topic, my-topic-52565379-1117408619-test-2, --group-instance-id, instance784942292, --bootstrap-server, my-cluster-7f6f1191-target-kafka-bootstrap.namespace-8.svc:9093, --max-messages, 200], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-7f6f1191-kafka-clients-5df55595d6-qwsrs', podNamespace='namespace-8', bootstrapServer='my-cluster-7f6f1191-target-kafka-bootstrap.namespace-8.svc:9093', topicName='my-topic-52565379-1117408619-test-2', maxMessages=200, kafkaUsername='my-cluster-7f6f1191-my-user-target', consumerGroupName='my-consumer-group-226119380', consumerInstanceId='instance784942292', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@12a88478}
2022-03-30 21:15:20 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-7f6f1191-target-kafka-bootstrap.namespace-8.svc:9093:my-topic-52565379-1117408619-test-2 from pod my-cluster-7f6f1191-kafka-clients-5df55595d6-qwsrs
2022-03-30 21:15:20 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-7f6f1191-kafka-clients-5df55595d6-qwsrs -n namespace-8 -- /opt/kafka/consumer.sh USER=my_cluster_7f6f1191_my_user_target --group-id my-consumer-group-226119380 --topic my-topic-52565379-1117408619-test-2 --group-instance-id instance784942292 --bootstrap-server my-cluster-7f6f1191-target-kafka-bootstrap.namespace-8.svc:9093 --max-messages 200
2022-03-30 21:15:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-7f6f1191-kafka-clients-5df55595d6-qwsrs -n namespace-8 -- /opt/kafka/consumer.sh USER=my_cluster_7f6f1191_my_user_target --group-id my-consumer-group-226119380 --topic my-topic-52565379-1117408619-test-2 --group-instance-id instance784942292 --bootstrap-server my-cluster-7f6f1191-target-kafka-bootstrap.namespace-8.svc:9093 --max-messages 200
2022-03-30 21:15:27 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 21:15:27 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-03-30 21:15:27 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker my-cluster-7f6f1191 in namespace namespace-8
2022-03-30 21:15:27 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-03-30 21:15:27 [ForkJoinPool-3-worker-15] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkamirrormakers' with unstable version 'v1beta2'
2022-03-30 21:15:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaMirrorMaker:my-cluster-7f6f1191
2022-03-30 21:15:27 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready
2022-03-30 21:15:27 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready
2022-03-30 21:15:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (479998ms till timeout)
2022-03-30 21:15:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (478994ms till timeout)
2022-03-30 21:15:29 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (477991ms till timeout)
2022-03-30 21:15:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (476987ms till timeout)
2022-03-30 21:15:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (475984ms till timeout)
2022-03-30 21:15:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (474980ms till timeout)
2022-03-30 21:15:33 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (473977ms till timeout)
2022-03-30 21:15:34 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (472974ms till timeout)
2022-03-30 21:15:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (471970ms till timeout)
2022-03-30 21:15:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (470967ms till timeout)
2022-03-30 21:15:37 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (469963ms till timeout)
2022-03-30 21:15:38 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (468958ms till timeout)
2022-03-30 21:15:39 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (467954ms till timeout)
2022-03-30 21:15:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (466951ms till timeout)
2022-03-30 21:15:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (465947ms till timeout)
2022-03-30 21:15:42 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (464944ms till timeout)
2022-03-30 21:15:43 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (463941ms till timeout)
2022-03-30 21:15:44 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (462937ms till timeout)
2022-03-30 21:15:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (461934ms till timeout)
2022-03-30 21:15:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (460931ms till timeout)
2022-03-30 21:15:47 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (459927ms till timeout)
2022-03-30 21:15:48 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (458924ms till timeout)
2022-03-30 21:15:49 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (457921ms till timeout)
2022-03-30 21:15:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (456918ms till timeout)
2022-03-30 21:15:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (455914ms till timeout)
2022-03-30 21:15:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (454911ms till timeout)
2022-03-30 21:15:53 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (453908ms till timeout)
2022-03-30 21:15:54 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (452904ms till timeout)
2022-03-30 21:15:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (451901ms till timeout)
2022-03-30 21:15:56 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (450898ms till timeout)
2022-03-30 21:15:57 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (449895ms till timeout)
2022-03-30 21:15:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (448892ms till timeout)
2022-03-30 21:15:59 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (447889ms till timeout)
2022-03-30 21:16:00 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (446885ms till timeout)
2022-03-30 21:16:01 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (445882ms till timeout)
2022-03-30 21:16:02 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (444879ms till timeout)
2022-03-30 21:16:03 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (443875ms till timeout)
2022-03-30 21:16:04 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (442872ms till timeout)
2022-03-30 21:16:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (441869ms till timeout)
2022-03-30 21:16:06 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (440865ms till timeout)
2022-03-30 21:16:07 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (439862ms till timeout)
2022-03-30 21:16:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (438859ms till timeout)
2022-03-30 21:16:09 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (437856ms till timeout)
2022-03-30 21:16:10 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (436852ms till timeout)
2022-03-30 21:16:11 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (435849ms till timeout)
2022-03-30 21:16:12 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (434846ms till timeout)
2022-03-30 21:16:13 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (433843ms till timeout)
2022-03-30 21:16:14 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (432840ms till timeout)
2022-03-30 21:16:15 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (431836ms till timeout)
2022-03-30 21:16:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (430833ms till timeout)
2022-03-30 21:16:17 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (429830ms till timeout)
2022-03-30 21:16:18 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (428827ms till timeout)
2022-03-30 21:16:19 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (427823ms till timeout)
2022-03-30 21:16:20 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (426820ms till timeout)
2022-03-30 21:16:21 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (425817ms till timeout)
2022-03-30 21:16:22 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (424814ms till timeout)
2022-03-30 21:16:23 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (423811ms till timeout)
2022-03-30 21:16:24 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (422808ms till timeout)
2022-03-30 21:16:25 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (421804ms till timeout)
2022-03-30 21:16:26 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (420801ms till timeout)
2022-03-30 21:16:27 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (419798ms till timeout)
2022-03-30 21:16:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (418795ms till timeout)
2022-03-30 21:16:29 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (417791ms till timeout)
2022-03-30 21:16:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] KafkaMirrorMaker: my-cluster-7f6f1191 will have desired state: Ready not ready, will try again in 1000 ms (416789ms till timeout)
2022-03-30 21:16:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-7f6f1191 is in desired state: Ready
2022-03-30 21:16:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Producer and consumer will successfully send and receive messages.
2022-03-30 21:16:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@37ef3569, which are set.
2022-03-30 21:16:31 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@3a86f57, messages=[], arguments=[USER=my_cluster_7f6f1191_my_user_source, --topic, my-topic-1151337119-304751814-source-820891023, --bootstrap-server, my-cluster-7f6f1191-source-kafka-bootstrap.namespace-8.svc:9093, --max-messages, 200], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-7f6f1191-kafka-clients-5df55595d6-qwsrs', podNamespace='namespace-8', bootstrapServer='my-cluster-7f6f1191-source-kafka-bootstrap.namespace-8.svc:9093', topicName='my-topic-1151337119-304751814-source-820891023', maxMessages=200, kafkaUsername='my-cluster-7f6f1191-my-user-source', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@37ef3569}
2022-03-30 21:16:31 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-7f6f1191-source-kafka-bootstrap.namespace-8.svc:9093:my-topic-1151337119-304751814-source-820891023 from pod my-cluster-7f6f1191-kafka-clients-5df55595d6-qwsrs
2022-03-30 21:16:31 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-7f6f1191-kafka-clients-5df55595d6-qwsrs -n namespace-8 -- /opt/kafka/producer.sh USER=my_cluster_7f6f1191_my_user_source --topic my-topic-1151337119-304751814-source-820891023 --bootstrap-server my-cluster-7f6f1191-source-kafka-bootstrap.namespace-8.svc:9093 --max-messages 200
2022-03-30 21:16:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-7f6f1191-kafka-clients-5df55595d6-qwsrs -n namespace-8 -- /opt/kafka/producer.sh USER=my_cluster_7f6f1191_my_user_source --topic my-topic-1151337119-304751814-source-820891023 --bootstrap-server my-cluster-7f6f1191-source-kafka-bootstrap.namespace-8.svc:9093 --max-messages 200
2022-03-30 21:16:34 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-03-30 21:16:34 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-03-30 21:16:34 [ForkJoinPool-3-worker-15] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@482ada67, which are set.
2022-03-30 21:16:34 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@620fa76, messages=[], arguments=[USER=my_cluster_7f6f1191_my_user_source, --group-id, my-consumer-group-601848669, --topic, my-topic-1151337119-304751814-source-820891023, --group-instance-id, instance271606218, --bootstrap-server, my-cluster-7f6f1191-source-kafka-bootstrap.namespace-8.svc:9093, --max-messages, 200], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-7f6f1191-kafka-clients-5df55595d6-qwsrs', podNamespace='namespace-8', bootstrapServer='my-cluster-7f6f1191-source-kafka-bootstrap.namespace-8.svc:9093', topicName='my-topic-1151337119-304751814-source-820891023', maxMessages=200, kafkaUsername='my-cluster-7f6f1191-my-user-source', consumerGroupName='my-consumer-group-601848669', consumerInstanceId='instance271606218', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@482ada67}
2022-03-30 21:16:34 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-7f6f1191-source-kafka-bootstrap.namespace-8.svc:9093:my-topic-1151337119-304751814-source-820891023 from pod my-cluster-7f6f1191-kafka-clients-5df55595d6-qwsrs
2022-03-30 21:16:34 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-7f6f1191-kafka-clients-5df55595d6-qwsrs -n namespace-8 -- /opt/kafka/consumer.sh USER=my_cluster_7f6f1191_my_user_source --group-id my-consumer-group-601848669 --topic my-topic-1151337119-304751814-source-820891023 --group-instance-id instance271606218 --bootstrap-server my-cluster-7f6f1191-source-kafka-bootstrap.namespace-8.svc:9093 --max-messages 200
2022-03-30 21:16:34 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-7f6f1191-kafka-clients-5df55595d6-qwsrs -n namespace-8 -- /opt/kafka/consumer.sh USER=my_cluster_7f6f1191_my_user_source --group-id my-consumer-group-601848669 --topic my-topic-1151337119-304751814-source-820891023 --group-instance-id instance271606218 --bootstrap-server my-cluster-7f6f1191-source-kafka-bootstrap.namespace-8.svc:9093 --max-messages 200
2022-03-30 21:16:41 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 21:16:41 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-03-30 21:16:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Consumer will successfully receive messages.
2022-03-30 21:16:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4632ef32, which are set.
2022-03-30 21:16:41 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@40dc80b6, messages=[], arguments=[USER=my_cluster_7f6f1191_my_user_target, --group-id, my-consumer-group-1339166388, --topic, my-topic-1151337119-304751814-source-820891023, --group-instance-id, instance946739300, --bootstrap-server, my-cluster-7f6f1191-target-kafka-bootstrap.namespace-8.svc:9093, --max-messages, 200], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-7f6f1191-kafka-clients-5df55595d6-qwsrs', podNamespace='namespace-8', bootstrapServer='my-cluster-7f6f1191-target-kafka-bootstrap.namespace-8.svc:9093', topicName='my-topic-1151337119-304751814-source-820891023', maxMessages=200, kafkaUsername='my-cluster-7f6f1191-my-user-target', consumerGroupName='my-consumer-group-1339166388', consumerInstanceId='instance946739300', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4632ef32}
2022-03-30 21:16:41 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-7f6f1191-target-kafka-bootstrap.namespace-8.svc:9093:my-topic-1151337119-304751814-source-820891023 from pod my-cluster-7f6f1191-kafka-clients-5df55595d6-qwsrs
2022-03-30 21:16:41 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-7f6f1191-kafka-clients-5df55595d6-qwsrs -n namespace-8 -- /opt/kafka/consumer.sh USER=my_cluster_7f6f1191_my_user_target --group-id my-consumer-group-1339166388 --topic my-topic-1151337119-304751814-source-820891023 --group-instance-id instance946739300 --bootstrap-server my-cluster-7f6f1191-target-kafka-bootstrap.namespace-8.svc:9093 --max-messages 200
2022-03-30 21:16:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl exec my-cluster-7f6f1191-kafka-clients-5df55595d6-qwsrs -n namespace-8 -- /opt/kafka/consumer.sh USER=my_cluster_7f6f1191_my_user_target --group-id my-consumer-group-1339166388 --topic my-topic-1151337119-304751814-source-820891023 --group-instance-id instance946739300 --bootstrap-server my-cluster-7f6f1191-target-kafka-bootstrap.namespace-8.svc:9093 --max-messages 200
2022-03-30 21:16:48 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-30 21:16:48 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-03-30 21:16:48 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:674] ============================================================================
2022-03-30 21:16:48 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:675] [mirrormaker.MirrorMakerIsolatedST - After Each] - Clean up after test
2022-03-30 21:16:48 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 21:16:48 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testMirrorMakerTlsAuthenticated
2022-03-30 21:16:48 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-7f6f1191-kafka-clients in namespace namespace-8
2022-03-30 21:16:48 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-52565379-1117408619-test-2 in namespace namespace-8
2022-03-30 21:16:48 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1151337119-304751814-source-820891023 in namespace namespace-8
2022-03-30 21:16:48 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-7f6f1191-my-user-target in namespace namespace-8
2022-03-30 21:16:48 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-7f6f1191-kafka-clients
2022-03-30 21:16:48 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-cluster-7f6f1191-my-user-target
2022-03-30 21:16:48 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-52565379-1117408619-test-2
2022-03-30 21:16:48 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1151337119-304751814-source-820891023
2022-03-30 21:16:48 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-cluster-7f6f1191-my-user-target not ready, will try again in 10000 ms (179990ms till timeout)
2022-03-30 21:16:48 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1151337119-304751814-source-820891023 not ready, will try again in 10000 ms (179989ms till timeout)
2022-03-30 21:16:48 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-7f6f1191-kafka-clients not ready, will try again in 10000 ms (479984ms till timeout)
2022-03-30 21:16:48 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-52565379-1117408619-test-2 not ready, will try again in 10000 ms (179987ms till timeout)
2022-03-30 21:16:58 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-52565379-1117408619-test-1 in namespace namespace-8
2022-03-30 21:16:58 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-7f6f1191-my-user-source in namespace namespace-8
2022-03-30 21:16:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker my-cluster-7f6f1191 in namespace namespace-8
2022-03-30 21:16:58 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-52565379-1117408619-test-1
2022-03-30 21:16:58 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-cluster-7f6f1191-my-user-source
2022-03-30 21:16:58 [ForkJoinPool-3-worker-1] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaMirrorMaker:my-cluster-7f6f1191
2022-03-30 21:16:58 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-7f6f1191-kafka-clients not ready, will try again in 10000 ms (469968ms till timeout)
2022-03-30 21:16:58 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-cluster-7f6f1191-my-user-source not ready, will try again in 10000 ms (179992ms till timeout)
2022-03-30 21:16:58 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-52565379-1117408619-test-1 not ready, will try again in 10000 ms (179990ms till timeout)
2022-03-30 21:16:58 [ForkJoinPool-3-worker-1] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaMirrorMaker:my-cluster-7f6f1191 not ready, will try again in 10000 ms (479953ms till timeout)
2022-03-30 21:17:08 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-7f6f1191-target in namespace namespace-8
2022-03-30 21:17:08 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-7f6f1191-source in namespace namespace-8
2022-03-30 21:17:08 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-7f6f1191-kafka-clients not ready, will try again in 10000 ms (459959ms till timeout)
2022-03-30 21:17:08 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-7f6f1191-target
2022-03-30 21:17:08 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-7f6f1191-source
2022-03-30 21:17:08 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-7f6f1191-target not ready, will try again in 10000 ms (839993ms till timeout)
2022-03-30 21:17:08 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-7f6f1191-source not ready, will try again in 10000 ms (839994ms till timeout)
2022-03-30 21:17:18 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-7f6f1191-kafka-clients not ready, will try again in 10000 ms (449951ms till timeout)
2022-03-30 21:17:28 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 21:17:28 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-8 for test case:testMirrorMakerTlsAuthenticated
2022-03-30 21:17:28 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Namespace namespace-8 removal
2022-03-30 21:17:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:28 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:28 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:17:28 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (479924ms till timeout)
2022-03-30 21:17:29 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:29 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:29 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:17:29 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (478853ms till timeout)
2022-03-30 21:17:30 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:31 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:17:31 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (477777ms till timeout)
2022-03-30 21:17:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:32 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:17:32 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (476705ms till timeout)
2022-03-30 21:17:33 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:33 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:17:33 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (475630ms till timeout)
2022-03-30 21:17:34 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:34 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:34 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:17:34 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (474557ms till timeout)
2022-03-30 21:17:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:35 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:35 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:17:35 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (473479ms till timeout)
2022-03-30 21:17:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:36 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:17:36 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (472408ms till timeout)
2022-03-30 21:17:37 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:37 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:17:37 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (471336ms till timeout)
2022-03-30 21:17:38 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:38 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:38 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:17:38 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (470261ms till timeout)
2022-03-30 21:17:39 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:39 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:39 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:17:39 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (469194ms till timeout)
2022-03-30 21:17:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:40 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:17:40 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (468121ms till timeout)
2022-03-30 21:17:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:41 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:17:41 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (467049ms till timeout)
2022-03-30 21:17:42 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:42 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:42 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:17:42 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (465966ms till timeout)
2022-03-30 21:17:43 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:43 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:43 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:17:43 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (464889ms till timeout)
2022-03-30 21:17:44 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:45 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:45 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:17:45 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (463816ms till timeout)
2022-03-30 21:17:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:46 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:17:46 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (462739ms till timeout)
2022-03-30 21:17:47 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:47 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:47 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:17:47 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (461664ms till timeout)
2022-03-30 21:17:48 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:48 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:48 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:17:48 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (460580ms till timeout)
2022-03-30 21:17:49 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:49 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:49 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:17:49 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (459508ms till timeout)
2022-03-30 21:17:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:50 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:50 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:17:50 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (458432ms till timeout)
2022-03-30 21:17:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:51 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:17:51 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (457352ms till timeout)
2022-03-30 21:17:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:52 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:52 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:17:52 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (456280ms till timeout)
2022-03-30 21:17:53 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:53 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:53 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:17:53 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (455205ms till timeout)
2022-03-30 21:17:54 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:54 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:54 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 0
2022-03-30 21:17:54 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (454132ms till timeout)
2022-03-30 21:17:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [Exec:248] Running command - kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Failed to exec command: kubectl --namespace namespace-8 get Namespace namespace-8 -o yaml
2022-03-30 21:17:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Return code: 1
2022-03-30 21:17:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] ======STDERR START=======
2022-03-30 21:17:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] Error from server (NotFound): namespaces "namespace-8" not found
2022-03-30 21:17:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [Exec:419] ======STDERR END======
2022-03-30 21:17:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@6a42f58d=[]}
2022-03-30 21:17:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:267] testMirrorMakerTlsAuthenticated - Notifies waiting test cases:[testCruiseControlBasicAPIRequestsWithSecurityDisabled, testSendSimpleMessageTls, testUpdateUser, testCruiseControlBasicAPIRequests, testReceiveSimpleMessageTls, testSendMessagesTlsScramSha, testAutoRenewAllCaCertsTriggeredByAnno, testKafkaAndZookeeperScaleUpScaleDown, testSendMessagesCustomListenerTlsScramSha, testMultiNodeKafkaConnectWithConnectorCreation, testKafkaExporterDifferentSetting, testKafkaConnectResponse, testKafkaConnectRequests, testTopicOperatorMetrics, testCruiseControlMetrics, testClusterOperatorMetrics, testKafkaMetricsSettings, testKafkaBridgeMetrics, testUserOperatorMetrics, testZookeeperWatchersCount, testKafkaConnectIoNetwork, testKafkaBrokersCount, testKafkaActiveControllers, testKafkaTopicUnderReplicatedPartitions, testZookeeperQuorumSize, testZookeeperAliveConnections, testKafkaTopicPartitions, testMirrorMaker2Metrics, testMirrorMaker2TlsAndTlsClientAuth, testMirrorMakerTlsAuthenticated] to and randomly select one to start execution
2022-03-30 21:17:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:93] [mirrormaker.MirrorMakerIsolatedST] - Removing parallel test: testMirrorMakerTlsAuthenticated
2022-03-30 21:17:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [SuiteThreadController:97] [mirrormaker.MirrorMakerIsolatedST] - Parallel test count: 0
2022-03-30 21:17:55 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST.testMirrorMakerTlsAuthenticated-FINISHED
2022-03-30 21:17:55 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-03-30 21:17:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:689] ============================================================================
2022-03-30 21:17:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [AbstractST:690] [mirrormaker.MirrorMakerIsolatedST - After All] - Clean up after test suite
2022-03-30 21:17:55 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 21:17:55 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:346] In context MirrorMakerIsolatedST is everything deleted.
2022-03-30 21:17:55 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3,504.888 s - in io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST
2022-03-30 21:17:55 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-03-30 21:17:55 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-30 21:17:55 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-03-30 21:17:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-03-30 21:17:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-30 21:17:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-30 21:17:55 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-30 21:17:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-30 21:17:55 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 21:17:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-30 21:17:55 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-30 21:17:55 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-30 21:17:55 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-30 21:17:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-30 21:17:55 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-30 21:17:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-30 21:17:55 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io not ready, will try again in 10000 ms (179977ms till timeout)
2022-03-30 21:17:55 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-30 21:17:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-30 21:17:55 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-30 21:17:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-30 21:17:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-30 21:17:55 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-30 21:17:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-30 21:17:55 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-30 21:17:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-30 21:17:55 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-30 21:17:55 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179964ms till timeout)
2022-03-30 21:17:55 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io not ready, will try again in 10000 ms (179991ms till timeout)
2022-03-30 21:17:55 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-30 21:17:55 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io not ready, will try again in 10000 ms (179979ms till timeout)
2022-03-30 21:18:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-30 21:18:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-30 21:18:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-30 21:18:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-30 21:18:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-30 21:18:05 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-30 21:18:05 [ForkJoinPool-3-worker-3] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-30 21:18:05 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-30 21:18:05 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-30 21:18:05 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-30 21:18:05 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io not ready, will try again in 10000 ms (179988ms till timeout)
2022-03-30 21:18:05 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-30 21:18:05 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-30 21:18:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-30 21:18:05 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-30 21:18:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-30 21:18:05 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-30 21:18:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-30 21:18:05 [ForkJoinPool-3-worker-3] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179949ms till timeout)
2022-03-30 21:18:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-30 21:18:05 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-30 21:18:05 [ForkJoinPool-3-worker-13] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-30 21:18:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-30 21:18:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-30 21:18:05 [ForkJoinPool-3-worker-13] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io not ready, will try again in 10000 ms (179987ms till timeout)
2022-03-30 21:18:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-30 21:18:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-30 21:18:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-30 21:18:05 [ForkJoinPool-3-worker-7] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-30 21:18:06 [ForkJoinPool-3-worker-7] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io not ready, will try again in 10000 ms (179921ms till timeout)
2022-03-30 21:18:15 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-30 21:18:15 [ForkJoinPool-3-worker-15] [36mDEBUG[m [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-30 21:18:16 [ForkJoinPool-3-worker-15] [30mTRACE[m [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179845ms till timeout)
2022-03-30 21:18:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-03-30 21:18:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-30 21:18:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-30 21:18:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-30 21:18:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v97008
2022-03-30 21:18:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v97008
2022-03-30 21:18:26 [ForkJoinPool-3-worker-3] [36mDEBUG[m [AbstractWatchManager:222] Watching https://192.168.49.2:8443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=97008&allowWatchBookmarks=true&watch=true...
2022-03-30 21:18:26 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-30 21:18:26 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 97009
2022-03-30 21:18:31 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received MODIFIED Namespace resourceVersion 97026
2022-03-30 21:18:36 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:139] Event received DELETED Namespace resourceVersion 97034
2022-03-30 21:18:36 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v97026 in namespace default
2022-03-30 21:18:36 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@5711d6a2
2022-03-30 21:18:36 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [Reflector:181] Watch gracefully closed
2022-03-30 21:18:36 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@532e5898
2022-03-30 21:18:36 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@532e5898
2022-03-30 21:18:36 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@532e5898
2022-03-30 21:18:36 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-30 21:18:36 [OkHttp https://192.168.49.2:8443/...] [36mDEBUG[m [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-30 21:18:36 [main] [32mINFO [m [TestExecutionListener:40] =======================================================================
2022-03-30 21:18:36 [main] [32mINFO [m [TestExecutionListener:41] =======================================================================
2022-03-30 21:18:36 [main] [32mINFO [m [TestExecutionListener:42]                         Test run finished
2022-03-30 21:18:36 [main] [32mINFO [m [TestExecutionListener:43] =======================================================================
2022-03-30 21:18:36 [main] [32mINFO [m [TestExecutionListener:44] =======================================================================
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 35, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] [1mReactor Summary for Strimzi - Apache Kafka on Kubernetes and OpenShift 0.29.0-SNAPSHOT:[m
[[1;34mINFO[m] 
[[1;34mINFO[m] Strimzi - Apache Kafka on Kubernetes and OpenShift . [1;32mSUCCESS[m [  2.591 s]
[[1;34mINFO[m] test ............................................... [1;32mSUCCESS[m [  0.981 s]
[[1;34mINFO[m] crd-annotations .................................... [1;32mSUCCESS[m [  1.002 s]
[[1;34mINFO[m] crd-generator ...................................... [1;32mSUCCESS[m [  2.556 s]
[[1;34mINFO[m] api ................................................ [1;32mSUCCESS[m [  6.807 s]
[[1;34mINFO[m] mockkube ........................................... [1;32mSUCCESS[m [  0.945 s]
[[1;34mINFO[m] config-model ....................................... [1;32mSUCCESS[m [  0.700 s]
[[1;34mINFO[m] certificate-manager ................................ [1;32mSUCCESS[m [  0.772 s]
[[1;34mINFO[m] operator-common .................................... [1;32mSUCCESS[m [  1.751 s]
[[1;34mINFO[m] systemtest ......................................... [1;32mSUCCESS[m [  01:18 h]
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] [1;32mBUILD SUCCESS[m
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] Total time:  01:18 h
[[1;34mINFO[m] Finished at: 2022-03-30T21:18:36Z
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
