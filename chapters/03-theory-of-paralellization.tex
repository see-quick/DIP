\chapter{Theory of paralelization}

This chapter describes the fundamental theory of parallelisation (i.e., Amdahl's law (\ref{04:amdalhlaw}), Shared memory (\ref{04:sharedmemory}), Threads and Processes (\ref{04:processesandthreads}), Mutual Exclusion (\ref{dependenciesandprotection}), Synchronization (\ref{04:synchronization}), Asynchronous tasks (\ref{04:asynctaks})). This chapter is based on the following books \emph{An Introduction to Parallel Programming} \cite{introductionToParallelProgramming} and \emph{The Art of Multiprocessor Programming} \cite{artOfMultiprocessorProgramming}.

In the past, computers did not have an operating system. They could only execute one program at a time from start to end. The programmers of the time were as respected as the virtuoso in music and the arts. Writing such programs has been highly challenging. This problem was solved by developing operating systems that can run several processes (programs). 
Processes use the so-called variant of coarse-grained communication. 
Coarse-grained communication includes primitives such as sockets, signals, semaphores, shared memory, and files. This variant allows them to communicate with each other using signals, files or shared memory.  These processes were virtually von Neumann computers, which contained their own memory space that included instructions and data. Subsequently, the processes executed these instructions according to the semantics of the assembly language. The last part was a set of I/O operations to communicate with each other. If we connect all the parts, we will have a model called Sequential. This model is used by most of today's programming languages.  Specifically, the sequential programming model is intuitive because it creates a sequence of operations that follow each other, thus making the expected result. However, this model has its limitations on performance and time consumption on specific tasks. During the twentieth century, technological advances brought a regular increase in the clock's speed, so that the software really "accelerated" itself over time. However, this scenario is not repeated in the twenty-first century. Today's advances in technology bring about a regular increase in parallelism, but only a slight increase in clock speeds. The use of this parallelism is one of the great challenges of modern informatics.

\section{Amdahl's law}
\label{04:amdalhlaw}

If we imagine ourselves as a team that would like to migrate from a single-processor program to a multi-processor program, it would be perfect to be sure that if we embark on the parallelisation of such a system, it will pay off. Moreover, many people live in a bubble, where they think that if we build a multi-processor program from a one-processor program and run it on 3-cores, the overall acceleration will be three times. This is an illusion, and we will never get such a result. The main problem is due to the division of labour which is not uniform for all parts. For clarity, we will illustrate with an example. Imagine that one has to construct a home table. In this case, it is a sequential approach. However, adding four identical tables (so there will be five) will take five times more time for one. Suppose four friends come to help him (we assume they are just as skilled and start simultaneously). The acceleration for such identical tables will be five times. Nevertheless, everything gets complicated if the tables are not the same. For example, the second table will be more complicated to build and take more time than the others. Furthermore, the first will be smaller, and thus the total time will be lower. This implies that the acceleration will not be close to 5-times, but it will probably be almost 3-times. This kind of analysis is crucial for concurrent computation, and thanks to Mr Amdahl, we have a formula for such calculation. It is called Amdahl's law, which can be seen in Equation \eqref{eqn:einstein}.
\begin{equation}
    \label{eqn:einstein}
    S = \frac{1}{1 - p + \frac{p}{n}}
    \tag{1}
\end{equation}
The formula defines the acceleration \emph{S}, which depends on the quantities \emph{n} and \emph{p}. \emph{n} is a non-zero positive number that represents the number of concurrent processors performing the same job. \emph{p} is a non-zero positive number that defines how much work is done in parallel. The sequential part that cannot be parallelised is defined as the difference between the total work and the work that can be parallelised (\emph{1 - p)}. The parallel component is expressed as the ratio of the parallel part and the number of competitors by the processor (\emph{p / n}). So if we sum up these two parts, we get the total time performed by parallel computation (\emph{1 - p + p / n}). Finally, we have to put the ratio between the sequential (single-processor) time and the parallel time, and we get already mentioned Equation \ref{eqn:einstein}. If we apply this formula to the previous example with five friends who want to build five tables, we get such an Equation \eqref{eqn:amdalhinpractice}.

\begin{equation}
    \label{eqn:amdalhinpractice}
    S = \frac{1}{1 - \frac{3}{5} + \frac{\frac{3}{5}}{\frac{5}{1}}} = 25/13 =\sim 2x \; acceleration
    \tag{2}
\end{equation}

Before we dive into the overall terminology and discuss the Critical section, Mutual exclusion, etc., it is necessary to know what the program is correct. The correctness of the program consists of two essential properties. The first is the safety property, which states: "\emph{Bad thing never happens}". To illustrate, imagine the concurrent program never end up in a deadlock\footnote{\textbf{Deadlock} \---\ is a situation where two processes or threads enters a waiting state because a requested system resource is held by another waiting process, which in turn is waiting for another resource held by another waiting process (toto prepisat)}. The second is the liveness property, which tells us: "\emph{An excellent thing will happen eventually}". For instance, the program always terminates. Thus, if we combine these two properties, then we say that the program is correct.

\section{Shared memory}
\label{04:sharedmemory}

The first aspect is memory. One needs to understand how memory is organised and how a computer accesses individual data. The speed of memory in a computer is usually much slower than the speed at which the processor operates, and if one processor overwrites data in memory, the others must wait. In this type of memory, all processors access the same memory in the global address space. 
\begin{definition}
  \textbf{Shared memory} \---\ is a type of memory, where all CPUs has access to the same address space.
\end{definition}
So if one processor makes a change to the data, all the other processors will know about it. The shared memory architecture is classified as UMA (Uniform memory access) and NUMA (Non-uniform memory access). This classification tells us how the individual processors are connected to the memory and how fast the data can be accessed. The wise reader might realise that memory access will be the same for all processors in Uniform memory access. While at Non-uniform memory access, the time will be different. In the UMA architecture, each processor has its cache memory, storing the most frequent data. However, if the processor uses cache memory, there is a very high risk for cache coherence\footnote {\textbf{Cache coherence} \---\ this is a situation where one of the processors obtains a value from shared memory and makes a change in its cache memory and fails to do so. Update to shared memory (while the other processor reads a value that has not yet been updated and will therefore work with the wrong value)}. Fortunately, this cache coherence is handled by hardware in multicore processors.

\section{Processes and Threads}
\label{04:processesandthreads}

If one imagines a shell script with a predefined set of instructions (bash commands), the moment someone runs it, it becomes a Process running in the Operating System. 
\begin{definition}
  \textbf{Process} \---\ is a dynamic object, which has its own global address space. 
\end{definition}
We can also imagine it as a static entity (written shell script) and a dynamic entity (shell script execution). In general, the Process contains program code, its data, and status information. Each Process is independent of the other and has its own address space in memory. On the other hand, there is also a subset of the Process, and it is a thread. 
\begin{definition}
  \textbf{Thread} \---\ is a lightweight variant of the Process that has an independent execution path and shares code and data within a specific Process. 
\end{definition}
Each thread must be part of a process. Thus, the data we work with is shared with all threads inside the Process. Furthermore, each thread has an independent path of program execution. One can imagine a thread as a lightweight variant of the Process. It is well known that threads take up less memory. Moreover, the operating system can switch faster between individual threads than between processes (context switching\footnote {\textbf{Context switching} \---\ it is a situation where the Process scheduler finds out that some processes have spent a fair share of its time on the processor and swap it with the different Process. When this happens, the Operating system stores the state of Process or thread and then load the state of a different process.}). In general, threads can be in one of four states:
\begin{enumerate}[itemsep=1mm, parsep=0pt]
        \item \textbf{New} \---\ If the main thread spawns a new thread, that thread will be in the \emph{New} state. Moreover, the descendants of the main thread can further create a tree hierarchy of new threads.
        \item \textbf{Runnable} \---\ If one creates a thread, it automatically acquires the \emph{New} state. Subsequently, in order to change to the \emph{Runnable} state, it is necessary to run the thread explicitly.
        \item \textbf {Blocked} \---\ If a thread needs to wait for an event, it switches to the \emph{Blocked} state. This is very useful in terms of resource utilisation. If the event occurs, the operating system assigns the CPU time and returns the thread to the \emph{Runnable} state.
         \item \textbf{Terminate} \---\ The thread returns to the \emph {Terminate} state if it was previously aborted abnormally (i.e., using inter-process communication) or complete its execution.
\end{enumerate}

\section{Dependencies and Protection}
\label{dependenciesandprotection}

One of the main challenges in parallel programming is detecting dependencies between threads. Imagine a situation where two threads access the shared variable \emph{x}. \emph{Thread A} reads a value from the shared variable \emph{x} and starts execution. Subsequently, the scheduler switches the context, and \emph{Thread B} reads the value of the shared variable \emph{x}. Then \emph{Thread B} modifies the value of \emph{x = 10}. The scheduler switches the context again, and \emph{Thread A} is currently operating with the wrong value. This is one of the possible faults that can occur in parallel programming. With this example, we have described the Data race failure.

\begin{definition}
    \textbf{Data race} \---\ is a situation where two or more concurrent threads access the same address space, and one of these threads are changing it.
\end{definition}
Fortunately, as programmers, we can eliminate such errors. Process begins with the detection of critical sections in the code.
\begin{definition}
	\label{04:criticalsection}
 	\textbf{Critical section} \---\ section of code, where two or more concurrent threads have write-access (simultaneously) and at least one of them can write to it and can produce erroneous behaviour.
\end{definition}
As can be seen from the Definition \ref{04:criticalsection}, the programmer must look for such places. It can be a simple increment of a shared variable or a complex structure or object change. If these places are detected, it is necessary to perform the next step. Use Mutual Exclusion briefly stated Mutex.
\begin {definition}
  \textbf{Mutual exclusion} \---\ two threads are excluded from being in the critical section at the same time.
\end {definition}
By using a mutex, we guarantee that only one thread will access the shared resource at a time. One will have to Aquire lock whenever one wants to modify a thread or read from a shared resource. Then one modifies the source and finally release the lock. Acquire lock is an Atomic operation performed as single action and cannot be interrupted by other threads.

We know several implementations of lock, but not all of them guarantee us the Liveness property. As a reminder, the Liveness property tells us that: \emph{"A particular good thing will happen eventually"}. For example, a program never "hangs". However, they usually guarantee the Safety property, and the attentive reader would undoubtedly notice that Mutual Exclusion has a Safety property. One of the leading implementations of lock are the following:
\begin {itemize}
	\item \textbf{Reentrant lock} \---\ This type of lock can be locked unlimited times. Nevertheless, the important thing is that if we want to unlock the lock, we have to do the same number of times. The use of this type of lock can be seen, for example, in recursive functions, when we lock the lock several times and unlock the same amount of times.
	\item \textbf{Try lock} \---\ Non-blocking version of the classic lock, if the Mutex is available, it acquires the lock and returns instantly true at the same time. Otherwise, it returns false. This behaviour is beneficial if the thread can do other things than in the critical section. Therefore, it will not be blocked as a classic lock.
	\item \textbf{Read-write lock} \---\ In case more readers want to read from a shared resource can. However, once a thread is locked in ReaderLock, it is not possible to get a thread that wants to modify the value of the shared resource. This is only possible if the thread that read the value subsequently released ReaderLock for the shared resource. At this point, the thread can be locked using WriterLock, and no other thread can access it. This type of lock is intended mainly for situations where we have more threads that will read from a given shared resource and fewer threads that will write (i.e., databases).
\end{itemize}

\section{Synchronisation}
\label{04:synchronization}

The main problems posed by mutexes are, for example, \emph {busy-waiting}, deadlock, livelock or starvation.

\begin{definition}
  \textbf{Busy waiting} \---\ waiting until thread, which is in the critical section, release lock or flag. The mutual exclusion problem requires waiting, and there is no way how to avoid it.
\end{definition}
Elimination of busy-waiting is possible using another synchronisation primitive such as Semaphore or Condition variable. The Condition variable represents a queue of threads waiting for a specific event and associated with a Mutex. Using these two parts, they implement a higher abstraction called the Monitor. The Monitor is a high-level synchronisation primitive that ensures mutual exclusion while giving threads the ability to wait until an event occurs. Noteworthy is the fact that the Condition variable involves three operations:
\begin{itemize}[itemsep=1mm, parsep=0pt]
\item \textbf {Wait} \---\ If a thread locks the Mutex and then verifies the Condition variable and finds that the condition is not satisfactory, it immediately switches to the Wait state, unlocks the Mutex, and queues the wait queue. to the \emph{notify()} signal, which automatically locks the Mutex again and tests the condition variable.
\item \textbf{Signal} \---\ If a thread has finished executing, it signals with \emph{notify()} and thus wakes one thread from the \emph{Waiting} state.
\item \textbf{Broadcast} \---\ A variant of the signal operation that wakes up all threads in the queue.
\end{itemize}
Another synchronisation mechanism is a Semaphore. Sometimes also referred to as a superset of a mutex. This is because if we imagine the simplest Semaphore, we get a mutex. The main difference between a mutex and a semaphore is that the Semaphore allows access to a critical section to more than one thread simultaneously. The amount added to such a section is conditioned by the number that one initialises in the Semaphore. The basic principle is that if a thread wants to access a critical section, it must increment this number. If the number reaches zero at that moment, no other thread can access the critical section. If the thread wants to exit the critical section, it decrements the counter. Another difference between a mutex and a semaphore is that a mutex can lock and unlock the same thread, whereas a semaphore can lock and unlock a different thread.


\section{Asynchronous tasks}
\label{04:asynctaks}

Another crucial aspect of parallelisation is knowing what an asynchronous task is. It is an object that is in charge of a predetermined task. This task is performed parallel to the main thread. Imagine a situation where we have to perform several tasks. For example, create several different objects that take a certain amount of time to create. If we used the classical strategy of creating one object after another, the whole Process would take a very long time. Hence, we have another alternative; for each of these objects, we submit an asynchronous task. However, it is essential to remember that if we have only four CPUs available and want to create more tasks, for example, twelve, this will result in a situation where the other eight will have to wait until these first threads are done. Therefore, it is better to use \emph{ThreadPool} to create a new thread for each task, and in the next paragraph, we argue why.

\emph{ThreadPool} is an object that creates and manages several threads, also called worker threads. What is so interesting about \emph{ThreadPool} is that if one thread completes its task,
\emph{ThreadPool} immediately assigns a new job to the free thread.  This eliminates the creation process and thus relieving the entire load on resources. However, this is nice, but if we want to submit one asynchronous task, then in the main thread, we want the future result. Thus, we created the \emph{Future} mechanism.

\emph{Future} is another object that creates one asynchronous task. According to intuition, we could deduce that the name was given to this mechanism because we do not know the value initially, but it will be available sometime in the closing \emph{Future}. It also provides access to asynchronous operations, so most languages have the \emph{get()} method. This operation is blocking and will usually be called if one is at a point where one needs a given result from an asynchronous task. The result will be available as soon as the task is completed.

We could go on to more complex parallelisation concepts, such as partitioning, mapping, agglomeration, concurrent objects, consensus algorithms. However, these topics are not necessary to understand the following chapters. Nevertheless, if the reader has these interesting ones, we recommend reading these facts from the books \emph{An Introduction to Parallel Programming} \cite{introductionToParallelProgramming} or \emph{The Art of Multiprocessor Programming} \cite{artOfMultiprocessorProgramming}.
